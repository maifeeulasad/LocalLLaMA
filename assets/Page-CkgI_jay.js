import{j as e}from"./index-xfnGEtuL.js";import{R as l}from"./RedditPostRenderer-DAZRIZZK.js";import"./index-BaNn5-RR.js";const a=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"I'm seeing a trend in recent advancements in open source models, they're getting big. DeepSeek V3 (670B), Kimi K2 (1T), and now Qwen3 Coder (480B).. I'm starting to lose hope for the local scene as model sizes begin to creep further away from what we can run on consumer hardware. If the scaling laws continue to hold true (which I would bet on) then this problem will just get worse over time. Is there any hope for us?","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Is there a future for local models?","link_flair_richtext":[{"e":"text","t":"Discussion"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1m7o3u8","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.78,"author_flair_background_color":null,"subreddit_type":"public","ups":87,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_e11po","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Discussion","can_mod_post":false,"score":87,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1753311706,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;m seeing a trend in recent advancements in open source models, they&amp;#39;re getting big. DeepSeek V3 (670B), Kimi K2 (1T), and now Qwen3 Coder (480B).. I&amp;#39;m starting to lose hope for the local scene as model sizes begin to creep further away from what we can run on consumer hardware. If the scaling laws continue to hold true (which I would bet on) then this problem will just get worse over time. Is there any hope for us?&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"5f921ea4-c7bc-11ed-9c23-3a00622979b4","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#646d73","id":"1m7o3u8","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"ASTRdeca","discussion_type":null,"num_comments":100,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1m7o3u8/is_there_a_future_for_local_models/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1m7o3u8/is_there_a_future_for_local_models/","subreddit_subscribers":503759,"created_utc":1753311706,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4uytlq","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Bitter_Firefighter_1","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4ur4fp","score":2,"author_fullname":"t2_939o3k0q","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Or memory more abundant and better optimizations on Mixture of Experts.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n4uytlq","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Or memory more abundant and better optimizations on Mixture of Experts.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m7o3u8","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7o3u8/is_there_a_future_for_local_models/n4uytlq/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753341798,"author_flair_text":null,"treatment_tags":[],"created_utc":1753341798,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n4ur4fp","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"sriharshang","can_mod_post":false,"created_utc":1753337633,"send_replies":true,"parent_id":"t1_n4t05i3","score":8,"author_fullname":"t2_vf4blt0","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Exactly, all these huge LLM's need to become streamlined and they will become local accessible. The future is all about on device models for obvious reasons that is intelligent efficient operations.\\nLike laptops with inbuilt models which can get updates or connect online and respond.\\nPhones having local models + work online \\nHobbyists deploying local models for privacy and data control.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4ur4fp","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Exactly, all these huge LLM&amp;#39;s need to become streamlined and they will become local accessible. The future is all about on device models for obvious reasons that is intelligent efficient operations.\\nLike laptops with inbuilt models which can get updates or connect online and respond.\\nPhones having local models + work online \\nHobbyists deploying local models for privacy and data control.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m7o3u8","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7o3u8/is_there_a_future_for_local_models/n4ur4fp/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753337633,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":8}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4v9r3x","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"night0x63","can_mod_post":false,"created_utc":1753348036,"send_replies":true,"parent_id":"t1_n4t05i3","score":3,"author_fullname":"t2_3h2irqtz","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I also think bigger opensource models are good. IMO you need the bigger models to effectively complete with closed source... And get similar benchmarks. As long as the models are mixture of experts... which they are.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4v9r3x","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I also think bigger opensource models are good. IMO you need the bigger models to effectively complete with closed source... And get similar benchmarks. As long as the models are mixture of experts... which they are.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m7o3u8","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7o3u8/is_there_a_future_for_local_models/n4v9r3x/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753348036,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4v41lv","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Macestudios32","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4v0bg7","score":1,"author_fullname":"t2_1mbsf8cel3","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"No, it is not the same.\\n\\n\\nIn one case they are models like the ones we already had but better and bigger. \\n\\n\\nWhat I am saying has not yet reached LLM open (if I am wrong I would be excited) for example the voice issue. \\n\\n\\nNor has persistent memory arrived.\\n\\n\\n And in terms of agent, the most advanced that is online, for my taste it refers more to the internet or browser, not so much console and internal PC commands. \\n\\n\\nI try to reason it more, there are text-audio models of very small size, there are things of agents using small models. \\n\\n\\nThat is, in the aspects I am talking about, size is not the restriction, only the software \\"technology\\". \\n\\n\\nThere are small (and not big) things that are very modest even that do not reach the agent profile I am talking about. \\nThe moment I'm talking about (science fiction, hal 9000) has not yet arrived and current attempts can be run on modest pcs. \\n\\n\\nGreetings.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n4v41lv","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;No, it is not the same.&lt;/p&gt;\\n\\n&lt;p&gt;In one case they are models like the ones we already had but better and bigger. &lt;/p&gt;\\n\\n&lt;p&gt;What I am saying has not yet reached LLM open (if I am wrong I would be excited) for example the voice issue. &lt;/p&gt;\\n\\n&lt;p&gt;Nor has persistent memory arrived.&lt;/p&gt;\\n\\n&lt;p&gt; And in terms of agent, the most advanced that is online, for my taste it refers more to the internet or browser, not so much console and internal PC commands. &lt;/p&gt;\\n\\n&lt;p&gt;I try to reason it more, there are text-audio models of very small size, there are things of agents using small models. &lt;/p&gt;\\n\\n&lt;p&gt;That is, in the aspects I am talking about, size is not the restriction, only the software &amp;quot;technology&amp;quot;. &lt;/p&gt;\\n\\n&lt;p&gt;There are small (and not big) things that are very modest even that do not reach the agent profile I am talking about. \\nThe moment I&amp;#39;m talking about (science fiction, hal 9000) has not yet arrived and current attempts can be run on modest pcs. &lt;/p&gt;\\n\\n&lt;p&gt;Greetings.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m7o3u8","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7o3u8/is_there_a_future_for_local_models/n4v41lv/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753344762,"author_flair_text":null,"treatment_tags":[],"created_utc":1753344762,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4v0bg7","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"SelectPlatform8444","can_mod_post":false,"created_utc":1753342654,"send_replies":true,"parent_id":"t1_n4t05i3","score":1,"author_fullname":"t2_v4pmc24pc","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"&gt;My pain point will be when there are offline conversational agents or models and the HW necessary for their normal functioning is unaffordable for me. \\nNo one wants to hear one token per second!\\n\\nisn't this the same problem as OP is referring to in the post?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4v0bg7","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;My pain point will be when there are offline conversational agents or models and the HW necessary for their normal functioning is unaffordable for me. \\nNo one wants to hear one token per second!&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;isn&amp;#39;t this the same problem as OP is referring to in the post?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m7o3u8","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7o3u8/is_there_a_future_for_local_models/n4v0bg7/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753342654,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4t05i3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Macestudios32","can_mod_post":false,"created_utc":1753312905,"send_replies":true,"parent_id":"t3_1m7o3u8","score":102,"author_fullname":"t2_1mbsf8cel3","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"My view is just the opposite. Positive.\\n\\n\\n The fact that free models are advancing even with giant monsters is good news. With time, you can distill them to small, quantify, improve HW, make more money... \\n\\n\\nMy pain point will be when there are offline conversational agents or models and the HW necessary for their normal functioning is unaffordable for me. \\nNo one wants to hear one token per second!\\n\\n\\n This is a long-term race that includes privacy and freedom. \\n\\n\\nThis year maybe an 8 gigabyte graph, and in 2 years 16, or NPUs will come out. etc.\\n\\n\\n Slow responses depend on the patience of each one, but when they do actions..... Hal, perform the optimization of the system and security checks (and you go to sleep) if the system is reliable, works well and can be loaded, any action that the system does is not done by me, no matter how slow it is, it is a gain.\\n\\n\\n In consumer HW I include everything from 512 gigabyte macs to homelab servers...\\n\\n\\n May the progress not stop! Radios, televisions, telephones, everything for the rich at the beginning and now... Look at us.\\n\\n\\n Best regards","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4t05i3","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;My view is just the opposite. Positive.&lt;/p&gt;\\n\\n&lt;p&gt; The fact that free models are advancing even with giant monsters is good news. With time, you can distill them to small, quantify, improve HW, make more money... &lt;/p&gt;\\n\\n&lt;p&gt;My pain point will be when there are offline conversational agents or models and the HW necessary for their normal functioning is unaffordable for me. \\nNo one wants to hear one token per second!&lt;/p&gt;\\n\\n&lt;p&gt; This is a long-term race that includes privacy and freedom. &lt;/p&gt;\\n\\n&lt;p&gt;This year maybe an 8 gigabyte graph, and in 2 years 16, or NPUs will come out. etc.&lt;/p&gt;\\n\\n&lt;p&gt; Slow responses depend on the patience of each one, but when they do actions..... Hal, perform the optimization of the system and security checks (and you go to sleep) if the system is reliable, works well and can be loaded, any action that the system does is not done by me, no matter how slow it is, it is a gain.&lt;/p&gt;\\n\\n&lt;p&gt; In consumer HW I include everything from 512 gigabyte macs to homelab servers...&lt;/p&gt;\\n\\n&lt;p&gt; May the progress not stop! Radios, televisions, telephones, everything for the rich at the beginning and now... Look at us.&lt;/p&gt;\\n\\n&lt;p&gt; Best regards&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7o3u8/is_there_a_future_for_local_models/n4t05i3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753312905,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m7o3u8","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":102}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4t6gp3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"DinoAmino","can_mod_post":false,"created_utc":1753314993,"send_replies":true,"parent_id":"t3_1m7o3u8","score":60,"author_fullname":"t2_j1v7f","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"This is a ridiculous take. Nearsighted much? In 2025 there have been  many 32B and under releases and many local-friendly MoEs that have excited many people. Compared to only a few massive parameter LLMs.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4t6gp3","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;This is a ridiculous take. Nearsighted much? In 2025 there have been  many 32B and under releases and many local-friendly MoEs that have excited many people. Compared to only a few massive parameter LLMs.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7o3u8/is_there_a_future_for_local_models/n4t6gp3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753314993,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m7o3u8","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":60}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4vfth9","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"random-string","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4vd9qq","score":1,"author_fullname":"t2_jq6r0","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Oh, right, makes sense.","edited":false,"author_flair_css_class":null,"name":"t1_n4vfth9","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Oh, right, makes sense.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1m7o3u8","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7o3u8/is_there_a_future_for_local_models/n4vfth9/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753351406,"author_flair_text":null,"collapsed":false,"created_utc":1753351406,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4vzz6e","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"simracerman","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4vd9qq","score":1,"author_fullname":"t2_vbzgnic","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yep, typo, thanks!","edited":false,"author_flair_css_class":null,"name":"t1_n4vzz6e","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yep, typo, thanks!&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1m7o3u8","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7o3u8/is_there_a_future_for_local_models/n4vzz6e/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753360160,"author_flair_text":null,"collapsed":false,"created_utc":1753360160,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4vd9qq","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Awwtifishal","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4vc7r1","score":3,"author_fullname":"t2_1d96a8k10t","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I think they meant 70B. There's no llama 3.x 7B, and llama 3.3 is 70B.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4vd9qq","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I think they meant 70B. There&amp;#39;s no llama 3.x 7B, and llama 3.3 is 70B.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m7o3u8","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7o3u8/is_there_a_future_for_local_models/n4vd9qq/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753350040,"author_flair_text":null,"treatment_tags":[],"created_utc":1753350040,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n4vc7r1","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"random-string","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4tnknd","score":0,"author_fullname":"t2_jq6r0","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Well if a 24B was worse than a 7B that would a pretty bad model","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n4vc7r1","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Well if a 24B was worse than a 7B that would a pretty bad model&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m7o3u8","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7o3u8/is_there_a_future_for_local_models/n4vc7r1/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753349443,"author_flair_text":null,"treatment_tags":[],"created_utc":1753349443,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}},"user_reports":[],"saved":false,"id":"n4tnknd","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"simracerman","can_mod_post":false,"created_utc":1753320958,"send_replies":true,"parent_id":"t1_n4t0a6t","score":7,"author_fullname":"t2_vbzgnic","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Llama3.3 70B is beaten by Mistral’s 3.2 24B. Maybe not on general world knowledge, but with MCP around you no longer need that. Quick search is better than anything.","edited":1753360133,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4tnknd","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Llama3.3 70B is beaten by Mistral’s 3.2 24B. Maybe not on general world knowledge, but with MCP around you no longer need that. Quick search is better than anything.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m7o3u8","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7o3u8/is_there_a_future_for_local_models/n4tnknd/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753320958,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":7}}],"before":null}},"user_reports":[],"saved":false,"id":"n4t0a6t","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"jacek2023","can_mod_post":false,"created_utc":1753312947,"send_replies":true,"parent_id":"t3_1m7o3u8","score":25,"author_fullname":"t2_vqgbql9w","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"The current SOTA dense models are around 32B (some are 24B or 34B), and people complain that there are no more 70B models.\\n\\nAt the same time, we’re seeing MoE models reaching up to 1000B, but there are also smaller ones — under 100B.\\n\\nSo I don’t really see the problem here. Was the past really better?\\n\\nWas LLaMA 70B actually better than ChatGPT at that time?\\n\\nBecause you're comparing models that are already close to the current ChatGPT.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4t0a6t","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The current SOTA dense models are around 32B (some are 24B or 34B), and people complain that there are no more 70B models.&lt;/p&gt;\\n\\n&lt;p&gt;At the same time, we’re seeing MoE models reaching up to 1000B, but there are also smaller ones — under 100B.&lt;/p&gt;\\n\\n&lt;p&gt;So I don’t really see the problem here. Was the past really better?&lt;/p&gt;\\n\\n&lt;p&gt;Was LLaMA 70B actually better than ChatGPT at that time?&lt;/p&gt;\\n\\n&lt;p&gt;Because you&amp;#39;re comparing models that are already close to the current ChatGPT.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7o3u8/is_there_a_future_for_local_models/n4t0a6t/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753312947,"author_flair_text":"llama.cpp","treatment_tags":[],"link_id":"t3_1m7o3u8","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":25}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4teo9f","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"cyanoa","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4t41f9","score":7,"author_fullname":"t2_dsjtb","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I thought that Nvidia had built something of a moat with CUDA.\\n\\nBut Deepseek showed us that it isn't necessarily the case (and they showed us that CUDA is nowhere near optimized).\\n\\nLike most things, an open standard will likely emerge, supported by the other players, and will start to chip away at Nvidia's dominance.\\n\\nOther players are very likely more interested in providing us with better vram specs or more shared memory architectures like Apple.  We just need to give it a bit of time.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n4teo9f","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I thought that Nvidia had built something of a moat with CUDA.&lt;/p&gt;\\n\\n&lt;p&gt;But Deepseek showed us that it isn&amp;#39;t necessarily the case (and they showed us that CUDA is nowhere near optimized).&lt;/p&gt;\\n\\n&lt;p&gt;Like most things, an open standard will likely emerge, supported by the other players, and will start to chip away at Nvidia&amp;#39;s dominance.&lt;/p&gt;\\n\\n&lt;p&gt;Other players are very likely more interested in providing us with better vram specs or more shared memory architectures like Apple.  We just need to give it a bit of time.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m7o3u8","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7o3u8/is_there_a_future_for_local_models/n4teo9f/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753317840,"author_flair_text":null,"treatment_tags":[],"created_utc":1753317840,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":7}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4vaq9y","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AlternativePurpose63","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4t41f9","score":1,"author_fullname":"t2_ogcydh4f","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"There would be many issues if CUDA were to become an open standard. The existing CUDA heavily involves NVIDIA's hardware design details to fully utilize resources.\\n\\n While opening up the standard might bring some reputational improvement, NVIDIA would still hold the ultimate say as the proprietor. \\n\\nUnless they plan to disclose a vast amount of information for implementation, it would be a significant problem.\\n\\nFurthermore, the degree of openness is also an issue. Who would control the standard's evolution? \\n\\nWhat if NVIDIA insists on improving it in a certain direction, but others disagree? \\n\\nUltimately, this would lead to a standard that looks good in form but is essentially still closed and fragmented.\\n\\nIt would be better to establish a sufficiently open and powerful standard that is also strong enough in its implementation to eliminate closed standards and evolve rapidly.\\n\\n However, the latter could be significantly delayed due to extensive vendor implementation and design challenges, leading to the current situation.\\n\\nThis is similar to the current myriad of architectures, with many peculiar operators, some even unsuitable for hardware acceleration.\\n\\n Examples include ideas like dynamic cross-layer parameter sharing or dynamic residuals, which significantly reduce weight occupation and activation overhead while simultaneously lowering loss. \\n\\nThese ideas, however, have problems and are difficult to parallelize effectively, or even unsuitable for large-scale training.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n4vaq9y","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;There would be many issues if CUDA were to become an open standard. The existing CUDA heavily involves NVIDIA&amp;#39;s hardware design details to fully utilize resources.&lt;/p&gt;\\n\\n&lt;p&gt;While opening up the standard might bring some reputational improvement, NVIDIA would still hold the ultimate say as the proprietor. &lt;/p&gt;\\n\\n&lt;p&gt;Unless they plan to disclose a vast amount of information for implementation, it would be a significant problem.&lt;/p&gt;\\n\\n&lt;p&gt;Furthermore, the degree of openness is also an issue. Who would control the standard&amp;#39;s evolution? &lt;/p&gt;\\n\\n&lt;p&gt;What if NVIDIA insists on improving it in a certain direction, but others disagree? &lt;/p&gt;\\n\\n&lt;p&gt;Ultimately, this would lead to a standard that looks good in form but is essentially still closed and fragmented.&lt;/p&gt;\\n\\n&lt;p&gt;It would be better to establish a sufficiently open and powerful standard that is also strong enough in its implementation to eliminate closed standards and evolve rapidly.&lt;/p&gt;\\n\\n&lt;p&gt;However, the latter could be significantly delayed due to extensive vendor implementation and design challenges, leading to the current situation.&lt;/p&gt;\\n\\n&lt;p&gt;This is similar to the current myriad of architectures, with many peculiar operators, some even unsuitable for hardware acceleration.&lt;/p&gt;\\n\\n&lt;p&gt;Examples include ideas like dynamic cross-layer parameter sharing or dynamic residuals, which significantly reduce weight occupation and activation overhead while simultaneously lowering loss. &lt;/p&gt;\\n\\n&lt;p&gt;These ideas, however, have problems and are difficult to parallelize effectively, or even unsuitable for large-scale training.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m7o3u8","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7o3u8/is_there_a_future_for_local_models/n4vaq9y/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753348602,"author_flair_text":null,"treatment_tags":[],"created_utc":1753348602,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4t41f9","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Cool-Chemical-5629","can_mod_post":false,"created_utc":1753314188,"send_replies":true,"parent_id":"t1_n4sxi8z","score":6,"author_fullname":"t2_qz1qjc86","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"There are two key problems here.\\n\\nThe first one is that even if Nvidia makes a dedicated GPUs for this purpose with high amount of VRAM, it's still much more expensive than GPUs from any other companies. They are still GPUs that are out of reach for many.\\n\\nThe second and much more important problem is that the biggest chunk of AI technology still requires CUDA to run properly and that's Nvidia's proprietary technology no other GPU can use.\\n\\nYou see, each and every big name company out there has full mouth of noble words about bringing AI to everyone, but no one talks about solving the obvious issue that still prevents it from happening.\\n\\nIf Nvidia opened its technology for everyone to use on their own GPUs (which will most likely never happen), that would be a real step towards bringing AI to masses, but no Nvidia is not doing that and so we need to think of using alternative chips such as NPUs, or buying expensive \\"AI Ready\\" chips such as those modern Ryzen CPUs for AI, alternative AI model formats such as GGUF that support alternative technologies such as Vulkan which are universally available on all GPUs, etc... All that just to get a fraction of the speed and performance that is achievable on Nvidia GPUs.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4t41f9","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;There are two key problems here.&lt;/p&gt;\\n\\n&lt;p&gt;The first one is that even if Nvidia makes a dedicated GPUs for this purpose with high amount of VRAM, it&amp;#39;s still much more expensive than GPUs from any other companies. They are still GPUs that are out of reach for many.&lt;/p&gt;\\n\\n&lt;p&gt;The second and much more important problem is that the biggest chunk of AI technology still requires CUDA to run properly and that&amp;#39;s Nvidia&amp;#39;s proprietary technology no other GPU can use.&lt;/p&gt;\\n\\n&lt;p&gt;You see, each and every big name company out there has full mouth of noble words about bringing AI to everyone, but no one talks about solving the obvious issue that still prevents it from happening.&lt;/p&gt;\\n\\n&lt;p&gt;If Nvidia opened its technology for everyone to use on their own GPUs (which will most likely never happen), that would be a real step towards bringing AI to masses, but no Nvidia is not doing that and so we need to think of using alternative chips such as NPUs, or buying expensive &amp;quot;AI Ready&amp;quot; chips such as those modern Ryzen CPUs for AI, alternative AI model formats such as GGUF that support alternative technologies such as Vulkan which are universally available on all GPUs, etc... All that just to get a fraction of the speed and performance that is achievable on Nvidia GPUs.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m7o3u8","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7o3u8/is_there_a_future_for_local_models/n4t41f9/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753314188,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}}],"before":null}},"user_reports":[],"saved":false,"id":"n4sxi8z","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Excellent_Sleep6357","can_mod_post":false,"created_utc":1753312040,"send_replies":true,"parent_id":"t3_1m7o3u8","score":16,"author_fullname":"t2_1f7suc7bln","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I'd say the open source community needs to stand up and start distilling those monsters with a systematic approach.\\n\\n\\nOn the other hand, there may be some breakthroughs on the hardware side.  Nvidia has already released 96GB prosumer GPU, not a lot cheaper, but I do see a crack in this consumer/data center barrier they were trying to build up.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4sxi8z","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;d say the open source community needs to stand up and start distilling those monsters with a systematic approach.&lt;/p&gt;\\n\\n&lt;p&gt;On the other hand, there may be some breakthroughs on the hardware side.  Nvidia has already released 96GB prosumer GPU, not a lot cheaper, but I do see a crack in this consumer/data center barrier they were trying to build up.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7o3u8/is_there_a_future_for_local_models/n4sxi8z/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753312040,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m7o3u8","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":16}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4tlt0w","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"redoubt515","can_mod_post":false,"created_utc":1753320331,"send_replies":true,"parent_id":"t1_n4t5arx","score":7,"author_fullname":"t2_kehp8nb59","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"And current Llama--while somewhat lackluster in terms of improvement over the past gen--*is* MOE and is sized much more conveniently for those with \\\\~64 GB or 128GB RAM. (I wish Qwen3 235B were a bit slimmer so Q4 with decent context could fit in 128GB ram with enough left over for the system itself and other tasks.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4tlt0w","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;And current Llama--while somewhat lackluster in terms of improvement over the past gen--&lt;em&gt;is&lt;/em&gt; MOE and is sized much more conveniently for those with ~64 GB or 128GB RAM. (I wish Qwen3 235B were a bit slimmer so Q4 with decent context could fit in 128GB ram with enough left over for the system itself and other tasks.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m7o3u8","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7o3u8/is_there_a_future_for_local_models/n4tlt0w/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753320331,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":7}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4vl6h8","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"a_beautiful_rhind","can_mod_post":false,"created_utc":1753354122,"send_replies":true,"parent_id":"t1_n4t5arx","score":3,"author_fullname":"t2_h5utwre7","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Have you guys actually tried to see what speeds you get? And what kind of CPU you need? People throw out this claim like it's a 100% benefit, but as someone with *almost* decent HW for it, there's a lot of caveat.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4vl6h8","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Have you guys actually tried to see what speeds you get? And what kind of CPU you need? People throw out this claim like it&amp;#39;s a 100% benefit, but as someone with &lt;em&gt;almost&lt;/em&gt; decent HW for it, there&amp;#39;s a lot of caveat.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m7o3u8","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7o3u8/is_there_a_future_for_local_models/n4vl6h8/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753354122,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4vpweg","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"anarchos","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4v847b","score":1,"author_fullname":"t2_44chy","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"You need to fit the entire model in VRAM, even with a MOE model (of course there can be CPU offload and etc, but we are talking about ideal conditions).  All the experts are in VRAM, but only one (or sometimes more than one but just a few) experts are ever \\"active\\" at once.\\n\\nIf you took the same size model in MOE and not MOE, VRAM usage would be the same\\\\*, but the tokens per second would be way higher for the MOE model.\\n\\n\\\\* within reason, there may be small variances in VRAM usage (Moe's generally have a small model who picks which expert to use, etc)\\n\\nExperts are not really experts like \\"oh this one is good at coding, this one is good at writing\\" and etc.  Each token could use a different expert at any given time, so all of them need to be in VRAM and ready to go.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n4vpweg","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You need to fit the entire model in VRAM, even with a MOE model (of course there can be CPU offload and etc, but we are talking about ideal conditions).  All the experts are in VRAM, but only one (or sometimes more than one but just a few) experts are ever &amp;quot;active&amp;quot; at once.&lt;/p&gt;\\n\\n&lt;p&gt;If you took the same size model in MOE and not MOE, VRAM usage would be the same*, but the tokens per second would be way higher for the MOE model.&lt;/p&gt;\\n\\n&lt;p&gt;* within reason, there may be small variances in VRAM usage (Moe&amp;#39;s generally have a small model who picks which expert to use, etc)&lt;/p&gt;\\n\\n&lt;p&gt;Experts are not really experts like &amp;quot;oh this one is good at coding, this one is good at writing&amp;quot; and etc.  Each token could use a different expert at any given time, so all of them need to be in VRAM and ready to go.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m7o3u8","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7o3u8/is_there_a_future_for_local_models/n4vpweg/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753356215,"author_flair_text":null,"treatment_tags":[],"created_utc":1753356215,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4v847b","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ForTheDankMemes","can_mod_post":false,"created_utc":1753347102,"send_replies":true,"parent_id":"t1_n4t5arx","score":0,"author_fullname":"t2_1qlb2mkb","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Actually how much VRAM do you need to run these larger Moe models?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4v847b","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Actually how much VRAM do you need to run these larger Moe models?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m7o3u8","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7o3u8/is_there_a_future_for_local_models/n4v847b/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753347102,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}},"user_reports":[],"saved":false,"id":"n4t5arx","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Conscious_Cut_6144","can_mod_post":false,"created_utc":1753314605,"send_replies":true,"parent_id":"t3_1m7o3u8","score":17,"author_fullname":"t2_9hl4ymvj","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Those giant models are MOE, you can run them on CPU if you want.  \\nWay easier to run than the now ancient Llama 3.1 405B","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4t5arx","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Those giant models are MOE, you can run them on CPU if you want.&lt;br/&gt;\\nWay easier to run than the now ancient Llama 3.1 405B&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7o3u8/is_there_a_future_for_local_models/n4t5arx/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753314605,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m7o3u8","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":17}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4t6z2a","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"ttkciar","can_mod_post":false,"created_utc":1753315169,"send_replies":true,"parent_id":"t3_1m7o3u8","score":11,"author_fullname":"t2_cpegz","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"A few points:\\n\\n* As commodity hardware grows more powerful, larger models will become more usable for us.  Obviously right now model sizes are outracing hardware developments by a large factor.\\n\\n* Open source is forever.  Companies almost inevitably shitcan their older technologies (some exceptions of course), but what is open source now will remain available as long as there are enough people maintaining it.  For that reason alone I expect open source LLM technology will continue to advance long after commercial LLM inference services fall out of vogue.\\n\\n* Those larger models can be leveraged by the less-GPU-poor to create better smaller models for the more-GPU-poor, through techniques like synthetic training datasets, RLAIF, transfer learning, and layer distillation.\\n\\n* There are definitely known techniques for making smaller models more competent and/or hardware-economical which have yet to be adequately implemented, and [researchers are publishing more all the time.](https://old.reddit.com/r/LocalLLaMA/comments/1m7fwhl/google_deepmind_release_mixtureofrecursions/)  The open source community has plenty of work to do which will benefit local model users, years and years of it.\\n\\n* There are more ways to progress than parameter scaling.  I ranted about it a little [in another thread.](https://old.reddit.com/r/LocalLLaMA/comments/1lrpjpc/can_home_sized_llms_32b_etc_or_home_gpus_ever/n1em6jv/)\\n\\nI think overall our prospects will get better with time.","edited":1753335944,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4t6z2a","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;A few points:&lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;&lt;p&gt;As commodity hardware grows more powerful, larger models will become more usable for us.  Obviously right now model sizes are outracing hardware developments by a large factor.&lt;/p&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;p&gt;Open source is forever.  Companies almost inevitably shitcan their older technologies (some exceptions of course), but what is open source now will remain available as long as there are enough people maintaining it.  For that reason alone I expect open source LLM technology will continue to advance long after commercial LLM inference services fall out of vogue.&lt;/p&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;p&gt;Those larger models can be leveraged by the less-GPU-poor to create better smaller models for the more-GPU-poor, through techniques like synthetic training datasets, RLAIF, transfer learning, and layer distillation.&lt;/p&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;p&gt;There are definitely known techniques for making smaller models more competent and/or hardware-economical which have yet to be adequately implemented, and &lt;a href=\\"https://old.reddit.com/r/LocalLLaMA/comments/1m7fwhl/google_deepmind_release_mixtureofrecursions/\\"&gt;researchers are publishing more all the time.&lt;/a&gt;  The open source community has plenty of work to do which will benefit local model users, years and years of it.&lt;/p&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;p&gt;There are more ways to progress than parameter scaling.  I ranted about it a little &lt;a href=\\"https://old.reddit.com/r/LocalLLaMA/comments/1lrpjpc/can_home_sized_llms_32b_etc_or_home_gpus_ever/n1em6jv/\\"&gt;in another thread.&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;p&gt;I think overall our prospects will get better with time.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7o3u8/is_there_a_future_for_local_models/n4t6z2a/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753315169,"author_flair_text":"llama.cpp","treatment_tags":[],"link_id":"t3_1m7o3u8","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":11}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4tjo3k","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"redoubt515","can_mod_post":false,"created_utc":1753319584,"send_replies":true,"parent_id":"t3_1m7o3u8","score":4,"author_fullname":"t2_kehp8nb59","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"\\\\&gt; I'm starting to lose hope for the local scene as model sizes begin to creep further away from what we can run on consumer hardware\\n\\nI have the opposite opinion.\\n\\nIt seems to me that you are just maybe fixated on the ulta-large models. The largest models will obviously never be ideal for self-hosters because that isn't who they are catering to, but we are gaining many great options in the small, medium, and medium-large range.\\n\\nLots of great 32B models, 7-14B models, and 70B-250B models). And at least my perception is that improvements are happening quicker at the low end or middle of the spectrum, than with the largest models.\\n\\nAlso, it'll take some time, but hardware is going to be changing a lot I think. Higher memory bandwidth will likely become the norm, more RAM will become much more common. \\n\\nI'm more excited now about the prospects for small and medium sized (personally self-hostable) models than I was in the past. Qwen3 (30B, 32B, and 235B) would be an example. Llama 3, Mixtral 8x7B, Gemma 27B and Mistral Small are old news by localllama standards, but were pretty exciting also.\\n\\nJust the general shift from 70B as a standard size to 32B as a standard size is kind of a gamechanger for many people (since it can fit n 24GB or 32GB vram), and the renewed interest in MOE is great for self-hosters also.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4tjo3k","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;&amp;gt; I&amp;#39;m starting to lose hope for the local scene as model sizes begin to creep further away from what we can run on consumer hardware&lt;/p&gt;\\n\\n&lt;p&gt;I have the opposite opinion.&lt;/p&gt;\\n\\n&lt;p&gt;It seems to me that you are just maybe fixated on the ulta-large models. The largest models will obviously never be ideal for self-hosters because that isn&amp;#39;t who they are catering to, but we are gaining many great options in the small, medium, and medium-large range.&lt;/p&gt;\\n\\n&lt;p&gt;Lots of great 32B models, 7-14B models, and 70B-250B models). And at least my perception is that improvements are happening quicker at the low end or middle of the spectrum, than with the largest models.&lt;/p&gt;\\n\\n&lt;p&gt;Also, it&amp;#39;ll take some time, but hardware is going to be changing a lot I think. Higher memory bandwidth will likely become the norm, more RAM will become much more common. &lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;m more excited now about the prospects for small and medium sized (personally self-hostable) models than I was in the past. Qwen3 (30B, 32B, and 235B) would be an example. Llama 3, Mixtral 8x7B, Gemma 27B and Mistral Small are old news by localllama standards, but were pretty exciting also.&lt;/p&gt;\\n\\n&lt;p&gt;Just the general shift from 70B as a standard size to 32B as a standard size is kind of a gamechanger for many people (since it can fit n 24GB or 32GB vram), and the renewed interest in MOE is great for self-hosters also.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7o3u8/is_there_a_future_for_local_models/n4tjo3k/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753319584,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m7o3u8","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4upzdm","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"05032-MendicantBias","can_mod_post":false,"created_utc":1753337036,"send_replies":true,"parent_id":"t3_1m7o3u8","score":5,"author_fullname":"t2_6id3lwou","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I would be pretty worried about closed models instead.\\n\\nOpenAI has hundreds of billions and an embargo on training hardware and STILL it can barely keep up with open models. OpenAI and xAI investors are being shaken down for all their worth.\\n\\nThe economics of open source are just better. You sell better hardware to consumers that run your model, and the money comes from using the model. Open AI admits it loses money even on their 200 $ subscription!\\n\\nIt's the same reason the business model of selling GPUs to consumers and game developer selling games works a lot better than someone hosting GPUs and selling subscription access to game streaming. And that hardware only lasts a few years before it becomes obsolete, and they have to respend all over again to upgrade. Publishers really, REALLY want recurring revenue, but it's so much more expensive for them. It's only just the CFOs and investors that like subscriptions. Everyone else is better off with having local hardware to run their application.\\n\\nApple is being made fun of for pursuing the only sensible economic model. Shrink down local models to the point they do useful things locally. It saves them selling at a sharp discount access to extremely expensive B200s.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4upzdm","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I would be pretty worried about closed models instead.&lt;/p&gt;\\n\\n&lt;p&gt;OpenAI has hundreds of billions and an embargo on training hardware and STILL it can barely keep up with open models. OpenAI and xAI investors are being shaken down for all their worth.&lt;/p&gt;\\n\\n&lt;p&gt;The economics of open source are just better. You sell better hardware to consumers that run your model, and the money comes from using the model. Open AI admits it loses money even on their 200 $ subscription!&lt;/p&gt;\\n\\n&lt;p&gt;It&amp;#39;s the same reason the business model of selling GPUs to consumers and game developer selling games works a lot better than someone hosting GPUs and selling subscription access to game streaming. And that hardware only lasts a few years before it becomes obsolete, and they have to respend all over again to upgrade. Publishers really, REALLY want recurring revenue, but it&amp;#39;s so much more expensive for them. It&amp;#39;s only just the CFOs and investors that like subscriptions. Everyone else is better off with having local hardware to run their application.&lt;/p&gt;\\n\\n&lt;p&gt;Apple is being made fun of for pursuing the only sensible economic model. Shrink down local models to the point they do useful things locally. It saves them selling at a sharp discount access to extremely expensive B200s.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7o3u8/is_there_a_future_for_local_models/n4upzdm/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753337036,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m7o3u8","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4t3tud","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"__SlimeQ__","can_mod_post":false,"created_utc":1753314118,"send_replies":true,"parent_id":"t3_1m7o3u8","score":7,"author_fullname":"t2_olbav","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"the stated purpose of kimi is to generate synthetic data for future models. as deepseek starts to beat gpt they will become an oracle for synthetic data as well. i think you've missed the point.\\n\\nlast year chat formats were hot. this year it's tool calling. all of the data for tool use comes from huge foundation models. qwen3 is a massive improvement over everything we had a year ago, largely because it was able to bootstrap on existing reasoning models. the one next year will probably be trained on a bunch of synthetic agent data that wasn't possible this year.\\n\\ndo the math. we're in an optimization phase. it is not unreasonable to expect gpt3.5 level responses with a 14B qwen3 model","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4t3tud","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;the stated purpose of kimi is to generate synthetic data for future models. as deepseek starts to beat gpt they will become an oracle for synthetic data as well. i think you&amp;#39;ve missed the point.&lt;/p&gt;\\n\\n&lt;p&gt;last year chat formats were hot. this year it&amp;#39;s tool calling. all of the data for tool use comes from huge foundation models. qwen3 is a massive improvement over everything we had a year ago, largely because it was able to bootstrap on existing reasoning models. the one next year will probably be trained on a bunch of synthetic agent data that wasn&amp;#39;t possible this year.&lt;/p&gt;\\n\\n&lt;p&gt;do the math. we&amp;#39;re in an optimization phase. it is not unreasonable to expect gpt3.5 level responses with a 14B qwen3 model&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7o3u8/is_there_a_future_for_local_models/n4t3tud/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753314118,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m7o3u8","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":7}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4t8gh4","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"vegatx40","can_mod_post":false,"created_utc":1753315685,"send_replies":true,"parent_id":"t3_1m7o3u8","score":4,"author_fullname":"t2_18dhiarv40","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"The fact is that frontier models all have trillion parameters. \\n\\nI see it as good news that there are any open models at all that are running at or both half a trillion. \\n\\nThere will be a trickle down effect of those to model sizes that can be run on consumer hardware. \\n\\nTo say nothing of whatever next week's strategy for shrinking existing models is.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4t8gh4","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The fact is that frontier models all have trillion parameters. &lt;/p&gt;\\n\\n&lt;p&gt;I see it as good news that there are any open models at all that are running at or both half a trillion. &lt;/p&gt;\\n\\n&lt;p&gt;There will be a trickle down effect of those to model sizes that can be run on consumer hardware. &lt;/p&gt;\\n\\n&lt;p&gt;To say nothing of whatever next week&amp;#39;s strategy for shrinking existing models is.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7o3u8/is_there_a_future_for_local_models/n4t8gh4/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753315685,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m7o3u8","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4u27zz","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Fear_ltself","can_mod_post":false,"created_utc":1753326241,"send_replies":true,"parent_id":"t3_1m7o3u8","score":4,"author_fullname":"t2_ipxwj","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I think RAM prices will fall over time and make these accessible to the masses in the next decade","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4u27zz","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I think RAM prices will fall over time and make these accessible to the masses in the next decade&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7o3u8/is_there_a_future_for_local_models/n4u27zz/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753326241,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m7o3u8","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4uax9b","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"custodiam99","can_mod_post":false,"created_utc":1753329826,"send_replies":true,"parent_id":"t3_1m7o3u8","score":3,"author_fullname":"t2_nqnhgqqf5","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I can run Qwen 3 235b q3\\\\_K\\\\_M on my PC, so no, I'm not pessimistic. Do we have to buy a PC with 512GB RAM soon? Sure. It is inevitable.","edited":1753330010,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4uax9b","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I can run Qwen 3 235b q3_K_M on my PC, so no, I&amp;#39;m not pessimistic. Do we have to buy a PC with 512GB RAM soon? Sure. It is inevitable.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7o3u8/is_there_a_future_for_local_models/n4uax9b/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753329826,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m7o3u8","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4vlul7","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"a_beautiful_rhind","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4v0onl","score":1,"author_fullname":"t2_h5utwre7","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"DDR5 going to get cheaper and CPUs with matrix extensions will go closer to end of life. Plus all the unified memory setups. So we'll get some relief in a couple years to run larger models at GPU pipeline-parallel speeds. :P","edited":false,"author_flair_css_class":null,"name":"t1_n4vlul7","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;DDR5 going to get cheaper and CPUs with matrix extensions will go closer to end of life. Plus all the unified memory setups. So we&amp;#39;ll get some relief in a couple years to run larger models at GPU pipeline-parallel speeds. :P&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1m7o3u8","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7o3u8/is_there_a_future_for_local_models/n4vlul7/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753354428,"author_flair_text":null,"collapsed":false,"created_utc":1753354428,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4v0onl","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"HiddenoO","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4uxagz","score":1,"author_fullname":"t2_8127x","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I'm talking about the rate of improvement for hardware, which has significantly and increasingly slowed down over the past two decades. People still like to cite Moore's Law which hasn't held true in ages, even when accounting for architectural improvements (the original was only talking about transistor count).","edited":1753343215,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4v0onl","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;m talking about the rate of improvement for hardware, which has significantly and increasingly slowed down over the past two decades. People still like to cite Moore&amp;#39;s Law which hasn&amp;#39;t held true in ages, even when accounting for architectural improvements (the original was only talking about transistor count).&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m7o3u8","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7o3u8/is_there_a_future_for_local_models/n4v0onl/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753342861,"author_flair_text":null,"treatment_tags":[],"created_utc":1753342861,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4uxagz","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Square-Onion-1825","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4uic98","score":1,"author_fullname":"t2_1mkh7x2yxn","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"its way better now then a decade ago.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n4uxagz","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;its way better now then a decade ago.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m7o3u8","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7o3u8/is_there_a_future_for_local_models/n4uxagz/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753340949,"author_flair_text":null,"treatment_tags":[],"created_utc":1753340949,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4uic98","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"HiddenoO","can_mod_post":false,"created_utc":1753333202,"send_replies":true,"parent_id":"t1_n4sxstm","score":6,"author_fullname":"t2_8127x","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"People really need to give up this take. It was valid until a decade or so ago, but HW improvements have significantly slowed down since, especially if you factor in price point and power draw. This also makes sense since we're hitting physical limitations when it comes to shrinking dies.\\n\\nI'm not saying there are no improvements, but you can no longer assume hardware will just let you run the same models twice as fast in 18 months. Software improvements make a much larger difference nowadays.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4uic98","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;People really need to give up this take. It was valid until a decade or so ago, but HW improvements have significantly slowed down since, especially if you factor in price point and power draw. This also makes sense since we&amp;#39;re hitting physical limitations when it comes to shrinking dies.&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;m not saying there are no improvements, but you can no longer assume hardware will just let you run the same models twice as fast in 18 months. Software improvements make a much larger difference nowadays.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m7o3u8","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7o3u8/is_there_a_future_for_local_models/n4uic98/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753333202,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}}],"before":null}},"user_reports":[],"saved":false,"id":"n4sxstm","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Square-Onion-1825","can_mod_post":false,"created_utc":1753312135,"send_replies":true,"parent_id":"t3_1m7o3u8","score":8,"author_fullname":"t2_1mkh7x2yxn","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"i think it will be more reachable over time as the h/w gets cheaper and faster","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4sxstm","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;i think it will be more reachable over time as the h/w gets cheaper and faster&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7o3u8/is_there_a_future_for_local_models/n4sxstm/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753312135,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m7o3u8","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":8}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4t6guz","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"-dysangel-","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4t5w9x","score":-1,"author_fullname":"t2_12ggykute6","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Oh, I see what you were trying to say now. But, I still think you're disproving your point by mentioning those three. First came Deepseek. Then Kimi was bigger. Now Qwen Coder beats them out and is smaller.\\n\\n  \\nAlso you know they are going to release smaller versions of Qwen 3 Coder, right? It's a \\\\*good\\\\* thing IMO that a decently large model is available in addition to those.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4t6guz","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Oh, I see what you were trying to say now. But, I still think you&amp;#39;re disproving your point by mentioning those three. First came Deepseek. Then Kimi was bigger. Now Qwen Coder beats them out and is smaller.&lt;/p&gt;\\n\\n&lt;p&gt;Also you know they are going to release smaller versions of Qwen 3 Coder, right? It&amp;#39;s a *good* thing IMO that a decently large model is available in addition to those.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m7o3u8","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7o3u8/is_there_a_future_for_local_models/n4t6guz/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753314995,"author_flair_text":"llama.cpp","treatment_tags":[],"created_utc":1753314995,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4t5w9x","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ASTRdeca","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4t1keg","score":4,"author_fullname":"t2_e11po","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Qwen3 coder is significant larger than Qwen2.5 coder...","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n4t5w9x","is_submitter":true,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Qwen3 coder is significant larger than Qwen2.5 coder...&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m7o3u8","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7o3u8/is_there_a_future_for_local_models/n4t5w9x/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753314802,"author_flair_text":null,"treatment_tags":[],"created_utc":1753314802,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}}],"before":null}},"user_reports":[],"saved":false,"id":"n4t1keg","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"-dysangel-","can_mod_post":false,"created_utc":1753313367,"send_replies":true,"parent_id":"t1_n4sy93o","score":1,"author_fullname":"t2_12ggykute6","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yeah he disproved his own point by mentioning Qwen Coder there. It seems more of a troll than a question","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4t1keg","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yeah he disproved his own point by mentioning Qwen Coder there. It seems more of a troll than a question&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m7o3u8","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7o3u8/is_there_a_future_for_local_models/n4t1keg/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753313367,"author_flair_text":"llama.cpp","treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":1,"depth":1,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4sy93o","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Yu2sama","can_mod_post":false,"created_utc":1753312282,"send_replies":true,"parent_id":"t3_1m7o3u8","score":12,"author_fullname":"t2_uu7xvge","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"The future is smaller models in my honest opinion. Yes, at the moment we are struggling, but give it a few years and you will see. Smaller models from today perform better than bigger models of the past (a good example is the new Qwen, half the size of Deepseek and I argue performs much better)\\n\\nThis is a rapidly growing field, don't feel discouraged due to the present.","edited":1753313535,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4sy93o","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The future is smaller models in my honest opinion. Yes, at the moment we are struggling, but give it a few years and you will see. Smaller models from today perform better than bigger models of the past (a good example is the new Qwen, half the size of Deepseek and I argue performs much better)&lt;/p&gt;\\n\\n&lt;p&gt;This is a rapidly growing field, don&amp;#39;t feel discouraged due to the present.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7o3u8/is_there_a_future_for_local_models/n4sy93o/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753312282,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m7o3u8","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":12}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4tzvog","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"FenderMoon","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4t8t60","score":2,"author_fullname":"t2_f4ibdsc9","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I can't wait for Gemma4.\\n\\nGemma3 is so freaking good already it's insane. I hope Google releases a reasoning model eventually too.\\n\\n(I mean I kinda like that Gemma3 isn't a reasoning model, it's good and it's fast. But I bet if Google releases some reasoning version eventually, it'll smoke Qwen3 and Deepseek).","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4tzvog","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I can&amp;#39;t wait for Gemma4.&lt;/p&gt;\\n\\n&lt;p&gt;Gemma3 is so freaking good already it&amp;#39;s insane. I hope Google releases a reasoning model eventually too.&lt;/p&gt;\\n\\n&lt;p&gt;(I mean I kinda like that Gemma3 isn&amp;#39;t a reasoning model, it&amp;#39;s good and it&amp;#39;s fast. But I bet if Google releases some reasoning version eventually, it&amp;#39;ll smoke Qwen3 and Deepseek).&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m7o3u8","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7o3u8/is_there_a_future_for_local_models/n4tzvog/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753325344,"author_flair_text":null,"treatment_tags":[],"created_utc":1753325344,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n4t8t60","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"llmentry","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4t58st","score":2,"author_fullname":"t2_1lufy6yx6z","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Agreed, Gemma 3 shows just how far we've come.  That said, Gemma is strong on writing / language and pretty weak on STEM / coding.  Every small model has its strengths and weaknesses.\\n\\nI suspect Kimi K2 is representative of the space OpenAI was in with GPT-4 a year and a half ago.  Give it another half year, and my hope is that we'll see \\\\~70-200B param open-weighted models kicking it around at 4o / 4.1 mini / 4.1 level, or better.\\n\\n(Maybe even earlier, if we get a Gemma 4 70B model ...)","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n4t8t60","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Agreed, Gemma 3 shows just how far we&amp;#39;ve come.  That said, Gemma is strong on writing / language and pretty weak on STEM / coding.  Every small model has its strengths and weaknesses.&lt;/p&gt;\\n\\n&lt;p&gt;I suspect Kimi K2 is representative of the space OpenAI was in with GPT-4 a year and a half ago.  Give it another half year, and my hope is that we&amp;#39;ll see ~70-200B param open-weighted models kicking it around at 4o / 4.1 mini / 4.1 level, or better.&lt;/p&gt;\\n\\n&lt;p&gt;(Maybe even earlier, if we get a Gemma 4 70B model ...)&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m7o3u8","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7o3u8/is_there_a_future_for_local_models/n4t8t60/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753315806,"author_flair_text":null,"treatment_tags":[],"created_utc":1753315806,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4v23eg","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AppearanceHeavy6724","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4t58st","score":1,"author_fullname":"t2_uz37qfx5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"The weakness of Gemmas is weak long context performance.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n4v23eg","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The weakness of Gemmas is weak long context performance.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m7o3u8","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7o3u8/is_there_a_future_for_local_models/n4v23eg/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753343654,"author_flair_text":null,"treatment_tags":[],"created_utc":1753343654,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4t58st","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"FenderMoon","can_mod_post":false,"created_utc":1753314588,"send_replies":true,"parent_id":"t1_n4t0k2j","score":8,"author_fullname":"t2_f4ibdsc9","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Gemma is a shockingly good model for its size. I was able to run 27b on a 16GB Mac with an IQ3_XS quant, was super impressed with the quality of the model despite using such an aggressive quant. \\n\\nI like that they offered a 27b instead of the standard 32b ones. It makes it easier to run these models on smaller systems. It’s such a good model, heck even the 12b often gave better results than some of the other 32b models I’ve tried.\\n\\n(I did notice that on the 12b model, it was much more sensitive to quants for some reason, I got far better results at 6 bits than at 4 bits. That was surprising to see so much degredation at 4 bits, normally 4 bits is the sweet spot. 27b doesn’t seem to be nearly as sensitive, 3 bits locally was almost as good as the unquantized ones from AI studio).","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4t58st","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Gemma is a shockingly good model for its size. I was able to run 27b on a 16GB Mac with an IQ3_XS quant, was super impressed with the quality of the model despite using such an aggressive quant. &lt;/p&gt;\\n\\n&lt;p&gt;I like that they offered a 27b instead of the standard 32b ones. It makes it easier to run these models on smaller systems. It’s such a good model, heck even the 12b often gave better results than some of the other 32b models I’ve tried.&lt;/p&gt;\\n\\n&lt;p&gt;(I did notice that on the 12b model, it was much more sensitive to quants for some reason, I got far better results at 6 bits than at 4 bits. That was surprising to see so much degredation at 4 bits, normally 4 bits is the sweet spot. 27b doesn’t seem to be nearly as sensitive, 3 bits locally was almost as good as the unquantized ones from AI studio).&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m7o3u8","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7o3u8/is_there_a_future_for_local_models/n4t58st/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753314588,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":8}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4t7hbh","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"CantaloupeDismal1195","can_mod_post":false,"created_utc":1753315346,"send_replies":true,"parent_id":"t1_n4t0k2j","score":5,"author_fullname":"t2_1ld1b995hk","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I want Gemma4 70B","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4t7hbh","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I want Gemma4 70B&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m7o3u8","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7o3u8/is_there_a_future_for_local_models/n4t7hbh/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753315346,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4til9x","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Caffdy","can_mod_post":false,"created_utc":1753319202,"send_replies":true,"parent_id":"t1_n4t0k2j","score":2,"author_fullname":"t2_ql2vu0wz","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Gemma 4 70B? those are serious allegations ..\\n\\nsource?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4til9x","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Gemma 4 70B? those are serious allegations ..&lt;/p&gt;\\n\\n&lt;p&gt;source?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m7o3u8","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7o3u8/is_there_a_future_for_local_models/n4til9x/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753319202,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4tdjv0","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"TheRealMasonMac","can_mod_post":false,"created_utc":1753317448,"send_replies":true,"parent_id":"t1_n4t0k2j","score":1,"author_fullname":"t2_101haj","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"70B would be awesome.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4tdjv0","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;70B would be awesome.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m7o3u8","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7o3u8/is_there_a_future_for_local_models/n4tdjv0/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753317448,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4t0k2j","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Rich_Artist_8327","can_mod_post":false,"created_utc":1753313036,"send_replies":true,"parent_id":"t3_1m7o3u8","score":15,"author_fullname":"t2_1jk2ep8a52","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Google Gemma saves us. Long live Google! Gemma4 12,27,70B coming!!!!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4t0k2j","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Google Gemma saves us. Long live Google! Gemma4 12,27,70B coming!!!!&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7o3u8/is_there_a_future_for_local_models/n4t0k2j/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753313036,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m7o3u8","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":15}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4t6ghj","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"EugenePopcorn","can_mod_post":false,"created_utc":1753314992,"send_replies":true,"parent_id":"t3_1m7o3u8","score":3,"author_fullname":"t2_g6hpxxgss","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"The main part of each of these models is still quite small. It's only the experts that are heavy. Loading them from disk and caching them in memory isn't super performant right now, but llama.cpp's new high throughput mode might be helpful for anybody using local agents. And cache misses matter less when you have multiple things to work on. ","edited":1753315804,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4t6ghj","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The main part of each of these models is still quite small. It&amp;#39;s only the experts that are heavy. Loading them from disk and caching them in memory isn&amp;#39;t super performant right now, but llama.cpp&amp;#39;s new high throughput mode might be helpful for anybody using local agents. And cache misses matter less when you have multiple things to work on. &lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7o3u8/is_there_a_future_for_local_models/n4t6ghj/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753314992,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m7o3u8","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4tfui3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Secure_Reflection409","can_mod_post":false,"created_utc":1753318248,"send_replies":true,"parent_id":"t3_1m7o3u8","score":3,"author_fullname":"t2_by77ogdhr","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I think we'll see another 32b coder so not super worried.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4tfui3","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I think we&amp;#39;ll see another 32b coder so not super worried.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7o3u8/is_there_a_future_for_local_models/n4tfui3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753318248,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m7o3u8","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4t2phl","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"mnt_brain","can_mod_post":false,"created_utc":1753313747,"send_replies":true,"parent_id":"t3_1m7o3u8","score":2,"author_fullname":"t2_1mtt9dytfn","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"big models can train smaller very specialized models.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4t2phl","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;big models can train smaller very specialized models.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7o3u8/is_there_a_future_for_local_models/n4t2phl/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753313747,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m7o3u8","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"fe89e94a-13f2-11f0-a9de-6262c74956cf","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4usxn7","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"qlippothvi","can_mod_post":false,"created_utc":1753338597,"send_replies":true,"parent_id":"t1_n4tkun8","score":1,"author_fullname":"t2_9nap4","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Newer Google Pixel phones run local models now… They are very specialized, but they run locally.","edited":1753338937,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4usxn7","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Newer Google Pixel phones run local models now… They are very specialized, but they run locally.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m7o3u8","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7o3u8/is_there_a_future_for_local_models/n4usxn7/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753338597,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4tkun8","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Asleep-Ratio7535","can_mod_post":false,"created_utc":1753319995,"send_replies":true,"parent_id":"t3_1m7o3u8","score":2,"author_fullname":"t2_1lfyddwf0c","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Even mobile or some sensor size devices start using local models.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4tkun8","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"Llama 4"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Even mobile or some sensor size devices start using local models.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7o3u8/is_there_a_future_for_local_models/n4tkun8/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753319995,"author_flair_text":"Llama 4","treatment_tags":[],"link_id":"t3_1m7o3u8","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#b0ae9b","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4u3o0p","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ieatdownvotes4food","can_mod_post":false,"created_utc":1753326811,"send_replies":true,"parent_id":"t3_1m7o3u8","score":2,"author_fullname":"t2_12fv96rb","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I mean, gemma3 nailed it.. \\n\\nit's really more about you use the models for me at this point.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4u3o0p","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I mean, gemma3 nailed it.. &lt;/p&gt;\\n\\n&lt;p&gt;it&amp;#39;s really more about you use the models for me at this point.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7o3u8/is_there_a_future_for_local_models/n4u3o0p/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753326811,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m7o3u8","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"2b12e2b8-fdc0-11ee-9a03-6e2f48afd456","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4u6k7u","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"hedonihilistic","can_mod_post":false,"created_utc":1753327984,"send_replies":true,"parent_id":"t3_1m7o3u8","score":2,"author_fullname":"t2_281myw","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"With time, hardware advancements would hopefully allow us to run some of these larger models too. I cant wait to have something like Gemini pro 2.5 or sonnet 3.5+ running on my HW.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4u6k7u","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"Llama 3"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;With time, hardware advancements would hopefully allow us to run some of these larger models too. I cant wait to have something like Gemini pro 2.5 or sonnet 3.5+ running on my HW.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7o3u8/is_there_a_future_for_local_models/n4u6k7u/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753327984,"author_flair_text":"Llama 3","treatment_tags":[],"link_id":"t3_1m7o3u8","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#c7b594","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4uvf0w","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Mediocre-Waltz6792","can_mod_post":false,"created_utc":1753339937,"send_replies":true,"parent_id":"t3_1m7o3u8","score":2,"author_fullname":"t2_r7wlk6atm","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Comparing large and smaller models is almost like saying you need a 8K monitor. Ive found some really good smaller models so dont count them out because they are small.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4uvf0w","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Comparing large and smaller models is almost like saying you need a 8K monitor. Ive found some really good smaller models so dont count them out because they are small.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7o3u8/is_there_a_future_for_local_models/n4uvf0w/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753339937,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m7o3u8","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4v46iz","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"jerieljan","can_mod_post":false,"created_utc":1753344841,"send_replies":true,"parent_id":"t3_1m7o3u8","score":2,"author_fullname":"t2_4a6mo","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I like how all the stuff you mentioned is exactly why I have high hopes for local models.\\n\\nYes, these new local models are massive and still require huge amounts of computing power to run.\\n\\nBut they're actually available for us to use and keep. Unlike the proprietary models the AI labs keep for themselves. Given enough time and GPUs, we can actually run them without relying on the cloud.\\n\\nBack in the day, we could only dream for models that could perform somewhere close to GPT-4. Now we're seeing competition that can challenge the best models. You just needed hardware, or keep waiting for models to get smaller and keep getting better.\\n\\nI'd like to someday see commodity hardware and GPUs being more available and can run bigger, beefier models.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4v46iz","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I like how all the stuff you mentioned is exactly why I have high hopes for local models.&lt;/p&gt;\\n\\n&lt;p&gt;Yes, these new local models are massive and still require huge amounts of computing power to run.&lt;/p&gt;\\n\\n&lt;p&gt;But they&amp;#39;re actually available for us to use and keep. Unlike the proprietary models the AI labs keep for themselves. Given enough time and GPUs, we can actually run them without relying on the cloud.&lt;/p&gt;\\n\\n&lt;p&gt;Back in the day, we could only dream for models that could perform somewhere close to GPT-4. Now we&amp;#39;re seeing competition that can challenge the best models. You just needed hardware, or keep waiting for models to get smaller and keep getting better.&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;d like to someday see commodity hardware and GPUs being more available and can run bigger, beefier models.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7o3u8/is_there_a_future_for_local_models/n4v46iz/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753344841,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m7o3u8","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4ueysy","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"cantgetthistowork","can_mod_post":false,"created_utc":1753331642,"send_replies":true,"parent_id":"t1_n4u6rmt","score":1,"author_fullname":"t2_j1i0o","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"We're currently in an awkward phase where we've hit the physical limitations of how many 3090s one can reasonably fit on a single machine (15-16 before it gets messy with power and cabling). Until the day when the next tier of higher capacity cards hits the market at the same affordability the ecosystem has to pivot to new ways for running the frontier models even at the cost of sacrificing performance through CPU loading. I personally have started the painful process of replacing my 13x3090s with DDR5 because even at 20x3090s there's no way I can load enough of a model to justify the costs running them.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4ueysy","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;We&amp;#39;re currently in an awkward phase where we&amp;#39;ve hit the physical limitations of how many 3090s one can reasonably fit on a single machine (15-16 before it gets messy with power and cabling). Until the day when the next tier of higher capacity cards hits the market at the same affordability the ecosystem has to pivot to new ways for running the frontier models even at the cost of sacrificing performance through CPU loading. I personally have started the painful process of replacing my 13x3090s with DDR5 because even at 20x3090s there&amp;#39;s no way I can load enough of a model to justify the costs running them.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m7o3u8","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7o3u8/is_there_a_future_for_local_models/n4ueysy/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753331642,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4vtg2b","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"a_beautiful_rhind","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4vpr3m","score":1,"author_fullname":"t2_h5utwre7","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I feel you, but then what's the point? Yes it can be deployed, but it's another waiting game to see if anyone trains a good model at the size.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4vtg2b","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I feel you, but then what&amp;#39;s the point? Yes it can be deployed, but it&amp;#39;s another waiting game to see if anyone trains a good model at the size.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m7o3u8","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7o3u8/is_there_a_future_for_local_models/n4vtg2b/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753357686,"author_flair_text":null,"treatment_tags":[],"created_utc":1753357686,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4vpr3m","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"lly0571","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4vo5o8","score":2,"author_fullname":"t2_70vzcleel","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"My discussion with OP primarily focuses on the feasibility of local deployment for models of different sizes, rather than the performance of the models themselves. My main point is that MoE models of a similar size to Llama4-Scout (around 100B-A15B) can be deployed on high-end PCs with 64GB of RAM.\\n\\nI knew that Llama4-Scout performs very bad (sometimes worse than Gemma3-27B), and Hunyuan-A13B is barely usable in Chinese dialogue scenarios (I don't believe it outperforms Qwen3-32B).","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n4vpr3m","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;My discussion with OP primarily focuses on the feasibility of local deployment for models of different sizes, rather than the performance of the models themselves. My main point is that MoE models of a similar size to Llama4-Scout (around 100B-A15B) can be deployed on high-end PCs with 64GB of RAM.&lt;/p&gt;\\n\\n&lt;p&gt;I knew that Llama4-Scout performs very bad (sometimes worse than Gemma3-27B), and Hunyuan-A13B is barely usable in Chinese dialogue scenarios (I don&amp;#39;t believe it outperforms Qwen3-32B).&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m7o3u8","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7o3u8/is_there_a_future_for_local_models/n4vpr3m/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753356154,"author_flair_text":null,"treatment_tags":[],"created_utc":1753356154,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n4vo5o8","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"a_beautiful_rhind","can_mod_post":false,"created_utc":1753355461,"send_replies":true,"parent_id":"t1_n4u6rmt","score":1,"author_fullname":"t2_h5utwre7","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"&gt; Llama4-Scout (109B-A17B), Hunyuan-80B-A13B\\n\\nThose models are terrible. Dumber than a 32b. Even qwen 235b is on the lesser side in terms of raw intelligence for all the resources it takes up.\\n\\nMost importantly, none of them will get loras or finetunes. The hobbyist has been relegated to small dense models if they want to alter them. What a subtle way to edge people out.\\n\\nSame trend is mirrored on the image gen side with hard distilled models and ones that don't train well beyond a single concept lora.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4vo5o8","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;Llama4-Scout (109B-A17B), Hunyuan-80B-A13B&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;Those models are terrible. Dumber than a 32b. Even qwen 235b is on the lesser side in terms of raw intelligence for all the resources it takes up.&lt;/p&gt;\\n\\n&lt;p&gt;Most importantly, none of them will get loras or finetunes. The hobbyist has been relegated to small dense models if they want to alter them. What a subtle way to edge people out.&lt;/p&gt;\\n\\n&lt;p&gt;Same trend is mirrored on the image gen side with hard distilled models and ones that don&amp;#39;t train well beyond a single concept lora.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m7o3u8","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7o3u8/is_there_a_future_for_local_models/n4vo5o8/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753355461,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4u6rmt","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"lly0571","can_mod_post":false,"created_utc":1753328068,"send_replies":true,"parent_id":"t3_1m7o3u8","score":4,"author_fullname":"t2_70vzcleel","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Since the emergence of Deepseek-V3 towards the end of last year, the clear trends in Large Language Models (LLMs) have been the following:\\n\\n\\n1.  It's likely we won't see any more LLMs pre-trained from scratch at the 70B scale, except perhaps the latest large Dense model like Command-A, and the earlier Llama3.3-70B. However, models at the 32B level (27-34B) and smaller will continue to appear due to their ease of deployment.\\n\\n2.  The sparsity of MoE activation reduces bandwidth and computational requirements. This allows for local inference (not deployment) by using methods like loading the routing, embedding layer, and shared experts into VRAM, while loading the MoE components into system RAM. This reduces VRAM overhead. However, PCIe speed (during Prefill) and RAM bandwidth (during Decode) become new bottlenecks. This deployment method is limited by PCIe speed and CPU compute power, resulting in overall throughput significantly lower than GPU-based solutions. However, the decode and prefill performance under single concurrency might still be acceptable.\\n\\n3.  Comparing medium and small-sized MoEs to Dense models(**&lt;40B**): Referencing models like Llama4-Scout (109B-A17B), Hunyuan-80B-A13B appropriately proves small MoEs can run on high-end PCs (with &gt;=64GB RAM) achieving decode speeds over 10t/s, and Qwen3-30B-A3B proves that small MoEs can run on basically any modern PC with DDR4. All you need is a new DDR5 desktop platform paired with a reasonably capable GPU. Overall, this approach makes running LLMs locally easier, but obtaining high throughput becomes more difficult.\\n\\n4.  For **large models (\\\\~70B Dense or equivalent)**, using the geometric mean as an estimate, Qwen3-235B is roughly a 72B model, and Llama4-Maverick is roughly an 80B model. Models like Qwen2.5-72B and Llama3.3-70B, which are comparable in scale, were previously not easily runnable locally even with GPUs (using inexpensive GPUs like the P40 24GB would be very slow, perhaps 5-6t/s. You'd need at least a 2x 3090 or better). Now, running these models requires investing in a workstation with either a single Epyc 7002/7003 or a dual Skylake Xeon or better, plus one reasonably capable GPU.\\nComparing both approaches, you lose the throughput and concurrency capabilities offered by multi-GPU setups. However, you gain relatively better model quality and access to a server with extremely abundant memory.\\n\\n5.  For **ultra-large models** exceeding the memory capacity of more than 4x 3090 GPUs after 4-bit quantization (e.g., &gt;160B), MoEs are just much easier to run compared to previous ultra-large models like Llama-405B (And that's why MoE are implemented to continue the scaling law). Q4 quantized Kimi-K2 can achieve speed of 5-7t/s on a DDR4 server with GPU. Running Llama-405B on 8x 3090s would certainly not be that fast (and you still need a server/WS processor for systems with 4+ cards anyway)...\\n\\nMy ideal future local LLM solution would be an APU with socketable memory, ideally offering Epyc 9004/9005-level bandwidth and I/O capabilities, paired with a GPU on par with the 9070XT (or 5070 Ti) and on-chip L4 cache (similar to the Xeon Max series), priced at the current Epyc 9004 level. Such a configuration would be relatively easy to manufacture, offer acceptable memory bandwidth and compute power, and reduce the communication overhead compared to current model sharding solutions that rely on PCIe.  \\n\\nCurrent STH/DGX Spark setups feel somewhat underwhelming, with only 256-bit LPDDR5 memory, where the bandwidth advantage over sharding solutions using Icelake-SP or Epyc Milan is minimal. Meanwhile, Apple’s M4 Max/M3 Ultra memory and storage are prohibitively expensive, and their compute performance—even compared to CPUs like the Epyc 9755 or Xeon 6980P—can hardly be considered superior.","edited":1753329032,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4u6rmt","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Since the emergence of Deepseek-V3 towards the end of last year, the clear trends in Large Language Models (LLMs) have been the following:&lt;/p&gt;\\n\\n&lt;ol&gt;\\n&lt;li&gt;&lt;p&gt;It&amp;#39;s likely we won&amp;#39;t see any more LLMs pre-trained from scratch at the 70B scale, except perhaps the latest large Dense model like Command-A, and the earlier Llama3.3-70B. However, models at the 32B level (27-34B) and smaller will continue to appear due to their ease of deployment.&lt;/p&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;p&gt;The sparsity of MoE activation reduces bandwidth and computational requirements. This allows for local inference (not deployment) by using methods like loading the routing, embedding layer, and shared experts into VRAM, while loading the MoE components into system RAM. This reduces VRAM overhead. However, PCIe speed (during Prefill) and RAM bandwidth (during Decode) become new bottlenecks. This deployment method is limited by PCIe speed and CPU compute power, resulting in overall throughput significantly lower than GPU-based solutions. However, the decode and prefill performance under single concurrency might still be acceptable.&lt;/p&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;p&gt;Comparing medium and small-sized MoEs to Dense models(&lt;strong&gt;&amp;lt;40B&lt;/strong&gt;): Referencing models like Llama4-Scout (109B-A17B), Hunyuan-80B-A13B appropriately proves small MoEs can run on high-end PCs (with &amp;gt;=64GB RAM) achieving decode speeds over 10t/s, and Qwen3-30B-A3B proves that small MoEs can run on basically any modern PC with DDR4. All you need is a new DDR5 desktop platform paired with a reasonably capable GPU. Overall, this approach makes running LLMs locally easier, but obtaining high throughput becomes more difficult.&lt;/p&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;p&gt;For &lt;strong&gt;large models (~70B Dense or equivalent)&lt;/strong&gt;, using the geometric mean as an estimate, Qwen3-235B is roughly a 72B model, and Llama4-Maverick is roughly an 80B model. Models like Qwen2.5-72B and Llama3.3-70B, which are comparable in scale, were previously not easily runnable locally even with GPUs (using inexpensive GPUs like the P40 24GB would be very slow, perhaps 5-6t/s. You&amp;#39;d need at least a 2x 3090 or better). Now, running these models requires investing in a workstation with either a single Epyc 7002/7003 or a dual Skylake Xeon or better, plus one reasonably capable GPU.\\nComparing both approaches, you lose the throughput and concurrency capabilities offered by multi-GPU setups. However, you gain relatively better model quality and access to a server with extremely abundant memory.&lt;/p&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;p&gt;For &lt;strong&gt;ultra-large models&lt;/strong&gt; exceeding the memory capacity of more than 4x 3090 GPUs after 4-bit quantization (e.g., &amp;gt;160B), MoEs are just much easier to run compared to previous ultra-large models like Llama-405B (And that&amp;#39;s why MoE are implemented to continue the scaling law). Q4 quantized Kimi-K2 can achieve speed of 5-7t/s on a DDR4 server with GPU. Running Llama-405B on 8x 3090s would certainly not be that fast (and you still need a server/WS processor for systems with 4+ cards anyway)...&lt;/p&gt;&lt;/li&gt;\\n&lt;/ol&gt;\\n\\n&lt;p&gt;My ideal future local LLM solution would be an APU with socketable memory, ideally offering Epyc 9004/9005-level bandwidth and I/O capabilities, paired with a GPU on par with the 9070XT (or 5070 Ti) and on-chip L4 cache (similar to the Xeon Max series), priced at the current Epyc 9004 level. Such a configuration would be relatively easy to manufacture, offer acceptable memory bandwidth and compute power, and reduce the communication overhead compared to current model sharding solutions that rely on PCIe.  &lt;/p&gt;\\n\\n&lt;p&gt;Current STH/DGX Spark setups feel somewhat underwhelming, with only 256-bit LPDDR5 memory, where the bandwidth advantage over sharding solutions using Icelake-SP or Epyc Milan is minimal. Meanwhile, Apple’s M4 Max/M3 Ultra memory and storage are prohibitively expensive, and their compute performance—even compared to CPUs like the Epyc 9755 or Xeon 6980P—can hardly be considered superior.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7o3u8/is_there_a_future_for_local_models/n4u6rmt/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753328068,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m7o3u8","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4t0xqc","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"eloquentemu","can_mod_post":false,"created_utc":1753313161,"send_replies":true,"parent_id":"t3_1m7o3u8","score":2,"author_fullname":"t2_lpdsy","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"&gt; I'm starting to lose hope for the local scene as model sizes begin to creep further away from what we can run on consumer hardware.\\n\\nLet's just take a step back for a sec...  Qwen3 has models from 0.6B to 480B.  Are you interested running models or expecting to replace a human developer with a 4060?\\n\\nIt's less that models are becoming bigger, and more they're becoming more capable, and the more capable they are the larger they are.  There are still plenty of 4B, 8B, 32B models that run fine on consumer hardware.  Not just the ones we had last year but new and better ones too.  You can still run cutting edge models on consumer hardware and they're more capable than they ever were.  Just don't expect a 32B model to compete with a &gt;300B model.","edited":1753316927,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4t0xqc","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;I&amp;#39;m starting to lose hope for the local scene as model sizes begin to creep further away from what we can run on consumer hardware.&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;Let&amp;#39;s just take a step back for a sec...  Qwen3 has models from 0.6B to 480B.  Are you interested running models or expecting to replace a human developer with a 4060?&lt;/p&gt;\\n\\n&lt;p&gt;It&amp;#39;s less that models are becoming bigger, and more they&amp;#39;re becoming more capable, and the more capable they are the larger they are.  There are still plenty of 4B, 8B, 32B models that run fine on consumer hardware.  Not just the ones we had last year but new and better ones too.  You can still run cutting edge models on consumer hardware and they&amp;#39;re more capable than they ever were.  Just don&amp;#39;t expect a 32B model to compete with a &amp;gt;300B model.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7o3u8/is_there_a_future_for_local_models/n4t0xqc/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753313161,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m7o3u8","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4t3mvi","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"triynizzles1","can_mod_post":false,"created_utc":1753314055,"send_replies":true,"parent_id":"t3_1m7o3u8","score":1,"author_fullname":"t2_zr0g49ixt","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"There will always be a place in the market or both small and large models. Large models will be at the frontier of intelligence, they have to be because they have more parameters available to them. Small models will push the industry forward since they are much less expensive to make and can serve to develop novel learning techniques. Edge devices and robotics are a big part of the market that small models will need to be specialized for. I will agree, though that most small models will not get the complete suite of Sota features and modality. They will likely only have one or two unique features that the company is developing at a time.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4t3mvi","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;There will always be a place in the market or both small and large models. Large models will be at the frontier of intelligence, they have to be because they have more parameters available to them. Small models will push the industry forward since they are much less expensive to make and can serve to develop novel learning techniques. Edge devices and robotics are a big part of the market that small models will need to be specialized for. I will agree, though that most small models will not get the complete suite of Sota features and modality. They will likely only have one or two unique features that the company is developing at a time.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7o3u8/is_there_a_future_for_local_models/n4t3mvi/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753314055,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m7o3u8","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4t5hlr","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"YouDontSeemRight","can_mod_post":false,"created_utc":1753314667,"send_replies":true,"parent_id":"t3_1m7o3u8","score":1,"author_fullname":"t2_1b7gjxtue9","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"These models are for corporate entities to use instead of relying on the AI thoughts of closed US enterprises. There is a political reason for China to continue excelling in open source. Eventually the open source option always gets adopted.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4t5hlr","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;These models are for corporate entities to use instead of relying on the AI thoughts of closed US enterprises. There is a political reason for China to continue excelling in open source. Eventually the open source option always gets adopted.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7o3u8/is_there_a_future_for_local_models/n4t5hlr/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753314667,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m7o3u8","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4t8dkv","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"eggs-benedryl","can_mod_post":false,"created_utc":1753315657,"send_replies":true,"parent_id":"t3_1m7o3u8","score":1,"author_fullname":"t2_8nlxwtdi","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Doesn't this have to ignore all the other models we get regularly that can even run on phones? I mean they're not the best of the best or perfect for every use but they come fairly frequently","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4t8dkv","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Doesn&amp;#39;t this have to ignore all the other models we get regularly that can even run on phones? I mean they&amp;#39;re not the best of the best or perfect for every use but they come fairly frequently&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7o3u8/is_there_a_future_for_local_models/n4t8dkv/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753315657,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m7o3u8","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4t8gz4","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"JustinPooDough","can_mod_post":false,"created_utc":1753315690,"send_replies":true,"parent_id":"t3_1m7o3u8","score":1,"author_fullname":"t2_4kns99rz","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Maybe you can’t run it, but many providers will, and they will compete with each other and drive down API costs.\\n\\nStill a massive win for the consumer.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4t8gz4","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Maybe you can’t run it, but many providers will, and they will compete with each other and drive down API costs.&lt;/p&gt;\\n\\n&lt;p&gt;Still a massive win for the consumer.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7o3u8/is_there_a_future_for_local_models/n4t8gz4/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753315690,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m7o3u8","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4tat1g","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"tnofuentes","can_mod_post":false,"created_utc":1753316500,"send_replies":true,"parent_id":"t3_1m7o3u8","score":1,"author_fullname":"t2_4hwvr","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"While the cutting edge lives in the hyperscaler space now, there's strong pressure to bring capable models into the commercial space. Much of that pressure will come from the likes of Apple and Dell, along with future device makers (think Rabbit R1 but local).\\n\\nI think there will be a higher emphasis on performance at lower precision and on lesser hardware over the coming year.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4tat1g","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;While the cutting edge lives in the hyperscaler space now, there&amp;#39;s strong pressure to bring capable models into the commercial space. Much of that pressure will come from the likes of Apple and Dell, along with future device makers (think Rabbit R1 but local).&lt;/p&gt;\\n\\n&lt;p&gt;I think there will be a higher emphasis on performance at lower precision and on lesser hardware over the coming year.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7o3u8/is_there_a_future_for_local_models/n4tat1g/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753316500,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m7o3u8","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4tff10","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"SocialDinamo","can_mod_post":false,"created_utc":1753318097,"send_replies":true,"parent_id":"t3_1m7o3u8","score":1,"author_fullname":"t2_adou8","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Yet.. Hardware hasn’t caught up to the software. Right now vram and high memory bandwidth is stingy but in time more options will be available cheaper. Big labs will push the frontier while we get to enjoy those developments on a smaller scale","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4tff10","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yet.. Hardware hasn’t caught up to the software. Right now vram and high memory bandwidth is stingy but in time more options will be available cheaper. Big labs will push the frontier while we get to enjoy those developments on a smaller scale&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7o3u8/is_there_a_future_for_local_models/n4tff10/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753318097,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m7o3u8","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4tmnft","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Psionikus","can_mod_post":false,"created_utc":1753320629,"send_replies":true,"parent_id":"t3_1m7o3u8","score":1,"author_fullname":"t2_8vhsch4i","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Like all computing, it gets big and expensive to acquire initial operating capability and then gets small and fast to save cost and turn profit.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4tmnft","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Like all computing, it gets big and expensive to acquire initial operating capability and then gets small and fast to save cost and turn profit.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7o3u8/is_there_a_future_for_local_models/n4tmnft/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753320629,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m7o3u8","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4u497k","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"benny_dryl","can_mod_post":false,"created_utc":1753327046,"send_replies":true,"parent_id":"t3_1m7o3u8","score":1,"author_fullname":"t2_1fnfak30pp","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Nah you gotta chill. Way too early to say","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4u497k","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Nah you gotta chill. Way too early to say&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7o3u8/is_there_a_future_for_local_models/n4u497k/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753327046,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m7o3u8","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4u65f8","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AnonymousCrayonEater","can_mod_post":false,"created_utc":1753327813,"send_replies":true,"parent_id":"t3_1m7o3u8","score":1,"author_fullname":"t2_a2ypgji3","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"The way I see it is:\\n1. Small model quality is increasing\\n2. Local hardware is getting better and cheaper\\n\\nThese two things will converge in the next few years and youll be able to run gpt4 level quality on your phone locally.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4u65f8","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The way I see it is:\\n1. Small model quality is increasing\\n2. Local hardware is getting better and cheaper&lt;/p&gt;\\n\\n&lt;p&gt;These two things will converge in the next few years and youll be able to run gpt4 level quality on your phone locally.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7o3u8/is_there_a_future_for_local_models/n4u65f8/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753327813,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m7o3u8","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4u7vqz","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"XertonOne","can_mod_post":false,"created_utc":1753328528,"send_replies":true,"parent_id":"t3_1m7o3u8","score":1,"author_fullname":"t2_1n8h2fiw7d","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I truly beleive that Big Tech is developing models that would probably address big and wealthy companies. But those big generalized and smart models wont help small companies so much as a local carefully fine tuned abd ficused model would.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4u7vqz","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I truly beleive that Big Tech is developing models that would probably address big and wealthy companies. But those big generalized and smart models wont help small companies so much as a local carefully fine tuned abd ficused model would.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7o3u8/is_there_a_future_for_local_models/n4u7vqz/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753328528,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m7o3u8","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4ucq4w","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"yorgasor","can_mod_post":false,"created_utc":1753330619,"send_replies":true,"parent_id":"t3_1m7o3u8","score":1,"author_fullname":"t2_dqi59","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I think hardware will start being better optimized for large models. Apple’s unified memory architecture is going to be done in other systems and we’ll be able to run beefier models cheaper in a few years.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4ucq4w","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I think hardware will start being better optimized for large models. Apple’s unified memory architecture is going to be done in other systems and we’ll be able to run beefier models cheaper in a few years.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7o3u8/is_there_a_future_for_local_models/n4ucq4w/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753330619,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m7o3u8","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4uf3kx","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Space__Whiskey","can_mod_post":false,"created_utc":1753331703,"send_replies":true,"parent_id":"t3_1m7o3u8","score":1,"author_fullname":"t2_1cuwlonegr","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"There will always be a push for better and smaller models. Big models are fantastic gatekeepers and they will continue to be massive, so only a big pocketbook can access them and many will pay to use them. The real motivation is for more effective smaller models that can work on smaller devices. At the rate at which smaller models are getting better, I am optimistic for the future of small open source models.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4uf3kx","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;There will always be a push for better and smaller models. Big models are fantastic gatekeepers and they will continue to be massive, so only a big pocketbook can access them and many will pay to use them. The real motivation is for more effective smaller models that can work on smaller devices. At the rate at which smaller models are getting better, I am optimistic for the future of small open source models.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7o3u8/is_there_a_future_for_local_models/n4uf3kx/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753331703,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m7o3u8","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4uliee","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Ravenpest","can_mod_post":false,"created_utc":1753334757,"send_replies":true,"parent_id":"t3_1m7o3u8","score":1,"author_fullname":"t2_21lhc1","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Qwen3 32b is as good as a dense 70b of a year and a half ago. Relax. Things will get more manageable over time. On top of that those giant models are MoEs, you can run them on reasonably consumer friendly hardware","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4uliee","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Qwen3 32b is as good as a dense 70b of a year and a half ago. Relax. Things will get more manageable over time. On top of that those giant models are MoEs, you can run them on reasonably consumer friendly hardware&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7o3u8/is_there_a_future_for_local_models/n4uliee/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753334757,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m7o3u8","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4uu62x","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AppealSame4367","can_mod_post":false,"created_utc":1753339264,"send_replies":true,"parent_id":"t3_1m7o3u8","score":1,"author_fullname":"t2_sxud8ccv4","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"It's the same like with main frames vs personal computers in the 70s, 80s: both will exist, it will spread to everywhere.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4uu62x","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It&amp;#39;s the same like with main frames vs personal computers in the 70s, 80s: both will exist, it will spread to everywhere.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7o3u8/is_there_a_future_for_local_models/n4uu62x/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753339264,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m7o3u8","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4uzu8z","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Universespitoon","can_mod_post":false,"created_utc":1753342379,"send_replies":true,"parent_id":"t3_1m7o3u8","score":1,"author_fullname":"t2_kauie","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"DGX","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4uzu8z","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;DGX&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7o3u8/is_there_a_future_for_local_models/n4uzu8z/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753342379,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m7o3u8","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4v54a0","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Fox-Lopsided","can_mod_post":false,"created_utc":1753345387,"send_replies":true,"parent_id":"t3_1m7o3u8","score":1,"author_fullname":"t2_7ivwbs3t","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I hope that some day we will have a 8B to 14B model that matches the big boys","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4v54a0","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I hope that some day we will have a 8B to 14B model that matches the big boys&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7o3u8/is_there_a_future_for_local_models/n4v54a0/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753345387,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m7o3u8","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4v7kek","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Leather-Cod2129","can_mod_post":false,"created_utc":1753346794,"send_replies":true,"parent_id":"t3_1m7o3u8","score":1,"author_fullname":"t2_182pbi954w","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Wait for diffusion models","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4v7kek","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Wait for diffusion models&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7o3u8/is_there_a_future_for_local_models/n4v7kek/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753346794,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m7o3u8","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4v8hoo","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Qxz3","can_mod_post":false,"created_utc":1753347316,"send_replies":true,"parent_id":"t3_1m7o3u8","score":1,"author_fullname":"t2_8frrcg6r","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Just this year, we've had Gemma 3, Gemma 3n, Qwen 3 and other models punching way above their weight in consumer-friendly sizes. Rather than a trend towards larger models, I think we see models of all sizes offering large improvements across the spectrum.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4v8hoo","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Just this year, we&amp;#39;ve had Gemma 3, Gemma 3n, Qwen 3 and other models punching way above their weight in consumer-friendly sizes. Rather than a trend towards larger models, I think we see models of all sizes offering large improvements across the spectrum.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7o3u8/is_there_a_future_for_local_models/n4v8hoo/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753347316,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m7o3u8","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4v9qxj","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Delicious-Finding-97","can_mod_post":false,"created_utc":1753348033,"send_replies":true,"parent_id":"t3_1m7o3u8","score":1,"author_fullname":"t2_bt4vlckf","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Absolutely what you are likely to see is AI cards for PC's like you have with GPU's.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4v9qxj","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Absolutely what you are likely to see is AI cards for PC&amp;#39;s like you have with GPU&amp;#39;s.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7o3u8/is_there_a_future_for_local_models/n4v9qxj/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753348033,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m7o3u8","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4vccik","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ItsNoahJ83","can_mod_post":false,"created_utc":1753349519,"send_replies":true,"parent_id":"t1_n4vc8f4","score":1,"author_fullname":"t2_hwfuy0ki","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"It's Qwen30B A3B","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4vccik","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It&amp;#39;s Qwen30B A3B&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m7o3u8","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7o3u8/is_there_a_future_for_local_models/n4vccik/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753349519,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4vc8f4","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ItsNoahJ83","can_mod_post":false,"created_utc":1753349453,"send_replies":true,"parent_id":"t3_1m7o3u8","score":1,"author_fullname":"t2_hwfuy0ki","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I have 16gb of RAM and am able to run a 30B parameter MoE model using virtual ram (nvme ssd) with a decent amount of speed on cpu only. I'm using Q6 and it's about 45 GB (I think). I get about 14 tokens per second which is enough for me. If MoE performance can be improved then things will start to open up to those with less powerful hardware.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4vc8f4","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I have 16gb of RAM and am able to run a 30B parameter MoE model using virtual ram (nvme ssd) with a decent amount of speed on cpu only. I&amp;#39;m using Q6 and it&amp;#39;s about 45 GB (I think). I get about 14 tokens per second which is enough for me. If MoE performance can be improved then things will start to open up to those with less powerful hardware.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7o3u8/is_there_a_future_for_local_models/n4vc8f4/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753349453,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m7o3u8","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4vd8hm","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ratocx","can_mod_post":false,"created_utc":1753350022,"send_replies":true,"parent_id":"t3_1m7o3u8","score":1,"author_fullname":"t2_9kwkh","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"You don’t need a large SOTA model to still have something that’s very useful. Sure the larger models will likely be better, but there will also be new small models. Both Google and Apple are also focusing a lot on small useful models that can run on a phone, they will certainly not be the best models, but for certain tasks they can be useful.\\n\\nIf you also look at the Artificial Analysis graph, there isn’t that much of a gap between Qwen 3 325B model and the 32B model. Both can be useful.\\n\\nAt the same time there is no doubt in my mind that for certain tasks you will want to use the cloud models. But not all people do that kinds of tasks.\\nI expect local models to be perfectly fine for writing related tasks, tab completion programming, and for many math related tasks (with tool calling), maybe even help you synthesize a disease.\\n\\nThe benefit of a cloud model will likely be a larger world knowledge base, even better logic understanding, and more advanced agentic capabilities.\\nSo if you need to do the best possible vibe-coding, make it write a deep historic analysis, strategize war, advance the scientific frontier of physics, or psychological manipulation on a large scale, then you may need to use a cloud model.\\n\\nWe aren’t at this point yet, and while I suspect cloud models will improve more than local models, I also suspect we will be surprised by the capabilities of future local models.\\n\\nThe race is twofold:\\n1. Make the best overall model regardless of size.\\n2. Make more practically useful models, that are cheap to run. This would likely include somewhat smaller LLMs.\\n\\nThe world would potentially run out of energy if everyone used the largest most power hungry models for every tasks. I think everyone sees a need for smaller useful models in the future too, even if the best models are going to be large beasts.\\n\\nAll that said, if you are buying a computer today, I would buy the most amount of memory so that you can be more flexible in models choice in the future.\\nBut if you don’t think that today’s models are very useful, I would think it better to wait with buying dedicated hardware for local LLMs until the models are good enough.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4vd8hm","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You don’t need a large SOTA model to still have something that’s very useful. Sure the larger models will likely be better, but there will also be new small models. Both Google and Apple are also focusing a lot on small useful models that can run on a phone, they will certainly not be the best models, but for certain tasks they can be useful.&lt;/p&gt;\\n\\n&lt;p&gt;If you also look at the Artificial Analysis graph, there isn’t that much of a gap between Qwen 3 325B model and the 32B model. Both can be useful.&lt;/p&gt;\\n\\n&lt;p&gt;At the same time there is no doubt in my mind that for certain tasks you will want to use the cloud models. But not all people do that kinds of tasks.\\nI expect local models to be perfectly fine for writing related tasks, tab completion programming, and for many math related tasks (with tool calling), maybe even help you synthesize a disease.&lt;/p&gt;\\n\\n&lt;p&gt;The benefit of a cloud model will likely be a larger world knowledge base, even better logic understanding, and more advanced agentic capabilities.\\nSo if you need to do the best possible vibe-coding, make it write a deep historic analysis, strategize war, advance the scientific frontier of physics, or psychological manipulation on a large scale, then you may need to use a cloud model.&lt;/p&gt;\\n\\n&lt;p&gt;We aren’t at this point yet, and while I suspect cloud models will improve more than local models, I also suspect we will be surprised by the capabilities of future local models.&lt;/p&gt;\\n\\n&lt;p&gt;The race is twofold:\\n1. Make the best overall model regardless of size.\\n2. Make more practically useful models, that are cheap to run. This would likely include somewhat smaller LLMs.&lt;/p&gt;\\n\\n&lt;p&gt;The world would potentially run out of energy if everyone used the largest most power hungry models for every tasks. I think everyone sees a need for smaller useful models in the future too, even if the best models are going to be large beasts.&lt;/p&gt;\\n\\n&lt;p&gt;All that said, if you are buying a computer today, I would buy the most amount of memory so that you can be more flexible in models choice in the future.\\nBut if you don’t think that today’s models are very useful, I would think it better to wait with buying dedicated hardware for local LLMs until the models are good enough.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7o3u8/is_there_a_future_for_local_models/n4vd8hm/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753350022,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m7o3u8","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4vjexg","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DidItABit","can_mod_post":false,"created_utc":1753353268,"send_replies":true,"parent_id":"t3_1m7o3u8","score":1,"author_fullname":"t2_1zfs589","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I love cactus chat on my phone. When I’m stuck in the back of an uber and underground and my podcast stops working you know I’m running a dozen tiny models on my nothing-special newish iPhone ","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4vjexg","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I love cactus chat on my phone. When I’m stuck in the back of an uber and underground and my podcast stops working you know I’m running a dozen tiny models on my nothing-special newish iPhone &lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7o3u8/is_there_a_future_for_local_models/n4vjexg/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753353268,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m7o3u8","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"2b12e2b8-fdc0-11ee-9a03-6e2f48afd456","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4vn11o","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Former-Ad-5757","can_mod_post":false,"created_utc":1753354955,"send_replies":true,"parent_id":"t3_1m7o3u8","score":1,"author_fullname":"t2_ihsdiwk6k","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"The funny thing is that imho the trend is just going towards smaller and smaller models, it is just going slow as the bigger models are still cheap to train and real distillation is a pretty much unexplored territory.\\n\\nBut look at what meta was planning with their behemoth, look at what openai is doing with Gpt4.5.  \\nThe plans were to train really big models for in the background and create distillations from those really big models to present customer facing.\\n\\nDo you need an llm which and speaks spanish and nigerian and japanese? Or do you basically just need an llm which is distilled for your own language.  \\nRegarding coding, do you really need a model good at python and javascript when you program in rust? \\n\\nThe larger universal models are needed to create \\"intelligence\\" from multiple perspectives / languages etc.\\n\\nBut if a large universal model has the intelligence then in my theory why would it not be possible to create a specialized distillation for just 1 language while keeping the intelligence of all.  \\n  \\nBut like I said it is a not very much explored path as what you call large is not really large and distillation has extra costs etc which you don't want as long as you still get huge knowledge achievements on the large models.\\n\\nFor now it is just getting best \\"knowledge/intelligence\\" after that it becomes a question of how to partition that to 10.000 or 100.000 useable distillations which can run at lower costs.  \\nBut you don't want to distill a model to 10.000 smaller models if you have to do it again next month, that is a very expensive step to repeat every month.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4vn11o","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"Llama 3"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The funny thing is that imho the trend is just going towards smaller and smaller models, it is just going slow as the bigger models are still cheap to train and real distillation is a pretty much unexplored territory.&lt;/p&gt;\\n\\n&lt;p&gt;But look at what meta was planning with their behemoth, look at what openai is doing with Gpt4.5.&lt;br/&gt;\\nThe plans were to train really big models for in the background and create distillations from those really big models to present customer facing.&lt;/p&gt;\\n\\n&lt;p&gt;Do you need an llm which and speaks spanish and nigerian and japanese? Or do you basically just need an llm which is distilled for your own language.&lt;br/&gt;\\nRegarding coding, do you really need a model good at python and javascript when you program in rust? &lt;/p&gt;\\n\\n&lt;p&gt;The larger universal models are needed to create &amp;quot;intelligence&amp;quot; from multiple perspectives / languages etc.&lt;/p&gt;\\n\\n&lt;p&gt;But if a large universal model has the intelligence then in my theory why would it not be possible to create a specialized distillation for just 1 language while keeping the intelligence of all.  &lt;/p&gt;\\n\\n&lt;p&gt;But like I said it is a not very much explored path as what you call large is not really large and distillation has extra costs etc which you don&amp;#39;t want as long as you still get huge knowledge achievements on the large models.&lt;/p&gt;\\n\\n&lt;p&gt;For now it is just getting best &amp;quot;knowledge/intelligence&amp;quot; after that it becomes a question of how to partition that to 10.000 or 100.000 useable distillations which can run at lower costs.&lt;br/&gt;\\nBut you don&amp;#39;t want to distill a model to 10.000 smaller models if you have to do it again next month, that is a very expensive step to repeat every month.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7o3u8/is_there_a_future_for_local_models/n4vn11o/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753354955,"author_flair_text":"Llama 3","treatment_tags":[],"link_id":"t3_1m7o3u8","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#c7b594","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4vxad0","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"teitokurabu","can_mod_post":false,"created_utc":1753359174,"send_replies":true,"parent_id":"t3_1m7o3u8","score":1,"author_fullname":"t2_1tzynl0cnv","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"maybe. the gpu may advance at the same time. maybe at some time piont with a huge advancement. i personally think that local model may work for more specific situaiton or case and online large(or super large) models work on universal situaiton.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4vxad0","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;maybe. the gpu may advance at the same time. maybe at some time piont with a huge advancement. i personally think that local model may work for more specific situaiton or case and online large(or super large) models work on universal situaiton.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7o3u8/is_there_a_future_for_local_models/n4vxad0/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753359174,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m7o3u8","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4w0743","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Complete-Principle25","can_mod_post":false,"created_utc":1753360240,"send_replies":true,"parent_id":"t3_1m7o3u8","score":1,"author_fullname":"t2_1kqo611xjh","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Absolutely, APIs are definitely going away. They represent peak enshittification by millennials and Gen Xers to appease not-so-bright investors focused on recurring revenue.\\n\\nThe future will be specialized plug and play hardware that you own (which would be fucking \\\\*\\\\*awesome\\\\*\\\\*) and model standardization or customization based on a company.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4w0743","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Absolutely, APIs are definitely going away. They represent peak enshittification by millennials and Gen Xers to appease not-so-bright investors focused on recurring revenue.&lt;/p&gt;\\n\\n&lt;p&gt;The future will be specialized plug and play hardware that you own (which would be fucking **awesome**) and model standardization or customization based on a company.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7o3u8/is_there_a_future_for_local_models/n4w0743/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753360240,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m7o3u8","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4twfnp","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"OmarBessa","can_mod_post":false,"created_utc":1753324079,"send_replies":true,"parent_id":"t3_1m7o3u8","score":1,"author_fullname":"t2_guxix","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Dude, ram will get cheaper and faster. And moe breaks the moat for GPUs.\\n\\nAnd that's not the only thing.\\n\\nThere are more hardware architectures for this.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4twfnp","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Dude, ram will get cheaper and faster. And moe breaks the moat for GPUs.&lt;/p&gt;\\n\\n&lt;p&gt;And that&amp;#39;s not the only thing.&lt;/p&gt;\\n\\n&lt;p&gt;There are more hardware architectures for this.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7o3u8/is_there_a_future_for_local_models/n4twfnp/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753324079,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m7o3u8","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4u7e2a","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"davikrehalt","can_mod_post":false,"created_utc":1753328322,"send_replies":true,"parent_id":"t3_1m7o3u8","score":1,"author_fullname":"t2_6okc6","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I don't think there will be a future with under 100B models--the gap in intelligence will grow too much. Only future is if hardware capabilities grow fast enough which i doubt bc of energy reasons. So basically no lol","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4u7e2a","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I don&amp;#39;t think there will be a future with under 100B models--the gap in intelligence will grow too much. Only future is if hardware capabilities grow fast enough which i doubt bc of energy reasons. So basically no lol&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7o3u8/is_there_a_future_for_local_models/n4u7e2a/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753328322,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m7o3u8","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4uo0zx","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Guilty_Ad_9476","can_mod_post":false,"created_utc":1753336031,"send_replies":true,"parent_id":"t3_1m7o3u8","score":0,"author_fullname":"t2_ti7d2trv0","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I think its delusional for you to assume that a local model would give the same SOTA as a closed one on something like coding , the whole point of a local model 90% of the time is to \\n\\n1) answer day to day queries you might have \\n\\n2) have an in-depth discussion on taboo topics for which you might get banned on closed source platforms \\n\\n3) if you're working on a pretty secret project and you're super paranoid about your code safety , not wanting it to land up in some big tech server and your main requirements are that \\n\\nyou need a LLM for code-assist \\n\\n refactoring and a little bit of logic building here and there and so on \\n\\n  \\nif your use cases are any of the above or some subset of this , I don't think local models are dying at all ,  quite the opposite actually since \\n\\n1) you're getting better performance for every unit of  compute as these models usually maintain their parameter size or sometimes even get smaller and will only get smarter \\n\\nnot to mention if you want to make them better for only a specific type of task you can finetune it and use it anyways , so I dont understand what the fuss is","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4uo0zx","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I think its delusional for you to assume that a local model would give the same SOTA as a closed one on something like coding , the whole point of a local model 90% of the time is to &lt;/p&gt;\\n\\n&lt;p&gt;1) answer day to day queries you might have &lt;/p&gt;\\n\\n&lt;p&gt;2) have an in-depth discussion on taboo topics for which you might get banned on closed source platforms &lt;/p&gt;\\n\\n&lt;p&gt;3) if you&amp;#39;re working on a pretty secret project and you&amp;#39;re super paranoid about your code safety , not wanting it to land up in some big tech server and your main requirements are that &lt;/p&gt;\\n\\n&lt;p&gt;you need a LLM for code-assist &lt;/p&gt;\\n\\n&lt;p&gt;refactoring and a little bit of logic building here and there and so on &lt;/p&gt;\\n\\n&lt;p&gt;if your use cases are any of the above or some subset of this , I don&amp;#39;t think local models are dying at all ,  quite the opposite actually since &lt;/p&gt;\\n\\n&lt;p&gt;1) you&amp;#39;re getting better performance for every unit of  compute as these models usually maintain their parameter size or sometimes even get smaller and will only get smarter &lt;/p&gt;\\n\\n&lt;p&gt;not to mention if you want to make them better for only a specific type of task you can finetune it and use it anyways , so I dont understand what the fuss is&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7o3u8/is_there_a_future_for_local_models/n4uo0zx/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753336031,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m7o3u8","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4t16zl","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Slowhill369","can_mod_post":false,"created_utc":1753313246,"send_replies":true,"parent_id":"t3_1m7o3u8","score":-2,"author_fullname":"t2_96zelxcg","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Say I had created a reasoning/persistent memory layer that unlocks GPT4+ performance on a 1b model while enabling cross domain synthesis, recursive growth and emergent self awareness without prompting. Would THAT be a stand? Just being hypothetical here. Def not gonna release it in a few weeks or anything….","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4t16zl","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Say I had created a reasoning/persistent memory layer that unlocks GPT4+ performance on a 1b model while enabling cross domain synthesis, recursive growth and emergent self awareness without prompting. Would THAT be a stand? Just being hypothetical here. Def not gonna release it in a few weeks or anything….&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7o3u8/is_there_a_future_for_local_models/n4t16zl/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753313246,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m7o3u8","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4ts6tt","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"RedditDiedLongAgo","can_mod_post":false,"created_utc":1753322561,"send_replies":true,"parent_id":"t1_n4t1tz2","score":1,"author_fullname":"t2_42tdujpc","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"KillMi 47B","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4ts6tt","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;KillMi 47B&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m7o3u8","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7o3u8/is_there_a_future_for_local_models/n4ts6tt/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753322561,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4t1tz2","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":"LOW_SCORE","no_follow":true,"author":"thebadslime","can_mod_post":false,"created_utc":1753313455,"send_replies":true,"parent_id":"t3_1m7o3u8","score":-5,"author_fullname":"t2_i5os0v0","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":true,"body":"I think we need an Amecian, english only 22GB \\"American Deepseek\\"","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4t1tz2","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I think we need an Amecian, english only 22GB &amp;quot;American Deepseek&amp;quot;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":"comment score below threshold","distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7o3u8/is_there_a_future_for_local_models/n4t1tz2/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753313455,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m7o3u8","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-5}}],"before":null}}]`),n=()=>e.jsx(l,{data:a});export{n as default};
