import{j as t}from"./index-xfnGEtuL.js";import{R as e}from"./RedditPostRenderer-DAZRIZZK.js";import"./index-BaNn5-RR.js";const n=JSON.parse('[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"So after doing some further research on the cost of self-hosting larger models I have come to this conclusion - and I am looking for feedback here.\\n\\nMy specific use case is an AI-assisted IDE I am building myself, and I am looking to dabble in self-hosting a capable model for inference for its users. I currently do **not** have a budget to do extensive testing and benchmarking but I have read up plenty on this (and argued quite a lot with ChatGPT and Gemini lol) for some days now.\\n\\nHere is what I\'ve got so far:\\n\\n* tokens per second is not a reliable metric as it actually *averages out* two very different speeds (input vs output):\\n\\n&gt;One additional note: I recently set up an inference setup for **llama-3-70b** on **8xH100**. I can get about **100,000 tok/s** on inputs which is pretty close to full utilization (1e15 flop/s \\\\* 8 gpus / 7e10 flop per forward pass). However, I get dramatically worse performance on generation, perhaps **3,200 tok/s**. I\'m doing generation with long prompts and llama-3-70b has no sparse attention or other feature for reducing KV cache (beyond multi-query attention which is standard these days), so KV cache bits pretty hard. - [link here](https://www.lesswrong.com/posts/g7H2sSGHAeYxCHzrz/how-much-ai-inference-can-we-do?commentId=RXnfe2ojyqmhLTXJm).\\n\\n* In IDE use we could expect our requests to **average out** 20k input tokens and 300 output per request. (This is my own estimate based on my own usage via OpernRouter).\\n\\n**Now for some math:**\\n\\nSingle H100 (Runpod): $ 2.59/hr\\n\\nMinimum of 8x H100 (required): $ 20.72/hr\\n\\nThis setup ***per second:*** 20.72 / 3600 = 0.0057 $/second\\n\\nQwen3-Coder-480B-A35B-Instruct: (half of llama-3-70B token/s?) **200k tokens/s input** \\\\+ **6400 tokens/s output**\\n\\n**Phase 1: Prompt Processing Time** (20,000 input tokens)\\n\\n* **Calculation:** `20,000 tokens / 200,000 tokens/sec`\\n* **Result:** **0.10 seconds**\\n\\n**Phase 2: Token Generation Time (300 output tokens)**\\n\\n* **Calculation:** `300 tokens / 6,400 tokens/sec`\\n* **Result:** **\\\\~0.047 seconds**\\n\\n**Total Time &amp; Cost per Request**\\n\\n* **Total Time:** `0.10s + 0.047s = **0.147 seconds**`\\n* **Total Cost:** `0.147 seconds * $0.0057/sec =` `~$0.0008`\\n\\n\\n\\nI mean... is this right? I think this is wrong but it is as far as I could get without actually going and renting these GPUs and testing it for myself. It just seems **so much cheaper** than what I end up paying via API in OpenRouter.","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Throughput: Input vs Output. Looking for help...","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1m7brg9","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":1,"author_flair_background_color":null,"subreddit_type":"public","ups":3,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_19mrnrt357","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":3,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":1753283519,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1753283238,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;So after doing some further research on the cost of self-hosting larger models I have come to this conclusion - and I am looking for feedback here.&lt;/p&gt;\\n\\n&lt;p&gt;My specific use case is an AI-assisted IDE I am building myself, and I am looking to dabble in self-hosting a capable model for inference for its users. I currently do &lt;strong&gt;not&lt;/strong&gt; have a budget to do extensive testing and benchmarking but I have read up plenty on this (and argued quite a lot with ChatGPT and Gemini lol) for some days now.&lt;/p&gt;\\n\\n&lt;p&gt;Here is what I&amp;#39;ve got so far:&lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;tokens per second is not a reliable metric as it actually &lt;em&gt;averages out&lt;/em&gt; two very different speeds (input vs output):&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;One additional note: I recently set up an inference setup for &lt;strong&gt;llama-3-70b&lt;/strong&gt; on &lt;strong&gt;8xH100&lt;/strong&gt;. I can get about &lt;strong&gt;100,000 tok/s&lt;/strong&gt; on inputs which is pretty close to full utilization (1e15 flop/s * 8 gpus / 7e10 flop per forward pass). However, I get dramatically worse performance on generation, perhaps &lt;strong&gt;3,200 tok/s&lt;/strong&gt;. I&amp;#39;m doing generation with long prompts and llama-3-70b has no sparse attention or other feature for reducing KV cache (beyond multi-query attention which is standard these days), so KV cache bits pretty hard. - &lt;a href=\\"https://www.lesswrong.com/posts/g7H2sSGHAeYxCHzrz/how-much-ai-inference-can-we-do?commentId=RXnfe2ojyqmhLTXJm\\"&gt;link here&lt;/a&gt;.&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;In IDE use we could expect our requests to &lt;strong&gt;average out&lt;/strong&gt; 20k input tokens and 300 output per request. (This is my own estimate based on my own usage via OpernRouter).&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;p&gt;&lt;strong&gt;Now for some math:&lt;/strong&gt;&lt;/p&gt;\\n\\n&lt;p&gt;Single H100 (Runpod): $ 2.59/hr&lt;/p&gt;\\n\\n&lt;p&gt;Minimum of 8x H100 (required): $ 20.72/hr&lt;/p&gt;\\n\\n&lt;p&gt;This setup &lt;strong&gt;&lt;em&gt;per second:&lt;/em&gt;&lt;/strong&gt; 20.72 / 3600 = 0.0057 $/second&lt;/p&gt;\\n\\n&lt;p&gt;Qwen3-Coder-480B-A35B-Instruct: (half of llama-3-70B token/s?) &lt;strong&gt;200k tokens/s input&lt;/strong&gt; + &lt;strong&gt;6400 tokens/s output&lt;/strong&gt;&lt;/p&gt;\\n\\n&lt;p&gt;&lt;strong&gt;Phase 1: Prompt Processing Time&lt;/strong&gt; (20,000 input tokens)&lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;&lt;strong&gt;Calculation:&lt;/strong&gt; &lt;code&gt;20,000 tokens / 200,000 tokens/sec&lt;/code&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;strong&gt;Result:&lt;/strong&gt; &lt;strong&gt;0.10 seconds&lt;/strong&gt;&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;p&gt;&lt;strong&gt;Phase 2: Token Generation Time (300 output tokens)&lt;/strong&gt;&lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;&lt;strong&gt;Calculation:&lt;/strong&gt; &lt;code&gt;300 tokens / 6,400 tokens/sec&lt;/code&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;strong&gt;Result:&lt;/strong&gt; &lt;strong&gt;~0.047 seconds&lt;/strong&gt;&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;p&gt;&lt;strong&gt;Total Time &amp;amp; Cost per Request&lt;/strong&gt;&lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;&lt;strong&gt;Total Time:&lt;/strong&gt; &lt;code&gt;0.10s + 0.047s = **0.147 seconds**&lt;/code&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;strong&gt;Total Cost:&lt;/strong&gt; &lt;code&gt;0.147 seconds * $0.0057/sec =&lt;/code&gt; &lt;code&gt;~$0.0008&lt;/code&gt;&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;p&gt;I mean... is this right? I think this is wrong but it is as far as I could get without actually going and renting these GPUs and testing it for myself. It just seems &lt;strong&gt;so much cheaper&lt;/strong&gt; than what I end up paying via API in OpenRouter.&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1m7brg9","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"Budget_Map_3333","discussion_type":null,"num_comments":1,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1m7brg9/throughput_input_vs_output_looking_for_help/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1m7brg9/throughput_input_vs_output_looking_for_help/","subreddit_subscribers":503757,"created_utc":1753283238,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[],"before":null}}]'),a=()=>t.jsx(e,{data:n});export{a as default};
