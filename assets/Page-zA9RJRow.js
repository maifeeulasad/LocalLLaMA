import{j as e}from"./index-BOnf-UhU.js";import{R as t}from"./RedditPostRenderer-Ce39qICS.js";import"./index-CDase6VD.js";const l=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Been using mostly Claude Code, works great. Yet feels like Im starting to hit the limits of what it can do. Im wondering what others are using for coding? Last time I checked Gemini 2.5 Pro and o3 and o4, they did not felt on par with Claude, maybe things changed recently?","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"What is the top model for coding?","link_flair_richtext":[{"e":"text","t":"Discussion"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1m5x04m","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.32,"author_flair_background_color":null,"subreddit_type":"public","ups":0,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_m8971","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Discussion","can_mod_post":false,"score":0,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1753136675,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Been using mostly Claude Code, works great. Yet feels like Im starting to hit the limits of what it can do. Im wondering what others are using for coding? Last time I checked Gemini 2.5 Pro and o3 and o4, they did not felt on par with Claude, maybe things changed recently?&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":true,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"5f921ea4-c7bc-11ed-9c23-3a00622979b4","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#646d73","id":"1m5x04m","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"estebansaa","discussion_type":null,"num_comments":21,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1m5x04m/what_is_the_top_model_for_coding/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1m5x04m/what_is_the_top_model_for_coding/","subreddit_subscribers":502981,"created_utc":1753136675,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4fs487","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"No_Efficiency_1144","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4fqr28","score":2,"author_fullname":"t2_1nkj9l14b0","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"It’s difficult because strong local coding model doesn’t currently exist yet.\\n\\n\\nKimi K2 comes closest in terms of actual syntax abilities but being a non-reasoning model puts a cap on the complexity it can handle.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n4fs487","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It’s difficult because strong local coding model doesn’t currently exist yet.&lt;/p&gt;\\n\\n&lt;p&gt;Kimi K2 comes closest in terms of actual syntax abilities but being a non-reasoning model puts a cap on the complexity it can handle.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m5x04m","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5x04m/what_is_the_top_model_for_coding/n4fs487/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753142991,"author_flair_text":null,"treatment_tags":[],"created_utc":1753142991,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n4fqr28","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"quuuub","can_mod_post":false,"created_utc":1753142514,"send_replies":true,"parent_id":"t1_n4ffy36","score":3,"author_fullname":"t2_cal7sdfa","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"i agree, this is off topic","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4fqr28","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;i agree, this is off topic&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m5x04m","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5x04m/what_is_the_top_model_for_coding/n4fqr28/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753142514,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4g083f","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"estebansaa","can_mod_post":false,"created_utc":1753145807,"send_replies":true,"parent_id":"t1_n4ffy36","score":3,"author_fullname":"t2_m8971","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"really wish there was something I could run locally that does better than CLaude.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4g083f","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;really wish there was something I could run locally that does better than CLaude.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m5x04m","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5x04m/what_is_the_top_model_for_coding/n4g083f/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753145807,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4hbm4e","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Voxandr","can_mod_post":false,"created_utc":1753165434,"send_replies":true,"parent_id":"t1_n4ffy36","score":1,"author_fullname":"t2_86dk0gye","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Had you tried qwen3 2.5 coder and qwen3 32B . Thru work fine for all my needs","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4hbm4e","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Had you tried qwen3 2.5 coder and qwen3 32B . Thru work fine for all my needs&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m5x04m","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5x04m/what_is_the_top_model_for_coding/n4hbm4e/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753165434,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4ffy36","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Voxandr","can_mod_post":false,"created_utc":1753138946,"send_replies":true,"parent_id":"t3_1m5x04m","score":18,"author_fullname":"t2_86dk0gye","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Can you keep things to locallms? Looks like new flood of vibe coders don't even know what local LLMs means","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4ffy36","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Can you keep things to locallms? Looks like new flood of vibe coders don&amp;#39;t even know what local LLMs means&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5x04m/what_is_the_top_model_for_coding/n4ffy36/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753138946,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m5x04m","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":18}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4ftnys","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Environmental-Metal9","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4fr1f0","score":2,"author_fullname":"t2_6x9o42az","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"You know, for certain tasks yes, hands down not even a comparison. Opus4 was swiftly able to debug and improve a custom ONNX pipeline I was working on in two prompts, which no other model even got close to, not even sonnet3.7. But the reality is that I still reach for it quite often for pretty mundane tasks like “ok, here are the models, these are the service objects, write me the fastapi endpoint for foo” because that’s pretty rote and id rather use a cheaper model. But here DeepSeek (and I’m hoping to test with kimi k2 and qwen max too) does just as good of a job.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4ftnys","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You know, for certain tasks yes, hands down not even a comparison. Opus4 was swiftly able to debug and improve a custom ONNX pipeline I was working on in two prompts, which no other model even got close to, not even sonnet3.7. But the reality is that I still reach for it quite often for pretty mundane tasks like “ok, here are the models, these are the service objects, write me the fastapi endpoint for foo” because that’s pretty rote and id rather use a cheaper model. But here DeepSeek (and I’m hoping to test with kimi k2 and qwen max too) does just as good of a job.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m5x04m","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5x04m/what_is_the_top_model_for_coding/n4ftnys/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753143530,"author_flair_text":null,"treatment_tags":[],"created_utc":1753143530,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n4fr1f0","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"No_Efficiency_1144","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4fbqz8","score":1,"author_fullname":"t2_1nkj9l14b0","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Is it a big step up from Claude 3.7 Sonnet?","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n4fr1f0","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Is it a big step up from Claude 3.7 Sonnet?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m5x04m","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5x04m/what_is_the_top_model_for_coding/n4fr1f0/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753142616,"author_flair_text":null,"treatment_tags":[],"created_utc":1753142616,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4fbqz8","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Environmental-Metal9","can_mod_post":false,"created_utc":1753137580,"send_replies":true,"parent_id":"t1_n4farm6","score":1,"author_fullname":"t2_6x9o42az","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Opus is the first model in a couple of years that feels like a real leap in ability to understand code, not just spit out code. But it is so damn expensive that I can’t help but feel like people like you and I aren’t the target consumers of it, but rather people hoping to augment their dev teams while shrinking their workforce. With Opus I mostly shift into product manager with a high degree of technical skills for 80% of the time, and senior dev the rest of the time where the app needs deep business logic. But the cost of using it ends up being a sizeable fraction of a software dev salary. I would say it is definitely about as good as a competent junior that you can delegate tasks to and just come back and check in from time to time. Sonnet 4 in comparison feels like the well intentioned little brother who definitely picked up a few tricks here and there but is largely cosplaying as the bigger brother","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4fbqz8","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Opus is the first model in a couple of years that feels like a real leap in ability to understand code, not just spit out code. But it is so damn expensive that I can’t help but feel like people like you and I aren’t the target consumers of it, but rather people hoping to augment their dev teams while shrinking their workforce. With Opus I mostly shift into product manager with a high degree of technical skills for 80% of the time, and senior dev the rest of the time where the app needs deep business logic. But the cost of using it ends up being a sizeable fraction of a software dev salary. I would say it is definitely about as good as a competent junior that you can delegate tasks to and just come back and check in from time to time. Sonnet 4 in comparison feels like the well intentioned little brother who definitely picked up a few tricks here and there but is largely cosplaying as the bigger brother&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m5x04m","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5x04m/what_is_the_top_model_for_coding/n4fbqz8/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753137580,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4farm6","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"ForsookComparison","can_mod_post":false,"created_utc":1753137256,"send_replies":true,"parent_id":"t3_1m5x04m","score":6,"author_fullname":"t2_on5es7pe3","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Claude 4.0 Sonnet is the best at implementing what you know you want to implement.\\n\\nDeepseek-R1-0528 beats Sonnet in problem solving and debugging, but isn't quite as strong a coder. When Sonnet fails to fix something and I can't guide it to exactly where the fault in logic exists, Deepseek-r1-0528 tends to be my savior.\\n\\nDeepseek-V3-0324 is the best open-weight straight-shot model. It is an order of magnitude cheaper than Sonnet and Opus and generally gets the job done.\\n\\nQwen3-235-a22b (the \\"old\\" one as of a few hours ago) is the best for quick edits where you know what you want changed. Llama4-Maverick isn't terrible for this, but I've phased it out since.\\n\\nOpus is ridiculously good but I can't afford to use it long enough to tell you more than that.\\n\\no3 pro is probably best but my wallet cannot survive the cost of Opus the *REASONING* tokens of o3.","edited":1753137845,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4farm6","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Claude 4.0 Sonnet is the best at implementing what you know you want to implement.&lt;/p&gt;\\n\\n&lt;p&gt;Deepseek-R1-0528 beats Sonnet in problem solving and debugging, but isn&amp;#39;t quite as strong a coder. When Sonnet fails to fix something and I can&amp;#39;t guide it to exactly where the fault in logic exists, Deepseek-r1-0528 tends to be my savior.&lt;/p&gt;\\n\\n&lt;p&gt;Deepseek-V3-0324 is the best open-weight straight-shot model. It is an order of magnitude cheaper than Sonnet and Opus and generally gets the job done.&lt;/p&gt;\\n\\n&lt;p&gt;Qwen3-235-a22b (the &amp;quot;old&amp;quot; one as of a few hours ago) is the best for quick edits where you know what you want changed. Llama4-Maverick isn&amp;#39;t terrible for this, but I&amp;#39;ve phased it out since.&lt;/p&gt;\\n\\n&lt;p&gt;Opus is ridiculously good but I can&amp;#39;t afford to use it long enough to tell you more than that.&lt;/p&gt;\\n\\n&lt;p&gt;o3 pro is probably best but my wallet cannot survive the cost of Opus the &lt;em&gt;REASONING&lt;/em&gt; tokens of o3.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5x04m/what_is_the_top_model_for_coding/n4farm6/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753137256,"author_flair_text":"llama.cpp","treatment_tags":[],"link_id":"t3_1m5x04m","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4lf1xo","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"No_Efficiency_1144","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4ksc8y","score":1,"author_fullname":"t2_1nkj9l14b0","approved_by":null,"mod_note":null,"all_awardings":[],"body":"2.5 Pro is much better yes the reasoning/thinking boost is big","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n4lf1xo","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;2.5 Pro is much better yes the reasoning/thinking boost is big&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1m5x04m","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5x04m/what_is_the_top_model_for_coding/n4lf1xo/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753217359,"author_flair_text":null,"treatment_tags":[],"created_utc":1753217359,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4ksc8y","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Creepy-Potential3408","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4kk5lq","score":0,"author_fullname":"t2_1tz5xea0uy","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Sorry my bad, I had a typo on 2 numbers that I edited from 1.5 to 2.5. I've looked in AI Studio and there is indeed Gemini 2.5 Pro which is better than 1.5 anyway right?","edited":false,"author_flair_css_class":null,"name":"t1_n4ksc8y","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Sorry my bad, I had a typo on 2 numbers that I edited from 1.5 to 2.5. I&amp;#39;ve looked in AI Studio and there is indeed Gemini 2.5 Pro which is better than 1.5 anyway right?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1m5x04m","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5x04m/what_is_the_top_model_for_coding/n4ksc8y/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753210964,"author_flair_text":null,"collapsed":false,"created_utc":1753210964,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}},"user_reports":[],"saved":false,"id":"n4kk5lq","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"No_Efficiency_1144","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4jgr9c","score":2,"author_fullname":"t2_1nkj9l14b0","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Multiple hallucinations in here for example Gemini 1.5 is not in AI Studio\\n\\n\\nYou can’t just ask LLMs questions like this and get a reliable answer","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4kk5lq","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Multiple hallucinations in here for example Gemini 1.5 is not in AI Studio&lt;/p&gt;\\n\\n&lt;p&gt;You can’t just ask LLMs questions like this and get a reliable answer&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m5x04m","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5x04m/what_is_the_top_model_for_coding/n4kk5lq/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753208658,"author_flair_text":null,"treatment_tags":[],"created_utc":1753208658,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n4jgr9c","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Creepy-Potential3408","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4ivadw","score":0,"author_fullname":"t2_1tz5xea0uy","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"You're right to notice that a lot of the initial buzz and common usage for Gemini models has been around the 1 million token context window. However, Gemini 2.5 Pro actually offers a 2 million token context window for developers, which was made generally available around June 2025.\\n\\nHere's the breakdown:\\n\\n* **Gemini 1.5 Pro** does indeed have a 2 million token context window. This was made generally available for developers via the Gemini API and Google AI Studio around June 2025. So, if you're using the latest versions of 2.5 Pro through the API or AI Studio, you should have access to it.\\n* **Deep Research mode** is a feature within Gemini Advanced (which uses Gemini 2.5 Pro, among other models). While Deep Research itself is designed to handle very long and complex information, the context window for *users* of Gemini Advanced (the consumer-facing product) for general chat is often cited as 1 million tokens. However, the underlying models are capable of more, and Deep Research *leverages* that longer context for its specific tasks of analyzing vast amounts of information.\\n\\nThe distinction is important:\\n\\n* **For developers** using the API or AI Studio, the 2 million token context for Gemini 1.5 Pro is available.\\n* **For consumer users** of Gemini Advanced, the \\"Deep Research\\" feature is built on top of the powerful capabilities of models like Gemini 2.5 Pro, and it's designed to *utilize* large contexts to provide detailed reports, even if the primary chat interface of Gemini Advanced itself has a stated 1 million token limit for direct user interaction.\\n\\nSo, when you see references to \\"Deep Research,\\" it's about the *capability* and how the model is being *applied* to handle large amounts of data, which benefits from the underlying 2 million token context window of Gemini 1.5 Pro (and now 2.5 Pro).\\n\\nHere's the relevant link, which confirms the 2 million token context for Gemini 1.5 Pro for developers:\\n\\n* **Google AI for Developers documentation on Long Context:**https://ai.google.dev/gemini-api/docs/long-context\\n\\nAnd to clarify the \\"Deep Research\\" aspect, which is a feature built upon these models:\\n\\n* **Gemini Apps' release updates &amp; improvements (Google Gemini Updates):** This page mentions Deep Research and its capabilities, which are powered by the advanced models like 2.5 Pro. It also mentions that Gemini AI Ultra subscribers get \\"the highest access to our best Gemini models, including 2.5 Pro, and powerful features like Deep Research... and a 1M token context window.\\" This further illustrates the difference between the *model's* maximum capability and the specific *user-facing product's* stated limit for general chat, while still acknowledging that Deep Research uses the underlying long-context power.\\n   * [https://gemini.google.com/updates](https://gemini.google.com/updates)\\n\\nHope that clears things up! It's a rapidly evolving space, so staying on top of the latest updates can be a challenge.","edited":1753210862,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n4jgr9c","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You&amp;#39;re right to notice that a lot of the initial buzz and common usage for Gemini models has been around the 1 million token context window. However, Gemini 2.5 Pro actually offers a 2 million token context window for developers, which was made generally available around June 2025.&lt;/p&gt;\\n\\n&lt;p&gt;Here&amp;#39;s the breakdown:&lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;&lt;strong&gt;Gemini 1.5 Pro&lt;/strong&gt; does indeed have a 2 million token context window. This was made generally available for developers via the Gemini API and Google AI Studio around June 2025. So, if you&amp;#39;re using the latest versions of 2.5 Pro through the API or AI Studio, you should have access to it.&lt;/li&gt;\\n&lt;li&gt;&lt;strong&gt;Deep Research mode&lt;/strong&gt; is a feature within Gemini Advanced (which uses Gemini 2.5 Pro, among other models). While Deep Research itself is designed to handle very long and complex information, the context window for &lt;em&gt;users&lt;/em&gt; of Gemini Advanced (the consumer-facing product) for general chat is often cited as 1 million tokens. However, the underlying models are capable of more, and Deep Research &lt;em&gt;leverages&lt;/em&gt; that longer context for its specific tasks of analyzing vast amounts of information.&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;p&gt;The distinction is important:&lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;&lt;strong&gt;For developers&lt;/strong&gt; using the API or AI Studio, the 2 million token context for Gemini 1.5 Pro is available.&lt;/li&gt;\\n&lt;li&gt;&lt;strong&gt;For consumer users&lt;/strong&gt; of Gemini Advanced, the &amp;quot;Deep Research&amp;quot; feature is built on top of the powerful capabilities of models like Gemini 2.5 Pro, and it&amp;#39;s designed to &lt;em&gt;utilize&lt;/em&gt; large contexts to provide detailed reports, even if the primary chat interface of Gemini Advanced itself has a stated 1 million token limit for direct user interaction.&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;p&gt;So, when you see references to &amp;quot;Deep Research,&amp;quot; it&amp;#39;s about the &lt;em&gt;capability&lt;/em&gt; and how the model is being &lt;em&gt;applied&lt;/em&gt; to handle large amounts of data, which benefits from the underlying 2 million token context window of Gemini 1.5 Pro (and now 2.5 Pro).&lt;/p&gt;\\n\\n&lt;p&gt;Here&amp;#39;s the relevant link, which confirms the 2 million token context for Gemini 1.5 Pro for developers:&lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;&lt;strong&gt;Google AI for Developers documentation on Long Context:&lt;/strong&gt;&lt;a href=\\"https://ai.google.dev/gemini-api/docs/long-context\\"&gt;https://ai.google.dev/gemini-api/docs/long-context&lt;/a&gt;&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;p&gt;And to clarify the &amp;quot;Deep Research&amp;quot; aspect, which is a feature built upon these models:&lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;&lt;strong&gt;Gemini Apps&amp;#39; release updates &amp;amp; improvements (Google Gemini Updates):&lt;/strong&gt; This page mentions Deep Research and its capabilities, which are powered by the advanced models like 2.5 Pro. It also mentions that Gemini AI Ultra subscribers get &amp;quot;the highest access to our best Gemini models, including 2.5 Pro, and powerful features like Deep Research... and a 1M token context window.&amp;quot; This further illustrates the difference between the &lt;em&gt;model&amp;#39;s&lt;/em&gt; maximum capability and the specific &lt;em&gt;user-facing product&amp;#39;s&lt;/em&gt; stated limit for general chat, while still acknowledging that Deep Research uses the underlying long-context power.\\n\\n&lt;ul&gt;\\n&lt;li&gt;&lt;a href=\\"https://gemini.google.com/updates\\"&gt;https://gemini.google.com/updates&lt;/a&gt;&lt;/li&gt;\\n&lt;/ul&gt;&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;p&gt;Hope that clears things up! It&amp;#39;s a rapidly evolving space, so staying on top of the latest updates can be a challenge.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m5x04m","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5x04m/what_is_the_top_model_for_coding/n4jgr9c/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753197837,"author_flair_text":null,"treatment_tags":[],"created_utc":1753197837,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}},"user_reports":[],"saved":false,"id":"n4ivadw","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"GroverOP","can_mod_post":false,"created_utc":1753191655,"send_replies":true,"parent_id":"t1_n4ffeu8","score":1,"author_fullname":"t2_14dogh","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"i can't find any information on the 2m token context. I keep seeing 1m everywhere (and in aistudio). Can you point me to a source?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4ivadw","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;i can&amp;#39;t find any information on the 2m token context. I keep seeing 1m everywhere (and in aistudio). Can you point me to a source?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m5x04m","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5x04m/what_is_the_top_model_for_coding/n4ivadw/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753191655,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4ffeu8","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Creepy-Potential3408","can_mod_post":false,"created_utc":1753138770,"send_replies":true,"parent_id":"t3_1m5x04m","score":3,"author_fullname":"t2_1tz5xea0uy","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Gemini 2.5 Pro now has a giant 2M token context, great code quality, and fewer “hallucinations,” while GPT-4o is close to Claude with 1M tokens and strong integration. Both now rival or surpass Claude in many tasks. definitely worth revisiting. I use each AI for it's strengths per the task.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4ffeu8","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Gemini 2.5 Pro now has a giant 2M token context, great code quality, and fewer “hallucinations,” while GPT-4o is close to Claude with 1M tokens and strong integration. Both now rival or surpass Claude in many tasks. definitely worth revisiting. I use each AI for it&amp;#39;s strengths per the task.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5x04m/what_is_the_top_model_for_coding/n4ffeu8/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753138770,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m5x04m","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4fsf8b","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"No_Efficiency_1144","can_mod_post":false,"created_utc":1753143098,"send_replies":true,"parent_id":"t1_n4f9wa4","score":2,"author_fullname":"t2_1nkj9l14b0","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"2.5 Flash handles agentic tasks well so even though it makes mistakes in its code if it is being walked through an agentic loop step by step it can fix them well.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4fsf8b","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;2.5 Flash handles agentic tasks well so even though it makes mistakes in its code if it is being walked through an agentic loop step by step it can fix them well.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m5x04m","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5x04m/what_is_the_top_model_for_coding/n4fsf8b/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753143098,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n4f9wa4","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"jkh911208","can_mod_post":false,"created_utc":1753136972,"send_replies":true,"parent_id":"t3_1m5x04m","score":2,"author_fullname":"t2_eq49q5k","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Claude is top performer right now, 2.5 pro and 2.5 flash works great.\\n\\nsince I have to pay for Claude, I use Gemini CLI exclusively it use 2.5pro and move on to 2.5 flash after certain quota\\n\\nI am absolutely happy with its performance.\\n\\nI mainly write Python, other programming languages might experience something different","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4f9wa4","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Claude is top performer right now, 2.5 pro and 2.5 flash works great.&lt;/p&gt;\\n\\n&lt;p&gt;since I have to pay for Claude, I use Gemini CLI exclusively it use 2.5pro and move on to 2.5 flash after certain quota&lt;/p&gt;\\n\\n&lt;p&gt;I am absolutely happy with its performance.&lt;/p&gt;\\n\\n&lt;p&gt;I mainly write Python, other programming languages might experience something different&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5x04m/what_is_the_top_model_for_coding/n4f9wa4/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753136972,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m5x04m","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4h4yle","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Clear-Ad-9312","can_mod_post":false,"created_utc":1753161966,"send_replies":true,"parent_id":"t3_1m5x04m","score":1,"author_fullname":"t2_13gn4f8kdq","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I am personally waiting for Qwen 3 coder. also we just got something called kimi k2.\\n\\nthe big diff between non-local and local models is the amount of work you would have to put in to setup your local environment to match the non-local performance. such as having fine tuning or just simply better RAG/other variations of pulling relevant context. making sure you can keep context low but high quality is extremely hard to do but its what really makes the non-local models perform just a tad better. I guess an ever evolving system prompt also adds a slight edge. Keep in mind, most are usually working with smaller models because of VRAM limitations. So trying to get high performance while keeping resource use low is a hard enough task.\\n\\nYou just need to constantly keep putting in the work to build it up. at the same time AI/LLM stuff is constantly evolving and you should chill with wanting the best of the best and enjoy the ride.","edited":1753162640,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4h4yle","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I am personally waiting for Qwen 3 coder. also we just got something called kimi k2.&lt;/p&gt;\\n\\n&lt;p&gt;the big diff between non-local and local models is the amount of work you would have to put in to setup your local environment to match the non-local performance. such as having fine tuning or just simply better RAG/other variations of pulling relevant context. making sure you can keep context low but high quality is extremely hard to do but its what really makes the non-local models perform just a tad better. I guess an ever evolving system prompt also adds a slight edge. Keep in mind, most are usually working with smaller models because of VRAM limitations. So trying to get high performance while keeping resource use low is a hard enough task.&lt;/p&gt;\\n\\n&lt;p&gt;You just need to constantly keep putting in the work to build it up. at the same time AI/LLM stuff is constantly evolving and you should chill with wanting the best of the best and enjoy the ride.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5x04m/what_is_the_top_model_for_coding/n4h4yle/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753161966,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m5x04m","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4g8i0z","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"complead","can_mod_post":false,"created_utc":1753148703,"send_replies":true,"parent_id":"t3_1m5x04m","score":0,"author_fullname":"t2_uzn88fhh","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"For coding tasks, exploring some emerging models may be worthwhile. Models like Mojo Coder and Heron Code are gaining traction for handling nuanced syntactic tasks, especially in languages like Python and JavaScript. Both offer flexible integrations and are designed to complement models like Claude and Opus by focusing on code execution improvements. Reviewing [this article](https://example.com) for a comparison might offer fresh insights.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4g8i0z","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;For coding tasks, exploring some emerging models may be worthwhile. Models like Mojo Coder and Heron Code are gaining traction for handling nuanced syntactic tasks, especially in languages like Python and JavaScript. Both offer flexible integrations and are designed to complement models like Claude and Opus by focusing on code execution improvements. Reviewing &lt;a href=\\"https://example.com\\"&gt;this article&lt;/a&gt; for a comparison might offer fresh insights.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5x04m/what_is_the_top_model_for_coding/n4g8i0z/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753148703,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m5x04m","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}}]`),r=()=>e.jsx(t,{data:l});export{r as default};
