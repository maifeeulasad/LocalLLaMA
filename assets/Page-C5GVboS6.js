import{j as e}from"./index-CNyNkRpk.js";import{R as l}from"./RedditPostRenderer-Dza0u9i2.js";import"./index-BUchu_-K.js";const t=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"So I use a 3090 on my main pc for image gen and various other things. Fine and dandy. Would be faster with a 4090 or 5090 (one day I'll upgrade) but it works fine.\\n\\nI also run Ollama on my homelab, which doesn't have a dedicated GPU but instead using a 13700k and 32gb of ram (will soon be 64gb).\\n\\nIt runs things like Qwen3 30b MoA pretty fast (fast enough anyway, though turning on thinking can add a bunch of pre-gen time so I usually don't bother). Gemma3-4b also works, though so far I think the Qwen3 MoA is outperforming it. (I know there's a new Gemma release as of yesterday that might be better still but I haven't tested it yet). I can run other models that are under aboutt 5gb in size at a decent speed (I aim for at least 12 to 15 tokens/s), most of the time once you get that small the quality becomes... problematic.\\n\\nI had been planning on throwing in a small GPU one day, when I find the time, but while thinking about it today I realised - All GPUs that aren't power hungry monsters, are limited to 8gb of vram for the most part. So while I'll have more 'processing power' which would speed up using small models (ones under 8gb) I'd still be left with the issue of those models not being that good. And bigger models end up spilling into ram, which would result in (I assume?) much slower speeds the same as I was getting on the CPU anyway.\\n\\nAm I missing something? (probably yes).\\n\\nIt seems that a GPU is only a significant benefit if you use models that fit inside the vram, and so it's only worth it if you have like.... 16gb+ of vram? maybe 12gb? I dunno.\\n\\nHence the question!\\n\\nEdit: I know (or at least think/believe) its the bandwidth/speed of the ram that effects the toks/s results, and not just the capacity, but I also know that the capacity is important in its own right. The vram will always be faster, but if its only faster on lower-quality (smaller) models and isn't noticeably faster on models that don't fit into vram then that's an issue. I guess?","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"(noob question) - At what point does a GPU with low vram outperform a CPU with lots of ram?","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lm32zh","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.5,"author_flair_background_color":null,"subreddit_type":"public","ups":0,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_vri94l9b","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":0,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1751053259,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;So I use a 3090 on my main pc for image gen and various other things. Fine and dandy. Would be faster with a 4090 or 5090 (one day I&amp;#39;ll upgrade) but it works fine.&lt;/p&gt;\\n\\n&lt;p&gt;I also run Ollama on my homelab, which doesn&amp;#39;t have a dedicated GPU but instead using a 13700k and 32gb of ram (will soon be 64gb).&lt;/p&gt;\\n\\n&lt;p&gt;It runs things like Qwen3 30b MoA pretty fast (fast enough anyway, though turning on thinking can add a bunch of pre-gen time so I usually don&amp;#39;t bother). Gemma3-4b also works, though so far I think the Qwen3 MoA is outperforming it. (I know there&amp;#39;s a new Gemma release as of yesterday that might be better still but I haven&amp;#39;t tested it yet). I can run other models that are under aboutt 5gb in size at a decent speed (I aim for at least 12 to 15 tokens/s), most of the time once you get that small the quality becomes... problematic.&lt;/p&gt;\\n\\n&lt;p&gt;I had been planning on throwing in a small GPU one day, when I find the time, but while thinking about it today I realised - All GPUs that aren&amp;#39;t power hungry monsters, are limited to 8gb of vram for the most part. So while I&amp;#39;ll have more &amp;#39;processing power&amp;#39; which would speed up using small models (ones under 8gb) I&amp;#39;d still be left with the issue of those models not being that good. And bigger models end up spilling into ram, which would result in (I assume?) much slower speeds the same as I was getting on the CPU anyway.&lt;/p&gt;\\n\\n&lt;p&gt;Am I missing something? (probably yes).&lt;/p&gt;\\n\\n&lt;p&gt;It seems that a GPU is only a significant benefit if you use models that fit inside the vram, and so it&amp;#39;s only worth it if you have like.... 16gb+ of vram? maybe 12gb? I dunno.&lt;/p&gt;\\n\\n&lt;p&gt;Hence the question!&lt;/p&gt;\\n\\n&lt;p&gt;Edit: I know (or at least think/believe) its the bandwidth/speed of the ram that effects the toks/s results, and not just the capacity, but I also know that the capacity is important in its own right. The vram will always be faster, but if its only faster on lower-quality (smaller) models and isn&amp;#39;t noticeably faster on models that don&amp;#39;t fit into vram then that&amp;#39;s an issue. I guess?&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":true,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1lm32zh","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"LFAdvice7984","discussion_type":null,"num_comments":25,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lm32zh/noob_question_at_what_point_does_a_gpu_with_low/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lm32zh/noob_question_at_what_point_does_a_gpu_with_low/","subreddit_subscribers":492233,"created_utc":1751053259,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n054d4e","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ttkciar","can_mod_post":false,"send_replies":true,"parent_id":"t1_n04uh5o","score":1,"author_fullname":"t2_cpegz","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Good to know! Thanks :-)\\n\\nI didn't realize llama.cpp was spoiling us by doing this reliably correctly.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n054d4e","is_submitter":false,"collapsed":false,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Good to know! Thanks :-)&lt;/p&gt;\\n\\n&lt;p&gt;I didn&amp;#39;t realize llama.cpp was spoiling us by doing this reliably correctly.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lm32zh","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lm32zh/noob_question_at_what_point_does_a_gpu_with_low/n054d4e/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751061453,"author_flair_text":"llama.cpp","treatment_tags":[],"created_utc":1751061453,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n04uh5o","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"ttkciar","can_mod_post":false,"created_utc":1751058282,"send_replies":true,"parent_id":"t1_n04glma","score":5,"author_fullname":"t2_cpegz","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Well, yes and no.  Some inference stacks (like llama.cpp) let you load as many layers into VRAM as will fit, and use the GPU to infer on those, and load the remainder into main system memory for CPU inference.\\n\\nOP obliquely referred to this.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n04uh5o","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Well, yes and no.  Some inference stacks (like llama.cpp) let you load as many layers into VRAM as will fit, and use the GPU to infer on those, and load the remainder into main system memory for CPU inference.&lt;/p&gt;\\n\\n&lt;p&gt;OP obliquely referred to this.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lm32zh","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lm32zh/noob_question_at_what_point_does_a_gpu_with_low/n04uh5o/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751058282,"author_flair_text":"llama.cpp","treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0509w0","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Double_Cause4609","can_mod_post":false,"send_replies":true,"parent_id":"t1_n04yd9f","score":5,"author_fullname":"t2_1kubzxt2ww","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Ollama wraps LlamaCPP but has gone through extensive effort to avoid crediting them or up-streaming any of their work to the greater LCPP project. Evil is a bit of an exaggeration, but a lot of people prefer using the raw LlamaCPP project. It allows fine-grained control of the configuration of your hardware and model.\\n\\nI believe OpenWebUI allows the use of arbitrary OpenAI compatible endpoints.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0509w0","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Ollama wraps LlamaCPP but has gone through extensive effort to avoid crediting them or up-streaming any of their work to the greater LCPP project. Evil is a bit of an exaggeration, but a lot of people prefer using the raw LlamaCPP project. It allows fine-grained control of the configuration of your hardware and model.&lt;/p&gt;\\n\\n&lt;p&gt;I believe OpenWebUI allows the use of arbitrary OpenAI compatible endpoints.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lm32zh","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lm32zh/noob_question_at_what_point_does_a_gpu_with_low/n0509w0/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751060117,"author_flair_text":null,"treatment_tags":[],"created_utc":1751060117,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}}],"before":null}},"user_reports":[],"saved":false,"id":"n04yd9f","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"LFAdvice7984","can_mod_post":false,"created_utc":1751059509,"send_replies":true,"parent_id":"t1_n04glma","score":2,"author_fullname":"t2_vri94l9b","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Is ollama evil? \\n\\nI use openwebui because it comes as a docker container (which is required). It then needs to use ollama to run certain models (like the ones I mention). \\n\\nIf there's an alternative that's better (or less evil) I am very happy to switch over, if you have a suggestion?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n04yd9f","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Is ollama evil? &lt;/p&gt;\\n\\n&lt;p&gt;I use openwebui because it comes as a docker container (which is required). It then needs to use ollama to run certain models (like the ones I mention). &lt;/p&gt;\\n\\n&lt;p&gt;If there&amp;#39;s an alternative that&amp;#39;s better (or less evil) I am very happy to switch over, if you have a suggestion?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lm32zh","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lm32zh/noob_question_at_what_point_does_a_gpu_with_low/n04yd9f/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751059509,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n04glma","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Double_Cause4609","can_mod_post":false,"created_utc":1751054080,"send_replies":true,"parent_id":"t3_1lm32zh","score":9,"author_fullname":"t2_1kubzxt2ww","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"There's not really a point where they're comparable; they're completely different.\\n\\nIf a GPU doesn't have enough VRAM, it literally can't load the tensors.\\n\\nSo it's more a question of: Can you get what you need done with faster generation of a smaller model, or slower generation of a larger model?\\n\\nIn terms of hardware dynamics, one option is actually to throw in a small GPU into your CPU only rig, and manually offload specific tensors to it to speed up generation. As an example, you could offload the KV cache to the GPU, and calculate the Attention there (No clue if Ollama lets you do this. They're evil anyway, just use LCPP), which lets you offload the computationally expensive component to the GPU with lots of compute, while keeping the bulk of the weights (majority of the memory use) on CPU.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n04glma","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;There&amp;#39;s not really a point where they&amp;#39;re comparable; they&amp;#39;re completely different.&lt;/p&gt;\\n\\n&lt;p&gt;If a GPU doesn&amp;#39;t have enough VRAM, it literally can&amp;#39;t load the tensors.&lt;/p&gt;\\n\\n&lt;p&gt;So it&amp;#39;s more a question of: Can you get what you need done with faster generation of a smaller model, or slower generation of a larger model?&lt;/p&gt;\\n\\n&lt;p&gt;In terms of hardware dynamics, one option is actually to throw in a small GPU into your CPU only rig, and manually offload specific tensors to it to speed up generation. As an example, you could offload the KV cache to the GPU, and calculate the Attention there (No clue if Ollama lets you do this. They&amp;#39;re evil anyway, just use LCPP), which lets you offload the computationally expensive component to the GPU with lots of compute, while keeping the bulk of the weights (majority of the memory use) on CPU.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lm32zh/noob_question_at_what_point_does_a_gpu_with_low/n04glma/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751054080,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lm32zh","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":9}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n04t7gt","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AppearanceHeavy6724","can_mod_post":false,"send_replies":true,"parent_id":"t1_n04s1d2","score":2,"author_fullname":"t2_uz37qfx5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Well, yeah even bloody $25 p106 or p104 would massively improve prompt processing. Now why prompt processing has strange timings w/o gpu - no idea.","edited":false,"author_flair_css_class":null,"name":"t1_n04t7gt","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Well, yeah even bloody $25 p106 or p104 would massively improve prompt processing. Now why prompt processing has strange timings w/o gpu - no idea.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lm32zh","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lm32zh/noob_question_at_what_point_does_a_gpu_with_low/n04t7gt/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751057892,"author_flair_text":null,"collapsed":false,"created_utc":1751057892,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n04s1d2","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"DeProgrammer99","can_mod_post":false,"send_replies":true,"parent_id":"t1_n04p7xt","score":3,"author_fullname":"t2_w4j8t","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Okay, that's a really important point to add, haha. So even a really cheap GPU is probably better than nothing. I wonder why it was so much slower (according to llama-server) when I didn't offload any layers to the GPU with the CUDA build, then.\\n\\nUsing the CPU build of llama.cpp...\\n\\nPrompt processing: **11.4** (so the 4060 is 120x as fast)\\n\\nIt seemed to take just as long for the last 2.4% of the prompt eval as it did for the entire previous 32%...","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n04s1d2","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Okay, that&amp;#39;s a really important point to add, haha. So even a really cheap GPU is probably better than nothing. I wonder why it was so much slower (according to llama-server) when I didn&amp;#39;t offload any layers to the GPU with the CUDA build, then.&lt;/p&gt;\\n\\n&lt;p&gt;Using the CPU build of llama.cpp...&lt;/p&gt;\\n\\n&lt;p&gt;Prompt processing: &lt;strong&gt;11.4&lt;/strong&gt; (so the 4060 is 120x as fast)&lt;/p&gt;\\n\\n&lt;p&gt;It seemed to take just as long for the last 2.4% of the prompt eval as it did for the entire previous 32%...&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lm32zh","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lm32zh/noob_question_at_what_point_does_a_gpu_with_low/n04s1d2/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751057539,"author_flair_text":null,"treatment_tags":[],"created_utc":1751057539,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n06p5ek","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"LFAdvice7984","can_mod_post":false,"created_utc":1751082607,"send_replies":true,"parent_id":"t1_n06n4qj","score":1,"author_fullname":"t2_vri94l9b","approved_by":null,"mod_note":null,"all_awardings":[],"body":"I did look at an a4000. seems to be small and low powered for the performance. Though it's like double the price of a 3060","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n06p5ek","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I did look at an a4000. seems to be small and low powered for the performance. Though it&amp;#39;s like double the price of a 3060&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lm32zh","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lm32zh/noob_question_at_what_point_does_a_gpu_with_low/n06p5ek/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751082607,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":7,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n06n4qj","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AppearanceHeavy6724","can_mod_post":false,"send_replies":true,"parent_id":"t1_n06cogs","score":1,"author_fullname":"t2_uz37qfx5","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Used 3060 would be one.","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n06n4qj","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Used 3060 would be one.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lm32zh","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lm32zh/noob_question_at_what_point_does_a_gpu_with_low/n06n4qj/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751081710,"author_flair_text":null,"treatment_tags":[],"created_utc":1751081710,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":6,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n073nb8","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"PermanentLiminality","can_mod_post":false,"send_replies":true,"parent_id":"t1_n06cogs","score":1,"author_fullname":"t2_19zqycaf","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"P102-100.  10gb and 450 GB/s for $50.  Eight watts at idle.","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n073nb8","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;P102-100.  10gb and 450 GB/s for $50.  Eight watts at idle.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lm32zh","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lm32zh/noob_question_at_what_point_does_a_gpu_with_low/n073nb8/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751089700,"author_flair_text":null,"treatment_tags":[],"created_utc":1751089700,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":6,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n06cogs","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"LFAdvice7984","can_mod_post":false,"send_replies":true,"parent_id":"t1_n051ilz","score":0,"author_fullname":"t2_vri94l9b","approved_by":null,"mod_note":null,"all_awardings":[],"body":"in which case, I may have to look out for the best value/performance small GPU","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n06cogs","is_submitter":true,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;in which case, I may have to look out for the best value/performance small GPU&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lm32zh","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lm32zh/noob_question_at_what_point_does_a_gpu_with_low/n06cogs/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751077379,"author_flair_text":null,"treatment_tags":[],"created_utc":1751077379,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}},"user_reports":[],"saved":false,"id":"n051ilz","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AppearanceHeavy6724","can_mod_post":false,"send_replies":true,"parent_id":"t1_n04yzbc","score":1,"author_fullname":"t2_uz37qfx5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yes, exactly ","edited":false,"author_flair_css_class":null,"name":"t1_n051ilz","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yes, exactly &lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lm32zh","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lm32zh/noob_question_at_what_point_does_a_gpu_with_low/n051ilz/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751060520,"author_flair_text":null,"collapsed":false,"created_utc":1751060520,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n04yzbc","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"LFAdvice7984","can_mod_post":false,"send_replies":true,"parent_id":"t1_n04p7xt","score":1,"author_fullname":"t2_vri94l9b","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Ahh, so I did refer to this in my post (possibly not very clearly) but what you seem to be saying is that a gpu without enough vram is still better to have, because it'll speed up some parts and then the rest will be done with cpu/system ram. But it'll still be a net gain cos of the parts it could speed up?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n04yzbc","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Ahh, so I did refer to this in my post (possibly not very clearly) but what you seem to be saying is that a gpu without enough vram is still better to have, because it&amp;#39;ll speed up some parts and then the rest will be done with cpu/system ram. But it&amp;#39;ll still be a net gain cos of the parts it could speed up?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lm32zh","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lm32zh/noob_question_at_what_point_does_a_gpu_with_low/n04yzbc/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751059704,"author_flair_text":null,"treatment_tags":[],"created_utc":1751059704,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n04p7xt","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"AppearanceHeavy6724","can_mod_post":false,"send_replies":true,"parent_id":"t1_n04mnab","score":8,"author_fullname":"t2_uz37qfx5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"If you have gpu plugged into motherboard it will be used for prompt processing regardless if you offloading or not the token generation to cpu. Use llama.cpp without Cuda and Vulkan support compiled in and you,ll get 20t/s prompt processing.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n04p7xt","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;If you have gpu plugged into motherboard it will be used for prompt processing regardless if you offloading or not the token generation to cpu. Use llama.cpp without Cuda and Vulkan support compiled in and you,ll get 20t/s prompt processing.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lm32zh","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lm32zh/noob_question_at_what_point_does_a_gpu_with_low/n04p7xt/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751056684,"author_flair_text":null,"treatment_tags":[],"created_utc":1751056684,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":8}}],"before":null}},"user_reports":[],"saved":false,"id":"n04mnab","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DeProgrammer99","can_mod_post":false,"created_utc":1751055905,"send_replies":true,"parent_id":"t1_n04fgii","score":2,"author_fullname":"t2_w4j8t","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Comparing Ryzen 5 7600 to RTX 4060 Ti (CUDA) and RX 7900 XTX (Vulkan) using Phi-4 Q4\\\\_K\\\\_M with Q8 KV cache.\\n\\nPrompt processing:\\n\\nCPU: 365\\n\\n4060: 1377\\n\\n7900: 689\\n\\nThat's 3.77x for the 4060, which is just barely higher than the difference between my VRAM and RAM bandwidth (288 to 81.25 GB/s). Only 1.89x for the 7900, although its memory bandwidth is 960 GB/s.\\n\\nInference (with \\\\~6300 tokens of context):\\n\\nCPU: 3.0\\n\\n4060: 24.0\\n\\n7900: 49.0\\n\\nI'm actually surprised the inference part was that different for my CPU.","edited":1751056590,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n04mnab","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Comparing Ryzen 5 7600 to RTX 4060 Ti (CUDA) and RX 7900 XTX (Vulkan) using Phi-4 Q4_K_M with Q8 KV cache.&lt;/p&gt;\\n\\n&lt;p&gt;Prompt processing:&lt;/p&gt;\\n\\n&lt;p&gt;CPU: 365&lt;/p&gt;\\n\\n&lt;p&gt;4060: 1377&lt;/p&gt;\\n\\n&lt;p&gt;7900: 689&lt;/p&gt;\\n\\n&lt;p&gt;That&amp;#39;s 3.77x for the 4060, which is just barely higher than the difference between my VRAM and RAM bandwidth (288 to 81.25 GB/s). Only 1.89x for the 7900, although its memory bandwidth is 960 GB/s.&lt;/p&gt;\\n\\n&lt;p&gt;Inference (with ~6300 tokens of context):&lt;/p&gt;\\n\\n&lt;p&gt;CPU: 3.0&lt;/p&gt;\\n\\n&lt;p&gt;4060: 24.0&lt;/p&gt;\\n\\n&lt;p&gt;7900: 49.0&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;m actually surprised the inference part was that different for my CPU.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lm32zh","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lm32zh/noob_question_at_what_point_does_a_gpu_with_low/n04mnab/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751055905,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n04fgii","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"AppearanceHeavy6724","can_mod_post":false,"created_utc":1751053735,"send_replies":true,"parent_id":"t3_1lm32zh","score":7,"author_fullname":"t2_uz37qfx5","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Prompt processing is massively, 30x faster with gpu, even if token generation is fully done on cpu.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n04fgii","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Prompt processing is massively, 30x faster with gpu, even if token generation is fully done on cpu.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lm32zh/noob_question_at_what_point_does_a_gpu_with_low/n04fgii/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751053735,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lm32zh","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":7}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n04zftk","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"LFAdvice7984","can_mod_post":false,"created_utc":1751059849,"send_replies":true,"parent_id":"t1_n04o361","score":1,"author_fullname":"t2_vri94l9b","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"That's the issue though I was trying to explain (I think?)\\n\\nThe CPU with lots of ram can do a 30gb model, just slowly cos it's 'one package at a time'.\\n\\nThe GPU with 8gb vram can slowly delivery the whole package at once.... but only if it's less than 8gb? Otherwise it does nothing. \\n\\nThough it seems it can offload the excess, but I don't know if that ends up being slower (or the same speed) as just using the cpu+ram.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n04zftk","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;That&amp;#39;s the issue though I was trying to explain (I think?)&lt;/p&gt;\\n\\n&lt;p&gt;The CPU with lots of ram can do a 30gb model, just slowly cos it&amp;#39;s &amp;#39;one package at a time&amp;#39;.&lt;/p&gt;\\n\\n&lt;p&gt;The GPU with 8gb vram can slowly delivery the whole package at once.... but only if it&amp;#39;s less than 8gb? Otherwise it does nothing. &lt;/p&gt;\\n\\n&lt;p&gt;Though it seems it can offload the excess, but I don&amp;#39;t know if that ends up being slower (or the same speed) as just using the cpu+ram.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lm32zh","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lm32zh/noob_question_at_what_point_does_a_gpu_with_low/n04zftk/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751059849,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n04o361","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"GatePorters","can_mod_post":false,"created_utc":1751056339,"send_replies":true,"parent_id":"t3_1lm32zh","score":2,"author_fullname":"t2_vryl95sg1","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"CPU with lots of RAM = 8-16 Ferraris quickly delivering packages. Only carry one order at a time\\n\\nGPU = a train slowly delivering 2800 quadcons. Can carry as many orders as will fit\\n\\nLLMs need many orders at once to return stuff","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n04o361","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;CPU with lots of RAM = 8-16 Ferraris quickly delivering packages. Only carry one order at a time&lt;/p&gt;\\n\\n&lt;p&gt;GPU = a train slowly delivering 2800 quadcons. Can carry as many orders as will fit&lt;/p&gt;\\n\\n&lt;p&gt;LLMs need many orders at once to return stuff&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lm32zh/noob_question_at_what_point_does_a_gpu_with_low/n04o361/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751056339,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lm32zh","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n05fze0","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"No-Consequence-1779","can_mod_post":false,"created_utc":1751065404,"send_replies":true,"parent_id":"t3_1lm32zh","score":2,"author_fullname":"t2_1ping1tiaw","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"At the starting point. ","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n05fze0","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;At the starting point. &lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lm32zh/noob_question_at_what_point_does_a_gpu_with_low/n05fze0/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751065404,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lm32zh","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n064k2k","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ArsNeph","can_mod_post":false,"created_utc":1751074270,"send_replies":true,"parent_id":"t3_1lm32zh","score":2,"author_fullname":"t2_vt0xkv60d","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"The main difference between VRAM and RAM is the memory bandwidth. A good GPU, like the RTX 3090 does about 936GB/s, whereas an average one does about 360GB/s. However, even the fastest DDR5 RAM on a dual channel setup will barely crack 100GB/s if you're lucky. The tradeoff is DDR5 RAM is (relatively) cheap, and VRAM is scarce + expensive.\\n\\nIf you have a speed sensitive use case, then it is important that your entire model fits in VRAM. However, the question becomes different when your use case is quality sensitive. How long are you willing to wait for a high quality response? \\n\\nIt's not as simple as having a GPU being useless, even if you only have part of the model on a GPU, the prompt processing times are far better that way, even if the tokens per second are bottlenecked by the RAM.You should also consider that not every AI application can run on RAM, so having a GPU can open up access to a lot of software.\\n\\nFrankly, a GPU with less than 12GB of VRAM is impractical for AI. That said, there are GPUs with 12GB with a relatively reasonable power consumption, such as the RTX 3060 12 GB, which you can undervolt to even further decrease power usage.\\n\\nConversely, there are times where RAM can be in a better solution than gpus, namely with eight or 12 channel servers with 8/12 channel RAM, which can allow people to run massive MoE models at reasonable speeds. Unified memory platforms are also good for this.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n064k2k","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The main difference between VRAM and RAM is the memory bandwidth. A good GPU, like the RTX 3090 does about 936GB/s, whereas an average one does about 360GB/s. However, even the fastest DDR5 RAM on a dual channel setup will barely crack 100GB/s if you&amp;#39;re lucky. The tradeoff is DDR5 RAM is (relatively) cheap, and VRAM is scarce + expensive.&lt;/p&gt;\\n\\n&lt;p&gt;If you have a speed sensitive use case, then it is important that your entire model fits in VRAM. However, the question becomes different when your use case is quality sensitive. How long are you willing to wait for a high quality response? &lt;/p&gt;\\n\\n&lt;p&gt;It&amp;#39;s not as simple as having a GPU being useless, even if you only have part of the model on a GPU, the prompt processing times are far better that way, even if the tokens per second are bottlenecked by the RAM.You should also consider that not every AI application can run on RAM, so having a GPU can open up access to a lot of software.&lt;/p&gt;\\n\\n&lt;p&gt;Frankly, a GPU with less than 12GB of VRAM is impractical for AI. That said, there are GPUs with 12GB with a relatively reasonable power consumption, such as the RTX 3060 12 GB, which you can undervolt to even further decrease power usage.&lt;/p&gt;\\n\\n&lt;p&gt;Conversely, there are times where RAM can be in a better solution than gpus, namely with eight or 12 channel servers with 8/12 channel RAM, which can allow people to run massive MoE models at reasonable speeds. Unified memory platforms are also good for this.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lm32zh/noob_question_at_what_point_does_a_gpu_with_low/n064k2k/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751074270,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lm32zh","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0504yh","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"LFAdvice7984","can_mod_post":false,"created_utc":1751060073,"send_replies":true,"parent_id":"t1_n04vn1l","score":1,"author_fullname":"t2_vri94l9b","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I think my issue was that it could work in two ways (A 20gb model, 64gb system ram, 8gb vram) - \\n\\n1. 8gb of the model fits in vram, the rest goes into system ram. The gpu churns through the fast part, and then swaps out a chunk and does the next part (or cpu handles the rest). Slower than pure gpu with all in vram but faster than pure cpu. \\n\\n2. 8gb of the model fits in vram, the model then chugs when it hits the limit, and either breaks entirely or gets bottlenecked so hard it ends up slower than just doing it purely on cpu. \\n\\n  \\nThe first would be the ideal, but many years of experience has taught me that the second happens more often than we would like lol. but I've never tested it with the world of LLMs specifically so I didn't know .","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0504yh","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I think my issue was that it could work in two ways (A 20gb model, 64gb system ram, 8gb vram) - &lt;/p&gt;\\n\\n&lt;ol&gt;\\n&lt;li&gt;&lt;p&gt;8gb of the model fits in vram, the rest goes into system ram. The gpu churns through the fast part, and then swaps out a chunk and does the next part (or cpu handles the rest). Slower than pure gpu with all in vram but faster than pure cpu. &lt;/p&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;p&gt;8gb of the model fits in vram, the model then chugs when it hits the limit, and either breaks entirely or gets bottlenecked so hard it ends up slower than just doing it purely on cpu. &lt;/p&gt;&lt;/li&gt;\\n&lt;/ol&gt;\\n\\n&lt;p&gt;The first would be the ideal, but many years of experience has taught me that the second happens more often than we would like lol. but I&amp;#39;ve never tested it with the world of LLMs specifically so I didn&amp;#39;t know .&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lm32zh","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lm32zh/noob_question_at_what_point_does_a_gpu_with_low/n0504yh/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751060073,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n04vn1l","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ttkciar","can_mod_post":false,"created_utc":1751058646,"send_replies":true,"parent_id":"t3_1lm32zh","score":1,"author_fullname":"t2_cpegz","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"If you're looking for a formula for predicting performance, I think you could get a pretty good first approximation with simply:\\n\\nP = (Pg x Lg + Pc x Lc) / L\\n\\nWhere:\\n\\n* P = overall performance in tokens/second\\n\\n* L = number of parameters in model (use layers as an approximation, though not all layers are the same size)\\n\\n* Pg = Performance of GPU inference in tokens/second\\n\\n* Lg = Number of parameters (or layers for approx) that fit in VRAM\\n\\n* Pc = Performance of CPU inference in tokens/second\\n\\n* Lc = Number of parameters (or layers for approx) that are loaded to main memory\\n\\nYMMV, though.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n04vn1l","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;If you&amp;#39;re looking for a formula for predicting performance, I think you could get a pretty good first approximation with simply:&lt;/p&gt;\\n\\n&lt;p&gt;P = (Pg x Lg + Pc x Lc) / L&lt;/p&gt;\\n\\n&lt;p&gt;Where:&lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;&lt;p&gt;P = overall performance in tokens/second&lt;/p&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;p&gt;L = number of parameters in model (use layers as an approximation, though not all layers are the same size)&lt;/p&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;p&gt;Pg = Performance of GPU inference in tokens/second&lt;/p&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;p&gt;Lg = Number of parameters (or layers for approx) that fit in VRAM&lt;/p&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;p&gt;Pc = Performance of CPU inference in tokens/second&lt;/p&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;p&gt;Lc = Number of parameters (or layers for approx) that are loaded to main memory&lt;/p&gt;&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;p&gt;YMMV, though.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lm32zh/noob_question_at_what_point_does_a_gpu_with_low/n04vn1l/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751058646,"author_flair_text":"llama.cpp","treatment_tags":[],"link_id":"t3_1lm32zh","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n06kp3t","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Fun-Wolf-2007","can_mod_post":false,"send_replies":true,"parent_id":"t1_n06cs6j","score":1,"author_fullname":"t2_lsixf36sr","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Thanks, I will do some research","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n06kp3t","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thanks, I will do some research&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lm32zh","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lm32zh/noob_question_at_what_point_does_a_gpu_with_low/n06kp3t/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751080655,"author_flair_text":null,"treatment_tags":[],"created_utc":1751080655,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n06cs6j","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"LFAdvice7984","can_mod_post":false,"created_utc":1751077419,"send_replies":true,"parent_id":"t1_n05dtqb","score":1,"author_fullname":"t2_vri94l9b","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"From my very limited knowledge, I didn't think ollama etc had any real support for NPUs yet","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n06cs6j","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;From my very limited knowledge, I didn&amp;#39;t think ollama etc had any real support for NPUs yet&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lm32zh","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lm32zh/noob_question_at_what_point_does_a_gpu_with_low/n06cs6j/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751077419,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n05dtqb","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Fun-Wolf-2007","can_mod_post":false,"created_utc":1751064666,"send_replies":true,"parent_id":"t3_1lm32zh","score":0,"author_fullname":"t2_lsixf36sr","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I read about NPU (Neural Processing Unit) on Dell computers.\\n\\nHas anyone tried it?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n05dtqb","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I read about NPU (Neural Processing Unit) on Dell computers.&lt;/p&gt;\\n\\n&lt;p&gt;Has anyone tried it?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lm32zh/noob_question_at_what_point_does_a_gpu_with_low/n05dtqb/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751064666,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lm32zh","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}}]`),r=()=>e.jsx(l,{data:t});export{r as default};
