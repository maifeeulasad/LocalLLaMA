import{j as e}from"./index-C_z07ZVC.js";import{R as l}from"./RedditPostRenderer-DPnSR41P.js";import"./index-DKzOAewW.js";const a=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Hello ,\\n\\nI am relatively new to local llm. I’ve run a few models, it’s quite slow.\\n\\nI currently have an M1 Pro 16 gb, and am thinking about trading it for an M4 pro. I mostly want to upgrade from 14 inch to 16 inch monitor, but will there be any significant improvement in my ability to run local models? ","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"M1 vs M4 pro","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lrrojr","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.5,"author_flair_background_color":null,"subreddit_type":"public","ups":0,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_eh5cn5fe8","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":0,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1751658489,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Hello ,&lt;/p&gt;\\n\\n&lt;p&gt;I am relatively new to local llm. I’ve run a few models, it’s quite slow.&lt;/p&gt;\\n\\n&lt;p&gt;I currently have an M1 Pro 16 gb, and am thinking about trading it for an M4 pro. I mostly want to upgrade from 14 inch to 16 inch monitor, but will there be any significant improvement in my ability to run local models? &lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":true,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1lrrojr","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"CulturalGrapefruit97","discussion_type":null,"num_comments":7,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lrrojr/m1_vs_m4_pro/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lrrojr/m1_vs_m4_pro/","subreddit_subscribers":494898,"created_utc":1751658489,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1d0c0h","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"BumbleSlob","can_mod_post":false,"created_utc":1751659025,"send_replies":true,"parent_id":"t3_1lrrojr","score":7,"author_fullname":"t2_1j7fhlcqkp","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"The chip difference alone with probably net you a 40-50% inference speed improvement at a minimum. I’d recommend you save up to get at least 32Gb if not 48Gb of RAM as that will enable you to run the next higher class of models (you have likely been limited to 8b param models, you’d be able to run like Qwen3 30b MoE at around 50tps or QwQ-32B at ~15-17tps \\n\\nThe real advantage with Mac’s is the unified memory which is blazingly fast. I bought a M2 Max 64Gb over 2 years ago and I have zero regrets","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1d0c0h","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The chip difference alone with probably net you a 40-50% inference speed improvement at a minimum. I’d recommend you save up to get at least 32Gb if not 48Gb of RAM as that will enable you to run the next higher class of models (you have likely been limited to 8b param models, you’d be able to run like Qwen3 30b MoE at around 50tps or QwQ-32B at ~15-17tps &lt;/p&gt;\\n\\n&lt;p&gt;The real advantage with Mac’s is the unified memory which is blazingly fast. I bought a M2 Max 64Gb over 2 years ago and I have zero regrets&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrrojr/m1_vs_m4_pro/n1d0c0h/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751659025,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lrrojr","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":7}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1d2f3k","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"-jujube-","can_mod_post":false,"created_utc":1751659695,"send_replies":true,"parent_id":"t3_1lrrojr","score":3,"author_fullname":"t2_s1up9","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"The M1 Pro will have 200 GB/S memory bandwidth vs 273 GB/S for the M4.  If running LLMs is your main concern consider the Max chip, 400 GB/S for the M1 Max or 546 GB/S for the M4 Max.\\n\\nHere are some benchmarks of llama on apple silicon: [https://github.com/ggml-org/llama.cpp/discussions/4167](https://github.com/ggml-org/llama.cpp/discussions/4167)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1d2f3k","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The M1 Pro will have 200 GB/S memory bandwidth vs 273 GB/S for the M4.  If running LLMs is your main concern consider the Max chip, 400 GB/S for the M1 Max or 546 GB/S for the M4 Max.&lt;/p&gt;\\n\\n&lt;p&gt;Here are some benchmarks of llama on apple silicon: &lt;a href=\\"https://github.com/ggml-org/llama.cpp/discussions/4167\\"&gt;https://github.com/ggml-org/llama.cpp/discussions/4167&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrrojr/m1_vs_m4_pro/n1d2f3k/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751659695,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lrrojr","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1d5l5f","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Careless_Garlic1438","can_mod_post":false,"created_utc":1751660721,"send_replies":true,"parent_id":"t3_1lrrojr","score":3,"author_fullname":"t2_w3uuzkpbi","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I have an M1 16GB and a M4 128GB Max ... day and night ... I know other budget, but if you plan on using / needing AI a lot for work ... it's a no brainer ... Also these machines keep their value way longer the second hand market for Max systems is quite limited in supply, they sell rather quickly ...","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1d5l5f","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I have an M1 16GB and a M4 128GB Max ... day and night ... I know other budget, but if you plan on using / needing AI a lot for work ... it&amp;#39;s a no brainer ... Also these machines keep their value way longer the second hand market for Max systems is quite limited in supply, they sell rather quickly ...&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrrojr/m1_vs_m4_pro/n1d5l5f/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751660721,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lrrojr","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1d0f4y","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"NoLeading4922","can_mod_post":false,"created_utc":1751659053,"send_replies":true,"parent_id":"t3_1lrrojr","score":2,"author_fullname":"t2_hsngwhbs","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"[https://github.com/ggml-org/llama.cpp/discussions/4167](https://github.com/ggml-org/llama.cpp/discussions/4167)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1d0f4y","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;&lt;a href=\\"https://github.com/ggml-org/llama.cpp/discussions/4167\\"&gt;https://github.com/ggml-org/llama.cpp/discussions/4167&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrrojr/m1_vs_m4_pro/n1d0f4y/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751659053,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lrrojr","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1dx147","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"NNN_Throwaway2","can_mod_post":false,"created_utc":1751670362,"send_replies":true,"parent_id":"t1_n1da7c8","score":1,"author_fullname":"t2_8rrihts9","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"If you can, wait. Macbook Pro will be getting a refresh with M5 chips, probably around october. And next year they are supposed to be getting an overhaul with M6.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1dx147","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;If you can, wait. Macbook Pro will be getting a refresh with M5 chips, probably around october. And next year they are supposed to be getting an overhaul with M6.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lrrojr","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrrojr/m1_vs_m4_pro/n1dx147/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751670362,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1da7c8","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"CulturalGrapefruit97","can_mod_post":false,"created_utc":1751662224,"send_replies":true,"parent_id":"t3_1lrrojr","score":1,"author_fullname":"t2_eh5cn5fe8","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Thank you everyone for the quick and informative responses. I think the m4 max is out of my price range, but m4 pro with 48 gb seems like a decent option. I appreciate your advice!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1da7c8","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thank you everyone for the quick and informative responses. I think the m4 max is out of my price range, but m4 pro with 48 gb seems like a decent option. I appreciate your advice!&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrrojr/m1_vs_m4_pro/n1da7c8/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751662224,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lrrojr","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1diqis","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"rorowhat","can_mod_post":false,"created_utc":1751665104,"send_replies":true,"parent_id":"t3_1lrrojr","score":0,"author_fullname":"t2_yq51a","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Get a windows PC instead. You can always upgrade later, multiple uses. Don't get handicapped by apple.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1diqis","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Get a windows PC instead. You can always upgrade later, multiple uses. Don&amp;#39;t get handicapped by apple.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrrojr/m1_vs_m4_pro/n1diqis/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751665104,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lrrojr","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":1,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}}]`),n=()=>e.jsx(l,{data:a});export{n as default};
