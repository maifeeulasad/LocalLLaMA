import{j as e}from"./index-BOnf-UhU.js";import{R as l}from"./RedditPostRenderer-Ce39qICS.js";import"./index-CDase6VD.js";const t=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"***TLDR***: VSCode + RooCode + LM Studio + Devstral + snowflake-arctic-embed2 + docs-mcp-server. A fast, cost-free, self-hosted AI coding assistant setup supports lesser-used languages and minimizes hallucinations on less powerful hardware.\\n\\n**Long Post:**\\n\\nHello everyone, sharing my findings on trying to find a self-hosted agentic AI coding assistant that:\\n\\n1. Responds reasonably well on a variety of hardware.\\n2. Doesn’t hallucinate outdated syntax.\\n3. Costs $0 (except electricity).\\n4. Understands less common languages, e.g., KQL, Flutter, etc.\\n\\nAfter experimenting with several setups, here’s the combo I found that actually works.  \\nPlease forgive any mistakes and feel free to let me know of any improvements you are aware of.\\n\\n**Hardware**  \\nTested on a Ryzen 5700 + RTX 3080 (10GB VRAM), 48GB RAM.  \\nShould work on both low, and high-end setups, your mileage may vary.\\n\\n**The Stack**\\n\\n\`VSCode +(with) RooCode +(connected to) LM Studio +(running both) Devstral +(and) snowflake-arctic-embed2 +(supported by) docs-mcp-server\`\\n\\n\\\\---\\n\\n**Edit 1:** Setup Process for users saying this is too complicated\\n\\n1. Install \`VSCode\` then get \`RooCode\` Extension\\n2. Install \`LMStudio\` and pull \`snowflake-arctic-embed2\`  embeddings model, as well as \`Devstral\` large language model which suits your computer. Start LM Studio server and load both models from \\"Power User\\" tab.\\n3. Install \`Docker\` or \`NodeJS\`, depending on which config you prefer *(recommend Docker)*\\n4. Include \`docs-mcp-server\` in your RooCode MCP configuration *(see json below)*\\n\\n**Edit 2**: I had been [misinformed](https://docs.useanything.com/setup/embedder-configuration/local/lmstudio) that running embeddings and LLM together via LM Studio is not possible, it certainly is! I have updated this guide to remove Ollama altogether and only use LM Studio.\\n\\nLM Studio made it slightly confusing because you cannot load embeddings model from \\"Chat\\" tab, you must load it from \\"Developer\\" tab.\\n\\n\\\\---\\n\\n**VSCode + RooCode**  \\nRooCode is a VS Code extension that enables agentic coding and has MCP support.\\n\\nVS Code: [https://code.visualstudio.com/download](https://code.visualstudio.com/download)  \\nAlternative - VSCodium: [https://github.com/VSCodium/vscodium/releases](https://github.com/VSCodium/vscodium/releases) \\\\- No telemetry\\n\\nRooCode: [https://marketplace.visualstudio.com/items?itemName=RooVeterinaryInc.roo-cline](https://marketplace.visualstudio.com/items?itemName=RooVeterinaryInc.roo-cline)\\n\\nAlternative to this setup is Zed Editor: [https://zed.dev/download](https://zed.dev/download)\\n\\n( Zed is nice, but you cannot yet pass problems as context. Released only for MacOS and Linux, coming soon for windows. Unofficial windows nightly here: [github.com/send-me-a-ticket/zedforwindows](https://github.com/send-me-a-ticket/zedforwindows) )\\n\\n**LM Studio**  \\n[https://lmstudio.ai/download](https://lmstudio.ai/download)\\n\\n* Nice UI with real-time logs\\n* GPU offloading is too simple. Changing AI model parameters is a breeze. You can achieve same effect in ollama by creating custom models with changed num\\\\_gpu and num\\\\_ctx parameters\\n* Good (better?) OpenAI-compatible API\\n\\n**Devstral (Unsloth finetune)**  \\nSolid coding model with good tool usage.\\n\\nI use \`devstral-small-2505@iq2_m\`, which fully fits within 10GB VRAM. token context 32768.  \\nOther variants &amp; parameters may work depending on your hardware.\\n\\n**snowflake-arctic-embed2**  \\nTiny embeddings model used with docs-mcp-server. Feel free to substitute for any better ones.  \\nI use \`text-embedding-snowflake-arctic-embed-l-v2.0\`\\n\\n**Docker**  \\n[https://www.docker.com/products/docker-desktop/](https://www.docker.com/products/docker-desktop/)  \\nRecommend Docker use instead of NPX, for security and ease of use.\\n\\nPortainer is my recommended extension for ease of use:  \\n[https://hub.docker.com/extensions/portainer/portainer-docker-extension](https://hub.docker.com/extensions/portainer/portainer-docker-extension)\\n\\n**docs-mcp-server**  \\n[https://github.com/arabold/docs-mcp-server](https://github.com/arabold/docs-mcp-server)\\n\\nThis is what makes it all click. MCP server scrapes documentation (with versioning) so the AI can look up the *correct* syntax for *your* version of language implementation, and avoid hallucinations.\\n\\nYou *should* also be able to run \`localhost:6281\` to open web UI for the \`docs-mcp-server\`, however web UI doesn't seem to be working for me, which I can ignore because AI is managing that anyway.\\n\\nYou can implement this MCP server as following -\\n\\n*Docker version (needs Docker Installed)*\\n\\n    {\\n      \\"mcpServers\\": {\\n        \\"docs-mcp-server\\": {\\n          \\"command\\": \\"docker\\",\\n          \\"args\\": [\\n            \\"run\\",\\n            \\"-i\\",\\n            \\"--rm\\",\\n            \\"-p\\",\\n            \\"6280:6280\\",\\n            \\"-p\\",\\n            \\"6281:6281\\",\\n            \\"-e\\",\\n            \\"OPENAI_API_KEY\\",\\n            \\"-e\\",\\n            \\"OPENAI_API_BASE\\",\\n            \\"-e\\",\\n            \\"DOCS_MCP_EMBEDDING_MODEL\\",\\n            \\"-v\\",\\n            \\"docs-mcp-data:/data\\",\\n            \\"ghcr.io/arabold/docs-mcp-server:latest\\"\\n          ],\\n          \\"env\\": {\\n            \\"OPENAI_API_KEY\\": \\"ollama\\",\\n            \\"OPENAI_API_BASE\\": \\"http://host.docker.internal:1234/v1\\",\\n            \\"DOCS_MCP_EMBEDDING_MODEL\\": \\"text-embedding-snowflake-arctic-embed-l-v2.0\\"\\n          }\\n        }\\n      }\\n    }\\n\\n*NPX version (needs NodeJS installed)*\\n\\n    {\\n      \\"mcpServers\\": {\\n        \\"docs-mcp-server\\": {\\n          \\"command\\": \\"npx\\",\\n          \\"args\\": [\\n            \\"@arabold/docs-mcp-server@latest\\"\\n          ],\\n          \\"env\\": {\\n            \\"OPENAI_API_KEY\\": \\"ollama\\",\\n            \\"OPENAI_API_BASE\\": \\"http://host.docker.internal:1234/v1\\",\\n            \\"DOCS_MCP_EMBEDDING_MODEL\\": \\"text-embedding-snowflake-arctic-embed-l-v2.0\\"\\n          }\\n        }\\n      }\\n    }\\n\\n**Adding documentation for your language**\\n\\nAsk AI to use the \`scrape_docs\`  tool with:\\n\\n* **url** (link to the documentation),\\n* **library** (name of the documentation/programming language),\\n* **version** (version of the documentation)\\n\\nyou can also provide (optional):\\n\\n* **maxPages** (maximum number of pages to scrape, default is 1000).\\n* **maxDepth** (maximum navigation depth, default is 3).\\n* **scope** (crawling boundary, which can be 'subpages', 'hostname', or 'domain', default is 'subpages').\\n* **followRedirects** (whether to follow HTTP 3xx redirects, default is true).\\n\\nYou can ask AI to use **search\\\\_docs** tool any time you want to make sure the syntax or code implementation is correct. It should also check docs automatically if it is smart enough.\\n\\nThis stack isn’t limited to coding, Devstral handles logical, non-coding tasks well too.  \\nThe MCP setup helps reduce hallucinations by grounding the AI in real documentation, making this a flexible and reliable solution for a variety of tasks.\\n\\n**Thanks for reading... If you have used and/or improved on this, I’d love to hear about it..!**","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Self-hosted AI coding that just works","link_flair_richtext":[{"e":"text","t":"Resources"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lt4y1z","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.97,"author_flair_background_color":null,"subreddit_type":"public","ups":579,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_toqw74vmo","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Resources","can_mod_post":false,"score":579,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":1751882516,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"post_hint":"self","content_categories":null,"is_self":true,"mod_note":null,"created":1751818168,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;&lt;strong&gt;&lt;em&gt;TLDR&lt;/em&gt;&lt;/strong&gt;: VSCode + RooCode + LM Studio + Devstral + snowflake-arctic-embed2 + docs-mcp-server. A fast, cost-free, self-hosted AI coding assistant setup supports lesser-used languages and minimizes hallucinations on less powerful hardware.&lt;/p&gt;\\n\\n&lt;p&gt;&lt;strong&gt;Long Post:&lt;/strong&gt;&lt;/p&gt;\\n\\n&lt;p&gt;Hello everyone, sharing my findings on trying to find a self-hosted agentic AI coding assistant that:&lt;/p&gt;\\n\\n&lt;ol&gt;\\n&lt;li&gt;Responds reasonably well on a variety of hardware.&lt;/li&gt;\\n&lt;li&gt;Doesn’t hallucinate outdated syntax.&lt;/li&gt;\\n&lt;li&gt;Costs $0 (except electricity).&lt;/li&gt;\\n&lt;li&gt;Understands less common languages, e.g., KQL, Flutter, etc.&lt;/li&gt;\\n&lt;/ol&gt;\\n\\n&lt;p&gt;After experimenting with several setups, here’s the combo I found that actually works.&lt;br/&gt;\\nPlease forgive any mistakes and feel free to let me know of any improvements you are aware of.&lt;/p&gt;\\n\\n&lt;p&gt;&lt;strong&gt;Hardware&lt;/strong&gt;&lt;br/&gt;\\nTested on a Ryzen 5700 + RTX 3080 (10GB VRAM), 48GB RAM.&lt;br/&gt;\\nShould work on both low, and high-end setups, your mileage may vary.&lt;/p&gt;\\n\\n&lt;p&gt;&lt;strong&gt;The Stack&lt;/strong&gt;&lt;/p&gt;\\n\\n&lt;p&gt;&lt;code&gt;VSCode +(with) RooCode +(connected to) LM Studio +(running both) Devstral +(and) snowflake-arctic-embed2 +(supported by) docs-mcp-server&lt;/code&gt;&lt;/p&gt;\\n\\n&lt;p&gt;---&lt;/p&gt;\\n\\n&lt;p&gt;&lt;strong&gt;Edit 1:&lt;/strong&gt; Setup Process for users saying this is too complicated&lt;/p&gt;\\n\\n&lt;ol&gt;\\n&lt;li&gt;Install &lt;code&gt;VSCode&lt;/code&gt; then get &lt;code&gt;RooCode&lt;/code&gt; Extension&lt;/li&gt;\\n&lt;li&gt;Install &lt;code&gt;LMStudio&lt;/code&gt; and pull &lt;code&gt;snowflake-arctic-embed2&lt;/code&gt;  embeddings model, as well as &lt;code&gt;Devstral&lt;/code&gt; large language model which suits your computer. Start LM Studio server and load both models from &amp;quot;Power User&amp;quot; tab.&lt;/li&gt;\\n&lt;li&gt;Install &lt;code&gt;Docker&lt;/code&gt; or &lt;code&gt;NodeJS&lt;/code&gt;, depending on which config you prefer &lt;em&gt;(recommend Docker)&lt;/em&gt;&lt;/li&gt;\\n&lt;li&gt;Include &lt;code&gt;docs-mcp-server&lt;/code&gt; in your RooCode MCP configuration &lt;em&gt;(see json below)&lt;/em&gt;&lt;/li&gt;\\n&lt;/ol&gt;\\n\\n&lt;p&gt;&lt;strong&gt;Edit 2&lt;/strong&gt;: I had been &lt;a href=\\"https://docs.useanything.com/setup/embedder-configuration/local/lmstudio\\"&gt;misinformed&lt;/a&gt; that running embeddings and LLM together via LM Studio is not possible, it certainly is! I have updated this guide to remove Ollama altogether and only use LM Studio.&lt;/p&gt;\\n\\n&lt;p&gt;LM Studio made it slightly confusing because you cannot load embeddings model from &amp;quot;Chat&amp;quot; tab, you must load it from &amp;quot;Developer&amp;quot; tab.&lt;/p&gt;\\n\\n&lt;p&gt;---&lt;/p&gt;\\n\\n&lt;p&gt;&lt;strong&gt;VSCode + RooCode&lt;/strong&gt;&lt;br/&gt;\\nRooCode is a VS Code extension that enables agentic coding and has MCP support.&lt;/p&gt;\\n\\n&lt;p&gt;VS Code: &lt;a href=\\"https://code.visualstudio.com/download\\"&gt;https://code.visualstudio.com/download&lt;/a&gt;&lt;br/&gt;\\nAlternative - VSCodium: &lt;a href=\\"https://github.com/VSCodium/vscodium/releases\\"&gt;https://github.com/VSCodium/vscodium/releases&lt;/a&gt; - No telemetry&lt;/p&gt;\\n\\n&lt;p&gt;RooCode: &lt;a href=\\"https://marketplace.visualstudio.com/items?itemName=RooVeterinaryInc.roo-cline\\"&gt;https://marketplace.visualstudio.com/items?itemName=RooVeterinaryInc.roo-cline&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;Alternative to this setup is Zed Editor: &lt;a href=\\"https://zed.dev/download\\"&gt;https://zed.dev/download&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;( Zed is nice, but you cannot yet pass problems as context. Released only for MacOS and Linux, coming soon for windows. Unofficial windows nightly here: &lt;a href=\\"https://github.com/send-me-a-ticket/zedforwindows\\"&gt;github.com/send-me-a-ticket/zedforwindows&lt;/a&gt; )&lt;/p&gt;\\n\\n&lt;p&gt;&lt;strong&gt;LM Studio&lt;/strong&gt;&lt;br/&gt;\\n&lt;a href=\\"https://lmstudio.ai/download\\"&gt;https://lmstudio.ai/download&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;Nice UI with real-time logs&lt;/li&gt;\\n&lt;li&gt;GPU offloading is too simple. Changing AI model parameters is a breeze. You can achieve same effect in ollama by creating custom models with changed num_gpu and num_ctx parameters&lt;/li&gt;\\n&lt;li&gt;Good (better?) OpenAI-compatible API&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;p&gt;&lt;strong&gt;Devstral (Unsloth finetune)&lt;/strong&gt;&lt;br/&gt;\\nSolid coding model with good tool usage.&lt;/p&gt;\\n\\n&lt;p&gt;I use &lt;code&gt;devstral-small-2505@iq2_m&lt;/code&gt;, which fully fits within 10GB VRAM. token context 32768.&lt;br/&gt;\\nOther variants &amp;amp; parameters may work depending on your hardware.&lt;/p&gt;\\n\\n&lt;p&gt;&lt;strong&gt;snowflake-arctic-embed2&lt;/strong&gt;&lt;br/&gt;\\nTiny embeddings model used with docs-mcp-server. Feel free to substitute for any better ones.&lt;br/&gt;\\nI use &lt;code&gt;text-embedding-snowflake-arctic-embed-l-v2.0&lt;/code&gt;&lt;/p&gt;\\n\\n&lt;p&gt;&lt;strong&gt;Docker&lt;/strong&gt;&lt;br/&gt;\\n&lt;a href=\\"https://www.docker.com/products/docker-desktop/\\"&gt;https://www.docker.com/products/docker-desktop/&lt;/a&gt;&lt;br/&gt;\\nRecommend Docker use instead of NPX, for security and ease of use.&lt;/p&gt;\\n\\n&lt;p&gt;Portainer is my recommended extension for ease of use:&lt;br/&gt;\\n&lt;a href=\\"https://hub.docker.com/extensions/portainer/portainer-docker-extension\\"&gt;https://hub.docker.com/extensions/portainer/portainer-docker-extension&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;&lt;strong&gt;docs-mcp-server&lt;/strong&gt;&lt;br/&gt;\\n&lt;a href=\\"https://github.com/arabold/docs-mcp-server\\"&gt;https://github.com/arabold/docs-mcp-server&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;This is what makes it all click. MCP server scrapes documentation (with versioning) so the AI can look up the &lt;em&gt;correct&lt;/em&gt; syntax for &lt;em&gt;your&lt;/em&gt; version of language implementation, and avoid hallucinations.&lt;/p&gt;\\n\\n&lt;p&gt;You &lt;em&gt;should&lt;/em&gt; also be able to run &lt;code&gt;localhost:6281&lt;/code&gt; to open web UI for the &lt;code&gt;docs-mcp-server&lt;/code&gt;, however web UI doesn&amp;#39;t seem to be working for me, which I can ignore because AI is managing that anyway.&lt;/p&gt;\\n\\n&lt;p&gt;You can implement this MCP server as following -&lt;/p&gt;\\n\\n&lt;p&gt;&lt;em&gt;Docker version (needs Docker Installed)&lt;/em&gt;&lt;/p&gt;\\n\\n&lt;pre&gt;&lt;code&gt;{\\n  &amp;quot;mcpServers&amp;quot;: {\\n    &amp;quot;docs-mcp-server&amp;quot;: {\\n      &amp;quot;command&amp;quot;: &amp;quot;docker&amp;quot;,\\n      &amp;quot;args&amp;quot;: [\\n        &amp;quot;run&amp;quot;,\\n        &amp;quot;-i&amp;quot;,\\n        &amp;quot;--rm&amp;quot;,\\n        &amp;quot;-p&amp;quot;,\\n        &amp;quot;6280:6280&amp;quot;,\\n        &amp;quot;-p&amp;quot;,\\n        &amp;quot;6281:6281&amp;quot;,\\n        &amp;quot;-e&amp;quot;,\\n        &amp;quot;OPENAI_API_KEY&amp;quot;,\\n        &amp;quot;-e&amp;quot;,\\n        &amp;quot;OPENAI_API_BASE&amp;quot;,\\n        &amp;quot;-e&amp;quot;,\\n        &amp;quot;DOCS_MCP_EMBEDDING_MODEL&amp;quot;,\\n        &amp;quot;-v&amp;quot;,\\n        &amp;quot;docs-mcp-data:/data&amp;quot;,\\n        &amp;quot;ghcr.io/arabold/docs-mcp-server:latest&amp;quot;\\n      ],\\n      &amp;quot;env&amp;quot;: {\\n        &amp;quot;OPENAI_API_KEY&amp;quot;: &amp;quot;ollama&amp;quot;,\\n        &amp;quot;OPENAI_API_BASE&amp;quot;: &amp;quot;http://host.docker.internal:1234/v1&amp;quot;,\\n        &amp;quot;DOCS_MCP_EMBEDDING_MODEL&amp;quot;: &amp;quot;text-embedding-snowflake-arctic-embed-l-v2.0&amp;quot;\\n      }\\n    }\\n  }\\n}\\n&lt;/code&gt;&lt;/pre&gt;\\n\\n&lt;p&gt;&lt;em&gt;NPX version (needs NodeJS installed)&lt;/em&gt;&lt;/p&gt;\\n\\n&lt;pre&gt;&lt;code&gt;{\\n  &amp;quot;mcpServers&amp;quot;: {\\n    &amp;quot;docs-mcp-server&amp;quot;: {\\n      &amp;quot;command&amp;quot;: &amp;quot;npx&amp;quot;,\\n      &amp;quot;args&amp;quot;: [\\n        &amp;quot;@arabold/docs-mcp-server@latest&amp;quot;\\n      ],\\n      &amp;quot;env&amp;quot;: {\\n        &amp;quot;OPENAI_API_KEY&amp;quot;: &amp;quot;ollama&amp;quot;,\\n        &amp;quot;OPENAI_API_BASE&amp;quot;: &amp;quot;http://host.docker.internal:1234/v1&amp;quot;,\\n        &amp;quot;DOCS_MCP_EMBEDDING_MODEL&amp;quot;: &amp;quot;text-embedding-snowflake-arctic-embed-l-v2.0&amp;quot;\\n      }\\n    }\\n  }\\n}\\n&lt;/code&gt;&lt;/pre&gt;\\n\\n&lt;p&gt;&lt;strong&gt;Adding documentation for your language&lt;/strong&gt;&lt;/p&gt;\\n\\n&lt;p&gt;Ask AI to use the &lt;code&gt;scrape_docs&lt;/code&gt;  tool with:&lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;&lt;strong&gt;url&lt;/strong&gt; (link to the documentation),&lt;/li&gt;\\n&lt;li&gt;&lt;strong&gt;library&lt;/strong&gt; (name of the documentation/programming language),&lt;/li&gt;\\n&lt;li&gt;&lt;strong&gt;version&lt;/strong&gt; (version of the documentation)&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;p&gt;you can also provide (optional):&lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;&lt;strong&gt;maxPages&lt;/strong&gt; (maximum number of pages to scrape, default is 1000).&lt;/li&gt;\\n&lt;li&gt;&lt;strong&gt;maxDepth&lt;/strong&gt; (maximum navigation depth, default is 3).&lt;/li&gt;\\n&lt;li&gt;&lt;strong&gt;scope&lt;/strong&gt; (crawling boundary, which can be &amp;#39;subpages&amp;#39;, &amp;#39;hostname&amp;#39;, or &amp;#39;domain&amp;#39;, default is &amp;#39;subpages&amp;#39;).&lt;/li&gt;\\n&lt;li&gt;&lt;strong&gt;followRedirects&lt;/strong&gt; (whether to follow HTTP 3xx redirects, default is true).&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;p&gt;You can ask AI to use &lt;strong&gt;search_docs&lt;/strong&gt; tool any time you want to make sure the syntax or code implementation is correct. It should also check docs automatically if it is smart enough.&lt;/p&gt;\\n\\n&lt;p&gt;This stack isn’t limited to coding, Devstral handles logical, non-coding tasks well too.&lt;br/&gt;\\nThe MCP setup helps reduce hallucinations by grounding the AI in real documentation, making this a flexible and reliable solution for a variety of tasks.&lt;/p&gt;\\n\\n&lt;p&gt;&lt;strong&gt;Thanks for reading... If you have used and/or improved on this, I’d love to hear about it..!&lt;/strong&gt;&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"preview":{"images":[{"source":{"url":"https://external-preview.redd.it/vlZpegiOP81x_r16_rT4LmaQbBjxreSVqZS08MiyODY.png?auto=webp&amp;s=d71804ee4194235942e3d32ce49695af47ae2931","width":1200,"height":630},"resolutions":[{"url":"https://external-preview.redd.it/vlZpegiOP81x_r16_rT4LmaQbBjxreSVqZS08MiyODY.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=e20521ee56cbc2d4a70339418c28f6b4802a2591","width":108,"height":56},{"url":"https://external-preview.redd.it/vlZpegiOP81x_r16_rT4LmaQbBjxreSVqZS08MiyODY.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=8f4858df95f192c288a54a807c884a7fca524a58","width":216,"height":113},{"url":"https://external-preview.redd.it/vlZpegiOP81x_r16_rT4LmaQbBjxreSVqZS08MiyODY.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=cb3c47addc61d1754610d9273b828b0a1cc6a3b0","width":320,"height":168},{"url":"https://external-preview.redd.it/vlZpegiOP81x_r16_rT4LmaQbBjxreSVqZS08MiyODY.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=f198eb5a5cd7d518778530fa8bc8cfcbef25f1bb","width":640,"height":336},{"url":"https://external-preview.redd.it/vlZpegiOP81x_r16_rT4LmaQbBjxreSVqZS08MiyODY.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=b11538ae52f8e379b9c1af43d0d3b4ca683ce85e","width":960,"height":504},{"url":"https://external-preview.redd.it/vlZpegiOP81x_r16_rT4LmaQbBjxreSVqZS08MiyODY.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=51d609c082f1e4bcf2e66fda02b1e312242161d2","width":1080,"height":567}],"variants":{},"id":"vlZpegiOP81x_r16_rT4LmaQbBjxreSVqZS08MiyODY"}],"enabled":false},"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#ccac2b","id":"1lt4y1z","is_robot_indexable":true,"num_duplicates":4,"report_reasons":null,"author":"send_me_a_ticket","discussion_type":null,"num_comments":79,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lt4y1z/selfhosted_ai_coding_that_just_works/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lt4y1z/selfhosted_ai_coding_that_just_works/","subreddit_subscribers":496034,"created_utc":1751818168,"num_crossposts":4,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1vvv85","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"henfiber","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1vnl0x","score":2,"author_fullname":"t2_lw9me25","approved_by":null,"mod_note":null,"all_awardings":[],"body":"That's how I created my first one. \\n\\nThen copy-paste the working model definitions for other models.\\n\\nThen moving some of the common runtime params to macros.\\n\\nAnd finally tweaking individual model definition parameters such as ttl, temperature etc.","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n1vvv85","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;That&amp;#39;s how I created my first one. &lt;/p&gt;\\n\\n&lt;p&gt;Then copy-paste the working model definitions for other models.&lt;/p&gt;\\n\\n&lt;p&gt;Then moving some of the common runtime params to macros.&lt;/p&gt;\\n\\n&lt;p&gt;And finally tweaking individual model definition parameters such as ttl, temperature etc.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lt4y1z","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lt4y1z/selfhosted_ai_coding_that_just_works/n1vvv85/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751927704,"author_flair_text":null,"treatment_tags":[],"created_utc":1751927704,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n1vnl0x","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"No-Statement-0001","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1p9ll2","score":2,"author_fullname":"t2_11gh93nhos","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"the example config was written to be copy and pasted into an LLM prompt and it should be able to help you write a decent working config.","edited":false,"author_flair_css_class":null,"name":"t1_n1vnl0x","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;the example config was written to be copy and pasted into an LLM prompt and it should be able to help you write a decent working config.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lt4y1z","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":"light","treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lt4y1z/selfhosted_ai_coding_that_just_works/n1vnl0x/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751925087,"author_flair_text":"llama.cpp","collapsed":false,"created_utc":1751925087,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n1p9ll2","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"henfiber","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1p3vp8","score":8,"author_fullname":"t2_lw9me25","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"They have added a wiki with examples. This, along with their inline comments in the default config example, should be enough to get you started.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1p9ll2","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;They have added a wiki with examples. This, along with their inline comments in the default config example, should be enough to get you started.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lt4y1z","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lt4y1z/selfhosted_ai_coding_that_just_works/n1p9ll2/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751836765,"author_flair_text":null,"treatment_tags":[],"created_utc":1751836765,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":8}}],"before":null}},"user_reports":[],"saved":false,"id":"n1p3vp8","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"texasdude11","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1nolsu","score":6,"author_fullname":"t2_ya9qn","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I need to start using llama-swap. Is there an easy tutorial for this? The docs there were a little confusing, either that or I didn't look hard enough. Most likely latter :)","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n1p3vp8","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I need to start using llama-swap. Is there an easy tutorial for this? The docs there were a little confusing, either that or I didn&amp;#39;t look hard enough. Most likely latter :)&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lt4y1z","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lt4y1z/selfhosted_ai_coding_that_just_works/n1p3vp8/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751834992,"author_flair_text":null,"treatment_tags":[],"created_utc":1751834992,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1th6v8","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"The_Noble_Lie","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1te5dp","score":2,"author_fullname":"t2_ovism","approved_by":null,"mod_note":null,"all_awardings":[],"body":"This is wholesome.","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n1th6v8","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;This is wholesome.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lt4y1z","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lt4y1z/selfhosted_ai_coding_that_just_works/n1th6v8/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751900658,"author_flair_text":null,"treatment_tags":[],"created_utc":1751900658,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1tt8uw","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"chromix","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1te5dp","score":2,"author_fullname":"t2_43ypa","approved_by":null,"mod_note":null,"all_awardings":[],"body":"LOL just my latest in a long train of hobbies you could trace back over decades. \\n\\nI'm a software engineer by day, so I'm sorry the benefit is so one sided.","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n1tt8uw","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;LOL just my latest in a long train of hobbies you could trace back over decades. &lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;m a software engineer by day, so I&amp;#39;m sorry the benefit is so one sided.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lt4y1z","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lt4y1z/selfhosted_ai_coding_that_just_works/n1tt8uw/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751904148,"author_flair_text":null,"treatment_tags":[],"created_utc":1751904148,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n1te5dp","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Chromix_","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1sv560","score":5,"author_fullname":"t2_k7w2h","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"👋  \\nThanks for adding to my... uh... wristwatch knowledge.","edited":false,"author_flair_css_class":null,"name":"t1_n1te5dp","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;👋&lt;br/&gt;\\nThanks for adding to my... uh... wristwatch knowledge.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lt4y1z","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lt4y1z/selfhosted_ai_coding_that_just_works/n1te5dp/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751899761,"author_flair_text":null,"collapsed":false,"created_utc":1751899761,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}}],"before":null}},"user_reports":[],"saved":false,"id":"n1sv560","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"chromix","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1sncfg","score":4,"author_fullname":"t2_43ypa","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"u/Chromix_ you mean \\\\^ (this happens a lot, but thanks for inadvertently adding to my LLM knowledge)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1sv560","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;&lt;a href=\\"/u/Chromix_\\"&gt;u/Chromix_&lt;/a&gt; you mean ^ (this happens a lot, but thanks for inadvertently adding to my LLM knowledge)&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lt4y1z","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lt4y1z/selfhosted_ai_coding_that_just_works/n1sv560/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751893637,"author_flair_text":null,"treatment_tags":[],"created_utc":1751893637,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}}],"before":null}},"user_reports":[],"saved":false,"id":"n1sncfg","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"The_Noble_Lie","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1nolsu","score":1,"author_fullname":"t2_ovism","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Any performance comparisons between the software / wrapper?\\n\\nFwd u/Chromix","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n1sncfg","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Any performance comparisons between the software / wrapper?&lt;/p&gt;\\n\\n&lt;p&gt;Fwd &lt;a href=\\"/u/Chromix\\"&gt;u/Chromix&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lt4y1z","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lt4y1z/selfhosted_ai_coding_that_just_works/n1sncfg/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751890663,"author_flair_text":null,"treatment_tags":[],"created_utc":1751890663,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1nolsu","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"FullstackSensei","can_mod_post":false,"created_utc":1751819271,"send_replies":true,"parent_id":"t1_n1no3p9","score":42,"author_fullname":"t2_17n3nqtj56","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Came to say this about using llama.cpp instead of ollama and lmstudio.\\n\\nAdd in [llama-swap](https://github.com/mostlygeek/llama-swap) for loading/unloading models automatically, especially now with groups support!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1nolsu","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Came to say this about using llama.cpp instead of ollama and lmstudio.&lt;/p&gt;\\n\\n&lt;p&gt;Add in &lt;a href=\\"https://github.com/mostlygeek/llama-swap\\"&gt;llama-swap&lt;/a&gt; for loading/unloading models automatically, especially now with groups support!&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lt4y1z","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lt4y1z/selfhosted_ai_coding_that_just_works/n1nolsu/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751819271,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":42}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1qx73a","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Revolutionalredstone","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1qhbme","score":3,"author_fullname":"t2_6crrj","approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yeah that's a much better point ☝️ 😉 \\n\\nGod I want an open source LMSTUDIO","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n1qx73a","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yeah that&amp;#39;s a much better point ☝️ 😉 &lt;/p&gt;\\n\\n&lt;p&gt;God I want an open source LMSTUDIO&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lt4y1z","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lt4y1z/selfhosted_ai_coding_that_just_works/n1qx73a/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751858300,"author_flair_text":null,"treatment_tags":[],"created_utc":1751858300,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n1qhbme","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"overand","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1p3z5v","score":11,"author_fullname":"t2_328ha","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"&gt; lmstudio is the right choice for all but the most backend dev.\\n\\nIt's also the wrong choice for people who want to use open source tools - LMStudio isn't open source, other than a few components.","edited":false,"author_flair_css_class":null,"name":"t1_n1qhbme","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;lmstudio is the right choice for all but the most backend dev.&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;It&amp;#39;s also the wrong choice for people who want to use open source tools - LMStudio isn&amp;#39;t open source, other than a few components.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lt4y1z","associated_award":null,"stickied":false,"author_premium":true,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lt4y1z/selfhosted_ai_coding_that_just_works/n1qhbme/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751852153,"author_flair_text":null,"collapsed":false,"created_utc":1751852153,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":11}}],"before":null}},"user_reports":[],"saved":false,"id":"n1p3z5v","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":"LOW_SCORE","no_follow":true,"author":"Revolutionalredstone","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1p1w05","score":-7,"author_fullname":"t2_6crrj","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":true,"body":"Your wrong dude, lmstudio does more than host, its a breeze to use and it has things like model search built in.\\n\\nUsing lamacpp may be more pure but it's not an advantage, lmstudio is the right choice for all but the most backend dev.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1p3z5v","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Your wrong dude, lmstudio does more than host, its a breeze to use and it has things like model search built in.&lt;/p&gt;\\n\\n&lt;p&gt;Using lamacpp may be more pure but it&amp;#39;s not an advantage, lmstudio is the right choice for all but the most backend dev.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":"comment score below threshold","distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lt4y1z","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lt4y1z/selfhosted_ai_coding_that_just_works/n1p3z5v/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751835021,"author_flair_text":null,"treatment_tags":[],"created_utc":1751835021,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-7}}],"before":null}},"user_reports":[],"saved":false,"id":"n1p1w05","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Marksta","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1ns7rm","score":8,"author_fullname":"t2_559a1","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"&gt;So far using wrappers means you do not have to think about the implementation\\n\\nI think you're talking about the standard OpenAI compatible API, right? Like, if somehow your Ollama endpoint got swapped with a llama.cpp endpoint, would you suddenly be worrying about the implementation now?\\n\\n&gt;and updates are managed\\n\\nDoes your wrappers not need updates? I mean, probably not unless you're trying something different with some new model anyways and thus you're already in tinker-ing mode, but one way or another updates are a thing.\\n\\nDefinitely applaud the post for discussing real world use, but a non-frontend-using related discussion where you just plug in an API and go vouching for why wrapper X is a really good standard API endpoint is bizarre. I think LM Studio's GUI is beautiful, but I can't see it while I'm coding [or not coding?] in Roo Code.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n1p1w05","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;So far using wrappers means you do not have to think about the implementation&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;I think you&amp;#39;re talking about the standard OpenAI compatible API, right? Like, if somehow your Ollama endpoint got swapped with a llama.cpp endpoint, would you suddenly be worrying about the implementation now?&lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;and updates are managed&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;Does your wrappers not need updates? I mean, probably not unless you&amp;#39;re trying something different with some new model anyways and thus you&amp;#39;re already in tinker-ing mode, but one way or another updates are a thing.&lt;/p&gt;\\n\\n&lt;p&gt;Definitely applaud the post for discussing real world use, but a non-frontend-using related discussion where you just plug in an API and go vouching for why wrapper X is a really good standard API endpoint is bizarre. I think LM Studio&amp;#39;s GUI is beautiful, but I can&amp;#39;t see it while I&amp;#39;m coding [or not coding?] in Roo Code.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lt4y1z","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lt4y1z/selfhosted_ai_coding_that_just_works/n1p1w05/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751834387,"author_flair_text":null,"treatment_tags":[],"created_utc":1751834387,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":8}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1ojl66","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Dudmaster","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1ns7rm","score":2,"author_fullname":"t2_7ff07","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I'm using Roo Code at 20k context but I have a bit more available to use. I use Qwen3, how does it compare with Devstral or GLM? I'm interested in trying both since I just overcame the context length issue\\n\\nEdit: I just tried Devstral and it's great, I am able to run 52k context","edited":1751842270,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n1ojl66","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;m using Roo Code at 20k context but I have a bit more available to use. I use Qwen3, how does it compare with Devstral or GLM? I&amp;#39;m interested in trying both since I just overcame the context length issue&lt;/p&gt;\\n\\n&lt;p&gt;Edit: I just tried Devstral and it&amp;#39;s great, I am able to run 52k context&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lt4y1z","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lt4y1z/selfhosted_ai_coding_that_just_works/n1ojl66/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751828676,"author_flair_text":null,"treatment_tags":[],"created_utc":1751828676,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n1ns7rm","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"send_me_a_ticket","can_mod_post":false,"created_utc":1751820379,"send_replies":true,"parent_id":"t1_n1no3p9","score":14,"author_fullname":"t2_toqw74vmo","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Thanks for your feedback.  \\nI will give Qwen3 0.6b embedding a try, I was not aware of this release.\\n\\nSo far using wrappers means you do not have to think about the implementation, and updates are managed, also LM Studio GUI has been handy for tinkering and debugging. Though, I see your point, using Llama.cpp indeed would reduce a lot of bloat, esp. Ollama is quite huge.\\n\\nRegarding Devstral, I find it worked best for me with tool use, and is just sized to fit under 10 GB VRAM for me. I have tried Gemma3n which keeps forgetting it has tool capability, and Phi4 which hallucinates much frequently.\\n\\nI am not sure of any incompatibility with RooCode, but I find RooCode will need around or over 24576 context (24 GB RAM?) to work well with any AI model.","edited":1751820785,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1ns7rm","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thanks for your feedback.&lt;br/&gt;\\nI will give Qwen3 0.6b embedding a try, I was not aware of this release.&lt;/p&gt;\\n\\n&lt;p&gt;So far using wrappers means you do not have to think about the implementation, and updates are managed, also LM Studio GUI has been handy for tinkering and debugging. Though, I see your point, using Llama.cpp indeed would reduce a lot of bloat, esp. Ollama is quite huge.&lt;/p&gt;\\n\\n&lt;p&gt;Regarding Devstral, I find it worked best for me with tool use, and is just sized to fit under 10 GB VRAM for me. I have tried Gemma3n which keeps forgetting it has tool capability, and Phi4 which hallucinates much frequently.&lt;/p&gt;\\n\\n&lt;p&gt;I am not sure of any incompatibility with RooCode, but I find RooCode will need around or over 24576 context (24 GB RAM?) to work well with any AI model.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lt4y1z","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lt4y1z/selfhosted_ai_coding_that_just_works/n1ns7rm/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751820379,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":14}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1qhs1d","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"cleverusernametry","can_mod_post":false,"created_utc":1751852322,"send_replies":true,"parent_id":"t1_n1no3p9","score":1,"author_fullname":"t2_17bfjs","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yes please - really hoping someone assembles more instructions to migrate from Ollama to llama.cpp","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1qhs1d","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yes please - really hoping someone assembles more instructions to migrate from Ollama to llama.cpp&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lt4y1z","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lt4y1z/selfhosted_ai_coding_that_just_works/n1qhs1d/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751852322,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1s48mh","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Chromix_","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1s24tz","score":2,"author_fullname":"t2_k7w2h","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Having one less component in the flow is an improvement. Your choice fell on LMStudio, a closed-source solution. I'm using llama.cpp instead. Either of them works.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n1s48mh","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Having one less component in the flow is an improvement. Your choice fell on LMStudio, a closed-source solution. I&amp;#39;m using llama.cpp instead. Either of them works.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lt4y1z","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lt4y1z/selfhosted_ai_coding_that_just_works/n1s48mh/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751881300,"author_flair_text":null,"treatment_tags":[],"created_utc":1751881300,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n1s24tz","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"send_me_a_ticket","can_mod_post":false,"created_utc":1751880039,"send_replies":true,"parent_id":"t1_n1no3p9","score":1,"author_fullname":"t2_toqw74vmo","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Hi u/Chromix\\\\_ , I have updated the guide to use only LM Studio for both embeddings and LLMs.  \\nI was misinformed that it is not possible, but tried it just now and it worked without issues.\\n\\nLoading embeddings is slightly obscured in LM Studio, you can only loading embeddings while on \\"Power User\\" tab. This documentation is wrong and should be updated - [https://docs.useanything.com/setup/embedder-configuration/local/lmstudio](https://docs.useanything.com/setup/embedder-configuration/local/lmstudio)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1s24tz","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Hi &lt;a href=\\"/u/Chromix\\"&gt;u/Chromix&lt;/a&gt;_ , I have updated the guide to use only LM Studio for both embeddings and LLMs.&lt;br/&gt;\\nI was misinformed that it is not possible, but tried it just now and it worked without issues.&lt;/p&gt;\\n\\n&lt;p&gt;Loading embeddings is slightly obscured in LM Studio, you can only loading embeddings while on &amp;quot;Power User&amp;quot; tab. This documentation is wrong and should be updated - &lt;a href=\\"https://docs.useanything.com/setup/embedder-configuration/local/lmstudio\\"&gt;https://docs.useanything.com/setup/embedder-configuration/local/lmstudio&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lt4y1z","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lt4y1z/selfhosted_ai_coding_that_just_works/n1s24tz/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751880039,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1ojbf7","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":"LOW_SCORE","no_follow":true,"author":"mantafloppy","can_mod_post":false,"created_utc":1751828592,"send_replies":false,"parent_id":"t1_n1no3p9","score":-14,"author_fullname":"t2_co2hf","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Ollama bad. Qwen good.\\n\\n\\nMe best commenter in the world.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1ojbf7","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Ollama bad. Qwen good.&lt;/p&gt;\\n\\n&lt;p&gt;Me best commenter in the world.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":"comment score below threshold","distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lt4y1z","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lt4y1z/selfhosted_ai_coding_that_just_works/n1ojbf7/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751828592,"author_flair_text":"llama.cpp","treatment_tags":[],"collapsed":true,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-14}}],"before":null}},"user_reports":[],"saved":false,"id":"n1no3p9","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Chromix_","can_mod_post":false,"created_utc":1751819116,"send_replies":true,"parent_id":"t3_1lt4y1z","score":143,"author_fullname":"t2_k7w2h","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"You could replace both LMStudio and ollama with plain llama.cpp here - one less software and one less wrapper that needs to be updated and used. Arctic is a nice and small embedding. In theory the small [Qwen3 0.6b embedding](https://www.reddit.com/r/LocalLLaMA/comments/1l3vt95/new_embedding_model_qwen3embedding06bgguf_just/) should beat it by now, when [used correctly](https://www.reddit.com/r/LocalLLaMA/comments/1lt18hg/are_qwen3_embedding_gguf_faulty/). This might not matter much for small projects as there isn't much to retrieve anyway.\\n\\nAside from that I wonder: Why Devstral instead of another model? It has an extensive default system prompt, been [trained to used OpenHands](https://www.reddit.com/r/LocalLLaMA/comments/1kryybf/comment/mtha570/?context=3), and Roo Code wasn't compatible to that last time I checked.","edited":1751820010,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1no3p9","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You could replace both LMStudio and ollama with plain llama.cpp here - one less software and one less wrapper that needs to be updated and used. Arctic is a nice and small embedding. In theory the small &lt;a href=\\"https://www.reddit.com/r/LocalLLaMA/comments/1l3vt95/new_embedding_model_qwen3embedding06bgguf_just/\\"&gt;Qwen3 0.6b embedding&lt;/a&gt; should beat it by now, when &lt;a href=\\"https://www.reddit.com/r/LocalLLaMA/comments/1lt18hg/are_qwen3_embedding_gguf_faulty/\\"&gt;used correctly&lt;/a&gt;. This might not matter much for small projects as there isn&amp;#39;t much to retrieve anyway.&lt;/p&gt;\\n\\n&lt;p&gt;Aside from that I wonder: Why Devstral instead of another model? It has an extensive default system prompt, been &lt;a href=\\"https://www.reddit.com/r/LocalLLaMA/comments/1kryybf/comment/mtha570/?context=3\\"&gt;trained to used OpenHands&lt;/a&gt;, and Roo Code wasn&amp;#39;t compatible to that last time I checked.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lt4y1z/selfhosted_ai_coding_that_just_works/n1no3p9/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751819116,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lt4y1z","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":143}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1racf8","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"HiddenoO","can_mod_post":false,"created_utc":1751864232,"send_replies":true,"parent_id":"t1_n1ogxd2","score":11,"author_fullname":"t2_8127x","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"People really need to start qualifying what they mean by \\"coding\\".\\n\\nIf all you're doing is creating a cookie-cutter React frontend, even the dumbest models can do a decent job, but the larger and more complex the code base and the less prevalent the language and libraries, the better your coding model has to be.\\n\\nAnd that's just the context, obviously it also matters what tasks you use the models for and what you find acceptable in terms of speed, accuracy, and quality.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1racf8","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;People really need to start qualifying what they mean by &amp;quot;coding&amp;quot;.&lt;/p&gt;\\n\\n&lt;p&gt;If all you&amp;#39;re doing is creating a cookie-cutter React frontend, even the dumbest models can do a decent job, but the larger and more complex the code base and the less prevalent the language and libraries, the better your coding model has to be.&lt;/p&gt;\\n\\n&lt;p&gt;And that&amp;#39;s just the context, obviously it also matters what tasks you use the models for and what you find acceptable in terms of speed, accuracy, and quality.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lt4y1z","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lt4y1z/selfhosted_ai_coding_that_just_works/n1racf8/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751864232,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":11}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1poz1a","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AppearanceHeavy6724","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1ovipu","score":1,"author_fullname":"t2_uz37qfx5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Strange. IQ4 of Mistral Small worked fine as coder in my case.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n1poz1a","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Strange. IQ4 of Mistral Small worked fine as coder in my case.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lt4y1z","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lt4y1z/selfhosted_ai_coding_that_just_works/n1poz1a/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751841952,"author_flair_text":null,"treatment_tags":[],"created_utc":1751841952,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1ovipu","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"JackedInAndAlive","can_mod_post":false,"created_utc":1751832418,"send_replies":true,"parent_id":"t1_n1ogxd2","score":14,"author_fullname":"t2_i1mbyh4q","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"There's no way you can do even casual recreational coding with iq2. I tried Q4_K_M the other day and it was still dumpster fire.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1ovipu","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;There&amp;#39;s no way you can do even casual recreational coding with iq2. I tried Q4_K_M the other day and it was still dumpster fire.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lt4y1z","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lt4y1z/selfhosted_ai_coding_that_just_works/n1ovipu/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751832418,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":14}}],"before":null}},"user_reports":[],"saved":false,"id":"n1ogxd2","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"wekede","can_mod_post":false,"created_utc":1751827853,"send_replies":true,"parent_id":"t3_1lt4y1z","score":14,"author_fullname":"t2_kn9o5","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"tbh I'm quite shocked iq2 works well for you, I'm running q8 devstral but it's slow for my meager hardware\\n\\nwhat are your prompts to this setup like, if you don't mind me asking? prompts where you believe this setup performs well on.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1ogxd2","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;tbh I&amp;#39;m quite shocked iq2 works well for you, I&amp;#39;m running q8 devstral but it&amp;#39;s slow for my meager hardware&lt;/p&gt;\\n\\n&lt;p&gt;what are your prompts to this setup like, if you don&amp;#39;t mind me asking? prompts where you believe this setup performs well on.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lt4y1z/selfhosted_ai_coding_that_just_works/n1ogxd2/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751827853,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lt4y1z","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":14}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1tu4m4","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"nava_7777","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1pn6dl","score":1,"author_fullname":"t2_mdukl","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Exactly","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n1tu4m4","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Exactly&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lt4y1z","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lt4y1z/selfhosted_ai_coding_that_just_works/n1tu4m4/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751904404,"author_flair_text":null,"treatment_tags":[],"created_utc":1751904404,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1pn6dl","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"ark1one","can_mod_post":false,"created_utc":1751841319,"send_replies":true,"parent_id":"t1_n1nnpu9","score":5,"author_fullname":"t2_dlsoz0c","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I read this and was going to respond, \\"they've become anything but free...\\" Then I'm seeing all the clones dropping because of their business practices and I was like... Ohhhh so true....","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1pn6dl","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I read this and was going to respond, &amp;quot;they&amp;#39;ve become anything but free...&amp;quot; Then I&amp;#39;m seeing all the clones dropping because of their business practices and I was like... Ohhhh so true....&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lt4y1z","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lt4y1z/selfhosted_ai_coding_that_just_works/n1pn6dl/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751841319,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}}],"before":null}},"user_reports":[],"saved":false,"id":"n1nnpu9","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"nava_7777","can_mod_post":false,"created_utc":1751819000,"send_replies":true,"parent_id":"t3_1lt4y1z","score":26,"author_fullname":"t2_mdukl","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Really nice post. I love that free Cursor will happen, one way or another","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1nnpu9","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Really nice post. I love that free Cursor will happen, one way or another&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lt4y1z/selfhosted_ai_coding_that_just_works/n1nnpu9/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751819000,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lt4y1z","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":26}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1vnr3m","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"_cadia","can_mod_post":false,"created_utc":1751925139,"send_replies":true,"parent_id":"t1_n1pm8an","score":2,"author_fullname":"t2_c23gxay","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Seems like he is using this docs-mcp-server as RAG. In theory, could you load all it needs to know about your project this way?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1vnr3m","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Seems like he is using this docs-mcp-server as RAG. In theory, could you load all it needs to know about your project this way?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lt4y1z","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lt4y1z/selfhosted_ai_coding_that_just_works/n1vnr3m/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751925139,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n1pm8an","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"RedZero76","can_mod_post":false,"created_utc":1751840981,"send_replies":true,"parent_id":"t3_1lt4y1z","score":18,"author_fullname":"t2_52gystxo","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"How do you get anything done with only 32k context?  And please know, my question sounds like it's picking apart your whole post with a single question, but I truly appreciate your post and the entire, detailed, awesome stack, along with the time you took to share it with everyone!  I'm not meaning to invalidate it in any way with my one question about it.  I just am so curious how you manage to get anything really done with only 32k context, because I've found that I need almost that much just to give my AI the context needed on a project before we even start working.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1pm8an","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;How do you get anything done with only 32k context?  And please know, my question sounds like it&amp;#39;s picking apart your whole post with a single question, but I truly appreciate your post and the entire, detailed, awesome stack, along with the time you took to share it with everyone!  I&amp;#39;m not meaning to invalidate it in any way with my one question about it.  I just am so curious how you manage to get anything really done with only 32k context, because I&amp;#39;ve found that I need almost that much just to give my AI the context needed on a project before we even start working.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lt4y1z/selfhosted_ai_coding_that_just_works/n1pm8an/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751840981,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lt4y1z","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":18}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1rzkx3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ATyp3","can_mod_post":false,"created_utc":1751878481,"send_replies":true,"parent_id":"t1_n1q5l8m","score":1,"author_fullname":"t2_8dsxq","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I used ChatGPT to make a docker compose other day day. It was OpenWebUI Artifacts. Try that maybe?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1rzkx3","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I used ChatGPT to make a docker compose other day day. It was OpenWebUI Artifacts. Try that maybe?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lt4y1z","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lt4y1z/selfhosted_ai_coding_that_just_works/n1rzkx3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751878481,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1q5l8m","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"hideo_kuze_","can_mod_post":false,"created_utc":1751847850,"send_replies":true,"parent_id":"t3_1lt4y1z","score":6,"author_fullname":"t2_47x1vr66","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Anyone brave enough to get all of this in a docker-compose.yaml? :)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1q5l8m","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Anyone brave enough to get all of this in a docker-compose.yaml? :)&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lt4y1z/selfhosted_ai_coding_that_just_works/n1q5l8m/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751847850,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lt4y1z","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1nqnqi","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Helios","can_mod_post":false,"created_utc":1751819903,"send_replies":true,"parent_id":"t3_1lt4y1z","score":3,"author_fullname":"t2_1osh","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Great post, thanks for sharing your configuration!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1nqnqi","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Great post, thanks for sharing your configuration!&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lt4y1z/selfhosted_ai_coding_that_just_works/n1nqnqi/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751819903,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lt4y1z","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1nulhf","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"onil_gova","can_mod_post":false,"created_utc":1751821103,"send_replies":true,"parent_id":"t3_1lt4y1z","score":3,"author_fullname":"t2_vcawomd","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Thanks for sharing. I'm going to try this out. I have been meaning to set up an actually useful local alternative to Cursor for smaller tasks.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1nulhf","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thanks for sharing. I&amp;#39;m going to try this out. I have been meaning to set up an actually useful local alternative to Cursor for smaller tasks.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lt4y1z/selfhosted_ai_coding_that_just_works/n1nulhf/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751821103,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lt4y1z","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1qihim","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"CouldHaveBeenAPun","can_mod_post":false,"created_utc":1751852585,"send_replies":true,"parent_id":"t3_1lt4y1z","score":3,"author_fullname":"t2_1672rs","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I know it is not self hosted, but for the sake of \\"it anyone is interested\\", I do basically all of this, but using free models from openrouter.ai and Gemini 2.5 pro, also still free for now.\\n\\nOpen router and it's free usage model was a game changer for me who doesn't have access to anything better than my Macbook air M2!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1qihim","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I know it is not self hosted, but for the sake of &amp;quot;it anyone is interested&amp;quot;, I do basically all of this, but using free models from openrouter.ai and Gemini 2.5 pro, also still free for now.&lt;/p&gt;\\n\\n&lt;p&gt;Open router and it&amp;#39;s free usage model was a game changer for me who doesn&amp;#39;t have access to anything better than my Macbook air M2!&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lt4y1z/selfhosted_ai_coding_that_just_works/n1qihim/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751852585,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lt4y1z","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1o5ukc","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Anuin","can_mod_post":false,"created_utc":1751824480,"send_replies":true,"parent_id":"t3_1lt4y1z","score":5,"author_fullname":"t2_ch11f","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Great work, thanks! I'm very interested in trying such a setup soon, but I still have some other things in the pipeline first. I hope you don't mind me asking some questions:\\n\\nCould you explain how the second embedding model and MCP are used exactly? Is it a kind of RAG served as an MCP after scraping online docs? Why not use Devstral for the embedding? Shouldn't the embedding model have the same architecture/base as the LLM that uses the information later? What if the LLM just hallucinates a library that does not exist and thus does not have any documentation?\\n\\nAlso, just out of interest, this may be helpful for context: https://deepwiki.com/","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1o5ukc","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Great work, thanks! I&amp;#39;m very interested in trying such a setup soon, but I still have some other things in the pipeline first. I hope you don&amp;#39;t mind me asking some questions:&lt;/p&gt;\\n\\n&lt;p&gt;Could you explain how the second embedding model and MCP are used exactly? Is it a kind of RAG served as an MCP after scraping online docs? Why not use Devstral for the embedding? Shouldn&amp;#39;t the embedding model have the same architecture/base as the LLM that uses the information later? What if the LLM just hallucinates a library that does not exist and thus does not have any documentation?&lt;/p&gt;\\n\\n&lt;p&gt;Also, just out of interest, this may be helpful for context: &lt;a href=\\"https://deepwiki.com/\\"&gt;https://deepwiki.com/&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lt4y1z/selfhosted_ai_coding_that_just_works/n1o5ukc/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751824480,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lt4y1z","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1ntn1w","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ResuTidderTset","can_mod_post":false,"created_utc":1751820812,"send_replies":true,"parent_id":"t3_1lt4y1z","score":2,"author_fullname":"t2_u7eztp0","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Very nice!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1ntn1w","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Very nice!&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lt4y1z/selfhosted_ai_coding_that_just_works/n1ntn1w/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751820812,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lt4y1z","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1s8s9s","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"kkb294","can_mod_post":false,"created_utc":1751883856,"send_replies":true,"parent_id":"t3_1lt4y1z","score":2,"author_fullname":"t2_9u3afpb8q","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Nice, thanks for sharing the detailed setup 😀","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1s8s9s","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Nice, thanks for sharing the detailed setup 😀&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lt4y1z/selfhosted_ai_coding_that_just_works/n1s8s9s/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751883856,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lt4y1z","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"body":"This is a very solid setup","subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1ug66o","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"lxe","can_mod_post":false,"created_utc":1751910804,"send_replies":true,"parent_id":"t3_1lt4y1z","score":2,"author_fullname":"t2_3jfnk","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"author_cakeday":true,"edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1ug66o","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;This is a very solid setup&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":true,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lt4y1z/selfhosted_ai_coding_that_just_works/n1ug66o/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751910804,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lt4y1z","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1r7l0b","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Guilty_Ad_9476","can_mod_post":false,"created_utc":1751862911,"send_replies":true,"parent_id":"t1_n1ojyaw","score":6,"author_fullname":"t2_ti7d2trv0","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"you cant be demanding privacy and not put in the  effort to make it actually private , that being said I think ollama and LM studio could be replaced by llama.cpp so its more like 5 tools now and you'd be using the rest of them in normal VSCode anyways","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1r7l0b","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;you cant be demanding privacy and not put in the  effort to make it actually private , that being said I think ollama and LM studio could be replaced by llama.cpp so its more like 5 tools now and you&amp;#39;d be using the rest of them in normal VSCode anyways&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lt4y1z","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lt4y1z/selfhosted_ai_coding_that_just_works/n1r7l0b/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751862911,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"d2642412-d9ce-11ed-ae30-32b11309f5bd","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1vmqbj","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ortegaalfredo","can_mod_post":false,"created_utc":1751924820,"send_replies":true,"parent_id":"t1_n1ojyaw","score":2,"author_fullname":"t2_g177e","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"It's just VSCode, a plugin and the server.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1vmqbj","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"Alpaca"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It&amp;#39;s just VSCode, a plugin and the server.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lt4y1z","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lt4y1z/selfhosted_ai_coding_that_just_works/n1vmqbj/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751924820,"author_flair_text":"Alpaca","treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":"#bd9e9e","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n1ojyaw","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"ILikeBubblyWater","can_mod_post":false,"created_utc":1751828790,"send_replies":false,"parent_id":"t3_1lt4y1z","score":7,"author_fullname":"t2_5aekl46a","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Just works: Needs 7 different tools","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1ojyaw","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Just works: Needs 7 different tools&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lt4y1z/selfhosted_ai_coding_that_just_works/n1ojyaw/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751828790,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lt4y1z","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":1,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":7}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1xpuof","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AppearanceHeavy6724","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1ubwc3","score":1,"author_fullname":"t2_uz37qfx5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"op has 3080. p104-100 will have to be installed as the second card.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n1xpuof","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;op has 3080. p104-100 will have to be installed as the second card.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lt4y1z","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lt4y1z/selfhosted_ai_coding_that_just_works/n1xpuof/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751951802,"author_flair_text":null,"treatment_tags":[],"created_utc":1751951802,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1ubwc3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"UsualResult","can_mod_post":false,"created_utc":1751909527,"send_replies":true,"parent_id":"t1_n1nroq4","score":1,"author_fullname":"t2_10iarzku","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Does that fit on a single p104-100? I thought the IQ4 quants were like 13GB??","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1ubwc3","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Does that fit on a single p104-100? I thought the IQ4 quants were like 13GB??&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lt4y1z","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lt4y1z/selfhosted_ai_coding_that_just_works/n1ubwc3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751909527,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1omsbo","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"AppearanceHeavy6724","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1ojt73","score":7,"author_fullname":"t2_uz37qfx5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"The op ran his setup with IQ2_M quant, which is normally borderline usable. You do not want to run SDE agent with model this severely compressed; IQ4_XS in my experience is the lowest useable quant. Even then IQ4_XS has often been to much for my taste, and I personally prefer Q4_K_M.","edited":false,"author_flair_css_class":null,"name":"t1_n1omsbo","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The op ran his setup with IQ2_M quant, which is normally borderline usable. You do not want to run SDE agent with model this severely compressed; IQ4_XS in my experience is the lowest useable quant. Even then IQ4_XS has often been to much for my taste, and I personally prefer Q4_K_M.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lt4y1z","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lt4y1z/selfhosted_ai_coding_that_just_works/n1omsbo/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751829665,"author_flair_text":null,"collapsed":false,"created_utc":1751829665,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":7}}],"before":null}},"user_reports":[],"saved":false,"id":"n1ojt73","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"BackgroundAmoebaNine","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1oa8p4","score":1,"author_fullname":"t2_1qxfvuvo","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Huh??","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1ojt73","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Huh??&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lt4y1z","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lt4y1z/selfhosted_ai_coding_that_just_works/n1ojt73/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751828745,"author_flair_text":null,"treatment_tags":[],"created_utc":1751828745,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1oa8p4","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"AppearanceHeavy6724","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1o1s3p","score":7,"author_fullname":"t2_uz37qfx5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Because IQ2 is well IQ2.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n1oa8p4","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Because IQ2 is well IQ2.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lt4y1z","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lt4y1z/selfhosted_ai_coding_that_just_works/n1oa8p4/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751825790,"author_flair_text":null,"treatment_tags":[],"created_utc":1751825790,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":7}}],"before":null}},"user_reports":[],"saved":false,"id":"n1o1s3p","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Kriztoz","can_mod_post":false,"created_utc":1751823263,"send_replies":true,"parent_id":"t1_n1nroq4","score":-1,"author_fullname":"t2_l4sqr","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Why?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1o1s3p","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Why?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lt4y1z","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lt4y1z/selfhosted_ai_coding_that_just_works/n1o1s3p/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751823263,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1nroq4","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"AppearanceHeavy6724","can_mod_post":false,"created_utc":1751820218,"send_replies":true,"parent_id":"t3_1lt4y1z","score":4,"author_fullname":"t2_uz37qfx5","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Shell out $25 for p104-100 and run IQ4 quant of devstral.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1nroq4","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Shell out $25 for p104-100 and run IQ4 quant of devstral.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lt4y1z/selfhosted_ai_coding_that_just_works/n1nroq4/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751820218,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lt4y1z","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1nuvav","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"XertonOne","can_mod_post":false,"created_utc":1751821186,"send_replies":true,"parent_id":"t3_1lt4y1z","score":2,"author_fullname":"t2_1n8h2fiw7d","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Really excellent post. Thank you for taking the time.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1nuvav","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Really excellent post. Thank you for taking the time.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lt4y1z/selfhosted_ai_coding_that_just_works/n1nuvav/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751821186,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lt4y1z","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1q37tl","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"IssueConnect7471","can_mod_post":false,"created_utc":1751846975,"send_replies":true,"parent_id":"t1_n1nq4er","score":5,"author_fullname":"t2_1r56gm7bpe","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"My take: containerize each model with vLLM so you can hot-swap weights without killing requests, then bolt docs-mcp-server in front for grounded code hints. I tried vLLM and Triton, but [APIWrapper.ai](http://APIWrapper.ai) ended up handling auth throttling and usage metrics without extra boilerplate. Set routing at nginx, point RooCode to the gateway, and expose an /embeddings endpoint that proxies to snowflake-arctic for smaller GPUs. Keep a shared token cache in redis to dodge cold starts. Keeping everything containerized with per-model volumes keeps reload times low and lets you tweak easily.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1q37tl","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;My take: containerize each model with vLLM so you can hot-swap weights without killing requests, then bolt docs-mcp-server in front for grounded code hints. I tried vLLM and Triton, but &lt;a href=\\"http://APIWrapper.ai\\"&gt;APIWrapper.ai&lt;/a&gt; ended up handling auth throttling and usage metrics without extra boilerplate. Set routing at nginx, point RooCode to the gateway, and expose an /embeddings endpoint that proxies to snowflake-arctic for smaller GPUs. Keep a shared token cache in redis to dodge cold starts. Keeping everything containerized with per-model volumes keeps reload times low and lets you tweak easily.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lt4y1z","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lt4y1z/selfhosted_ai_coding_that_just_works/n1q37tl/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751846975,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}}],"before":null}},"user_reports":[],"saved":false,"id":"n1nq4er","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AbortedFajitas","can_mod_post":false,"created_utc":1751819738,"send_replies":true,"parent_id":"t3_1lt4y1z","score":1,"author_fullname":"t2_3hv9p","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I run an inference network and aim to provide it for free and very cheap to consumers. We run open source LLM models and video/image gen models and frameworks. I keep dreaming of setting up a vibe coding stack that works well and can be powered by our API. great work!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1nq4er","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I run an inference network and aim to provide it for free and very cheap to consumers. We run open source LLM models and video/image gen models and frameworks. I keep dreaming of setting up a vibe coding stack that works well and can be powered by our API. great work!&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lt4y1z/selfhosted_ai_coding_that_just_works/n1nq4er/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751819738,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lt4y1z","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1rqmmb","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"CatEatsDogs","can_mod_post":false,"created_utc":1751873097,"send_replies":true,"parent_id":"t1_n1nt90u","score":2,"author_fullname":"t2_1lkw52frej","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Look for Context7","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1rqmmb","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Look for Context7&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lt4y1z","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lt4y1z/selfhosted_ai_coding_that_just_works/n1rqmmb/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751873097,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n1nt90u","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Pedalnomica","can_mod_post":false,"created_utc":1751820693,"send_replies":true,"parent_id":"t3_1lt4y1z","score":1,"author_fullname":"t2_b0d7j6x9","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I'd been thinking something like the docs MCP server might help cut down on coding hallucinations. Glad to hear someone already built it!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1nt90u","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;d been thinking something like the docs MCP server might help cut down on coding hallucinations. Glad to hear someone already built it!&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lt4y1z/selfhosted_ai_coding_that_just_works/n1nt90u/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751820693,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lt4y1z","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1ofsct","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"HornyGooner4401","can_mod_post":false,"created_utc":1751827499,"send_replies":true,"parent_id":"t3_1lt4y1z","score":1,"author_fullname":"t2_1fetkxu1xm","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"You can't pass problems as context in Zed, but you can tell it to check the diagnostics manually","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1ofsct","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You can&amp;#39;t pass problems as context in Zed, but you can tell it to check the diagnostics manually&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lt4y1z/selfhosted_ai_coding_that_just_works/n1ofsct/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751827499,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lt4y1z","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1pqluo","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Turkino","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1pouoy","score":1,"author_fullname":"t2_ai06o","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"excellent, thanks!","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n1pqluo","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;excellent, thanks!&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lt4y1z","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lt4y1z/selfhosted_ai_coding_that_just_works/n1pqluo/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751842516,"author_flair_text":null,"treatment_tags":[],"created_utc":1751842516,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1pouoy","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"popsumbong","can_mod_post":false,"created_utc":1751841910,"send_replies":true,"parent_id":"t1_n1pmfn4","score":2,"author_fullname":"t2_xq83l","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"If you're referring to the mcp-server json, that goes in roocode's mcp_settings.json","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1pouoy","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;If you&amp;#39;re referring to the mcp-server json, that goes in roocode&amp;#39;s mcp_settings.json&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lt4y1z","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lt4y1z/selfhosted_ai_coding_that_just_works/n1pouoy/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751841910,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n1pmfn4","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Turkino","can_mod_post":false,"created_utc":1751841054,"send_replies":true,"parent_id":"t3_1lt4y1z","score":1,"author_fullname":"t2_ai06o","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"VERY beginner question as I've not set up an MCP server before.  \\nWhere does that json config for docker go?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1pmfn4","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;VERY beginner question as I&amp;#39;ve not set up an MCP server before.&lt;br/&gt;\\nWhere does that json config for docker go?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lt4y1z/selfhosted_ai_coding_that_just_works/n1pmfn4/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751841054,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lt4y1z","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1porfw","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Brave-Car-9482","can_mod_post":false,"created_utc":1751841878,"send_replies":true,"parent_id":"t3_1lt4y1z","score":1,"author_fullname":"t2_c9k3mn0b","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Cool i am ganna try this","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1porfw","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Cool i am ganna try this&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lt4y1z/selfhosted_ai_coding_that_just_works/n1porfw/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751841878,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lt4y1z","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1ptuj6","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MumeiNoName","can_mod_post":false,"created_utc":1751843638,"send_replies":true,"parent_id":"t3_1lt4y1z","score":1,"author_fullname":"t2_bki8njpu","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Thanks so much exactly what I was looking for. Will read it tonight","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1ptuj6","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thanks so much exactly what I was looking for. Will read it tonight&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lt4y1z/selfhosted_ai_coding_that_just_works/n1ptuj6/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751843638,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lt4y1z","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1qr2qz","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Ylsid","can_mod_post":false,"created_utc":1751855834,"send_replies":true,"parent_id":"t3_1lt4y1z","score":1,"author_fullname":"t2_6lmlc","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"This is a really great experiment. I wasn't sure it was even possible to work well. I'm looking forward to seeing how people refine this cumbersome workflow.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1qr2qz","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;This is a really great experiment. I wasn&amp;#39;t sure it was even possible to work well. I&amp;#39;m looking forward to seeing how people refine this cumbersome workflow.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lt4y1z/selfhosted_ai_coding_that_just_works/n1qr2qz/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751855834,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lt4y1z","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1qu5ry","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Awkward_Sympathy4475","can_mod_post":false,"created_utc":1751857058,"send_replies":true,"parent_id":"t3_1lt4y1z","score":1,"author_fullname":"t2_1j39ni9rp1","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Whats the speed like.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1qu5ry","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Whats the speed like.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lt4y1z/selfhosted_ai_coding_that_just_works/n1qu5ry/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751857058,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lt4y1z","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1rbypl","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Agreeable-Prompt-666","can_mod_post":false,"created_utc":1751865046,"send_replies":true,"parent_id":"t3_1lt4y1z","score":1,"author_fullname":"t2_1l3z4stvkq","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Awesome write-up, thanks for putting this together. Holy moly lots of moving parts anxiety:)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1rbypl","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Awesome write-up, thanks for putting this together. Holy moly lots of moving parts anxiety:)&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lt4y1z/selfhosted_ai_coding_that_just_works/n1rbypl/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751865046,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lt4y1z","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1s28o3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"robberviet","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1s1tx0","score":1,"author_fullname":"t2_jxc5a","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Strange again: Why do you use docs of anythingLLM for LMStudio? Anw, thank for the update.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n1s28o3","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Strange again: Why do you use docs of anythingLLM for LMStudio? Anw, thank for the update.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lt4y1z","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lt4y1z/selfhosted_ai_coding_that_just_works/n1s28o3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751880105,"author_flair_text":null,"treatment_tags":[],"created_utc":1751880105,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1s1tx0","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"send_me_a_ticket","can_mod_post":false,"created_utc":1751879853,"send_replies":true,"parent_id":"t1_n1rz0c4","score":1,"author_fullname":"t2_toqw74vmo","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Hello u/robberviet, I understand your confusion, I was misinformed that it is not possible to run both LLM and Embeddings via LM Studio, which is why I went to Ollama.\\n\\nTurns out you can, but it is slightly obscured in LM Studio, you can only loading embeddings while on \\"Developer\\" tab. When experimenting, I came across this documentation and just assumed it to be true, and that I would need to run embeddings another way.\\n\\nThis documentation is wrong and should be updated - [https://docs.useanything.com/setup/embedder-configuration/local/lmstudio](https://docs.useanything.com/setup/embedder-configuration/local/lmstudio)","edited":1751882193,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1s1tx0","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Hello &lt;a href=\\"/u/robberviet\\"&gt;u/robberviet&lt;/a&gt;, I understand your confusion, I was misinformed that it is not possible to run both LLM and Embeddings via LM Studio, which is why I went to Ollama.&lt;/p&gt;\\n\\n&lt;p&gt;Turns out you can, but it is slightly obscured in LM Studio, you can only loading embeddings while on &amp;quot;Developer&amp;quot; tab. When experimenting, I came across this documentation and just assumed it to be true, and that I would need to run embeddings another way.&lt;/p&gt;\\n\\n&lt;p&gt;This documentation is wrong and should be updated - &lt;a href=\\"https://docs.useanything.com/setup/embedder-configuration/local/lmstudio\\"&gt;https://docs.useanything.com/setup/embedder-configuration/local/lmstudio&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lt4y1z","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lt4y1z/selfhosted_ai_coding_that_just_works/n1s1tx0/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751879853,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1rz0c4","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"robberviet","can_mod_post":false,"created_utc":1751878145,"send_replies":true,"parent_id":"t3_1lt4y1z","score":1,"author_fullname":"t2_jxc5a","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Still not understand why you need both Ollama and LMStudio, and why ollama just for \\\\\`\`snowflake-arctic-embed2\`, I had bugs with Ollama embedding and looks like still is: [https://github.com/ollama/ollama/issues/6094](https://github.com/ollama/ollama/issues/6094)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1rz0c4","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Still not understand why you need both Ollama and LMStudio, and why ollama just for \`&lt;code&gt;snowflake-arctic-embed2&lt;/code&gt;, I had bugs with Ollama embedding and looks like still is: &lt;a href=\\"https://github.com/ollama/ollama/issues/6094\\"&gt;https://github.com/ollama/ollama/issues/6094&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lt4y1z/selfhosted_ai_coding_that_just_works/n1rz0c4/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751878145,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lt4y1z","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1uiu02","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Spirited_Example_341","can_mod_post":false,"created_utc":1751911618,"send_replies":true,"parent_id":"t3_1lt4y1z","score":1,"author_fullname":"t2_122x8ksifg","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"im not kidding - Todd Howard","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1uiu02","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;im not kidding - Todd Howard&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lt4y1z/selfhosted_ai_coding_that_just_works/n1uiu02/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751911618,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lt4y1z","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"d2642412-d9ce-11ed-ae30-32b11309f5bd","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1vmxxp","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ortegaalfredo","can_mod_post":false,"created_utc":1751924886,"send_replies":true,"parent_id":"t3_1lt4y1z","score":1,"author_fullname":"t2_g177e","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I use this setup but connected to VLLM+Qwen3-32B, and I see no difference in speed or capabilities compared to the free version of Cursor, in fact the gui is better IMHO.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1vmxxp","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"Alpaca"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I use this setup but connected to VLLM+Qwen3-32B, and I see no difference in speed or capabilities compared to the free version of Cursor, in fact the gui is better IMHO.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lt4y1z/selfhosted_ai_coding_that_just_works/n1vmxxp/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751924886,"author_flair_text":"Alpaca","treatment_tags":[],"link_id":"t3_1lt4y1z","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bd9e9e","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1wkshg","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ii_social","can_mod_post":false,"created_utc":1751935941,"send_replies":true,"parent_id":"t3_1lt4y1z","score":1,"author_fullname":"t2_tohvxz80x","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Thanks for the great post!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1wkshg","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thanks for the great post!&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lt4y1z/selfhosted_ai_coding_that_just_works/n1wkshg/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751935941,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lt4y1z","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1xf8u1","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Superb_Intention2783","can_mod_post":false,"created_utc":1751946999,"send_replies":true,"parent_id":"t3_1lt4y1z","score":1,"author_fullname":"t2_7sorxm7k","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Great Thanks","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1xf8u1","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Great Thanks&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lt4y1z/selfhosted_ai_coding_that_just_works/n1xf8u1/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751946999,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lt4y1z","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1ogq0t","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"vegatx40","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1o2zsp","score":3,"author_fullname":"t2_18dhiarv40","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Nope just chats. Nearly instant. Faster than Gemini CLI","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n1ogq0t","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Nope just chats. Nearly instant. Faster than Gemini CLI&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lt4y1z","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lt4y1z/selfhosted_ai_coding_that_just_works/n1ogq0t/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751827789,"author_flair_text":null,"treatment_tags":[],"created_utc":1751827789,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n1o2zsp","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Hekel1989","can_mod_post":false,"created_utc":1751823631,"send_replies":true,"parent_id":"t1_n1nv7m3","score":1,"author_fullname":"t2_gvd6f","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"What's the time per answer with your 4090? I'm assuming you're talking about agentic mode.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1o2zsp","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;What&amp;#39;s the time per answer with your 4090? I&amp;#39;m assuming you&amp;#39;re talking about agentic mode.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lt4y1z","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lt4y1z/selfhosted_ai_coding_that_just_works/n1o2zsp/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751823631,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1nv7m3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"vegatx40","can_mod_post":false,"created_utc":1751821289,"send_replies":true,"parent_id":"t3_1lt4y1z","score":1,"author_fullname":"t2_18dhiarv40","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"cool. I am just running vscode with Copilot pointed at Ollama deepseek-coder:33b on my rtx4090. very happy! deepseek feels a bit better than either devstral or codestral (one of which just gives you answers, doesn't explain)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1nv7m3","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;cool. I am just running vscode with Copilot pointed at Ollama deepseek-coder:33b on my rtx4090. very happy! deepseek feels a bit better than either devstral or codestral (one of which just gives you answers, doesn&amp;#39;t explain)&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lt4y1z/selfhosted_ai_coding_that_just_works/n1nv7m3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751821289,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lt4y1z","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1qaq74","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"apel-sin","can_mod_post":false,"created_utc":1751849753,"send_replies":true,"parent_id":"t3_1lt4y1z","score":1,"author_fullname":"t2_9gr3xlu1","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Hi! Thanx for sharing pipeline! This proxy might help you collect all your access points in one place :)  \\n[https://github.com/kreolsky/llm-router-api](https://github.com/kreolsky/llm-router-api)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1qaq74","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Hi! Thanx for sharing pipeline! This proxy might help you collect all your access points in one place :)&lt;br/&gt;\\n&lt;a href=\\"https://github.com/kreolsky/llm-router-api\\"&gt;https://github.com/kreolsky/llm-router-api&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lt4y1z/selfhosted_ai_coding_that_just_works/n1qaq74/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751849753,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lt4y1z","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1nxsdq","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"doc-acula","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1nx9t2","score":1,"author_fullname":"t2_uznbm2nk","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Thanks, I wasn't aware of that. And yes, I use VSCodium instead of VS Code already.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n1nxsdq","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thanks, I wasn&amp;#39;t aware of that. And yes, I use VSCodium instead of VS Code already.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lt4y1z","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lt4y1z/selfhosted_ai_coding_that_just_works/n1nxsdq/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751822065,"author_flair_text":null,"treatment_tags":[],"created_utc":1751822065,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1nx9t2","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"send_me_a_ticket","can_mod_post":false,"created_utc":1751821912,"send_replies":true,"parent_id":"t1_n1ntd9s","score":3,"author_fullname":"t2_toqw74vmo","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Hi u/doc-acula, I have indeed tried Void editor, it is promising, but still has a long way to go.  \\nZed editor is much ahead in terms of finish, but Void benefits from the vast vscode marketplace that Zed misses out on.\\n\\nStill, being able to pass \\\\\`@problems\\\\\` as context is reason enough to be using RooCode, which can be added to Void anyway.\\n\\nIt is certainly something to keep an eye on, it already does agentic coding, and I believe lightly than RooCode, so if RooCode doesn't work well for someone, Void may be a better fit, and maybe one day it can replace VSCode as primary code editor.\\n\\nI would recommend this as alternative to VSCode but seems like for privacy-minded folks, VSCodium is still a better choice. (https://github.com/voideditor/void/issues/764)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1nx9t2","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Hi &lt;a href=\\"/u/doc-acula\\"&gt;u/doc-acula&lt;/a&gt;, I have indeed tried Void editor, it is promising, but still has a long way to go.&lt;br/&gt;\\nZed editor is much ahead in terms of finish, but Void benefits from the vast vscode marketplace that Zed misses out on.&lt;/p&gt;\\n\\n&lt;p&gt;Still, being able to pass \`@problems\` as context is reason enough to be using RooCode, which can be added to Void anyway.&lt;/p&gt;\\n\\n&lt;p&gt;It is certainly something to keep an eye on, it already does agentic coding, and I believe lightly than RooCode, so if RooCode doesn&amp;#39;t work well for someone, Void may be a better fit, and maybe one day it can replace VSCode as primary code editor.&lt;/p&gt;\\n\\n&lt;p&gt;I would recommend this as alternative to VSCode but seems like for privacy-minded folks, VSCodium is still a better choice. (&lt;a href=\\"https://github.com/voideditor/void/issues/764\\"&gt;https://github.com/voideditor/void/issues/764&lt;/a&gt;)&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lt4y1z","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lt4y1z/selfhosted_ai_coding_that_just_works/n1nx9t2/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751821912,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n1ntd9s","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"doc-acula","can_mod_post":false,"created_utc":1751820728,"send_replies":true,"parent_id":"t3_1lt4y1z","score":-3,"author_fullname":"t2_uznbm2nk","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"What do you think about void? [https://voideditor.com/](https://voideditor.com/)\\n\\nIt is a fork of VS Code and has llm chat/coding and mcp integrated. I am only very casually coding, so I am not sure if it fits your needs. But please comment on disadventages of void over other solutions. I think it is quite solid and makes things comfortable.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1ntd9s","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;What do you think about void? &lt;a href=\\"https://voideditor.com/\\"&gt;https://voideditor.com/&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;It is a fork of VS Code and has llm chat/coding and mcp integrated. I am only very casually coding, so I am not sure if it fits your needs. But please comment on disadventages of void over other solutions. I think it is quite solid and makes things comfortable.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lt4y1z/selfhosted_ai_coding_that_just_works/n1ntd9s/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751820728,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lt4y1z","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-3}}],"before":null}}]`),r=()=>e.jsx(l,{data:t});export{r as default};
