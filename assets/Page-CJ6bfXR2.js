import{j as e}from"./index-xfnGEtuL.js";import{R as l}from"./RedditPostRenderer-DAZRIZZK.js";import"./index-BaNn5-RR.js";const a=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"I know this is a stupid question, but how can I find out which 8b models are the strongest for math or coding (in python)?  \\n\\nReally I want the strongest model that fits in 16GB of RAM.","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Newbie question, how do I see which 8b models are the strongest at math or coding?","link_flair_richtext":[{"e":"text","t":"Discussion"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1m3oma3","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.66,"author_flair_background_color":null,"subreddit_type":"public","ups":4,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_sm168dt0h","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Discussion","can_mod_post":false,"score":4,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":1752904638,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1752904136,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;I know this is a stupid question, but how can I find out which 8b models are the strongest for math or coding (in python)?  &lt;/p&gt;\\n\\n&lt;p&gt;Really I want the strongest model that fits in 16GB of RAM.&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"5f921ea4-c7bc-11ed-9c23-3a00622979b4","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#646d73","id":"1m3oma3","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"MrMrsPotts","discussion_type":null,"num_comments":9,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1m3oma3/newbie_question_how_do_i_see_which_8b_models_are/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1m3oma3/newbie_question_how_do_i_see_which_8b_models_are/","subreddit_subscribers":501752,"created_utc":1752904136,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"50c36eba-fdca-11ee-9735-92a88d7e3b87","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n408dj4","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"iKy1e","can_mod_post":false,"created_utc":1752937438,"send_replies":true,"parent_id":"t3_1m3oma3","score":2,"author_fullname":"t2_aqewd","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"It’s a lame answer but it’s normally the newest ones. Maths performance is one of the main benchmarks they compare themselves to on release, as it’s easy to score and fairly objective.\\n\\nThe other answer is to look at the models new releases compare themselves too.\\n\\nAs for what’s best at the moment. For maths specifically reasoning models which can &lt;think&gt; score way higher, they get a chance to fix mistakes and do questions step by step.\\n\\nReasoning models which can execute Python code is another ‘cheat’ which boosts model performance massively, but that also needs the tools setup to work.\\n\\nA good base at the moment is Qwen3 8b, though there is probably a math specific fine tune I’ve missed that’s been released since.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n408dj4","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"Ollama"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It’s a lame answer but it’s normally the newest ones. Maths performance is one of the main benchmarks they compare themselves to on release, as it’s easy to score and fairly objective.&lt;/p&gt;\\n\\n&lt;p&gt;The other answer is to look at the models new releases compare themselves too.&lt;/p&gt;\\n\\n&lt;p&gt;As for what’s best at the moment. For maths specifically reasoning models which can &amp;lt;think&amp;gt; score way higher, they get a chance to fix mistakes and do questions step by step.&lt;/p&gt;\\n\\n&lt;p&gt;Reasoning models which can execute Python code is another ‘cheat’ which boosts model performance massively, but that also needs the tools setup to work.&lt;/p&gt;\\n\\n&lt;p&gt;A good base at the moment is Qwen3 8b, though there is probably a math specific fine tune I’ve missed that’s been released since.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3oma3/newbie_question_how_do_i_see_which_8b_models_are/n408dj4/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752937438,"author_flair_text":"Ollama","treatment_tags":[],"link_id":"t3_1m3oma3","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n409mm5","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"tmvr","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3zt7ej","score":1,"author_fullname":"t2_11qlhv","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"There is nothing like what you are looking for. We are still waiting for Qwen3 Coder, the best for coding, especially at the sizes you can run, is still Qwen2.5 Coder.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n409mm5","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;There is nothing like what you are looking for. We are still waiting for Qwen3 Coder, the best for coding, especially at the sizes you can run, is still Qwen2.5 Coder.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m3oma3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3oma3/newbie_question_how_do_i_see_which_8b_models_are/n409mm5/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752937828,"author_flair_text":null,"treatment_tags":[],"created_utc":1752937828,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3zt7ej","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"wooden-guy","can_mod_post":false,"created_utc":1752932516,"send_replies":true,"parent_id":"t1_n3ykszq","score":0,"author_fullname":"t2_16to413y2o","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"This is a very stupid question, I'm not that deep in the llm world, but why wouldn't you say go with a fine tune for coding using the newer qwen 3 models or llama and those generally newer models.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3zt7ej","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;This is a very stupid question, I&amp;#39;m not that deep in the llm world, but why wouldn&amp;#39;t you say go with a fine tune for coding using the newer qwen 3 models or llama and those generally newer models.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m3oma3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3oma3/newbie_question_how_do_i_see_which_8b_models_are/n3zt7ej/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752932516,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}},"user_reports":[],"saved":false,"id":"n3ykszq","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"tmvr","can_mod_post":false,"created_utc":1752911459,"send_replies":true,"parent_id":"t3_1m3oma3","score":3,"author_fullname":"t2_11qlhv","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"For coding I'd still stick to Qwen2.5 Coder 7B which you can run at Q8 with 16GB VRAM, but with that much VRAM you can also go with Qwen2.5 Coder 14B at Q6 (12GB) or Q4 (9GB) because the larger model gives noticeably better results.","edited":1752922809,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3ykszq","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;For coding I&amp;#39;d still stick to Qwen2.5 Coder 7B which you can run at Q8 with 16GB VRAM, but with that much VRAM you can also go with Qwen2.5 Coder 14B at Q6 (12GB) or Q4 (9GB) because the larger model gives noticeably better results.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3oma3/newbie_question_how_do_i_see_which_8b_models_are/n3ykszq/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752911459,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m3oma3","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3yc4si","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DorphinPack","can_mod_post":false,"created_utc":1752906640,"send_replies":true,"parent_id":"t3_1m3oma3","score":1,"author_fullname":"t2_zebuyjw9s","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"There isn’t a real “strength” measurement. Smaller models can excel if specialized. Larger models can be more generically useful. You’ll find leaderboards but they need to be read with a huge grain of salt. They’re only as good as the evaluations and plenty of them are meh to say the least.\\n\\nIt all starts with your problem space and data sets. Everything is workload dependent.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3yc4si","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;There isn’t a real “strength” measurement. Smaller models can excel if specialized. Larger models can be more generically useful. You’ll find leaderboards but they need to be read with a huge grain of salt. They’re only as good as the evaluations and plenty of them are meh to say the least.&lt;/p&gt;\\n\\n&lt;p&gt;It all starts with your problem space and data sets. Everything is workload dependent.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3oma3/newbie_question_how_do_i_see_which_8b_models_are/n3yc4si/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752906640,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m3oma3","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3y9cmf","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ArchdukeofHyperbole","can_mod_post":false,"created_utc":1752905153,"send_replies":true,"parent_id":"t3_1m3oma3","score":0,"author_fullname":"t2_1p41v97q5d","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Maybe there's a leaderboard for math somewhere. Did you try mathstral?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3y9cmf","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Maybe there&amp;#39;s a leaderboard for math somewhere. Did you try mathstral?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3oma3/newbie_question_how_do_i_see_which_8b_models_are/n3y9cmf/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752905153,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m3oma3","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}},{"kind":"t1","data":{"total_awards_received":0,"approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"ups":-1,"removal_reason":null,"link_id":"t3_1m3oma3","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3yeo2e","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"FunnyAsparagus1253","can_mod_post":false,"created_utc":1752908023,"send_replies":true,"parent_id":"t1_n3y899w","score":2,"author_fullname":"t2_i6c8tay3w","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Sillyish comment but username checks out lol","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3yeo2e","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Sillyish comment but username checks out lol&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m3oma3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3oma3/newbie_question_how_do_i_see_which_8b_models_are/n3yeo2e/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752908023,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n3y899w","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":"DELETED","no_follow":true,"author":"[deleted]","can_mod_post":false,"send_replies":true,"parent_id":"t3_1m3oma3","score":-1,"approved_by":null,"report_reasons":null,"all_awardings":[],"subreddit_id":"t5_81eyvm","body":"[deleted]","edited":false,"downs":0,"author_flair_css_class":null,"collapsed":true,"is_submitter":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;[deleted]&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"associated_award":null,"stickied":false,"subreddit_type":"public","can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":"dark","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3oma3/newbie_question_how_do_i_see_which_8b_models_are/n3y899w/","num_reports":null,"locked":false,"name":"t1_n3y899w","created":1752904580,"subreddit":"LocalLLaMA","author_flair_text":null,"treatment_tags":[],"created_utc":1752904580,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":1,"depth":0,"author_flair_background_color":"","collapsed_because_crowd_control":null,"mod_reports":[],"mod_note":null,"distinguished":null}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n40y82d","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ArsNeph","can_mod_post":false,"created_utc":1752945461,"send_replies":true,"parent_id":"t3_1m3oma3","score":2,"author_fullname":"t2_vt0xkv60d","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Probably Qwen 3 8B at 8 bit, or you could squeeze in Qwen 3 14B at a lower quant but it would be pretty slow. Qwen 3 30B MoE would be ideal, but I don't think you have enough RAM to run a decent quant","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n40y82d","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Probably Qwen 3 8B at 8 bit, or you could squeeze in Qwen 3 14B at a lower quant but it would be pretty slow. Qwen 3 30B MoE would be ideal, but I don&amp;#39;t think you have enough RAM to run a decent quant&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3oma3/newbie_question_how_do_i_see_which_8b_models_are/n40y82d/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752945461,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m3oma3","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}}]`),s=()=>e.jsx(l,{data:a});export{s as default};
