import{j as e}from"./index-DLSqWzaI.js";import{R as l}from"./RedditPostRenderer-CysRo2D_.js";import"./index-COXiL3Lo.js";const t=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"## Edit 1\\nI want to reiterate this is not using llama cpp. This does not appear like an inference engine specific problem because I have tried with multiple different inference engines [vLLM, infinity-embed, HuggingFace TEI] and even sentence_transformers. \\n\\n\\n## Background &amp; Brief Setup\\nWe need a robust intent/sentiment classification and RAG pipeline, for which we plan on using embeddings, for a latency sensitive consumer facing product. We are planning to deploy a small embedding model on a inference optimized GCE VM for the same. \\n\\nI am currently running TEI (by HuggingFace) using the official docker image from the repo for inference [output identical with vLLM and infinity-embed]. Using OpenAI python client [results are no different if I switch to direct http requests].\\n\\n**Model** : Qwen 3 Embeddings 0.6B [should not matter but _downloaded locally_]\\n\\nNot using any custom instructions or prompts with the embedding since we are creating clusters for our semantic search. We were earlier using BAAI/bge-m3 which was giving good results.\\n\\n## Problem\\n \\nLike I don't know how to put this, but the embeddings feel really.. 'bad'? Like same sentence with capitalization and without capitalization have a lower similarity score. Does not work with our existing query clusters which used to capture the intents and semantic meaning of each query quite well. Capitalization changes everything. Clustering followed by BAAI/bge-m3 used to give fantastic results. Qwen3 is routing plain wrong. I can't understand what am I doing wrong. The models are so high up on MTEB and seem to excel at all aspects so I am flabbergasted.\\n\\n## Questions\\n\\nIs there something obvious I am missing here?\\n\\nHas someone else faced similar issues with Qwen3 Embeddings?\\n\\nAre embeddings tuned for instructions fundamentally different from 'normal' embedding models in any way? \\n\\nAre there any embedding models less than 1B parameters, that are multilingual and not trained with anglosphere centric data, with demonstrated track record in semantic clustering, that I can use for semantic clustering?","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Qwen 3 Embeddings 0.6B faring really poorly inspite of high score on benchmarks","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lxvf0j","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.99,"author_flair_background_color":null,"subreddit_type":"public","ups":42,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_alrxvbt1","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":42,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":1752409332,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1752308700,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;h2&gt;Edit 1&lt;/h2&gt;\\n\\n&lt;p&gt;I want to reiterate this is not using llama cpp. This does not appear like an inference engine specific problem because I have tried with multiple different inference engines [vLLM, infinity-embed, HuggingFace TEI] and even sentence_transformers. &lt;/p&gt;\\n\\n&lt;h2&gt;Background &amp;amp; Brief Setup&lt;/h2&gt;\\n\\n&lt;p&gt;We need a robust intent/sentiment classification and RAG pipeline, for which we plan on using embeddings, for a latency sensitive consumer facing product. We are planning to deploy a small embedding model on a inference optimized GCE VM for the same. &lt;/p&gt;\\n\\n&lt;p&gt;I am currently running TEI (by HuggingFace) using the official docker image from the repo for inference [output identical with vLLM and infinity-embed]. Using OpenAI python client [results are no different if I switch to direct http requests].&lt;/p&gt;\\n\\n&lt;p&gt;&lt;strong&gt;Model&lt;/strong&gt; : Qwen 3 Embeddings 0.6B [should not matter but &lt;em&gt;downloaded locally&lt;/em&gt;]&lt;/p&gt;\\n\\n&lt;p&gt;Not using any custom instructions or prompts with the embedding since we are creating clusters for our semantic search. We were earlier using BAAI/bge-m3 which was giving good results.&lt;/p&gt;\\n\\n&lt;h2&gt;Problem&lt;/h2&gt;\\n\\n&lt;p&gt;Like I don&amp;#39;t know how to put this, but the embeddings feel really.. &amp;#39;bad&amp;#39;? Like same sentence with capitalization and without capitalization have a lower similarity score. Does not work with our existing query clusters which used to capture the intents and semantic meaning of each query quite well. Capitalization changes everything. Clustering followed by BAAI/bge-m3 used to give fantastic results. Qwen3 is routing plain wrong. I can&amp;#39;t understand what am I doing wrong. The models are so high up on MTEB and seem to excel at all aspects so I am flabbergasted.&lt;/p&gt;\\n\\n&lt;h2&gt;Questions&lt;/h2&gt;\\n\\n&lt;p&gt;Is there something obvious I am missing here?&lt;/p&gt;\\n\\n&lt;p&gt;Has someone else faced similar issues with Qwen3 Embeddings?&lt;/p&gt;\\n\\n&lt;p&gt;Are embeddings tuned for instructions fundamentally different from &amp;#39;normal&amp;#39; embedding models in any way? &lt;/p&gt;\\n\\n&lt;p&gt;Are there any embedding models less than 1B parameters, that are multilingual and not trained with anglosphere centric data, with demonstrated track record in semantic clustering, that I can use for semantic clustering?&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1lxvf0j","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"i4858i","discussion_type":null,"num_comments":24,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lxvf0j/qwen_3_embeddings_06b_faring_really_poorly/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lxvf0j/qwen_3_embeddings_06b_faring_really_poorly/","subreddit_subscribers":498346,"created_utc":1752308700,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2t3a53","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"uber-linny","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2rgnv3","score":1,"author_fullname":"t2_14166b","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Nice idea about the -small ,,, I dont use it for story telling , i want it pull specific data. what ive been doing is asking it 10-20 questions , review the output (since i dont know how to measure tokens in LM studio)  and throw the whole chat into Google Studio to verify what i think is correct. \\n\\nInteresting observation for me , was that the Q4k\\\\_m performed better than the Q8. the Q4k\\\\_M would build tables and format it how how i would.","edited":false,"author_flair_css_class":null,"name":"t1_n2t3a53","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Nice idea about the -small ,,, I dont use it for story telling , i want it pull specific data. what ive been doing is asking it 10-20 questions , review the output (since i dont know how to measure tokens in LM studio)  and throw the whole chat into Google Studio to verify what i think is correct. &lt;/p&gt;\\n\\n&lt;p&gt;Interesting observation for me , was that the Q4k_m performed better than the Q8. the Q4k_M would build tables and format it how how i would.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lxvf0j","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxvf0j/qwen_3_embeddings_06b_faring_really_poorly/n2t3a53/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752361668,"author_flair_text":null,"collapsed":false,"created_utc":1752361668,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2rgnv3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"sciencewarrior","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2q41wv","score":2,"author_fullname":"t2_4feaa","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"You could also check https://huggingface.co/intfloat/multilingual-e5-small to see if it performs better than a quantization of the larger one. It's worth noting that these E5 models perform better when you add question: and passage: as prefix.\\n\\nIf you need something smaller and still multilingual, IBM Granite has worked well for me: https://huggingface.co/bartowski/granite-embedding-107m-multilingual-GGUF","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2rgnv3","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You could also check &lt;a href=\\"https://huggingface.co/intfloat/multilingual-e5-small\\"&gt;https://huggingface.co/intfloat/multilingual-e5-small&lt;/a&gt; to see if it performs better than a quantization of the larger one. It&amp;#39;s worth noting that these E5 models perform better when you add question: and passage: as prefix.&lt;/p&gt;\\n\\n&lt;p&gt;If you need something smaller and still multilingual, IBM Granite has worked well for me: &lt;a href=\\"https://huggingface.co/bartowski/granite-embedding-107m-multilingual-GGUF\\"&gt;https://huggingface.co/bartowski/granite-embedding-107m-multilingual-GGUF&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lxvf0j","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxvf0j/qwen_3_embeddings_06b_faring_really_poorly/n2rgnv3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752342469,"author_flair_text":null,"treatment_tags":[],"created_utc":1752342469,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n2q41wv","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"uber-linny","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2p6a80","score":4,"author_fullname":"t2_14166b","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"never realised what i was missing out on .... i compared :\\n\\n[https://huggingface.co/MesTruck/multilingual-e5-large-instruct-GGUF](https://huggingface.co/MesTruck/multilingual-e5-large-instruct-GGUF)\\n\\nwhich was next on the MTEB list because i was using 0.6B embed,,,, night and day difference... the things you learn hey !","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2q41wv","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;never realised what i was missing out on .... i compared :&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://huggingface.co/MesTruck/multilingual-e5-large-instruct-GGUF\\"&gt;https://huggingface.co/MesTruck/multilingual-e5-large-instruct-GGUF&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;which was next on the MTEB list because i was using 0.6B embed,,,, night and day difference... the things you learn hey !&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lxvf0j","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxvf0j/qwen_3_embeddings_06b_faring_really_poorly/n2q41wv/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752327001,"author_flair_text":null,"treatment_tags":[],"created_utc":1752327001,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}}],"before":null}},"user_reports":[],"saved":false,"id":"n2p6a80","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"uber-linny","can_mod_post":false,"created_utc":1752310552,"send_replies":true,"parent_id":"t1_n2p53ko","score":4,"author_fullname":"t2_14166b","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Thanks I'm interested... I'm using LM studio,But a absolute rookie at it. \\n\\nWhat embedding model does everyone suggest until this is merged ?\\n\\nMy setup interfaces to anythingllm","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2p6a80","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thanks I&amp;#39;m interested... I&amp;#39;m using LM studio,But a absolute rookie at it. &lt;/p&gt;\\n\\n&lt;p&gt;What embedding model does everyone suggest until this is merged ?&lt;/p&gt;\\n\\n&lt;p&gt;My setup interfaces to anythingllm&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lxvf0j","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxvf0j/qwen_3_embeddings_06b_faring_really_poorly/n2p6a80/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752310552,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}}],"before":null}},"user_reports":[],"saved":false,"id":"n2p53ko","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Chromix_","can_mod_post":false,"created_utc":1752309842,"send_replies":true,"parent_id":"t3_1lxvf0j","score":34,"author_fullname":"t2_k7w2h","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"That's been discussed very recently. If you're using llama.cpp you need to include a patch that hasn't been merged yet. Aside from that it's important to prompt indexing, search and clustering correctly, with the correct settings as documented in their readme.\\n\\nSee these threads for further information:\\n\\n[https://www.reddit.com/r/LocalLLaMA/comments/1lt18hg/are\\\\_qwen3\\\\_embedding\\\\_gguf\\\\_faulty/](https://www.reddit.com/r/LocalLLaMA/comments/1lt18hg/are_qwen3_embedding_gguf_faulty/)\\n\\n[https://www.reddit.com/r/LocalLLaMA/comments/1lx66on/issues\\\\_with\\\\_qwen\\\\_3\\\\_embedding\\\\_models\\\\_4b\\\\_and\\\\_06b/](https://www.reddit.com/r/LocalLLaMA/comments/1lx66on/issues_with_qwen_3_embedding_models_4b_and_06b/)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2p53ko","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;That&amp;#39;s been discussed very recently. If you&amp;#39;re using llama.cpp you need to include a patch that hasn&amp;#39;t been merged yet. Aside from that it&amp;#39;s important to prompt indexing, search and clustering correctly, with the correct settings as documented in their readme.&lt;/p&gt;\\n\\n&lt;p&gt;See these threads for further information:&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://www.reddit.com/r/LocalLLaMA/comments/1lt18hg/are_qwen3_embedding_gguf_faulty/\\"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1lt18hg/are_qwen3_embedding_gguf_faulty/&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://www.reddit.com/r/LocalLLaMA/comments/1lx66on/issues_with_qwen_3_embedding_models_4b_and_06b/\\"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1lx66on/issues_with_qwen_3_embedding_models_4b_and_06b/&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxvf0j/qwen_3_embeddings_06b_faring_really_poorly/n2p53ko/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752309842,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lxvf0j","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":34}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2p45k2","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"hapliniste","can_mod_post":false,"created_utc":1752309269,"send_replies":true,"parent_id":"t3_1lxvf0j","score":4,"author_fullname":"t2_fc7rd","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I'm very interested as well because I planned on using it based on its rank in the leaderborard 😅","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2p45k2","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;m very interested as well because I planned on using it based on its rank in the leaderborard 😅&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxvf0j/qwen_3_embeddings_06b_faring_really_poorly/n2p45k2/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752309269,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lxvf0j","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2qgl0q","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"BadSkater0729","can_mod_post":false,"created_utc":1752331305,"send_replies":true,"parent_id":"t3_1lxvf0j","score":3,"author_fullname":"t2_u4deq6r0","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"So 1) your query to the VDB you’re using matters a TON and 2) you MUST use the exact query prompt they have provided in their examples for both the embedder AND reranker. Without this accuracy completely tanks - Qwen’s recommendation is more of a requirement.\\n\\nIn regards to #1, remember that this is a LAST TOKEN POOLING embedder. Most of the embedders on the MTEB leaderboard are average pooling, meaning that they are much less susceptible to noise but also are less precise on average.\\n\\nWe found that adding generic filler to the VDB query significantly hurt recall. For example, let’s say you’re working on a corpus for the University of Michigan. If you include “University of Michigan” in your query then Qwen’s extra sensitivity tanks recall. Therefore remove ALL filler whenever possible. Additionally, it seems like ending your query in the most “relevant” noun helps with recall.\\n\\nTBH overall this embedder is very good but quite temperamental due to that last token pooling bit and the instruct. Hope this helps\\n\\nEDIT: this is on vLLM. Llama.cpp might still have a few bugs to iron out","edited":1752331845,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2qgl0q","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;So 1) your query to the VDB you’re using matters a TON and 2) you MUST use the exact query prompt they have provided in their examples for both the embedder AND reranker. Without this accuracy completely tanks - Qwen’s recommendation is more of a requirement.&lt;/p&gt;\\n\\n&lt;p&gt;In regards to #1, remember that this is a LAST TOKEN POOLING embedder. Most of the embedders on the MTEB leaderboard are average pooling, meaning that they are much less susceptible to noise but also are less precise on average.&lt;/p&gt;\\n\\n&lt;p&gt;We found that adding generic filler to the VDB query significantly hurt recall. For example, let’s say you’re working on a corpus for the University of Michigan. If you include “University of Michigan” in your query then Qwen’s extra sensitivity tanks recall. Therefore remove ALL filler whenever possible. Additionally, it seems like ending your query in the most “relevant” noun helps with recall.&lt;/p&gt;\\n\\n&lt;p&gt;TBH overall this embedder is very good but quite temperamental due to that last token pooling bit and the instruct. Hope this helps&lt;/p&gt;\\n\\n&lt;p&gt;EDIT: this is on vLLM. Llama.cpp might still have a few bugs to iron out&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxvf0j/qwen_3_embeddings_06b_faring_really_poorly/n2qgl0q/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752331305,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lxvf0j","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2v7i2p","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"terminoid_","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2uy9ch","score":1,"author_fullname":"t2_1iu07dnz2i","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"your GGUFs need to be updated with the tokenizer changes from the safetensors repo. also, i believe CISC added limited llama.cpp support for \\"post_processor\\" section from tokenizer.json, (the part which adds the needed EOS token) but i would double-check the output.  i have a suspicion your re-ranker models might need some attention, too.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2v7i2p","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;your GGUFs need to be updated with the tokenizer changes from the safetensors repo. also, i believe CISC added limited llama.cpp support for &amp;quot;post_processor&amp;quot; section from tokenizer.json, (the part which adds the needed EOS token) but i would double-check the output.  i have a suspicion your re-ranker models might need some attention, too.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lxvf0j","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxvf0j/qwen_3_embeddings_06b_faring_really_poorly/n2v7i2p/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752395124,"author_flair_text":null,"treatment_tags":[],"created_utc":1752395124,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2vye40","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"i4858i","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2uy9ch","score":1,"author_fullname":"t2_alrxvbt1","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Hello. Since it looks like you're someone involved with the project as a dev, I wanted to understand if you could see if I was doing something obviously wrong with the setup","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2vye40","is_submitter":true,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Hello. Since it looks like you&amp;#39;re someone involved with the project as a dev, I wanted to understand if you could see if I was doing something obviously wrong with the setup&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lxvf0j","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxvf0j/qwen_3_embeddings_06b_faring_really_poorly/n2vye40/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752409549,"author_flair_text":null,"treatment_tags":[],"created_utc":1752409549,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2uy9ch","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Diff_Yue","can_mod_post":false,"created_utc":1752389837,"send_replies":true,"parent_id":"t1_n2pgoln","score":1,"author_fullname":"t2_4ue37hu5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Could you please tell us what problems exist with the official gguf? We will attempt to correct it.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2uy9ch","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Could you please tell us what problems exist with the official gguf? We will attempt to correct it.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lxvf0j","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxvf0j/qwen_3_embeddings_06b_faring_really_poorly/n2uy9ch/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752389837,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2vy1pj","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"i4858i","can_mod_post":false,"created_utc":1752409400,"send_replies":true,"parent_id":"t1_n2pgoln","score":1,"author_fullname":"t2_alrxvbt1","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I am not using GGUFs at all. Going to try with the instructions provided in the model card, thanks","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2vy1pj","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I am not using GGUFs at all. Going to try with the instructions provided in the model card, thanks&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lxvf0j","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxvf0j/qwen_3_embeddings_06b_faring_really_poorly/n2vy1pj/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752409400,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2qqv2g","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"giblesnot","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2qim8i","score":4,"author_fullname":"t2_nb3tt","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"But it would be dramatically more useful to say \\"unsloths gguf is much better than the official ones\\" (if unsloth has quants, we dont know because they only said what was broken not what works.)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2qqv2g","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;But it would be dramatically more useful to say &amp;quot;unsloths gguf is much better than the official ones&amp;quot; (if unsloth has quants, we dont know because they only said what was broken not what works.)&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lxvf0j","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxvf0j/qwen_3_embeddings_06b_faring_really_poorly/n2qqv2g/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752334551,"author_flair_text":null,"treatment_tags":[],"created_utc":1752334551,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2taxvr","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"BadSkater0729","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2rvgoy","score":1,"author_fullname":"t2_u4deq6r0","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Fair enough! Just thought the way that reply was phrased was very weird.","edited":false,"author_flair_css_class":null,"name":"t1_n2taxvr","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Fair enough! Just thought the way that reply was phrased was very weird.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lxvf0j","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxvf0j/qwen_3_embeddings_06b_faring_really_poorly/n2taxvr/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752364361,"author_flair_text":null,"collapsed":false,"created_utc":1752364361,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2rvgoy","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Specialist-String598","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2qim8i","score":1,"author_fullname":"t2_1t5o3lghq5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"This is false, his instructions are incomplete. \\n\\n\\"also, don't use the official GGUFs, they're busted\\" What ggufs do i use instead? Do i not use a gguf at all and use the raw safetensors? Safetensors at fp8? Exllama?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2rvgoy","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;This is false, his instructions are incomplete. &lt;/p&gt;\\n\\n&lt;p&gt;&amp;quot;also, don&amp;#39;t use the official GGUFs, they&amp;#39;re busted&amp;quot; What ggufs do i use instead? Do i not use a gguf at all and use the raw safetensors? Safetensors at fp8? Exllama?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lxvf0j","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxvf0j/qwen_3_embeddings_06b_faring_really_poorly/n2rvgoy/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752347116,"author_flair_text":null,"treatment_tags":[],"created_utc":1752347116,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2qim8i","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"BadSkater0729","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2pvci3","score":0,"author_fullname":"t2_u4deq6r0","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"??? His instructions look pretty clear to me, you were certainly right on the unsolicited part","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2qim8i","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;??? His instructions look pretty clear to me, you were certainly right on the unsolicited part&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lxvf0j","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxvf0j/qwen_3_embeddings_06b_faring_really_poorly/n2qim8i/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752331959,"author_flair_text":null,"treatment_tags":[],"created_utc":1752331959,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":1,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}},"user_reports":[],"saved":false,"id":"n2pvci3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"__JockY__","can_mod_post":false,"created_utc":1752323667,"send_replies":true,"parent_id":"t1_n2pgoln","score":-3,"author_fullname":"t2_qf8h7ka8","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"May I offer a piece of unsolicited advice? Thanks.\\n\\n&lt;old_man_unsolicited_life_hacks&gt;\\nWhen advising someone on any topic (assuming no interference from Messrs. Dunning and Kruger) your advice can be made more actionable - and therefore more useful - by including recommendations _on what to do instead_. \\n\\nConsider approaching it with a positive angle: “hey, head’s up: the official GGUFs are busted, make sure to use the &lt;notbusted&gt; ones instead”. \\n\\nThink of it this way: who do you go to for advice about tricky problems? The “try this” person or the “fuck that” person?\\n\\nFinally, to paraphrase Baz Luhrman: if you succeed at doing this, please tell me how.\\n\\n&lt;/old_man_unsolicited_life_hacks&gt;","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2pvci3","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;May I offer a piece of unsolicited advice? Thanks.&lt;/p&gt;\\n\\n&lt;p&gt;&amp;lt;old_man_unsolicited_life_hacks&amp;gt;\\nWhen advising someone on any topic (assuming no interference from Messrs. Dunning and Kruger) your advice can be made more actionable - and therefore more useful - by including recommendations &lt;em&gt;on what to do instead&lt;/em&gt;. &lt;/p&gt;\\n\\n&lt;p&gt;Consider approaching it with a positive angle: “hey, head’s up: the official GGUFs are busted, make sure to use the &amp;lt;notbusted&amp;gt; ones instead”. &lt;/p&gt;\\n\\n&lt;p&gt;Think of it this way: who do you go to for advice about tricky problems? The “try this” person or the “fuck that” person?&lt;/p&gt;\\n\\n&lt;p&gt;Finally, to paraphrase Baz Luhrman: if you succeed at doing this, please tell me how.&lt;/p&gt;\\n\\n&lt;p&gt;&amp;lt;/old_man_unsolicited_life_hacks&amp;gt;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lxvf0j","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxvf0j/qwen_3_embeddings_06b_faring_really_poorly/n2pvci3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752323667,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":1,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-3}}],"before":null}},"user_reports":[],"saved":false,"id":"n2pgoln","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"terminoid_","can_mod_post":false,"created_utc":1752316765,"send_replies":true,"parent_id":"t3_1lxvf0j","score":8,"author_fullname":"t2_1iu07dnz2i","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"you have to use the instruction format in the model card, otherwise performance drops a lot, you can't just use it like a normal embedding model.\\n\\nalso, don't use the official GGUFs, they're busted","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2pgoln","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;you have to use the instruction format in the model card, otherwise performance drops a lot, you can&amp;#39;t just use it like a normal embedding model.&lt;/p&gt;\\n\\n&lt;p&gt;also, don&amp;#39;t use the official GGUFs, they&amp;#39;re busted&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxvf0j/qwen_3_embeddings_06b_faring_really_poorly/n2pgoln/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752316765,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lxvf0j","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":8}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2psut2","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"__JockY__","can_mod_post":false,"created_utc":1752322630,"send_replies":true,"parent_id":"t3_1lxvf0j","score":2,"author_fullname":"t2_qf8h7ka8","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Ah, what you need is \`tolower(3)\`.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2psut2","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Ah, what you need is &lt;code&gt;tolower(3)&lt;/code&gt;.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxvf0j/qwen_3_embeddings_06b_faring_really_poorly/n2psut2/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752322630,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lxvf0j","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2vy7bm","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"i4858i","can_mod_post":false,"created_utc":1752409468,"send_replies":true,"parent_id":"t1_n2po7sn","score":1,"author_fullname":"t2_alrxvbt1","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"This does not appear like an inference engine specific problem because I have tried with multiple different inference engines \\\\[vLLM, infinity-embed, HuggingFace TEI\\\\] and even sentence\\\\_transformers.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2vy7bm","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;This does not appear like an inference engine specific problem because I have tried with multiple different inference engines [vLLM, infinity-embed, HuggingFace TEI] and even sentence_transformers.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lxvf0j","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxvf0j/qwen_3_embeddings_06b_faring_really_poorly/n2vy7bm/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752409468,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2po7sn","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"teamclouday","can_mod_post":false,"created_utc":1752320574,"send_replies":true,"parent_id":"t3_1lxvf0j","score":1,"author_fullname":"t2_210m41fn","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Have you tested the same inputs with sentence transformers? Check out this issue: [https://github.com/huggingface/text-embeddings-inference/issues/668](https://github.com/huggingface/text-embeddings-inference/issues/668)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2po7sn","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Have you tested the same inputs with sentence transformers? Check out this issue: &lt;a href=\\"https://github.com/huggingface/text-embeddings-inference/issues/668\\"&gt;https://github.com/huggingface/text-embeddings-inference/issues/668&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxvf0j/qwen_3_embeddings_06b_faring_really_poorly/n2po7sn/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752320574,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lxvf0j","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2vy6pv","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"i4858i","can_mod_post":false,"created_utc":1752409461,"send_replies":true,"parent_id":"t1_n2ps2ds","score":1,"author_fullname":"t2_alrxvbt1","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"This does not appear like an inference engine specific problem because I have tried with multiple different inference engines \\\\[vLLM, infinity-embed, HuggingFace TEI\\\\] and even sentence\\\\_transformers.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2vy6pv","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;This does not appear like an inference engine specific problem because I have tried with multiple different inference engines [vLLM, infinity-embed, HuggingFace TEI] and even sentence_transformers.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lxvf0j","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxvf0j/qwen_3_embeddings_06b_faring_really_poorly/n2vy6pv/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752409461,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2ps2ds","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AskAmbitious5697","can_mod_post":false,"created_utc":1752322293,"send_replies":true,"parent_id":"t3_1lxvf0j","score":1,"author_fullname":"t2_gm9w5nsf","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Works good for me running with llama cpp, although texts are very simple… wierd\\n\\nEdit: GGUF that I used is 100% faulty, it’s not the same as using it with sentencetransformer.","edited":1752325703,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2ps2ds","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Works good for me running with llama cpp, although texts are very simple… wierd&lt;/p&gt;\\n\\n&lt;p&gt;Edit: GGUF that I used is 100% faulty, it’s not the same as using it with sentencetransformer.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxvf0j/qwen_3_embeddings_06b_faring_really_poorly/n2ps2ds/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752322293,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lxvf0j","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2q1bnn","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"celsowm","can_mod_post":false,"created_utc":1752325986,"send_replies":true,"parent_id":"t3_1lxvf0j","score":1,"author_fullname":"t2_dyvrh","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Not for me in pt-br texts using native transformers + fastapi","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2q1bnn","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Not for me in pt-br texts using native transformers + fastapi&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxvf0j/qwen_3_embeddings_06b_faring_really_poorly/n2q1bnn/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752325986,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lxvf0j","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2q9x61","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"cwefelscheid","can_mod_post":false,"created_utc":1752329087,"send_replies":true,"parent_id":"t3_1lxvf0j","score":1,"author_fullname":"t2_uca99ugj","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I use qwen3 0.6B for wikillm.com. In total its over 25+ Million paragraphs from English Wikipedia. I think the performance is decent, sometimes it does not find obvious articles but overall performance is much better then what I used before.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2q9x61","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I use qwen3 0.6B for wikillm.com. In total its over 25+ Million paragraphs from English Wikipedia. I think the performance is decent, sometimes it does not find obvious articles but overall performance is much better then what I used before.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxvf0j/qwen_3_embeddings_06b_faring_really_poorly/n2q9x61/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752329087,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lxvf0j","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),o=()=>e.jsx(l,{data:t});export{o as default};
