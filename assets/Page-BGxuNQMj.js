import{j as e}from"./index-Bu7qcPAU.js";import{R as l}from"./RedditPostRenderer-CbHA7O5q.js";import"./index-BKgbfxhf.js";const t=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"OpenCodeReasoning-Nemotron-1.1-7B is a large language model (LLM) which is a derivative of Qwen2.5-7B-Instruct (AKA the reference model). It is a reasoning model that is post-trained for reasoning for code generation. The model supports a context length of 64k tokens.  \\n\\n\\nThis model is ready for commercial/non-commercial use.\\n\\n\\n\\n||LiveCodeBench|\\n|:-|:-|\\n|QwQ-32B|61.3|\\n|**OpenCodeReasoning-Nemotron-1.1-14B**|**65.9**|\\n|OpenCodeReasoning-Nemotron-14B|59.4|\\n|**OpenCodeReasoning-Nemotron-1.1-32B**|**69.9**|\\n|OpenCodeReasoning-Nemotron-32B|61.7|\\n|DeepSeek-R1-0528|73.4|\\n|DeepSeek-R1|65.6|\\n\\n\\n\\n\\n\\n[https://huggingface.co/nvidia/OpenCodeReasoning-Nemotron-1.1-7B](https://huggingface.co/nvidia/OpenCodeReasoning-Nemotron-1.1-7B)\\n\\n[https://huggingface.co/nvidia/OpenCodeReasoning-Nemotron-1.1-14B](https://huggingface.co/nvidia/OpenCodeReasoning-Nemotron-1.1-14B)\\n\\n[https://huggingface.co/nvidia/OpenCodeReasoning-Nemotron-1.1-32B](https://huggingface.co/nvidia/OpenCodeReasoning-Nemotron-1.1-32B)\\n\\n","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"new models from NVIDIA: OpenCodeReasoning-Nemotron-1.1 7B/14B/32B","link_flair_richtext":[{"e":"text","t":"New Model"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lus2yw","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.98,"author_flair_background_color":"#bbbdbf","subreddit_type":"public","ups":180,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","is_original_content":false,"author_fullname":"t2_vqgbql9w","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"New Model","can_mod_post":false,"score":180,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"gildings":{},"post_hint":"self","content_categories":null,"is_self":true,"mod_note":null,"created":1751989711,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"richtext","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;OpenCodeReasoning-Nemotron-1.1-7B is a large language model (LLM) which is a derivative of Qwen2.5-7B-Instruct (AKA the reference model). It is a reasoning model that is post-trained for reasoning for code generation. The model supports a context length of 64k tokens.  &lt;/p&gt;\\n\\n&lt;p&gt;This model is ready for commercial/non-commercial use.&lt;/p&gt;\\n\\n&lt;table&gt;&lt;thead&gt;\\n&lt;tr&gt;\\n&lt;th align=\\"left\\"&gt;&lt;/th&gt;\\n&lt;th align=\\"left\\"&gt;LiveCodeBench&lt;/th&gt;\\n&lt;/tr&gt;\\n&lt;/thead&gt;&lt;tbody&gt;\\n&lt;tr&gt;\\n&lt;td align=\\"left\\"&gt;QwQ-32B&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;61.3&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;tr&gt;\\n&lt;td align=\\"left\\"&gt;&lt;strong&gt;OpenCodeReasoning-Nemotron-1.1-14B&lt;/strong&gt;&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;&lt;strong&gt;65.9&lt;/strong&gt;&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;tr&gt;\\n&lt;td align=\\"left\\"&gt;OpenCodeReasoning-Nemotron-14B&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;59.4&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;tr&gt;\\n&lt;td align=\\"left\\"&gt;&lt;strong&gt;OpenCodeReasoning-Nemotron-1.1-32B&lt;/strong&gt;&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;&lt;strong&gt;69.9&lt;/strong&gt;&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;tr&gt;\\n&lt;td align=\\"left\\"&gt;OpenCodeReasoning-Nemotron-32B&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;61.7&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;tr&gt;\\n&lt;td align=\\"left\\"&gt;DeepSeek-R1-0528&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;73.4&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;tr&gt;\\n&lt;td align=\\"left\\"&gt;DeepSeek-R1&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;65.6&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;/tbody&gt;&lt;/table&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://huggingface.co/nvidia/OpenCodeReasoning-Nemotron-1.1-7B\\"&gt;https://huggingface.co/nvidia/OpenCodeReasoning-Nemotron-1.1-7B&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://huggingface.co/nvidia/OpenCodeReasoning-Nemotron-1.1-14B\\"&gt;https://huggingface.co/nvidia/OpenCodeReasoning-Nemotron-1.1-14B&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://huggingface.co/nvidia/OpenCodeReasoning-Nemotron-1.1-32B\\"&gt;https://huggingface.co/nvidia/OpenCodeReasoning-Nemotron-1.1-32B&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"preview":{"images":[{"source":{"url":"https://external-preview.redd.it/xydcboaWr0AtFYvdA_VYKzaGbb6J3DC7YWd6PyBFtp0.png?auto=webp&amp;s=c046dee229da79f38621e6f0294ad12f961a6cb6","width":1200,"height":648},"resolutions":[{"url":"https://external-preview.redd.it/xydcboaWr0AtFYvdA_VYKzaGbb6J3DC7YWd6PyBFtp0.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=64ec5a05cd96cb87a4112fc8bfe7de098998dfb3","width":108,"height":58},{"url":"https://external-preview.redd.it/xydcboaWr0AtFYvdA_VYKzaGbb6J3DC7YWd6PyBFtp0.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=e3a19d7dc3807669af215390b9226fe990e62a93","width":216,"height":116},{"url":"https://external-preview.redd.it/xydcboaWr0AtFYvdA_VYKzaGbb6J3DC7YWd6PyBFtp0.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=ceacf3cf885d899d7bd7dd681ad5e7c095d46f14","width":320,"height":172},{"url":"https://external-preview.redd.it/xydcboaWr0AtFYvdA_VYKzaGbb6J3DC7YWd6PyBFtp0.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=5bc5c5ab390ef3369333633fa85044571e121cce","width":640,"height":345},{"url":"https://external-preview.redd.it/xydcboaWr0AtFYvdA_VYKzaGbb6J3DC7YWd6PyBFtp0.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=e091e24575e0af7c9f8a54d691665da5d4e3b46c","width":960,"height":518},{"url":"https://external-preview.redd.it/xydcboaWr0AtFYvdA_VYKzaGbb6J3DC7YWd6PyBFtp0.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=8ed5d57035bfc07d513aff72e320481a92cf70cb","width":1080,"height":583}],"variants":{},"id":"xydcboaWr0AtFYvdA_VYKzaGbb6J3DC7YWd6PyBFtp0"}],"enabled":false},"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"ced98442-f5d3-11ed-b657-66d3b15490c6","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":"llama.cpp","treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#ffb000","id":"1lus2yw","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"jacek2023","discussion_type":null,"num_comments":48,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":"light","permalink":"/r/LocalLLaMA/comments/1lus2yw/new_models_from_nvidia/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lus2yw/new_models_from_nvidia/","subreddit_subscribers":497025,"created_utc":1751989711,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n22w6v1","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"TheRealMasonMac","can_mod_post":false,"send_replies":true,"parent_id":"t1_n20ihva","score":13,"author_fullname":"t2_101haj","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Their datasets are not very high quality. For example, the dataset used for Nemotron prunes of Llama 3 had derived all its \\"safety\\" alignment prompts by adversarially prompting a WizardLM model seemingly without a jailbreak, thus leading to soft (implicitly redirecting to \\"safer\\" non-adversarial outputs) and hard refusals (I refuse) directly in the dataset. So, you have innocuous prompts like, \\"Is it hard being a police officer?\\" in the dataset, training the model using the dataset to refuse them.\\n\\nTheir math datasets also have instances where questions are unsolvable because they don't contain the information necessary to solve them and so models have to hallucinate information to be able to solve them (e.g. \\"Referring to the functions in problem 13, \\\\[...\\\\]\\"), they're poorly worded, or their ground truth is incomplete (e.g. a question that has multiple possible solutions only lists a single solution in its ground truth).\\n\\nTo be fair, most datasets out there right now are mind-boggingly poor quality given how many entire classes of errors could be trivially detected. AllenAI's datasets have a similar issue with respect to using an existing censored model to create adversarial prompts.\\n\\nKudos to groups like [https://huggingface.co/NousResearch](https://huggingface.co/NousResearch) that do put effort into cleaning up their datasets (e.g. [Minos](https://huggingface.co/NousResearch/Minos-v1) is great for detecting soft/hard refusals).\\n\\n(I enjoy letting the leopards eat the faces of these \\"ethical\\" researchers with how their adversarial prompt generation is failing them because of the very same alignment they're pursuing, leading them to create ever worse models. Oh, but only psychopaths would get value from uncensored models, right?)","edited":1752029841,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n22w6v1","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Their datasets are not very high quality. For example, the dataset used for Nemotron prunes of Llama 3 had derived all its &amp;quot;safety&amp;quot; alignment prompts by adversarially prompting a WizardLM model seemingly without a jailbreak, thus leading to soft (implicitly redirecting to &amp;quot;safer&amp;quot; non-adversarial outputs) and hard refusals (I refuse) directly in the dataset. So, you have innocuous prompts like, &amp;quot;Is it hard being a police officer?&amp;quot; in the dataset, training the model using the dataset to refuse them.&lt;/p&gt;\\n\\n&lt;p&gt;Their math datasets also have instances where questions are unsolvable because they don&amp;#39;t contain the information necessary to solve them and so models have to hallucinate information to be able to solve them (e.g. &amp;quot;Referring to the functions in problem 13, [...]&amp;quot;), they&amp;#39;re poorly worded, or their ground truth is incomplete (e.g. a question that has multiple possible solutions only lists a single solution in its ground truth).&lt;/p&gt;\\n\\n&lt;p&gt;To be fair, most datasets out there right now are mind-boggingly poor quality given how many entire classes of errors could be trivially detected. AllenAI&amp;#39;s datasets have a similar issue with respect to using an existing censored model to create adversarial prompts.&lt;/p&gt;\\n\\n&lt;p&gt;Kudos to groups like &lt;a href=\\"https://huggingface.co/NousResearch\\"&gt;https://huggingface.co/NousResearch&lt;/a&gt; that do put effort into cleaning up their datasets (e.g. &lt;a href=\\"https://huggingface.co/NousResearch/Minos-v1\\"&gt;Minos&lt;/a&gt; is great for detecting soft/hard refusals).&lt;/p&gt;\\n\\n&lt;p&gt;(I enjoy letting the leopards eat the faces of these &amp;quot;ethical&amp;quot; researchers with how their adversarial prompt generation is failing them because of the very same alignment they&amp;#39;re pursuing, leading them to create ever worse models. Oh, but only psychopaths would get value from uncensored models, right?)&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lus2yw","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lus2yw/new_models_from_nvidia/n22w6v1/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752017568,"author_flair_text":null,"treatment_tags":[],"created_utc":1752017568,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":13}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n22fllo","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"mj3815","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2212qo","score":3,"author_fullname":"t2_4a870z4c","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Mistral-Nemotron isn’t even open weights","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n22fllo","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Mistral-Nemotron isn’t even open weights&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lus2yw","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lus2yw/new_models_from_nvidia/n22fllo/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752012332,"author_flair_text":null,"treatment_tags":[],"created_utc":1752012332,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":6,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n2212qo","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"DinoAmino","can_mod_post":false,"send_replies":true,"parent_id":"t1_n21x71p","score":3,"author_fullname":"t2_j1v7f","approved_by":null,"mod_note":null,"all_awardings":[],"body":"The OpenCodeReasoning models are in their own collection:\\n\\nhttps://huggingface.co/collections/nvidia/opencodereasoning-67ec462892673a326c0696c1\\n\\nThe Nemotrons have their own collection:\\n\\nhttps://huggingface.co/collections/nvidia/llama-nemotron-67d92346030a2691293f200b\\n\\nWhether I am right or wrong - not all Nvidia models are open source - is easy to verify.","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n2212qo","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The OpenCodeReasoning models are in their own collection:&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://huggingface.co/collections/nvidia/opencodereasoning-67ec462892673a326c0696c1\\"&gt;https://huggingface.co/collections/nvidia/opencodereasoning-67ec462892673a326c0696c1&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;The Nemotrons have their own collection:&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://huggingface.co/collections/nvidia/llama-nemotron-67d92346030a2691293f200b\\"&gt;https://huggingface.co/collections/nvidia/llama-nemotron-67d92346030a2691293f200b&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;Whether I am right or wrong - not all Nvidia models are open source - is easy to verify.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lus2yw","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lus2yw/new_models_from_nvidia/n2212qo/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752008144,"author_flair_text":null,"treatment_tags":[],"created_utc":1752008144,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n21x71p","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"silenceimpaired","can_mod_post":false,"send_replies":true,"parent_id":"t1_n21uy56","score":2,"author_fullname":"t2_dissgzyl","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"This model has Nemotron in the name so technically… are you right? :)","edited":false,"author_flair_css_class":null,"name":"t1_n21x71p","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;This model has Nemotron in the name so technically… are you right? :)&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lus2yw","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lus2yw/new_models_from_nvidia/n21x71p/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752007112,"author_flair_text":null,"collapsed":false,"created_utc":1752007112,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n226rds","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"MosaicCantab","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2231c2","score":6,"author_fullname":"t2_1ldnbkzx5a","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Touchè brother you’re more than correct I had never noticed.","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n226rds","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Touchè brother you’re more than correct I had never noticed.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lus2yw","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lus2yw/new_models_from_nvidia/n226rds/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752009729,"author_flair_text":null,"treatment_tags":[],"created_utc":1752009729,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":6,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}}],"before":null}},"user_reports":[],"saved":false,"id":"n2231c2","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DinoAmino","can_mod_post":false,"send_replies":true,"parent_id":"t1_n220l8g","score":3,"author_fullname":"t2_j1v7f","approved_by":null,"mod_note":null,"all_awardings":[],"body":"Sorry for splitting hairs. Those Nemotron models don't list the datasets in the model card \\"metadata\\" in the same way these coders do. They mention at the end of the Nemotron readmes that they released a _sample_ of their post training dataset. It's not really the entire dataset that they actually used.","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n2231c2","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Sorry for splitting hairs. Those Nemotron models don&amp;#39;t list the datasets in the model card &amp;quot;metadata&amp;quot; in the same way these coders do. They mention at the end of the Nemotron readmes that they released a &lt;em&gt;sample&lt;/em&gt; of their post training dataset. It&amp;#39;s not really the entire dataset that they actually used.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lus2yw","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lus2yw/new_models_from_nvidia/n2231c2/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752008679,"author_flair_text":null,"treatment_tags":[],"created_utc":1752008679,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n220l8g","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MosaicCantab","can_mod_post":false,"send_replies":true,"parent_id":"t1_n21uy56","score":0,"author_fullname":"t2_1ldnbkzx5a","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"The entire nemotrom dataset is available and all of its variants. \\n\\nhttps://huggingface.co/datasets/nvidia/Llama-Nemotron-Post-Training-Dataset","edited":false,"author_flair_css_class":null,"name":"t1_n220l8g","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The entire nemotrom dataset is available and all of its variants. &lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://huggingface.co/datasets/nvidia/Llama-Nemotron-Post-Training-Dataset\\"&gt;https://huggingface.co/datasets/nvidia/Llama-Nemotron-Post-Training-Dataset&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lus2yw","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lus2yw/new_models_from_nvidia/n220l8g/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752008014,"author_flair_text":null,"collapsed":false,"created_utc":1752008014,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}},"user_reports":[],"saved":false,"id":"n21uy56","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"DinoAmino","can_mod_post":false,"send_replies":true,"parent_id":"t1_n21tfzg","score":5,"author_fullname":"t2_j1v7f","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"If you mean the models from this collection then you're correct. But not all Nvidia open weight models are open source. None of the models in their Nemotron collection have their datasets published.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n21uy56","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;If you mean the models from this collection then you&amp;#39;re correct. But not all Nvidia open weight models are open source. None of the models in their Nemotron collection have their datasets published.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lus2yw","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lus2yw/new_models_from_nvidia/n21uy56/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752006512,"author_flair_text":null,"treatment_tags":[],"created_utc":1752006512,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}}],"before":null}},"user_reports":[],"saved":false,"id":"n21tfzg","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MosaicCantab","can_mod_post":false,"send_replies":true,"parent_id":"t1_n20ihva","score":3,"author_fullname":"t2_1ldnbkzx5a","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"All of them have released datasets.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n21tfzg","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;All of them have released datasets.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lus2yw","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lus2yw/new_models_from_nvidia/n21tfzg/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752006114,"author_flair_text":null,"treatment_tags":[],"created_utc":1752006114,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n20ihva","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"DinoAmino","can_mod_post":false,"created_utc":1751993276,"send_replies":true,"parent_id":"t1_n207fjf","score":27,"author_fullname":"t2_j1v7f","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yeah, Nvidia does some good things with their models. A few of them have their datasets released on HF, making them truly open source.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n20ihva","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yeah, Nvidia does some good things with their models. A few of them have their datasets released on HF, making them truly open source.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lus2yw","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lus2yw/new_models_from_nvidia/n20ihva/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751993276,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":27}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n21tbhu","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"emprahsFury","can_mod_post":false,"created_utc":1752006081,"send_replies":false,"parent_id":"t1_n207fjf","score":2,"author_fullname":"t2_177r8n","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"This is vertical integration","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n21tbhu","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;This is vertical integration&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lus2yw","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lus2yw/new_models_from_nvidia/n21tbhu/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752006081,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n207fjf","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"silenceimpaired","can_mod_post":false,"created_utc":1751990132,"send_replies":true,"parent_id":"t3_1lus2yw","score":61,"author_fullname":"t2_dissgzyl","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Wow licensed without additional restrictions. I’m impressed.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n207fjf","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Wow licensed without additional restrictions. I’m impressed.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lus2yw/new_models_from_nvidia/n207fjf/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751990132,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lus2yw","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":61}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n22e8mj","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Lazy-Pattern-5171","can_mod_post":false,"created_utc":1752011919,"send_replies":true,"parent_id":"t1_n216tdp","score":4,"author_fullname":"t2_1lyjk8is25","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"It caught up, that’s step 1, it means the team has the basics down and can play, but just like R2, an OpenCodeReasoning 2 will fail to impress or be delayed for some unknown reason.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n22e8mj","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It caught up, that’s step 1, it means the team has the basics down and can play, but just like R2, an OpenCodeReasoning 2 will fail to impress or be delayed for some unknown reason.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lus2yw","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lus2yw/new_models_from_nvidia/n22e8mj/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752011919,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}}],"before":null}},"user_reports":[],"saved":false,"id":"n216tdp","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Professional-Bear857","can_mod_post":false,"created_utc":1751999874,"send_replies":true,"parent_id":"t3_1lus2yw","score":11,"author_fullname":"t2_yrl9ztfsa","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"It looks like it was fine tuned on responses from R1-0528 which explains why it performs so well.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n216tdp","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It looks like it was fine tuned on responses from R1-0528 which explains why it performs so well.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lus2yw/new_models_from_nvidia/n216tdp/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751999874,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lus2yw","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":11}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n20vkm9","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"rerri","can_mod_post":false,"send_replies":true,"parent_id":"t1_n20t3s9","score":10,"author_fullname":"t2_12aeph","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Dunno about 32B but Qwen3-235B-A22B scores ~~65.9~~ according to [https://livecodebench.github.io/leaderboard.html](https://livecodebench.github.io/leaderboard.html)\\n\\nedit: oh, actually Qwen3-235B-A22B scores **70.2** when setting the dates to **2408-2501** as Nvidia sites.","edited":1751997193,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n20vkm9","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Dunno about 32B but Qwen3-235B-A22B scores &lt;del&gt;65.9&lt;/del&gt; according to &lt;a href=\\"https://livecodebench.github.io/leaderboard.html\\"&gt;https://livecodebench.github.io/leaderboard.html&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;edit: oh, actually Qwen3-235B-A22B scores &lt;strong&gt;70.2&lt;/strong&gt; when setting the dates to &lt;strong&gt;2408-2501&lt;/strong&gt; as Nvidia sites.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lus2yw","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lus2yw/new_models_from_nvidia/n20vkm9/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751996861,"author_flair_text":null,"treatment_tags":[],"created_utc":1751996861,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":10}}],"before":null}},"user_reports":[],"saved":false,"id":"n20t3s9","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Secure_Reflection409","can_mod_post":false,"created_utc":1751996179,"send_replies":true,"parent_id":"t1_n20hb8k","score":2,"author_fullname":"t2_by77ogdhr","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"What did qwen score?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n20t3s9","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;What did qwen score?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lus2yw","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lus2yw/new_models_from_nvidia/n20t3s9/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751996179,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n20hb8k","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"AaronFeng47","can_mod_post":false,"created_utc":1751992941,"send_replies":true,"parent_id":"t3_1lus2yw","score":20,"author_fullname":"t2_4gc7hf3m","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Wow the 32b one actually scored higher than qwen3 32B","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n20hb8k","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Wow the 32b one actually scored higher than qwen3 32B&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lus2yw/new_models_from_nvidia/n20hb8k/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751992941,"author_flair_text":"llama.cpp","treatment_tags":[],"link_id":"t3_1lus2yw","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":20}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n25cl5s","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"SkyFeistyLlama8","can_mod_post":false,"created_utc":1752055670,"send_replies":true,"parent_id":"t1_n211edb","score":3,"author_fullname":"t2_1hgbaqgbnq","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I had just downloaded Microsoft's NextCoder 32B which is also based on Qwen 2.5 Coder.\\n\\nIf a 14B does coding better than QwQ 32B, we could be seeing the next jump in capability for smaller models. Previously, 70B models were the best for local inference on unified RAM architectures, before 32B models took that crown. Now it could be 14B next.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n25cl5s","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I had just downloaded Microsoft&amp;#39;s NextCoder 32B which is also based on Qwen 2.5 Coder.&lt;/p&gt;\\n\\n&lt;p&gt;If a 14B does coding better than QwQ 32B, we could be seeing the next jump in capability for smaller models. Previously, 70B models were the best for local inference on unified RAM architectures, before 32B models took that crown. Now it could be 14B next.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lus2yw","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lus2yw/new_models_from_nvidia/n25cl5s/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752055670,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n259x7t","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"uber-linny","can_mod_post":false,"send_replies":true,"parent_id":"t1_n21k71k","score":1,"author_fullname":"t2_14166b","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yeah I just asked it to make a batch file for a ping sweep ... Couldnt do it .","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n259x7t","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yeah I just asked it to make a batch file for a ping sweep ... Couldnt do it .&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lus2yw","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lus2yw/new_models_from_nvidia/n259x7t/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752054283,"author_flair_text":null,"treatment_tags":[],"created_utc":1752054283,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n21k71k","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Secure_Reflection409","can_mod_post":false,"created_utc":1752003548,"send_replies":true,"parent_id":"t1_n211edb","score":4,"author_fullname":"t2_by77ogdhr","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"We need more quants, capn!\\n\\nInitial findings = meh","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n21k71k","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;We need more quants, capn!&lt;/p&gt;\\n\\n&lt;p&gt;Initial findings = meh&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lus2yw","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lus2yw/new_models_from_nvidia/n21k71k/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752003548,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}}],"before":null}},"user_reports":[],"saved":false,"id":"n211edb","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Secure_Reflection409","can_mod_post":false,"created_utc":1751998421,"send_replies":true,"parent_id":"t3_1lus2yw","score":18,"author_fullname":"t2_by77ogdhr","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"That's a 14b model that allegedly outperforms the old R1?\\n\\n\\nThis is amazing news for us 16GB plebs, if true.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n211edb","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;That&amp;#39;s a 14b model that allegedly outperforms the old R1?&lt;/p&gt;\\n\\n&lt;p&gt;This is amazing news for us 16GB plebs, if true.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lus2yw/new_models_from_nvidia/n211edb/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751998421,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lus2yw","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":18}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n21dhsb","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"smahs9","can_mod_post":false,"created_utc":1752001683,"send_replies":true,"parent_id":"t3_1lus2yw","score":7,"author_fullname":"t2_neyagc1uz","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"There appears to be a chat template problem in llama.cpp. The reasoning is generated without the starting &lt;think&gt; tag, but does generate a &lt;/think&gt; tag later. Not sure if its just me, or others who tried also observed this. Otherwise, the \\"thoughts\\" of the 14B variant are in proper markdown syntax.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n21dhsb","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;There appears to be a chat template problem in llama.cpp. The reasoning is generated without the starting &amp;lt;think&amp;gt; tag, but does generate a &amp;lt;/think&amp;gt; tag later. Not sure if its just me, or others who tried also observed this. Otherwise, the &amp;quot;thoughts&amp;quot; of the 14B variant are in proper markdown syntax.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lus2yw/new_models_from_nvidia/n21dhsb/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752001683,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lus2yw","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":7}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n25cnu9","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"SkyFeistyLlama8","can_mod_post":false,"created_utc":1752055708,"send_replies":true,"parent_id":"t3_1lus2yw","score":6,"author_fullname":"t2_1hgbaqgbnq","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"The 32B and 14B need to be compared against THUD GLM-4 32B. That's been my gold standard for local coding models so far.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n25cnu9","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The 32B and 14B need to be compared against THUD GLM-4 32B. That&amp;#39;s been my gold standard for local coding models so far.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lus2yw/new_models_from_nvidia/n25cnu9/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752055708,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lus2yw","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}},{"kind":"t1","data":{"total_awards_received":0,"approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"ups":3,"removal_reason":null,"link_id":"t3_1lus2yw","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"total_awards_received":0,"approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"ups":1,"removal_reason":null,"link_id":"t3_1lus2yw","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n223tjc","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Rrraptr","can_mod_post":false,"send_replies":true,"parent_id":"t1_n222tbh","score":1,"author_fullname":"t2_i7v1u","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"As the model is based on Qwen Coder 2.5, I'll provide the recommended settings for it: 'temperature': 0.7, 'top\\\\_p': 0.8, 'top\\\\_k': 20","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n223tjc","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;As the model is based on Qwen Coder 2.5, I&amp;#39;ll provide the recommended settings for it: &amp;#39;temperature&amp;#39;: 0.7, &amp;#39;top_p&amp;#39;: 0.8, &amp;#39;top_k&amp;#39;: 20&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lus2yw","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lus2yw/new_models_from_nvidia/n223tjc/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752008896,"author_flair_text":null,"treatment_tags":[],"created_utc":1752008896,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n222tbh","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":"DELETED","no_follow":true,"author":"[deleted]","can_mod_post":false,"created_utc":1752008618,"send_replies":true,"parent_id":"t1_n221q05","score":1,"approved_by":null,"report_reasons":null,"all_awardings":[],"subreddit_id":"t5_81eyvm","body":"[removed]","edited":false,"author_flair_css_class":null,"downs":0,"is_submitter":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;[removed]&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"permalink":"/r/LocalLLaMA/comments/1lus2yw/new_models_from_nvidia/n222tbh/","associated_award":null,"stickied":false,"subreddit_type":"public","can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":"dark","score_hidden":false,"media_metadata":{"uhat5jevspbf1":{"status":"valid","e":"Image","m":"image/png","p":[{"y":75,"x":108,"u":"https://preview.redd.it/uhat5jevspbf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=609750a12b4ec6593a3fec61083d83d6a939065a"},{"y":151,"x":216,"u":"https://preview.redd.it/uhat5jevspbf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=cd3861c1bb01e405c6737b14094f525c846e56e4"},{"y":224,"x":320,"u":"https://preview.redd.it/uhat5jevspbf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=f9f8b1bf49011c947072e8623dd514fa70ec390f"},{"y":449,"x":640,"u":"https://preview.redd.it/uhat5jevspbf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=b1a2e44f3d1d9098af1c46b723816c34cd15fd25"},{"y":674,"x":960,"u":"https://preview.redd.it/uhat5jevspbf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=4e93aab9a87782c3a98ba00c1a2a35b455311ecc"}],"s":{"y":688,"x":979,"u":"https://preview.redd.it/uhat5jevspbf1.png?width=979&amp;format=png&amp;auto=webp&amp;s=a294a0c5c4afbb26767f77f2a63ff5b0546b2be8"},"id":"uhat5jevspbf1"}},"num_reports":null,"locked":false,"name":"t1_n222tbh","created":1752008618,"subreddit":"LocalLLaMA","author_flair_text":null,"treatment_tags":[],"collapsed":true,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":"","collapsed_because_crowd_control":null,"mod_reports":[],"mod_note":null,"distinguished":null}}],"before":null}},"user_reports":[],"saved":false,"id":"n221q05","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Rrraptr","can_mod_post":false,"created_utc":1752008319,"send_replies":true,"parent_id":"t1_n21wcqx","score":2,"author_fullname":"t2_i7v1u","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"wrong inference settings?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n221q05","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;wrong inference settings?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lus2yw","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lus2yw/new_models_from_nvidia/n221q05/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752008319,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n22ts65","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"jonydevidson","can_mod_post":false,"created_utc":1752016799,"send_replies":true,"parent_id":"t1_n21wcqx","score":1,"author_fullname":"t2_14ytk0","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"That's a shitty-ass prompt, though.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n22ts65","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;That&amp;#39;s a shitty-ass prompt, though.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lus2yw","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lus2yw/new_models_from_nvidia/n22ts65/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752016799,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n21wcqx","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":"DELETED","no_follow":true,"author":"[deleted]","can_mod_post":false,"send_replies":true,"parent_id":"t3_1lus2yw","score":3,"approved_by":null,"report_reasons":null,"subreddit":"LocalLLaMA","all_awardings":[],"subreddit_id":"t5_81eyvm","body":"[removed]","edited":1752007872,"downs":0,"author_flair_css_class":null,"collapsed":true,"is_submitter":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;[removed]&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"associated_award":null,"stickied":false,"subreddit_type":"public","can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":"dark","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lus2yw/new_models_from_nvidia/n21wcqx/","num_reports":null,"locked":false,"name":"t1_n21wcqx","created":1752006886,"media_metadata":{"ndhly00znpbf1":{"status":"valid","e":"Image","m":"image/png","p":[{"y":76,"x":108,"u":"https://preview.redd.it/ndhly00znpbf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=fdaa70b4dd6710b96018eaa413ed4fd410c8cf8c"},{"y":153,"x":216,"u":"https://preview.redd.it/ndhly00znpbf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=f7a9e0c744d38b0e852a0b89132a16e8e2f85e8d"},{"y":227,"x":320,"u":"https://preview.redd.it/ndhly00znpbf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=b73f419bdbe3c44c038dc3875586f0a5e8c8e8cd"},{"y":454,"x":640,"u":"https://preview.redd.it/ndhly00znpbf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=280612fa3829a2bb3a6e6651316b46b2dc5a14f3"},{"y":682,"x":960,"u":"https://preview.redd.it/ndhly00znpbf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=3009ccba2c47aa93766ec28d836b99732d013808"}],"s":{"y":692,"x":974,"u":"https://preview.redd.it/ndhly00znpbf1.png?width=974&amp;format=png&amp;auto=webp&amp;s=c89fa5d713cba6b8ddbc24df7c1700b583dafa80"},"id":"ndhly00znpbf1"}},"author_flair_text":null,"treatment_tags":[],"created_utc":1752006886,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"","collapsed_because_crowd_control":null,"mod_reports":[],"mod_note":null,"distinguished":null}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n22f64a","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"jacek2023","can_mod_post":false,"send_replies":true,"parent_id":"t1_n22a0di","score":5,"author_fullname":"t2_vqgbql9w","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"In that case here is 14B\\n\\nhttps://huggingface.co/mradermacher/OpenCodeReasoning-Nemotron-1.1-14B-GGUF","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n22f64a","is_submitter":true,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;In that case here is 14B&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://huggingface.co/mradermacher/OpenCodeReasoning-Nemotron-1.1-14B-GGUF\\"&gt;https://huggingface.co/mradermacher/OpenCodeReasoning-Nemotron-1.1-14B-GGUF&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lus2yw","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lus2yw/new_models_from_nvidia/n22f64a/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752012200,"author_flair_text":"llama.cpp","treatment_tags":[],"created_utc":1752012200,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}}],"before":null}},"user_reports":[],"saved":false,"id":"n22a0di","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"taltoris","can_mod_post":false,"send_replies":true,"parent_id":"t1_n229nze","score":1,"author_fullname":"t2_4ahit","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Looked for these, but didn't see any! Good find!","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n22a0di","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Looked for these, but didn&amp;#39;t see any! Good find!&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lus2yw","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lus2yw/new_models_from_nvidia/n22a0di/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752010675,"author_flair_text":null,"treatment_tags":[],"created_utc":1752010675,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n229nze","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"jacek2023","can_mod_post":false,"created_utc":1752010574,"send_replies":true,"parent_id":"t1_n229fqb","score":6,"author_fullname":"t2_vqgbql9w","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Why not?\\nhttps://huggingface.co/mradermacher/OpenCodeReasoning-Nemotron-1.1-32B-GGUF","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n229nze","is_submitter":true,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Why not?\\n&lt;a href=\\"https://huggingface.co/mradermacher/OpenCodeReasoning-Nemotron-1.1-32B-GGUF\\"&gt;https://huggingface.co/mradermacher/OpenCodeReasoning-Nemotron-1.1-32B-GGUF&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lus2yw","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lus2yw/new_models_from_nvidia/n229nze/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752010574,"author_flair_text":"llama.cpp","treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}}],"before":null}},"user_reports":[],"saved":false,"id":"n229fqb","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"taltoris","can_mod_post":false,"created_utc":1752010505,"send_replies":true,"parent_id":"t3_1lus2yw","score":2,"author_fullname":"t2_4ahit","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Looks good. Can we get some Quants?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n229fqb","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Looks good. Can we get some Quants?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lus2yw/new_models_from_nvidia/n229fqb/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752010505,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lus2yw","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n25k0fw","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"TechnoRhythmic","can_mod_post":false,"created_utc":1752059124,"send_replies":true,"parent_id":"t3_1lus2yw","score":1,"author_fullname":"t2_1jupk37iwu","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Tried a quantized version. And things escalated quickly with an Hi. (Had to abort)\\n\\n\`ollama run  hf.co/mradermacher/OpenCodeReasoning-Nemotron-1.1-32B-GGUF\`\\n\\n\`pulling manifest\`\\n\\n\`......\`\\n\\n\`success\`\\n\\n\`&gt;&gt;&gt;\` Hi\\n\\nHello! How can I help you today?\\n\\n\\\\- If you have a question, please ask it.\\n\\n\\\\- If you want to generate an image, you can describe what you want and\\n\\nI'll try my best.\\n\\n\\\\- Alternatively, if you need a story or a poem, let me know.\\n\\nBut note: I cannot generate images directly. However, I can provide text\\n\\nthat describes the image, or if you have an idea for an image, we might\\n\\nuse an API? But since I'm an AI without direct image generation\\n\\ncapabilities, I'll stick to providing textual responses.\\n\\nSo, what's on your mind?\\n\\n&lt;/think&gt;\\n\\n\\nThe task is to generate a response for the user input. The user can ask\\n\\nany question or request any type of content (like an image, story, etc.).\\n\\nHowever, since the assistant cannot generate images directly, it should\\n\\nhandle such requests appropriately by either describing the image in text\\n\\nor redirecting to another service? But note: the problem says \\"if you want\\n\\nto generate an image, you can describe what you want and I'll try my best\\"\\n\\n— but the assistant is text-based.\\n\\nActually, the assistant should not generate images. Instead, for image\\n\\nrequests, it should provide a detailed description of the requested image\\n\\nin text form? Or perhaps the assistant can use an external API? But the\\n\\nproblem does not specify that.\\n\\nRe-reading the instructions: \\"If you want to generate an image, you can\\n\\ndescribe what you want and I'll try my best.\\" — meaning the assistant will\\n\\ngenerate a textual description of the image?\\n\\n**................... 4000 more words with lots of musings and some psuedo code .......**","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n25k0fw","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Tried a quantized version. And things escalated quickly with an Hi. (Had to abort)&lt;/p&gt;\\n\\n&lt;p&gt;&lt;code&gt;ollama run  hf.co/mradermacher/OpenCodeReasoning-Nemotron-1.1-32B-GGUF&lt;/code&gt;&lt;/p&gt;\\n\\n&lt;p&gt;&lt;code&gt;pulling manifest&lt;/code&gt;&lt;/p&gt;\\n\\n&lt;p&gt;&lt;code&gt;......&lt;/code&gt;&lt;/p&gt;\\n\\n&lt;p&gt;&lt;code&gt;success&lt;/code&gt;&lt;/p&gt;\\n\\n&lt;p&gt;&lt;code&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/code&gt; Hi&lt;/p&gt;\\n\\n&lt;p&gt;Hello! How can I help you today?&lt;/p&gt;\\n\\n&lt;p&gt;- If you have a question, please ask it.&lt;/p&gt;\\n\\n&lt;p&gt;- If you want to generate an image, you can describe what you want and&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;ll try my best.&lt;/p&gt;\\n\\n&lt;p&gt;- Alternatively, if you need a story or a poem, let me know.&lt;/p&gt;\\n\\n&lt;p&gt;But note: I cannot generate images directly. However, I can provide text&lt;/p&gt;\\n\\n&lt;p&gt;that describes the image, or if you have an idea for an image, we might&lt;/p&gt;\\n\\n&lt;p&gt;use an API? But since I&amp;#39;m an AI without direct image generation&lt;/p&gt;\\n\\n&lt;p&gt;capabilities, I&amp;#39;ll stick to providing textual responses.&lt;/p&gt;\\n\\n&lt;p&gt;So, what&amp;#39;s on your mind?&lt;/p&gt;\\n\\n&lt;p&gt;&amp;lt;/think&amp;gt;&lt;/p&gt;\\n\\n&lt;p&gt;The task is to generate a response for the user input. The user can ask&lt;/p&gt;\\n\\n&lt;p&gt;any question or request any type of content (like an image, story, etc.).&lt;/p&gt;\\n\\n&lt;p&gt;However, since the assistant cannot generate images directly, it should&lt;/p&gt;\\n\\n&lt;p&gt;handle such requests appropriately by either describing the image in text&lt;/p&gt;\\n\\n&lt;p&gt;or redirecting to another service? But note: the problem says &amp;quot;if you want&lt;/p&gt;\\n\\n&lt;p&gt;to generate an image, you can describe what you want and I&amp;#39;ll try my best&amp;quot;&lt;/p&gt;\\n\\n&lt;p&gt;— but the assistant is text-based.&lt;/p&gt;\\n\\n&lt;p&gt;Actually, the assistant should not generate images. Instead, for image&lt;/p&gt;\\n\\n&lt;p&gt;requests, it should provide a detailed description of the requested image&lt;/p&gt;\\n\\n&lt;p&gt;in text form? Or perhaps the assistant can use an external API? But the&lt;/p&gt;\\n\\n&lt;p&gt;problem does not specify that.&lt;/p&gt;\\n\\n&lt;p&gt;Re-reading the instructions: &amp;quot;If you want to generate an image, you can&lt;/p&gt;\\n\\n&lt;p&gt;describe what you want and I&amp;#39;ll try my best.&amp;quot; — meaning the assistant will&lt;/p&gt;\\n\\n&lt;p&gt;generate a textual description of the image?&lt;/p&gt;\\n\\n&lt;p&gt;&lt;strong&gt;................... 4000 more words with lots of musings and some psuedo code .......&lt;/strong&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lus2yw/new_models_from_nvidia/n25k0fw/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752059124,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lus2yw","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n20regy","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"LocoMod","can_mod_post":false,"created_utc":1751995714,"send_replies":true,"parent_id":"t1_n20e06f","score":18,"author_fullname":"t2_6uuoq","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Most models start degrading significantly after \\\\~16k tokens which is why context engineering is a thing to this day.\\n\\nhttps://preview.redd.it/pvv1ueuqqobf1.jpeg?width=1434&amp;format=pjpg&amp;auto=webp&amp;s=ff63d5e61a3a078efe47d963559435a7de6a2840","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n20regy","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Most models start degrading significantly after ~16k tokens which is why context engineering is a thing to this day.&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://preview.redd.it/pvv1ueuqqobf1.jpeg?width=1434&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=ff63d5e61a3a078efe47d963559435a7de6a2840\\"&gt;https://preview.redd.it/pvv1ueuqqobf1.jpeg?width=1434&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=ff63d5e61a3a078efe47d963559435a7de6a2840&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lus2yw","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lus2yw/new_models_from_nvidia/n20regy/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751995714,"media_metadata":{"pvv1ueuqqobf1":{"status":"valid","e":"Image","m":"image/jpeg","p":[{"y":113,"x":108,"u":"https://preview.redd.it/pvv1ueuqqobf1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=95788a3dc1a09c5007c4ee466b9f94f39ce1b321"},{"y":227,"x":216,"u":"https://preview.redd.it/pvv1ueuqqobf1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=2948a4581407469f278af97188283b82842fd5a0"},{"y":337,"x":320,"u":"https://preview.redd.it/pvv1ueuqqobf1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=0b483d801ae805690658acd7e15b84f3673ac344"},{"y":675,"x":640,"u":"https://preview.redd.it/pvv1ueuqqobf1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=8613528032285ec9b9e782817b74717a8bd72f86"},{"y":1012,"x":960,"u":"https://preview.redd.it/pvv1ueuqqobf1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=b8b8eab4351fb2b324d3f0fe666338660b06dd6c"},{"y":1139,"x":1080,"u":"https://preview.redd.it/pvv1ueuqqobf1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=52b8ba5f0cd388b5e26728159d51cdab060d92e6"}],"s":{"y":1513,"x":1434,"u":"https://preview.redd.it/pvv1ueuqqobf1.jpeg?width=1434&amp;format=pjpg&amp;auto=webp&amp;s=ff63d5e61a3a078efe47d963559435a7de6a2840"},"id":"pvv1ueuqqobf1"}},"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":18}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n221swe","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"madsheep","can_mod_post":false,"send_replies":true,"parent_id":"t1_n20p8qu","score":1,"author_fullname":"t2_3hlhf","approved_by":null,"mod_note":null,"all_awardings":[],"body":"ah that makes sense, thanks for the insight ","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n221swe","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;ah that makes sense, thanks for the insight &lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lus2yw","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lus2yw/new_models_from_nvidia/n221swe/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752008341,"author_flair_text":null,"treatment_tags":[],"created_utc":1752008341,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n20p8qu","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"tomz17","can_mod_post":false,"send_replies":true,"parent_id":"t1_n20ni05","score":7,"author_fullname":"t2_1mhx5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"The typical modality is that you strip out the thinking from the context before sending the next prompt.  Most LLM templtes do that automatically, but it may require a checkbox or a flag in whatever software you are using.  In that way, it should not use any more context than a non-thinking model (in fact it may use less, since the thinking models tend to produce more concise outputs, in my experience).","edited":false,"author_flair_css_class":null,"name":"t1_n20p8qu","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The typical modality is that you strip out the thinking from the context before sending the next prompt.  Most LLM templtes do that automatically, but it may require a checkbox or a flag in whatever software you are using.  In that way, it should not use any more context than a non-thinking model (in fact it may use less, since the thinking models tend to produce more concise outputs, in my experience).&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lus2yw","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lus2yw/new_models_from_nvidia/n20p8qu/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751995129,"author_flair_text":null,"collapsed":false,"created_utc":1751995129,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":7}}],"before":null}},"user_reports":[],"saved":false,"id":"n20ni05","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"madsheep","can_mod_post":false,"send_replies":true,"parent_id":"t1_n20m8g8","score":2,"author_fullname":"t2_3hlhf","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"yeah, I wasn’t sure thats why I was asking - looking around now.\\n\\nIn this case 64k sound good but its a reasoning model so might be not that much after all","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n20ni05","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;yeah, I wasn’t sure thats why I was asking - looking around now.&lt;/p&gt;\\n\\n&lt;p&gt;In this case 64k sound good but its a reasoning model so might be not that much after all&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lus2yw","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lus2yw/new_models_from_nvidia/n20ni05/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751994653,"author_flair_text":null,"treatment_tags":[],"created_utc":1751994653,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n20m8g8","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"tomz17","can_mod_post":false,"send_replies":true,"parent_id":"t1_n20g7mh","score":4,"author_fullname":"t2_1mhx5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"didn't qwen 2.5 coder have a 128k context?","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n20m8g8","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;didn&amp;#39;t qwen 2.5 coder have a 128k context?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lus2yw","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lus2yw/new_models_from_nvidia/n20m8g8/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751994309,"author_flair_text":null,"treatment_tags":[],"created_utc":1751994309,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n24d8cc","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"madsheep","can_mod_post":false,"send_replies":true,"parent_id":"t1_n22uthw","score":0,"author_fullname":"t2_3hlhf","approved_by":null,"mod_note":null,"all_awardings":[],"body":"I know I said I am out, but this is just too funny. So now your point is that the Local community should expect larger models, only a few of us can afford to run?","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n24d8cc","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I know I said I am out, but this is just too funny. So now your point is that the Local community should expect larger models, only a few of us can afford to run?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lus2yw","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lus2yw/new_models_from_nvidia/n24d8cc/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752036725,"author_flair_text":null,"treatment_tags":[],"created_utc":1752036725,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}},"user_reports":[],"saved":false,"id":"n22uthw","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"cantgetthistowork","can_mod_post":false,"send_replies":true,"parent_id":"t1_n20lm94","score":0,"author_fullname":"t2_j1i0o","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Just because it's free doesn't mean it's good. R1 is free, 128k context and amazing. More of that is what we need. Not more 32b garbage that is unusable halfway through the context.","edited":false,"author_flair_css_class":null,"name":"t1_n22uthw","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Just because it&amp;#39;s free doesn&amp;#39;t mean it&amp;#39;s good. R1 is free, 128k context and amazing. More of that is what we need. Not more 32b garbage that is unusable halfway through the context.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lus2yw","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lus2yw/new_models_from_nvidia/n22uthw/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752017132,"author_flair_text":null,"collapsed":false,"created_utc":1752017132,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}},"user_reports":[],"saved":false,"id":"n20lm94","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"madsheep","can_mod_post":false,"send_replies":true,"parent_id":"t1_n20h38a","score":4,"author_fullname":"t2_3hlhf","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"oh so your point is we got the biggest ctx size at 32b for free in probably quite a decent quality model and in return we should call their efforts pathetic? Got ya.\\n\\nI’m out.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n20lm94","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;oh so your point is we got the biggest ctx size at 32b for free in probably quite a decent quality model and in return we should call their efforts pathetic? Got ya.&lt;/p&gt;\\n\\n&lt;p&gt;I’m out.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lus2yw","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lus2yw/new_models_from_nvidia/n20lm94/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751994139,"author_flair_text":null,"treatment_tags":[],"created_utc":1751994139,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}}],"before":null}},"user_reports":[],"saved":false,"id":"n20h38a","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"cantgetthistowork","can_mod_post":false,"send_replies":true,"parent_id":"t1_n20g7mh","score":-4,"author_fullname":"t2_j1i0o","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Nothing. They should have made a bigger model","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n20h38a","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Nothing. They should have made a bigger model&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lus2yw","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lus2yw/new_models_from_nvidia/n20h38a/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751992878,"author_flair_text":null,"treatment_tags":[],"created_utc":1751992878,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-4}}],"before":null}},"user_reports":[],"saved":false,"id":"n20g7mh","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"madsheep","can_mod_post":false,"created_utc":1751992628,"send_replies":true,"parent_id":"t1_n20e06f","score":6,"author_fullname":"t2_3hlhf","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Which 32b model has bigger context and similar scores? Glm comes to mind but thats 32k ctx right?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n20g7mh","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Which 32b model has bigger context and similar scores? Glm comes to mind but thats 32k ctx right?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lus2yw","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lus2yw/new_models_from_nvidia/n20g7mh/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751992628,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}}],"before":null}},"user_reports":[],"saved":false,"id":"n20e06f","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":"LOW_SCORE","no_follow":true,"author":"cantgetthistowork","can_mod_post":false,"created_utc":1751992007,"send_replies":true,"parent_id":"t3_1lus2yw","score":-12,"author_fullname":"t2_j1i0o","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":true,"body":"64K for a small model is pathetic because you'll burn through context trying to handhold it","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n20e06f","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;64K for a small model is pathetic because you&amp;#39;ll burn through context trying to handhold it&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":"comment score below threshold","distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lus2yw/new_models_from_nvidia/n20e06f/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751992007,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lus2yw","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-12}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n27oj15","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"UsualResult","can_mod_post":false,"created_utc":1752082523,"send_replies":true,"parent_id":"t3_1lus2yw","score":1,"author_fullname":"t2_10iarzku","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I tried out 7b last night (q8_0 GGUF) and it falls into loops where it thinks the same thoughts over and over and over again and hardly ever gets to implementation. I'm not able to run the larger models at an acceptable speed, so I have no info on them. I didn't play with repetition penalty, temperature or anything else, but I guess the defaults were not that great.\\n\\nI'll be sticking with the regular qwen for now. Waiting to see what other feedback happens about these.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n27oj15","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I tried out 7b last night (q8_0 GGUF) and it falls into loops where it thinks the same thoughts over and over and over again and hardly ever gets to implementation. I&amp;#39;m not able to run the larger models at an acceptable speed, so I have no info on them. I didn&amp;#39;t play with repetition penalty, temperature or anything else, but I guess the defaults were not that great.&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;ll be sticking with the regular qwen for now. Waiting to see what other feedback happens about these.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lus2yw/new_models_from_nvidia/n27oj15/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752082523,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lus2yw","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),o=()=>e.jsx(l,{data:t});export{o as default};
