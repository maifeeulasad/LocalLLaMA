import{j as e}from"./index-BOnf-UhU.js";import{R as t}from"./RedditPostRenderer-Ce39qICS.js";import"./index-CDase6VD.js";const a=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"On the website of [https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct](https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct), there is an \\"Inference Providers\\" section where I can chat with Llama-3.2-1B-Instruct. It gives reasonable responses like the following.\\n\\nhttps://preview.redd.it/r7n08nqxzt9f1.png?width=1238&amp;format=png&amp;auto=webp&amp;s=bbb16c1049feafba2d026e2d93e2a0de65199440\\n\\nHowever, when I download and run the model with the following code, it does not run properly. I have asked the same questions, but got bad responses. \\n\\nI am new to LLMs and wondering what causes the difference. Do I use the model not in the right way?\\n\\n    from transformers import AutoModelForCausalLM, AutoTokenizer\\n    import torch\\n    import ipdb\\n    \\n    model_name = \\"Llama-3.2-1B-Instruct\\"\\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\\n    model = AutoModelForCausalLM.from_pretrained(\\n        model_name,\\n        device_map=\\"cuda\\", \\n        torch_dtype=torch.float16,)\\n    \\n    def format_prompt(instruction: str, system_prompt: str = \\"You are a helpful assistant.\\"):\\n        if system_prompt:\\n            return f\\"&lt;s&gt;[INST] &lt;&lt;SYS&gt;&gt;\\\\n{system_prompt}\\\\n&lt;&lt;/SYS&gt;&gt;\\\\n\\\\n{instruction.strip()} [/INST]\\"\\n        else:\\n            return f\\"&lt;s&gt;[INST] {instruction.strip()} [/INST]\\"\\n    \\n    def generate_response(prompt, max_new_tokens=256):\\n        inputs = tokenizer(prompt, return_tensors=\\"pt\\").to(model.device)\\n        with torch.no_grad():\\n            outputs = model.generate(\\n                input_ids=inputs[\\"input_ids\\"],\\n                attention_mask=inputs[\\"attention_mask\\"],\\n                max_new_tokens=max_new_tokens,\\n                temperature=0.7,\\n                top_p=0.9,\\n                do_sample=True,\\n                pad_token_id=tokenizer.eos_token_id,\\n                eos_token_id=tokenizer.eos_token_id\\n            )\\n        decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\\n        response = decoded.split(\\"[/INST]\\")[-1].strip()\\n        return response\\n    \\n    if __name__ == \\"__main__\\":\\n        print(\\"Chat with LLaMA-3.2-1B-Instruct. Type 'exit' to stop.\\")\\n        while True:\\n            user_input = input(\\"You: \\")\\n            if user_input.lower() in [\\"exit\\", \\"quit\\"]:\\n                break\\n            prompt = format_prompt(user_input)\\n            response = generate_response(prompt)\\n            print(\\"LLaMA:\\", response)\\n\\nhttps://preview.redd.it/d203h6p71u9f1.png?width=1914&amp;format=png&amp;auto=webp&amp;s=0a6a82adfe03861ce268a8e64c9298c443d871fd\\n\\n","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Why the local Llama-3.2-1B-Instruct is not as smart as the one provided on Hugging Face?","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":75,"top_awarded_type":null,"hide_score":false,"media_metadata":{"d203h6p71u9f1":{"status":"valid","e":"Image","m":"image/png","p":[{"y":15,"x":108,"u":"https://preview.redd.it/d203h6p71u9f1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=de697fe366c5ac29ab2d5cbe17ecfb1a1fb2c50b"},{"y":31,"x":216,"u":"https://preview.redd.it/d203h6p71u9f1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=a50b3c9ff282c57b9ac1f042a564b1e8b7ebaeae"},{"y":47,"x":320,"u":"https://preview.redd.it/d203h6p71u9f1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=da93f7f6918a4cd9d6f6144c1b9135e319b5de06"},{"y":94,"x":640,"u":"https://preview.redd.it/d203h6p71u9f1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=6aace5072eca8cbebd9d1d911ce9ba61a0659d11"},{"y":141,"x":960,"u":"https://preview.redd.it/d203h6p71u9f1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=6fe5c63a1057d7030d9d0a6547b936fa458ff6b8"},{"y":159,"x":1080,"u":"https://preview.redd.it/d203h6p71u9f1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b03d8b98d9d1e0fae4405224fe7f86220a7744ca"}],"s":{"y":282,"x":1914,"u":"https://preview.redd.it/d203h6p71u9f1.png?width=1914&amp;format=png&amp;auto=webp&amp;s=0a6a82adfe03861ce268a8e64c9298c443d871fd"},"id":"d203h6p71u9f1"},"r7n08nqxzt9f1":{"status":"valid","e":"Image","m":"image/png","p":[{"y":89,"x":108,"u":"https://preview.redd.it/r7n08nqxzt9f1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=509694e291df3e487312b45b50622d2cc5893af4"},{"y":178,"x":216,"u":"https://preview.redd.it/r7n08nqxzt9f1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=3ed7da3e235d2b5ca689cb2b281afce9d795cb34"},{"y":264,"x":320,"u":"https://preview.redd.it/r7n08nqxzt9f1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=fade3decfd6359dee49b775f059d1047fbdc089b"},{"y":529,"x":640,"u":"https://preview.redd.it/r7n08nqxzt9f1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=a4d80025c3ca36c48821f196df85812809248fde"},{"y":794,"x":960,"u":"https://preview.redd.it/r7n08nqxzt9f1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=e6da8785bdd762a1686d2b8f7cec88a813c2b5d8"},{"y":893,"x":1080,"u":"https://preview.redd.it/r7n08nqxzt9f1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=3dda55db028da03b5086ddfb744a3494f0229383"}],"s":{"y":1024,"x":1238,"u":"https://preview.redd.it/r7n08nqxzt9f1.png?width=1238&amp;format=png&amp;auto=webp&amp;s=bbb16c1049feafba2d026e2d93e2a0de65199440"},"id":"r7n08nqxzt9f1"}},"name":"t3_1lnacbb","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.6,"author_flair_background_color":null,"ups":6,"total_awards_received":0,"media_embed":{},"thumbnail_width":140,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_1mztkegia6","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":6,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"https://external-preview.redd.it/RiiSR8W28H0Xiz1FF_p6kKoYDp7PN9LmuOIWUiMJBCs.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=68163bbac1fd692738e265226b7b8002d844e11a","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"post_hint":"self","content_categories":null,"is_self":true,"subreddit_type":"public","created":1751188392,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;On the website of &lt;a href=\\"https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct\\"&gt;https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct&lt;/a&gt;, there is an &amp;quot;Inference Providers&amp;quot; section where I can chat with Llama-3.2-1B-Instruct. It gives reasonable responses like the following.&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://preview.redd.it/r7n08nqxzt9f1.png?width=1238&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bbb16c1049feafba2d026e2d93e2a0de65199440\\"&gt;https://preview.redd.it/r7n08nqxzt9f1.png?width=1238&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=bbb16c1049feafba2d026e2d93e2a0de65199440&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;However, when I download and run the model with the following code, it does not run properly. I have asked the same questions, but got bad responses. &lt;/p&gt;\\n\\n&lt;p&gt;I am new to LLMs and wondering what causes the difference. Do I use the model not in the right way?&lt;/p&gt;\\n\\n&lt;pre&gt;&lt;code&gt;from transformers import AutoModelForCausalLM, AutoTokenizer\\nimport torch\\nimport ipdb\\n\\nmodel_name = &amp;quot;Llama-3.2-1B-Instruct&amp;quot;\\ntokenizer = AutoTokenizer.from_pretrained(model_name)\\nmodel = AutoModelForCausalLM.from_pretrained(\\n    model_name,\\n    device_map=&amp;quot;cuda&amp;quot;, \\n    torch_dtype=torch.float16,)\\n\\ndef format_prompt(instruction: str, system_prompt: str = &amp;quot;You are a helpful assistant.&amp;quot;):\\n    if system_prompt:\\n        return f&amp;quot;&amp;lt;s&amp;gt;[INST] &amp;lt;&amp;lt;SYS&amp;gt;&amp;gt;\\\\n{system_prompt}\\\\n&amp;lt;&amp;lt;/SYS&amp;gt;&amp;gt;\\\\n\\\\n{instruction.strip()} [/INST]&amp;quot;\\n    else:\\n        return f&amp;quot;&amp;lt;s&amp;gt;[INST] {instruction.strip()} [/INST]&amp;quot;\\n\\ndef generate_response(prompt, max_new_tokens=256):\\n    inputs = tokenizer(prompt, return_tensors=&amp;quot;pt&amp;quot;).to(model.device)\\n    with torch.no_grad():\\n        outputs = model.generate(\\n            input_ids=inputs[&amp;quot;input_ids&amp;quot;],\\n            attention_mask=inputs[&amp;quot;attention_mask&amp;quot;],\\n            max_new_tokens=max_new_tokens,\\n            temperature=0.7,\\n            top_p=0.9,\\n            do_sample=True,\\n            pad_token_id=tokenizer.eos_token_id,\\n            eos_token_id=tokenizer.eos_token_id\\n        )\\n    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)\\n    response = decoded.split(&amp;quot;[/INST]&amp;quot;)[-1].strip()\\n    return response\\n\\nif __name__ == &amp;quot;__main__&amp;quot;:\\n    print(&amp;quot;Chat with LLaMA-3.2-1B-Instruct. Type &amp;#39;exit&amp;#39; to stop.&amp;quot;)\\n    while True:\\n        user_input = input(&amp;quot;You: &amp;quot;)\\n        if user_input.lower() in [&amp;quot;exit&amp;quot;, &amp;quot;quit&amp;quot;]:\\n            break\\n        prompt = format_prompt(user_input)\\n        response = generate_response(prompt)\\n        print(&amp;quot;LLaMA:&amp;quot;, response)\\n&lt;/code&gt;&lt;/pre&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://preview.redd.it/d203h6p71u9f1.png?width=1914&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0a6a82adfe03861ce268a8e64c9298c443d871fd\\"&gt;https://preview.redd.it/d203h6p71u9f1.png?width=1914&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0a6a82adfe03861ce268a8e64c9298c443d871fd&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":true,"is_crosspostable":false,"pinned":false,"over_18":false,"preview":{"images":[{"source":{"url":"https://external-preview.redd.it/RiiSR8W28H0Xiz1FF_p6kKoYDp7PN9LmuOIWUiMJBCs.png?auto=webp&amp;s=1ff3891f4de6d0e1f4df9470a706b4e72624c796","width":1200,"height":648},"resolutions":[{"url":"https://external-preview.redd.it/RiiSR8W28H0Xiz1FF_p6kKoYDp7PN9LmuOIWUiMJBCs.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=b07a77e76d7e80c0852251f2c6141e975971ddeb","width":108,"height":58},{"url":"https://external-preview.redd.it/RiiSR8W28H0Xiz1FF_p6kKoYDp7PN9LmuOIWUiMJBCs.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=512dca3736fc57cd366d1bf67736c8c3faf5f797","width":216,"height":116},{"url":"https://external-preview.redd.it/RiiSR8W28H0Xiz1FF_p6kKoYDp7PN9LmuOIWUiMJBCs.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=ccef885f014afbb18e98e7b3f69cc0d7ec415b78","width":320,"height":172},{"url":"https://external-preview.redd.it/RiiSR8W28H0Xiz1FF_p6kKoYDp7PN9LmuOIWUiMJBCs.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=2bfb801d9de9bf9739d847c79dca9f8dfe661ec0","width":640,"height":345},{"url":"https://external-preview.redd.it/RiiSR8W28H0Xiz1FF_p6kKoYDp7PN9LmuOIWUiMJBCs.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=de740b79ccf12b3254cbbdb5b112a918bd848eda","width":960,"height":518},{"url":"https://external-preview.redd.it/RiiSR8W28H0Xiz1FF_p6kKoYDp7PN9LmuOIWUiMJBCs.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=081b4e57405177a82fa477420fddbd30fbc1dc7e","width":1080,"height":583}],"variants":{},"id":"RiiSR8W28H0Xiz1FF_p6kKoYDp7PN9LmuOIWUiMJBCs"}],"enabled":false},"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"mod_note":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"num_reports":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1lnacbb","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"OkLengthiness2286","discussion_type":null,"num_comments":10,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lnacbb/why_the_local_llama321binstruct_is_not_as_smart/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lnacbb/why_the_local_llama321binstruct_is_not_as_smart/","subreddit_subscribers":492929,"created_utc":1751188392,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0drriv","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"AppearanceHeavy6724","can_mod_post":false,"created_utc":1751189536,"send_replies":true,"parent_id":"t3_1lnacbb","score":36,"author_fullname":"t2_uz37qfx5","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"max_new_tokens=256 too low.\\n\\nmin_p has to be set too at 0.05.\\n\\nEDIT: Just checked your screenshot  - you have wrong chat template.\\n\\nUse  llama3 from here: https://github.com/ggml-org/llama.cpp/wiki/Templates-supported-by-llama_chat_apply_template","edited":1751189975,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0drriv","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;max_new_tokens=256 too low.&lt;/p&gt;\\n\\n&lt;p&gt;min_p has to be set too at 0.05.&lt;/p&gt;\\n\\n&lt;p&gt;EDIT: Just checked your screenshot  - you have wrong chat template.&lt;/p&gt;\\n\\n&lt;p&gt;Use  llama3 from here: &lt;a href=\\"https://github.com/ggml-org/llama.cpp/wiki/Templates-supported-by-llama_chat_apply_template\\"&gt;https://github.com/ggml-org/llama.cpp/wiki/Templates-supported-by-llama_chat_apply_template&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnacbb/why_the_local_llama321binstruct_is_not_as_smart/n0drriv/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751189536,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnacbb","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":36}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0ell59","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"YieldMeAlone","can_mod_post":false,"created_utc":1751204029,"send_replies":true,"parent_id":"t3_1lnacbb","score":10,"author_fullname":"t2_1hzcbuoc1y","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"You're using the wrong instruct template. \\"[INST]\\" is for mistral not llama. It doesn't recognise it but treats it as some random string you said.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0ell59","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You&amp;#39;re using the wrong instruct template. &amp;quot;[INST]&amp;quot; is for mistral not llama. It doesn&amp;#39;t recognise it but treats it as some random string you said.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnacbb/why_the_local_llama321binstruct_is_not_as_smart/n0ell59/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751204029,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnacbb","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":10}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0ds1ll","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"LoSboccacc","can_mod_post":false,"created_utc":1751189709,"send_replies":true,"parent_id":"t3_1lnacbb","score":6,"author_fullname":"t2_dievh","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"on top of the other max token and temp suggestion, ditch format prompt, use\\n\\n    tokenizer.apply_chat_template(chat, tokenize=False)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0ds1ll","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;on top of the other max token and temp suggestion, ditch format prompt, use&lt;/p&gt;\\n\\n&lt;pre&gt;&lt;code&gt;tokenizer.apply_chat_template(chat, tokenize=False)\\n&lt;/code&gt;&lt;/pre&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnacbb/why_the_local_llama321binstruct_is_not_as_smart/n0ds1ll/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751189709,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnacbb","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0fhw5n","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Wheynelau","can_mod_post":false,"created_utc":1751214619,"send_replies":true,"parent_id":"t3_1lnacbb","score":2,"author_fullname":"t2_vezlk","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Hey man, understand you might be new. Have you tried using the pipeline methods instead? That will isolate the other issues like tokenizers and handling responses.\\n\\n\\n1. Your code does not handle chat history, so its never knows whats your previous prompt\\n\\n2. Wrong system template, as mentioned by others. Use the apply chat template format, then use generate, that handles all the necessary formatting for generation. You can see here https://huggingface.co/docs/transformers/en/chat_templating\\n\\nPointers below are not breaking changes, but just small issues to take note of. \\n\\n3. Where do you get your values for generation, such as temperature? These may not replicate what is shown in the inference provider, as I'm not sure about the params that are set\\n\\n4. Usually model dtypes are in bfloat16, again this should not affect too much. \\n\\n5. Your question is quite short, max new tokens should not affect it. You will encounter an error or abrupt sentences if you face this issue.\\n\\n\\nTLDR, I would say it's a code issue. Use the chat template with generation. And add a history if needed.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0fhw5n","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Hey man, understand you might be new. Have you tried using the pipeline methods instead? That will isolate the other issues like tokenizers and handling responses.&lt;/p&gt;\\n\\n&lt;ol&gt;\\n&lt;li&gt;&lt;p&gt;Your code does not handle chat history, so its never knows whats your previous prompt&lt;/p&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;p&gt;Wrong system template, as mentioned by others. Use the apply chat template format, then use generate, that handles all the necessary formatting for generation. You can see here &lt;a href=\\"https://huggingface.co/docs/transformers/en/chat_templating\\"&gt;https://huggingface.co/docs/transformers/en/chat_templating&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;\\n&lt;/ol&gt;\\n\\n&lt;p&gt;Pointers below are not breaking changes, but just small issues to take note of. &lt;/p&gt;\\n\\n&lt;ol&gt;\\n&lt;li&gt;&lt;p&gt;Where do you get your values for generation, such as temperature? These may not replicate what is shown in the inference provider, as I&amp;#39;m not sure about the params that are set&lt;/p&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;p&gt;Usually model dtypes are in bfloat16, again this should not affect too much. &lt;/p&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;p&gt;Your question is quite short, max new tokens should not affect it. You will encounter an error or abrupt sentences if you face this issue.&lt;/p&gt;&lt;/li&gt;\\n&lt;/ol&gt;\\n\\n&lt;p&gt;TLDR, I would say it&amp;#39;s a code issue. Use the chat template with generation. And add a history if needed.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnacbb/why_the_local_llama321binstruct_is_not_as_smart/n0fhw5n/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751214619,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnacbb","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0drk2y","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"OkLengthiness2286","can_mod_post":false,"created_utc":1751189407,"send_replies":true,"parent_id":"t1_n0dq9hz","score":0,"author_fullname":"t2_1mztkegia6","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Thanks for the reply. Sorry, I still don't get it. Is Novita a method to use the model? I have searched the internet but didn't get useful information.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0drk2y","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thanks for the reply. Sorry, I still don&amp;#39;t get it. Is Novita a method to use the model? I have searched the internet but didn&amp;#39;t get useful information.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnacbb","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnacbb/why_the_local_llama321binstruct_is_not_as_smart/n0drk2y/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751189407,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}},"user_reports":[],"saved":false,"id":"n0dq9hz","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Amon_star","can_mod_post":false,"created_utc":1751188596,"send_replies":true,"parent_id":"t3_1lnacbb","score":2,"author_fullname":"t2_5hk8oe0h","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"novitas ones specs are 1 temp and 16000 max token","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0dq9hz","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;novitas ones specs are 1 temp and 16000 max token&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnacbb/why_the_local_llama321binstruct_is_not_as_smart/n0dq9hz/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751188596,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnacbb","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0ds8br","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Amon_star","can_mod_post":false,"created_utc":1751189822,"send_replies":true,"parent_id":"t1_n0ds2du","score":2,"author_fullname":"t2_5hk8oe0h","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"if you start new watch 1or2 lmstudio tutorial and learn what is gguf","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0ds8br","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;if you start new watch 1or2 lmstudio tutorial and learn what is gguf&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnacbb","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnacbb/why_the_local_llama321binstruct_is_not_as_smart/n0ds8br/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751189822,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n0ds2du","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Amon_star","can_mod_post":false,"created_utc":1751189722,"send_replies":true,"parent_id":"t3_1lnacbb","score":1,"author_fullname":"t2_5hk8oe0h","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"    max_new_tokens=256 \\n    \\nThink of it as the character it can give, yours is only 3 medium sentences long.But his is about 64 times as much as yours.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0ds2du","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;pre&gt;&lt;code&gt;max_new_tokens=256 \\n&lt;/code&gt;&lt;/pre&gt;\\n\\n&lt;p&gt;Think of it as the character it can give, yours is only 3 medium sentences long.But his is about 64 times as much as yours.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnacbb/why_the_local_llama321binstruct_is_not_as_smart/n0ds2du/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751189722,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnacbb","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0dup38","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"No_Afternoon_4260","can_mod_post":false,"created_utc":1751191314,"send_replies":true,"parent_id":"t3_1lnacbb","score":0,"author_fullname":"t2_cj9kap4bx","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Because you use it quantized?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0dup38","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Because you use it quantized?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnacbb/why_the_local_llama321binstruct_is_not_as_smart/n0dup38/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751191314,"author_flair_text":"llama.cpp","treatment_tags":[],"link_id":"t3_1lnacbb","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":1,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0dy4w6","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"NNN_Throwaway2","can_mod_post":false,"created_utc":1751193362,"send_replies":true,"parent_id":"t3_1lnacbb","score":-4,"author_fullname":"t2_8rrihts9","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"This code scares me.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0dy4w6","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;This code scares me.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnacbb/why_the_local_llama321binstruct_is_not_as_smart/n0dy4w6/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751193362,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnacbb","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-4}}],"before":null}}]`),o=()=>e.jsx(t,{data:a});export{o as default};
