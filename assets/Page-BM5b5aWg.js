import{j as e}from"./index-BOnf-UhU.js";import{R as l}from"./RedditPostRenderer-Ce39qICS.js";import"./index-CDase6VD.js";const a=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"(like deepseek-r1 1.5b)\\nI just can't think of any simple straightforward examples of tasks they're useful / good enough for. And answers on the internet and from other LLMs are just too vague.\\n\\nWhat kind of task with what kind of prompt, system prompt, overall setup worth doing with it? \\n","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"What are the use cases for 1.5B model?","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1m6d6um","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.7,"author_flair_background_color":null,"subreddit_type":"public","ups":4,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_16jnzh","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":4,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1753188511,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;(like deepseek-r1 1.5b)\\nI just can&amp;#39;t think of any simple straightforward examples of tasks they&amp;#39;re useful / good enough for. And answers on the internet and from other LLMs are just too vague.&lt;/p&gt;\\n\\n&lt;p&gt;What kind of task with what kind of prompt, system prompt, overall setup worth doing with it? &lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1m6d6um","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"nathman999","discussion_type":null,"num_comments":10,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1m6d6um/what_are_the_use_cases_for_15b_model/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1m6d6um/what_are_the_use_cases_for_15b_model/","subreddit_subscribers":503254,"created_utc":1753188511,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4im823","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"bick_nyers","can_mod_post":false,"created_utc":1753188655,"send_replies":true,"parent_id":"t3_1m6d6um","score":8,"author_fullname":"t2_6nwld4d3","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Fine-tune classifiers, speculative decoding, making sure your training code works on a cheap GPU.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4im823","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Fine-tune classifiers, speculative decoding, making sure your training code works on a cheap GPU.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6d6um/what_are_the_use_cases_for_15b_model/n4im823/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753188655,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m6d6um","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":8}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4jb47o","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Awwtifishal","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4isyh1","score":5,"author_fullname":"t2_1d96a8k10t","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Any model perfectly sticks to JSON if you use a [grammar](https://github.com/ggml-org/llama.cpp/blob/master/grammars/README.md).","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n4jb47o","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Any model perfectly sticks to JSON if you use a &lt;a href=\\"https://github.com/ggml-org/llama.cpp/blob/master/grammars/README.md\\"&gt;grammar&lt;/a&gt;.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m6d6um","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6d6um/what_are_the_use_cases_for_15b_model/n4jb47o/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753196271,"author_flair_text":null,"treatment_tags":[],"created_utc":1753196271,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4j5l6y","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Amazing_Athlete_2265","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4isyh1","score":1,"author_fullname":"t2_1nw9fzb7dt","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Sometimes JSON, sometimes not.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n4j5l6y","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Sometimes JSON, sometimes not.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m6d6um","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6d6um/what_are_the_use_cases_for_15b_model/n4j5l6y/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753194722,"author_flair_text":null,"treatment_tags":[],"created_utc":1753194722,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4jvfa3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"offlinesir","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4isyh1","score":2,"author_fullname":"t2_jn5ft2le","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Very poorly unless if you restrict their token output, which is possible with many API's and local solutions. You can also prefill the first token to be \\"{\\" to start JSON.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n4jvfa3","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Very poorly unless if you restrict their token output, which is possible with many API&amp;#39;s and local solutions. You can also prefill the first token to be &amp;quot;{&amp;quot; to start JSON.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m6d6um","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6d6um/what_are_the_use_cases_for_15b_model/n4jvfa3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753201922,"author_flair_text":null,"treatment_tags":[],"created_utc":1753201922,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n4isyh1","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Pvt_Twinkietoes","can_mod_post":false,"created_utc":1753190905,"send_replies":true,"parent_id":"t1_n4imq7a","score":2,"author_fullname":"t2_3k9qfjsr","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"How well do they stick to responding in JSON?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4isyh1","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;How well do they stick to responding in JSON?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m6d6um","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6d6um/what_are_the_use_cases_for_15b_model/n4isyh1/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753190905,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n4imq7a","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"offlinesir","can_mod_post":false,"created_utc":1753188827,"send_replies":true,"parent_id":"t3_1m6d6um","score":5,"author_fullname":"t2_jn5ft2le","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I've found it useful for sentiment analysis, eg, how does this person in this tweet feel about x. It's also able to do basic math.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4imq7a","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;ve found it useful for sentiment analysis, eg, how does this person in this tweet feel about x. It&amp;#39;s also able to do basic math.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6d6um/what_are_the_use_cases_for_15b_model/n4imq7a/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753188827,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m6d6um","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4izxl5","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"celsowm","can_mod_post":false,"created_utc":1753193075,"send_replies":true,"parent_id":"t3_1m6d6um","score":2,"author_fullname":"t2_dyvrh","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Summarize docs","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4izxl5","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Summarize docs&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6d6um/what_are_the_use_cases_for_15b_model/n4izxl5/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753193075,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m6d6um","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4jd6m7","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"-dysangel-","can_mod_post":false,"created_utc":1753196845,"send_replies":true,"parent_id":"t3_1m6d6um","score":2,"author_fullname":"t2_12ggykute6","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Smaller models are good for adding a bit of intelligence to utilities, like for example summarising results from a vector database search. Though I usually use 8B models for that kind of thing, because they are fast enough for my use cases, and slightly smarter","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4jd6m7","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Smaller models are good for adding a bit of intelligence to utilities, like for example summarising results from a vector database search. Though I usually use 8B models for that kind of thing, because they are fast enough for my use cases, and slightly smarter&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6d6um/what_are_the_use_cases_for_15b_model/n4jd6m7/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753196845,"author_flair_text":"llama.cpp","treatment_tags":[],"link_id":"t3_1m6d6um","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4jqviz","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"steezy13312","can_mod_post":false,"created_utc":1753200657,"send_replies":true,"parent_id":"t3_1m6d6um","score":2,"author_fullname":"t2_rfjj2","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"In my server, I have a secondary GPU (WX3200 I think, 4GB) and I really just have for occasional need to hook up a monitor, like tweaking BIOS settings, since my V620 doesn't have video out. It doesn't even have an external power connector, just uses power from the PCIE slot.\\n\\nI've found that using Vulkan, I can run small models with long enough context (such as Qwen-0.6B) in there with satisfactory speeds for Open WebUI to use for naming chats, adding tags, etc, and I can just leave that model loaded with a really long TTL, and it's not consuming any precious resources on my more powerful card.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4jqviz","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;In my server, I have a secondary GPU (WX3200 I think, 4GB) and I really just have for occasional need to hook up a monitor, like tweaking BIOS settings, since my V620 doesn&amp;#39;t have video out. It doesn&amp;#39;t even have an external power connector, just uses power from the PCIE slot.&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;ve found that using Vulkan, I can run small models with long enough context (such as Qwen-0.6B) in there with satisfactory speeds for Open WebUI to use for naming chats, adding tags, etc, and I can just leave that model loaded with a really long TTL, and it&amp;#39;s not consuming any precious resources on my more powerful card.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6d6um/what_are_the_use_cases_for_15b_model/n4jqviz/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753200657,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m6d6um","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}}]`),o=()=>e.jsx(l,{data:a});export{o as default};
