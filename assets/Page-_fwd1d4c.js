import{j as e}from"./index-F0NXdzZX.js";import{R as l}from"./RedditPostRenderer-CoEZB1dt.js";import"./index-DrtravXm.js";const t=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Are Mac Studios the best value for money to run big LLMs locally? From what I can see, you can get a Mac Studio for $4-5k with 96GB Ram and you can go up to 512GB. \\n\\nIn comparison, Nvidia GPUs donâ€™t have that much memory and the cards that do are super expensive. I believe an A100 with 40GB is $3k for half the ram. \\n\\nAm I missing something here?","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Does Apple have the best value for money for running LLMs?","link_flair_richtext":[{"e":"text","t":"Discussion"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1m0h7sx","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.52,"author_flair_background_color":null,"subreddit_type":"public","ups":1,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_d2o5eadk","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Discussion","can_mod_post":false,"score":1,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1752584653,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Are Mac Studios the best value for money to run big LLMs locally? From what I can see, you can get a Mac Studio for $4-5k with 96GB Ram and you can go up to 512GB. &lt;/p&gt;\\n\\n&lt;p&gt;In comparison, Nvidia GPUs donâ€™t have that much memory and the cards that do are super expensive. I believe an A100 with 40GB is $3k for half the ram. &lt;/p&gt;\\n\\n&lt;p&gt;Am I missing something here?&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":true,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"5f921ea4-c7bc-11ed-9c23-3a00622979b4","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#646d73","id":"1m0h7sx","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"Jolly-Phone8982","discussion_type":null,"num_comments":34,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1m0h7sx/does_apple_have_the_best_value_for_money_for/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1m0h7sx/does_apple_have_the_best_value_for_money_for/","subreddit_subscribers":499773,"created_utc":1752584653,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n39enza","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"xflareon","can_mod_post":false,"send_replies":true,"parent_id":"t1_n39clyu","score":5,"author_fullname":"t2_a4z6s","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Well generally speaking you don't need to worry about nvlink or anything, as that doesn't affect inference, only training. You really just plug the GPUs into the motherboard and power them, and there's no other setup required. It's super simple, and sometimes surprises people with how little work actually goes into it. \\n\\nOn the flip side, you don't necessarily need more than one GPU. As I said earlier, you may decide that speed is paramount, and opt for the rtx 6000 pro, which is currently 8200usd. It has only 96gb of VRAM, which is significantly lower than the maximum on a Mac Studio, but it also will run 120b models at like 1100tps prompt processing and 20ish tps output.\\n\\nMy current rig is 3 5090s and a 6000 pro, and while it was much more expensive than a Mac at like 15k, I also use it every day, and with every request I save potentially minutes of waiting. Before this i had 4 3090s and an a6000 (non ada), which wasn't bad, but half the tps on output and a third of the prompt processing on the new one. I can still use those cards if I need the vram capacity, but speed matters a lot to me, so it's really a question you have to ask yourself, as at the end of the day if you buy a Mac you also just have a Mac, and some people really enjoy them.\\n\\nI'm a Linux guy personally, and tend to value my time pretty highly, so I opted for the faster hardware.","edited":false,"author_flair_css_class":null,"name":"t1_n39enza","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Well generally speaking you don&amp;#39;t need to worry about nvlink or anything, as that doesn&amp;#39;t affect inference, only training. You really just plug the GPUs into the motherboard and power them, and there&amp;#39;s no other setup required. It&amp;#39;s super simple, and sometimes surprises people with how little work actually goes into it. &lt;/p&gt;\\n\\n&lt;p&gt;On the flip side, you don&amp;#39;t necessarily need more than one GPU. As I said earlier, you may decide that speed is paramount, and opt for the rtx 6000 pro, which is currently 8200usd. It has only 96gb of VRAM, which is significantly lower than the maximum on a Mac Studio, but it also will run 120b models at like 1100tps prompt processing and 20ish tps output.&lt;/p&gt;\\n\\n&lt;p&gt;My current rig is 3 5090s and a 6000 pro, and while it was much more expensive than a Mac at like 15k, I also use it every day, and with every request I save potentially minutes of waiting. Before this i had 4 3090s and an a6000 (non ada), which wasn&amp;#39;t bad, but half the tps on output and a third of the prompt processing on the new one. I can still use those cards if I need the vram capacity, but speed matters a lot to me, so it&amp;#39;s really a question you have to ask yourself, as at the end of the day if you buy a Mac you also just have a Mac, and some people really enjoy them.&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;m a Linux guy personally, and tend to value my time pretty highly, so I opted for the faster hardware.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1m0h7sx","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0h7sx/does_apple_have_the_best_value_for_money_for/n39enza/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752587418,"author_flair_text":null,"collapsed":false,"created_utc":1752587418,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}}],"before":null}},"user_reports":[],"saved":false,"id":"n39clyu","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Jolly-Phone8982","can_mod_post":false,"send_replies":true,"parent_id":"t1_n39b5hf","score":1,"author_fullname":"t2_d2o5eadk","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Yeah, that makes sense. For me itâ€™s more that from Apple itâ€™s just 1 chip thatâ€™s capable of this whereas with nvidia you have to combine multiple gpus and think about the components and NVlink and whatnot to inter-connect them. \\n\\nItâ€™s gonna be interesting to see how the hardware changes with time","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n39clyu","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yeah, that makes sense. For me itâ€™s more that from Apple itâ€™s just 1 chip thatâ€™s capable of this whereas with nvidia you have to combine multiple gpus and think about the components and NVlink and whatnot to inter-connect them. &lt;/p&gt;\\n\\n&lt;p&gt;Itâ€™s gonna be interesting to see how the hardware changes with time&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0h7sx","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0h7sx/does_apple_have_the_best_value_for_money_for/n39clyu/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752586792,"author_flair_text":null,"treatment_tags":[],"created_utc":1752586792,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3af3fw","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Cergorach","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3a9mpo","score":1,"author_fullname":"t2_cs4w88d2l","approved_by":null,"mod_note":null,"all_awardings":[],"body":"I would use a meter that shows you what it's actually drawing from the wall. Gives a LOT better perspective on actual usage.\\n\\nThe 512GB M3 Ultra makes sense when you need a large model that fits in this amount of memory, you don't have an option to use any service or cloud server due to confidentiality clauses, don't have the budget for an H200 and don't want to have a space heater that can warm the whole building during winter...\\n\\nIt's a niche, but it's the same niche that makes you run local LLMs in the first place. Is it ideal? No! Is it workable, yeah... You just need to change your work process and be very accurate in what you ask of the LLM.\\n\\nI run a Mac Mini M4 Pro (20c GPU) 64GB and can run 70b models, sure it's not the fastest on an M4 Pro, but it works. It doesn't work as well as free models though... So when I use LLM for my hobby projects, I just use the 'cloud' LLMs. When working for a client I could use the approved LLM service they use, and if they don't have one, then they prefer not to use it and I won't either. My IT job tends to be... Not really compatible with LLMs, it's often fact finding missions where the marketing material of a solution usually promises to do more then it can actually do OR the documentation is behind a paywall OR the documentation is updated regularly or is very new. The amount of hallucinations that would generate is scary! And I often find that the documentation is often not correct...\\n\\nI *might* eventually get a 4090/5090 or it's ilk for specific projects where cuda works (better) then MLX.","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n3af3fw","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I would use a meter that shows you what it&amp;#39;s actually drawing from the wall. Gives a LOT better perspective on actual usage.&lt;/p&gt;\\n\\n&lt;p&gt;The 512GB M3 Ultra makes sense when you need a large model that fits in this amount of memory, you don&amp;#39;t have an option to use any service or cloud server due to confidentiality clauses, don&amp;#39;t have the budget for an H200 and don&amp;#39;t want to have a space heater that can warm the whole building during winter...&lt;/p&gt;\\n\\n&lt;p&gt;It&amp;#39;s a niche, but it&amp;#39;s the same niche that makes you run local LLMs in the first place. Is it ideal? No! Is it workable, yeah... You just need to change your work process and be very accurate in what you ask of the LLM.&lt;/p&gt;\\n\\n&lt;p&gt;I run a Mac Mini M4 Pro (20c GPU) 64GB and can run 70b models, sure it&amp;#39;s not the fastest on an M4 Pro, but it works. It doesn&amp;#39;t work as well as free models though... So when I use LLM for my hobby projects, I just use the &amp;#39;cloud&amp;#39; LLMs. When working for a client I could use the approved LLM service they use, and if they don&amp;#39;t have one, then they prefer not to use it and I won&amp;#39;t either. My IT job tends to be... Not really compatible with LLMs, it&amp;#39;s often fact finding missions where the marketing material of a solution usually promises to do more then it can actually do OR the documentation is behind a paywall OR the documentation is updated regularly or is very new. The amount of hallucinations that would generate is scary! And I often find that the documentation is often not correct...&lt;/p&gt;\\n\\n&lt;p&gt;I &lt;em&gt;might&lt;/em&gt; eventually get a 4090/5090 or it&amp;#39;s ilk for specific projects where cuda works (better) then MLX.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1m0h7sx","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0h7sx/does_apple_have_the_best_value_for_money_for/n3af3fw/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752597762,"author_flair_text":null,"treatment_tags":[],"created_utc":1752597762,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3a9mpo","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"xflareon","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3a7xf7","score":1,"author_fullname":"t2_a4z6s","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I do think that the 512gb version makes nearly no sense, as if you actually try to use all of that memory it's going to be like tens of minutes per response. \\n\\nAs for power draw, I don't think you would scale up 3090s to that size, it doesn't really make much sense. If you want to run SOTA models in the hundreds of billions of parameters at any kind of useful speed, a few of the 6000 pro is probably your only reasonable option. If you just want to run it at all, you can get away with less than that, but I don't see the point myself. Also, it's not really fair to compare the power consumption like that. It pulls more power while inferencing but also finishes faster. Idle power draw is definitely higher though, my 3 5090s tend to pull 10w each idle if nvtop is to be believed, the 6000 pro is a little higher.","edited":false,"author_flair_css_class":null,"name":"t1_n3a9mpo","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I do think that the 512gb version makes nearly no sense, as if you actually try to use all of that memory it&amp;#39;s going to be like tens of minutes per response. &lt;/p&gt;\\n\\n&lt;p&gt;As for power draw, I don&amp;#39;t think you would scale up 3090s to that size, it doesn&amp;#39;t really make much sense. If you want to run SOTA models in the hundreds of billions of parameters at any kind of useful speed, a few of the 6000 pro is probably your only reasonable option. If you just want to run it at all, you can get away with less than that, but I don&amp;#39;t see the point myself. Also, it&amp;#39;s not really fair to compare the power consumption like that. It pulls more power while inferencing but also finishes faster. Idle power draw is definitely higher though, my 3 5090s tend to pull 10w each idle if nvtop is to be believed, the 6000 pro is a little higher.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1m0h7sx","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0h7sx/does_apple_have_the_best_value_for_money_for/n3a9mpo/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752596252,"author_flair_text":null,"collapsed":false,"created_utc":1752596252,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3a7xf7","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Cergorach","can_mod_post":false,"send_replies":true,"parent_id":"t1_n39b5hf","score":1,"author_fullname":"t2_cs4w88d2l","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Keep in mind that it only has an old M2 Ultra and a M3 Max on there as the fastests Macs. An M3 Ultra has about twice the memory bandwidth as the M3 Max, so I expect about 2x the performance, which very close to the 3090x4 performance. Not surprising, as the 3090 is about 15% faster. But the 3090 is probably faster to first token.\\n\\nBUT... The biggest issue with 4x 3090 is the power draw, both under load is way, way higher. Just the 4x 3090 will do 1400+W under load, add to that the rest of the computer. The Mac Studio M3 Ultra does &lt;200W. Idle the Nvidia cards alone draw 6-7x more power at idle, with the rest of the computer it's probably x10 the power draw... Which you also need to cool.\\n\\nAlso, 4x 3090 = 4x 24GB = 96GB, you can scale up the Mac M3 Ultra up to 512GB of unified RAM. If you were to do that with 3090 cards... 7,5kW of power under load and 350W idle... and about twice the price in secondhand 3090 cars...\\n\\nBut each solution has it's own strengths. But a RTX 6000 Pro is also about 10k, also has 96GB of VRAM, is about twice as fast as the 3090 cards and only draws around 600W under load. It's a win on all fronts compared to a 3090 solution, except price, which isn't really 'fair' due to comparing a second hand product vs a new product.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3a7xf7","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Keep in mind that it only has an old M2 Ultra and a M3 Max on there as the fastests Macs. An M3 Ultra has about twice the memory bandwidth as the M3 Max, so I expect about 2x the performance, which very close to the 3090x4 performance. Not surprising, as the 3090 is about 15% faster. But the 3090 is probably faster to first token.&lt;/p&gt;\\n\\n&lt;p&gt;BUT... The biggest issue with 4x 3090 is the power draw, both under load is way, way higher. Just the 4x 3090 will do 1400+W under load, add to that the rest of the computer. The Mac Studio M3 Ultra does &amp;lt;200W. Idle the Nvidia cards alone draw 6-7x more power at idle, with the rest of the computer it&amp;#39;s probably x10 the power draw... Which you also need to cool.&lt;/p&gt;\\n\\n&lt;p&gt;Also, 4x 3090 = 4x 24GB = 96GB, you can scale up the Mac M3 Ultra up to 512GB of unified RAM. If you were to do that with 3090 cards... 7,5kW of power under load and 350W idle... and about twice the price in secondhand 3090 cars...&lt;/p&gt;\\n\\n&lt;p&gt;But each solution has it&amp;#39;s own strengths. But a RTX 6000 Pro is also about 10k, also has 96GB of VRAM, is about twice as fast as the 3090 cards and only draws around 600W under load. It&amp;#39;s a win on all fronts compared to a 3090 solution, except price, which isn&amp;#39;t really &amp;#39;fair&amp;#39; due to comparing a second hand product vs a new product.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0h7sx","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0h7sx/does_apple_have_the_best_value_for_money_for/n3a7xf7/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752595778,"author_flair_text":null,"treatment_tags":[],"created_utc":1752595778,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n39b5hf","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"xflareon","can_mod_post":false,"send_replies":true,"parent_id":"t1_n399rqu","score":7,"author_fullname":"t2_a4z6s","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"It really depends, but it will be pretty far in the future if their current rate of progression is anything to go by. The issue is that the GPUs on Mac devices are just pretty weak, and the memory bandwidth isn't spectacular either. They aren't terrible by any stretch, and for someone that just wants an in to mess with LLMs, it's by far the simplest solution. \\n\\nhttps://github.com/XiongjieDai/GPU-Benchmarks-on-LLM-Inference\\n\\nHere's some useful benchmarks, in particular when stacked up against 4x 3090s which I consider to be the direct competition, you can see the huge speed difference, but like I said, that's also a huge difference in maximum memory. If your concern is being able to run say deepseek *at all*, then a Mac makes sense, though you should also expect it to be very, very extremely slow. As I said though, there's a big difference between running it at all and being literally incapable.\\n\\nOn the flip side though, 4x 3090s gets you basically real time generation at reading speed for 120b models in roughly the same budget, so it's a matter of priorities.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n39b5hf","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It really depends, but it will be pretty far in the future if their current rate of progression is anything to go by. The issue is that the GPUs on Mac devices are just pretty weak, and the memory bandwidth isn&amp;#39;t spectacular either. They aren&amp;#39;t terrible by any stretch, and for someone that just wants an in to mess with LLMs, it&amp;#39;s by far the simplest solution. &lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://github.com/XiongjieDai/GPU-Benchmarks-on-LLM-Inference\\"&gt;https://github.com/XiongjieDai/GPU-Benchmarks-on-LLM-Inference&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;Here&amp;#39;s some useful benchmarks, in particular when stacked up against 4x 3090s which I consider to be the direct competition, you can see the huge speed difference, but like I said, that&amp;#39;s also a huge difference in maximum memory. If your concern is being able to run say deepseek &lt;em&gt;at all&lt;/em&gt;, then a Mac makes sense, though you should also expect it to be very, very extremely slow. As I said though, there&amp;#39;s a big difference between running it at all and being literally incapable.&lt;/p&gt;\\n\\n&lt;p&gt;On the flip side though, 4x 3090s gets you basically real time generation at reading speed for 120b models in roughly the same budget, so it&amp;#39;s a matter of priorities.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0h7sx","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0h7sx/does_apple_have_the_best_value_for_money_for/n39b5hf/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752586334,"author_flair_text":null,"treatment_tags":[],"created_utc":1752586334,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":7}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n39u9mu","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Such_Advantage_6949","can_mod_post":false,"send_replies":true,"parent_id":"t1_n399rqu","score":3,"author_fullname":"t2_a548b491","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"It is not really metal or neural engine at the moment. Raw processing is slower on the mac side. On nvidia camp, if u buy 2x3090 u will be able to use tensor parallel, and it will be quite abit even faster. But with the trend now edging toward moe model. Mac does have its benefit. Nonethless, i have m4 max, and prompt processing is a bit too low for big model","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n39u9mu","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It is not really metal or neural engine at the moment. Raw processing is slower on the mac side. On nvidia camp, if u buy 2x3090 u will be able to use tensor parallel, and it will be quite abit even faster. But with the trend now edging toward moe model. Mac does have its benefit. Nonethless, i have m4 max, and prompt processing is a bit too low for big model&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0h7sx","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0h7sx/does_apple_have_the_best_value_for_money_for/n39u9mu/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752591961,"author_flair_text":null,"treatment_tags":[],"created_utc":1752591961,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n399rqu","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Jolly-Phone8982","can_mod_post":false,"created_utc":1752585884,"send_replies":true,"parent_id":"t1_n3999l9","score":2,"author_fullname":"t2_d2o5eadk","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Do you think at some point Appleâ€™s Neural Engine or Metal (whatever their graphics engine is) will catch up to cuda?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n399rqu","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Do you think at some point Appleâ€™s Neural Engine or Metal (whatever their graphics engine is) will catch up to cuda?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0h7sx","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0h7sx/does_apple_have_the_best_value_for_money_for/n399rqu/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752585884,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n3999l9","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"xflareon","can_mod_post":false,"created_utc":1752585719,"send_replies":true,"parent_id":"t3_1m0h7sx","score":15,"author_fullname":"t2_a4z6s","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"In the sense that they will be capable of running the models yes, as there's arguably a big difference between being literally incapable of running something and running it very, very slowly.\\n\\nThe difference is largely that while Nvidia cards have a lower amount of available memory, they are a couple orders of magnitude faster in prompt processing, and generally also faster in token output, though that depends on the card. \\n\\nIf you're most concerned with being able to run something at all, a Mac may be the right fit. \\n\\nFor models 70b and over though, it's difficult to get performance that I would consider useful on a day-to-day level, however your personal tolerance determines at what speed you no longer can stomach the wait. If you plan to use it often or for extended periods, the money spent on a rig with enough vram via Nvidia cards could make sense as well, but the budget may well exceed the cost of a Mac.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3999l9","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;In the sense that they will be capable of running the models yes, as there&amp;#39;s arguably a big difference between being literally incapable of running something and running it very, very slowly.&lt;/p&gt;\\n\\n&lt;p&gt;The difference is largely that while Nvidia cards have a lower amount of available memory, they are a couple orders of magnitude faster in prompt processing, and generally also faster in token output, though that depends on the card. &lt;/p&gt;\\n\\n&lt;p&gt;If you&amp;#39;re most concerned with being able to run something at all, a Mac may be the right fit. &lt;/p&gt;\\n\\n&lt;p&gt;For models 70b and over though, it&amp;#39;s difficult to get performance that I would consider useful on a day-to-day level, however your personal tolerance determines at what speed you no longer can stomach the wait. If you plan to use it often or for extended periods, the money spent on a rig with enough vram via Nvidia cards could make sense as well, but the budget may well exceed the cost of a Mac.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0h7sx/does_apple_have_the_best_value_for_money_for/n3999l9/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752585719,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m0h7sx","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":15}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3bf58z","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"notdaria53","can_mod_post":false,"send_replies":true,"parent_id":"t1_n39aoup","score":1,"author_fullname":"t2_12qn7g5u8c","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Well it depends on the exact Mac model since they have different bandwidth. 5060ti 16GB which is great for its cost is still 1/2 speed of 3090, keep this in mind. However unless you really want to tinker Mac OS going to be the superior solution, especially when we are talking higher size models. \\n\\nAnd yes, gpu, especially from nvidia will crush if it fully fits the model into its vram, which is problematic if you need to run something huge, but works wonders if you choose smaller models.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3bf58z","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Well it depends on the exact Mac model since they have different bandwidth. 5060ti 16GB which is great for its cost is still 1/2 speed of 3090, keep this in mind. However unless you really want to tinker Mac OS going to be the superior solution, especially when we are talking higher size models. &lt;/p&gt;\\n\\n&lt;p&gt;And yes, gpu, especially from nvidia will crush if it fully fits the model into its vram, which is problematic if you need to run something huge, but works wonders if you choose smaller models.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0h7sx","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0h7sx/does_apple_have_the_best_value_for_money_for/n3bf58z/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752607671,"author_flair_text":null,"treatment_tags":[],"created_utc":1752607671,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n39aoup","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Jolly-Phone8982","can_mod_post":false,"created_utc":1752586183,"send_replies":true,"parent_id":"t1_n399xpi","score":2,"author_fullname":"t2_d2o5eadk","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Currently I have an M1 Pro mbp with 32GB ram. I run 7 to 14b models quite comfortably. \\n\\nI also have an old desktop with ryzen 7 2700, 32gb ddr4 ram, gtx 1060 6GB. \\n\\nIâ€™m thinking of upgrading the gtx 1060 6GB to rtx 5070 16GB or something like that. Would it provide better performance than the Mac for the same 7 to 14b models?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n39aoup","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Currently I have an M1 Pro mbp with 32GB ram. I run 7 to 14b models quite comfortably. &lt;/p&gt;\\n\\n&lt;p&gt;I also have an old desktop with ryzen 7 2700, 32gb ddr4 ram, gtx 1060 6GB. &lt;/p&gt;\\n\\n&lt;p&gt;Iâ€™m thinking of upgrading the gtx 1060 6GB to rtx 5070 16GB or something like that. Would it provide better performance than the Mac for the same 7 to 14b models?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0h7sx","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0h7sx/does_apple_have_the_best_value_for_money_for/n39aoup/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752586183,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n399xpi","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"notdaria53","can_mod_post":false,"created_utc":1752585937,"send_replies":true,"parent_id":"t3_1m0h7sx","score":5,"author_fullname":"t2_12qn7g5u8c","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"It really depends on your goals. If running the biggest model is the thing you wish for, Mac is best in terms of power consumption, ease of use etc., however: \\n\\n3090 24gb VRAM with a whopping 936gb/s memory bandwidth, costs 600-700$ on average, you can get multiple as well to get to the VRAM needed. You better love tinkering though. You can run 30b models with decent context and the speed is mind blowing.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n399xpi","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It really depends on your goals. If running the biggest model is the thing you wish for, Mac is best in terms of power consumption, ease of use etc., however: &lt;/p&gt;\\n\\n&lt;p&gt;3090 24gb VRAM with a whopping 936gb/s memory bandwidth, costs 600-700$ on average, you can get multiple as well to get to the VRAM needed. You better love tinkering though. You can run 30b models with decent context and the speed is mind blowing.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0h7sx/does_apple_have_the_best_value_for_money_for/n399xpi/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752585937,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m0h7sx","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n39bx5q","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"lxgrf","can_mod_post":false,"created_utc":1752586579,"send_replies":true,"parent_id":"t1_n39bcn0","score":7,"author_fullname":"t2_55cvbroq","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Wrapping this comment up in religious language is quite appropriate.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n39bx5q","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Wrapping this comment up in religious language is quite appropriate.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0h7sx","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0h7sx/does_apple_have_the_best_value_for_money_for/n39bx5q/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752586579,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":7}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n39bkuh","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Jolly-Phone8982","can_mod_post":false,"created_utc":1752586472,"send_replies":true,"parent_id":"t1_n39bcn0","score":2,"author_fullname":"t2_d2o5eadk","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"ðŸ˜‚ðŸ˜‚","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n39bkuh","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;ðŸ˜‚ðŸ˜‚&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0h7sx","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0h7sx/does_apple_have_the_best_value_for_money_for/n39bkuh/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752586472,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3bwhg2","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Hunting-Succcubus","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3bpi62","score":1,"author_fullname":"t2_3wxyen0t","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yeah, rolex and patek phillipe watches are good value for money too unilke cosio and seiko.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3bwhg2","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yeah, rolex and patek phillipe watches are good value for money too unilke cosio and seiko.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0h7sx","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0h7sx/does_apple_have_the_best_value_for_money_for/n3bwhg2/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752612532,"author_flair_text":null,"treatment_tags":[],"created_utc":1752612532,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3bpi62","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"power97992","can_mod_post":false,"created_utc":1752610597,"send_replies":true,"parent_id":"t1_n39bcn0","score":1,"author_fullname":"t2_64yf00b9","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"M4 macbook air is pretty good value for your money.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3bpi62","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;M4 macbook air is pretty good value for your money.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0h7sx","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0h7sx/does_apple_have_the_best_value_for_money_for/n3bpi62/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752610597,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n39bcn0","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Hunting-Succcubus","can_mod_post":false,"created_utc":1752586398,"send_replies":true,"parent_id":"t3_1m0h7sx","score":8,"author_fullname":"t2_3wxyen0t","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Cardinal Sin. Apple and value for money should never be used in same sentence. Its Blasphemy.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n39bcn0","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Cardinal Sin. Apple and value for money should never be used in same sentence. Its Blasphemy.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0h7sx/does_apple_have_the_best_value_for_money_for/n39bcn0/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752586398,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m0h7sx","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":8}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3979hi","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"PinkyPonk10","can_mod_post":false,"created_utc":1752585065,"send_replies":true,"parent_id":"t3_1m0h7sx","score":6,"author_fullname":"t2_waiyn","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Some people have had success with one or more mi50 32gb cards for $100 ish from alibaba.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3979hi","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Some people have had success with one or more mi50 32gb cards for $100 ish from alibaba.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0h7sx/does_apple_have_the_best_value_for_money_for/n3979hi/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752585065,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m0h7sx","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n39c7kt","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"lly0571","can_mod_post":false,"created_utc":1752586670,"send_replies":true,"parent_id":"t3_1m0h7sx","score":6,"author_fullname":"t2_70vzcleel","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Server class CPU is always better than Apple Silicon for value.\\n\\nA 96GB RAM M3 Ultra Mac costs close to 4x 3090, and RTX 3090s just have way higher FLOPs.\\n\\nA 512GB M3 Ultra Mac studio costs about $10K, you can get a 2S EMR or 1S Epyc Turin Server with a 4090/5090 at that point, and both of them has more than 600GB/s RAM bandwidth and much better scalability for multiple GPUs.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n39c7kt","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Server class CPU is always better than Apple Silicon for value.&lt;/p&gt;\\n\\n&lt;p&gt;A 96GB RAM M3 Ultra Mac costs close to 4x 3090, and RTX 3090s just have way higher FLOPs.&lt;/p&gt;\\n\\n&lt;p&gt;A 512GB M3 Ultra Mac studio costs about $10K, you can get a 2S EMR or 1S Epyc Turin Server with a 4090/5090 at that point, and both of them has more than 600GB/s RAM bandwidth and much better scalability for multiple GPUs.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0h7sx/does_apple_have_the_best_value_for_money_for/n39c7kt/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752586670,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m0h7sx","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n39im0q","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"datbackup","can_mod_post":false,"created_utc":1752588602,"send_replies":true,"parent_id":"t3_1m0h7sx","score":3,"author_fullname":"t2_ielo6","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I wrote my opinion about mac vs others for LLM here:\\n\\nhttps://www.reddit.com/r/LocalLLaMA/s/HRENM99iW1\\n\\nTLDR best value for money? Maybe, itâ€™s quite debatable\\n\\nBest value for time and hassle? Far more likely\\n\\nI think between buying a Mac and building a multichannel RAM system, the only big mistake would be buying a Mac that isnâ€™t the 512GB m3 ultra.\\n\\nIf you go Mac youâ€™re going to have slow prompt processing and slow large-context handling so you might as well be getting SOTA level answers after waiting for themâ€”and only the 512GB model has enough RAM to handle SOTA models like Deepseek v3/R1 and Kimi K2. Getting a smaller Mac is the worst of both worlds.\\n\\nIf you have lots of time, willingness to troubleshoot, and/or system-building knowhow, I do think a blackwell + multichannel RAM setup is better value for moneyâ€¦ but nothing beats the sheer â€œpoint A to point Bâ€ of the m3 ultra if you just want to run SOTA at home asap","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n39im0q","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I wrote my opinion about mac vs others for LLM here:&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://www.reddit.com/r/LocalLLaMA/s/HRENM99iW1\\"&gt;https://www.reddit.com/r/LocalLLaMA/s/HRENM99iW1&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;TLDR best value for money? Maybe, itâ€™s quite debatable&lt;/p&gt;\\n\\n&lt;p&gt;Best value for time and hassle? Far more likely&lt;/p&gt;\\n\\n&lt;p&gt;I think between buying a Mac and building a multichannel RAM system, the only big mistake would be buying a Mac that isnâ€™t the 512GB m3 ultra.&lt;/p&gt;\\n\\n&lt;p&gt;If you go Mac youâ€™re going to have slow prompt processing and slow large-context handling so you might as well be getting SOTA level answers after waiting for themâ€”and only the 512GB model has enough RAM to handle SOTA models like Deepseek v3/R1 and Kimi K2. Getting a smaller Mac is the worst of both worlds.&lt;/p&gt;\\n\\n&lt;p&gt;If you have lots of time, willingness to troubleshoot, and/or system-building knowhow, I do think a blackwell + multichannel RAM setup is better value for moneyâ€¦ but nothing beats the sheer â€œpoint A to point Bâ€ of the m3 ultra if you just want to run SOTA at home asap&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0h7sx/does_apple_have_the_best_value_for_money_for/n39im0q/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752588602,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m0h7sx","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3a6ppt","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"taimusrs","can_mod_post":false,"created_utc":1752595437,"send_replies":true,"parent_id":"t1_n3999t1","score":1,"author_fullname":"t2_nrjel","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I got one for work - yes it's crazy efficient compared to Nvidia. Uses about 120W for LLM inference. For ASR (MLX Whisper), it uses just 30W. If you found a way to use the Neural Engine - 12W, just sipping power. But honestly, unless your power is really expensive, it's not that big of a deal. \\n\\nThe other side of its efficiency prowess though, is that it's hopelessly unrepairable. You will be shit outta luck if it somehow dies. Apple's stuff I bought has been very reliable, but there are people who got burnt by Apple too","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3a6ppt","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I got one for work - yes it&amp;#39;s crazy efficient compared to Nvidia. Uses about 120W for LLM inference. For ASR (MLX Whisper), it uses just 30W. If you found a way to use the Neural Engine - 12W, just sipping power. But honestly, unless your power is really expensive, it&amp;#39;s not that big of a deal. &lt;/p&gt;\\n\\n&lt;p&gt;The other side of its efficiency prowess though, is that it&amp;#39;s hopelessly unrepairable. You will be shit outta luck if it somehow dies. Apple&amp;#39;s stuff I bought has been very reliable, but there are people who got burnt by Apple too&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0h7sx","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0h7sx/does_apple_have_the_best_value_for_money_for/n3a6ppt/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752595437,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3999t1","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Jolly-Phone8982","can_mod_post":false,"created_utc":1752585721,"send_replies":true,"parent_id":"t3_1m0h7sx","score":2,"author_fullname":"t2_d2o5eadk","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Itâ€™s also weird that Apple isnâ€™t marketing their machines more for LLMs. \\n\\nFrom their keynotes it also looks like their efficiency is way better than the competition (no idea if anyone verified their graphs)\\n\\nSo when you add their SoC performance and power efficiency which lowers electricity bills, they can be making a killing selling their pcs if they just bothered marking them more aggressively","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3999t1","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Itâ€™s also weird that Apple isnâ€™t marketing their machines more for LLMs. &lt;/p&gt;\\n\\n&lt;p&gt;From their keynotes it also looks like their efficiency is way better than the competition (no idea if anyone verified their graphs)&lt;/p&gt;\\n\\n&lt;p&gt;So when you add their SoC performance and power efficiency which lowers electricity bills, they can be making a killing selling their pcs if they just bothered marking them more aggressively&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0h7sx/does_apple_have_the_best_value_for_money_for/n3999t1/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752585721,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m0h7sx","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n39e0yc","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"dsartori","can_mod_post":false,"created_utc":1752587226,"send_replies":true,"parent_id":"t3_1m0h7sx","score":2,"author_fullname":"t2_4d6m0","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"For department-level deployments or smaller I think a Mac is the best value, yes, but it's also not as fast as NVIDIA hardware.\\n\\nMy local LLM setup at work has two Ollama servers: one running on an M2 Mac Mini and the other is on a PC with a 16GB 4060. The PC is about 30% faster for inference.","edited":1752593186,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n39e0yc","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;For department-level deployments or smaller I think a Mac is the best value, yes, but it&amp;#39;s also not as fast as NVIDIA hardware.&lt;/p&gt;\\n\\n&lt;p&gt;My local LLM setup at work has two Ollama servers: one running on an M2 Mac Mini and the other is on a PC with a 16GB 4060. The PC is about 30% faster for inference.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0h7sx/does_apple_have_the_best_value_for_money_for/n39e0yc/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752587226,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m0h7sx","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n39efwj","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Stickybunfun","can_mod_post":false,"created_utc":1752587351,"send_replies":true,"parent_id":"t3_1m0h7sx","score":2,"author_fullname":"t2_1dd7f8zg","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Performance per dollar? No. For everything else - debatable. Buy it now, install lmstudio, download a model and GO.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n39efwj","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Performance per dollar? No. For everything else - debatable. Buy it now, install lmstudio, download a model and GO.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0h7sx/does_apple_have_the_best_value_for_money_for/n39efwj/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752587351,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m0h7sx","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n399e4s","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Trepedation","can_mod_post":false,"created_utc":1752585760,"send_replies":true,"parent_id":"t3_1m0h7sx","score":2,"author_fullname":"t2_59t4l717","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Macs definitely provide good value but you can find used 3090s now for not terrible prices and they are pretty much the sweet spot for value at the moment. Nvidia cards still run circles around Mac gpus in terms of token generation too. Macs are definitely good but thereâ€™s more to consider than just vram.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n399e4s","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Macs definitely provide good value but you can find used 3090s now for not terrible prices and they are pretty much the sweet spot for value at the moment. Nvidia cards still run circles around Mac gpus in terms of token generation too. Macs are definitely good but thereâ€™s more to consider than just vram.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0h7sx/does_apple_have_the_best_value_for_money_for/n399e4s/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752585760,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m0h7sx","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n39c0z9","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"JLeonsarmiento","can_mod_post":false,"created_utc":1752586613,"send_replies":true,"parent_id":"t3_1m0h7sx","score":1,"author_fullname":"t2_9b9s4a7g","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"For the average Joe, yes.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n39c0z9","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;For the average Joe, yes.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0h7sx/does_apple_have_the_best_value_for_money_for/n39c0z9/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752586613,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m0h7sx","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n39j375","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MarinatedPickachu","can_mod_post":false,"created_utc":1752588743,"send_replies":true,"parent_id":"t3_1m0h7sx","score":1,"author_fullname":"t2_bwdb8qqfj","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"You can get a 192GB Orange Pi Ai Studio Pro for under 3k, but no idea about software support","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n39j375","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You can get a 192GB Orange Pi Ai Studio Pro for under 3k, but no idea about software support&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0h7sx/does_apple_have_the_best_value_for_money_for/n39j375/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752588743,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m0h7sx","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n39nhbh","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Willing_Landscape_61","can_mod_post":false,"created_utc":1752590029,"send_replies":true,"parent_id":"t3_1m0h7sx","score":1,"author_fullname":"t2_8lvrytgw","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Short answer: no.\\nLonger answer: it depends on the models you want to use and the context size.\\n$4k to $5k goes a long way for an Epyc Gen 2 server with AMD or Intel GPUs or even 3090.\\nWith 5k I would get an Epyc Gen 2 with 512GB of DDR4 3200 on 8 channels and 4 or more MI50 with 32GB of VRAM each or 4 x 3090.\\nNot sure for with models/ uses would the Mac Studio be better except for electricity consumption.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n39nhbh","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Short answer: no.\\nLonger answer: it depends on the models you want to use and the context size.\\n$4k to $5k goes a long way for an Epyc Gen 2 server with AMD or Intel GPUs or even 3090.\\nWith 5k I would get an Epyc Gen 2 with 512GB of DDR4 3200 on 8 channels and 4 or more MI50 with 32GB of VRAM each or 4 x 3090.\\nNot sure for with models/ uses would the Mac Studio be better except for electricity consumption.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0h7sx/does_apple_have_the_best_value_for_money_for/n39nhbh/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752590029,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m0h7sx","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n39rq70","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"segmond","can_mod_post":false,"created_utc":1752591249,"send_replies":true,"parent_id":"t3_1m0h7sx","score":1,"author_fullname":"t2_ah13x","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"yeah, you are missing being resourceful, creative, smart, frugal.  these are the best means to run an LLM","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n39rq70","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;yeah, you are missing being resourceful, creative, smart, frugal.  these are the best means to run an LLM&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0h7sx/does_apple_have_the_best_value_for_money_for/n39rq70/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752591249,"author_flair_text":"llama.cpp","treatment_tags":[],"link_id":"t3_1m0h7sx","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n396mm5","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Illustrious-Dot-6888","can_mod_post":false,"created_utc":1752584851,"send_replies":true,"parent_id":"t3_1m0h7sx","score":0,"author_fullname":"t2_gelzgtkby","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Yes,without a doubt","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n396mm5","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yes,without a doubt&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0h7sx/does_apple_have_the_best_value_for_money_for/n396mm5/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752584851,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m0h7sx","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":1,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n399kap","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Far_Note6719","can_mod_post":false,"created_utc":1752585816,"send_replies":true,"parent_id":"t1_n397o2g","score":3,"author_fullname":"t2_1j219c2ws5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Someone did some calculations on the performance to expect. Memory bandwidth may be a bottleneck, where Apple beats it.\\n\\nSee Comments here:\\n\\n[https://www.reddit.com/r/LocalLLaMA/comments/1kq4ey4/nvidia\\\\_says\\\\_dgx\\\\_spark\\\\_releasing\\\\_in\\\\_july/](https://www.reddit.com/r/LocalLLaMA/comments/1kq4ey4/nvidia_says_dgx_spark_releasing_in_july/)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n399kap","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Someone did some calculations on the performance to expect. Memory bandwidth may be a bottleneck, where Apple beats it.&lt;/p&gt;\\n\\n&lt;p&gt;See Comments here:&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://www.reddit.com/r/LocalLLaMA/comments/1kq4ey4/nvidia_says_dgx_spark_releasing_in_july/\\"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1kq4ey4/nvidia_says_dgx_spark_releasing_in_july/&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0h7sx","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0h7sx/does_apple_have_the_best_value_for_money_for/n399kap/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752585816,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n399p4t","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"alamacra","can_mod_post":false,"created_utc":1752585860,"send_replies":true,"parent_id":"t1_n397o2g","score":2,"author_fullname":"t2_o5fyioqm","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"It's only got 128GB though, which is nowhere near enough to run Deepseek or Kimi. Even 256 isn't, essentially you need the 512GB Mac Studio, which is actually a decent deal, for Apple that is.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n399p4t","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It&amp;#39;s only got 128GB though, which is nowhere near enough to run Deepseek or Kimi. Even 256 isn&amp;#39;t, essentially you need the 512GB Mac Studio, which is actually a decent deal, for Apple that is.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0h7sx","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0h7sx/does_apple_have_the_best_value_for_money_for/n399p4t/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752585860,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n398dtz","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Federal-Effective879","can_mod_post":false,"created_utc":1752585431,"send_replies":true,"parent_id":"t1_n397o2g","score":3,"author_fullname":"t2_702zh1r2","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"DGX Spark has low memory bandwidth and so slow generation, making large dense models nearly unusable. M4 Max has double the memory bandwidth, resulting in roughly double the generation speed.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n398dtz","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;DGX Spark has low memory bandwidth and so slow generation, making large dense models nearly unusable. M4 Max has double the memory bandwidth, resulting in roughly double the generation speed.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0h7sx","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0h7sx/does_apple_have_the_best_value_for_money_for/n398dtz/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752585431,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n397o2g","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"TechMaven-Geospatial","can_mod_post":false,"created_utc":1752585197,"send_replies":true,"parent_id":"t3_1m0h7sx","score":0,"author_fullname":"t2_vyn28m","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Best value is new Nvidia AI mini computer DGX SPARK $4000 or $8000\\nBut nothing can compete with how many tops and power it has","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n397o2g","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Best value is new Nvidia AI mini computer DGX SPARK $4000 or $8000\\nBut nothing can compete with how many tops and power it has&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0h7sx/does_apple_have_the_best_value_for_money_for/n397o2g/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752585197,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m0h7sx","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":1,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}}]`),n=()=>e.jsx(l,{data:t});export{n as default};
