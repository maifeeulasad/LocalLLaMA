import{j as e}from"./index-CWmJdUH_.js";import{R as l}from"./RedditPostRenderer-D2iunoQ9.js";import"./index-BCg9RP6g.js";const a=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Looking at getting this machine for running local llms. New to running them locally. Wondering if 128GB is worth it, or if the larger models start becoming too slow to make the extra memory meaningful? I would love to hear some opinions.","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"AI 395+ 64GB vs 128GB?","link_flair_richtext":[{"e":"text","t":"Discussion"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1m5s6d1","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.92,"author_flair_background_color":null,"subreddit_type":"public","ups":30,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_y0abrfm","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Discussion","can_mod_post":false,"score":30,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1753125511,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Looking at getting this machine for running local llms. New to running them locally. Wondering if 128GB is worth it, or if the larger models start becoming too slow to make the extra memory meaningful? I would love to hear some opinions.&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"5f921ea4-c7bc-11ed-9c23-3a00622979b4","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#646d73","id":"1m5s6d1","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"cfogrady","discussion_type":null,"num_comments":87,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1m5s6d1/ai_395_64gb_vs_128gb/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1m5s6d1/ai_395_64gb_vs_128gb/","subreddit_subscribers":502981,"created_utc":1753125511,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4fwm7d","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"rbit4","can_mod_post":false,"created_utc":1753144560,"send_replies":true,"parent_id":"t1_n4e8bdb","score":1,"author_fullname":"t2_g4rgng0","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Got a 96 core 192t epyc system with 512gb ddr5 ram, dual 5090s and dual 4090s. 784gb is easy though getting to 1.5tb would need dual socket mb, or multiple dimms per channel. What kinda system do you have","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4fwm7d","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Got a 96 core 192t epyc system with 512gb ddr5 ram, dual 5090s and dual 4090s. 784gb is easy though getting to 1.5tb would need dual socket mb, or multiple dimms per channel. What kinda system do you have&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m5s6d1","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5s6d1/ai_395_64gb_vs_128gb/n4fwm7d/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753144560,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4fd0il","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Rich_Repeat_22","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4ea4zy","score":2,"author_fullname":"t2_viufiki6","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Sort of. The iGPU has access to double the bandwidth compared to the CPU. So is paramount to set on BIOS the VRAM otherwise allowing the auto setting for unified memory cripples the perf.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4fd0il","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Sort of. The iGPU has access to double the bandwidth compared to the CPU. So is paramount to set on BIOS the VRAM otherwise allowing the auto setting for unified memory cripples the perf.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m5s6d1","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5s6d1/ai_395_64gb_vs_128gb/n4fd0il/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753137993,"author_flair_text":null,"treatment_tags":[],"created_utc":1753137993,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4eb0rr","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"LagOps91","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4ea4zy","score":2,"author_fullname":"t2_3wi6j7vwh","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"that's true. there are still external gpus you could connect tho if i'm not wrong. but yeah, 128GB of unified memory is a big step up from 64GB. unless the price difference is a dealbreaker, i would go with that.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4eb0rr","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;that&amp;#39;s true. there are still external gpus you could connect tho if i&amp;#39;m not wrong. but yeah, 128GB of unified memory is a big step up from 64GB. unless the price difference is a dealbreaker, i would go with that.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m5s6d1","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5s6d1/ai_395_64gb_vs_128gb/n4eb0rr/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753126580,"author_flair_text":null,"treatment_tags":[],"created_utc":1753126580,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n4ea4zy","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"AbyssianOne","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4e938m","score":15,"author_fullname":"t2_1651c3kskq","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"They said they're considering a machine with the AI 395 processor. It has no onboard VRAM and uses system ram as unified memory, with a cap of 128gb.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n4ea4zy","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;They said they&amp;#39;re considering a machine with the AI 395 processor. It has no onboard VRAM and uses system ram as unified memory, with a cap of 128gb.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m5s6d1","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5s6d1/ai_395_64gb_vs_128gb/n4ea4zy/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753126324,"author_flair_text":null,"treatment_tags":[],"created_utc":1753126324,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":15}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4fpsps","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"simracerman","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4eo9cw","score":1,"author_fullname":"t2_vbzgnic","approved_by":null,"mod_note":null,"all_awardings":[],"body":"Only on comments in reply to the main post. If you post as a reply to other users, OP won't get a notification.","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n4fpsps","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Only on comments in reply to the main post. If you post as a reply to other users, OP won&amp;#39;t get a notification.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1m5s6d1","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5s6d1/ai_395_64gb_vs_128gb/n4fpsps/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753142190,"author_flair_text":null,"treatment_tags":[],"created_utc":1753142190,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4ks7sd","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"randomqhacker","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4fi7ec","score":1,"author_fullname":"t2_4nw3v","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"If OP doesn't read every reply to every thread he doesn't deserve our reddit wisdoms!","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n4ks7sd","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;If OP doesn&amp;#39;t read every reply to every thread he doesn&amp;#39;t deserve our reddit wisdoms!&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1m5s6d1","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5s6d1/ai_395_64gb_vs_128gb/n4ks7sd/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753210927,"author_flair_text":null,"treatment_tags":[],"created_utc":1753210927,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":6,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4fi7ec","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"teh_spazz","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4eo9cw","score":0,"author_fullname":"t2_33dpl","approved_by":null,"mod_note":null,"all_awardings":[],"body":"No he doesn’t.","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n4fi7ec","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;No he doesn’t.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1m5s6d1","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5s6d1/ai_395_64gb_vs_128gb/n4fi7ec/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753139688,"author_flair_text":null,"treatment_tags":[],"created_utc":1753139688,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}},"user_reports":[],"saved":false,"id":"n4eo9cw","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"LagOps91","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4em1wv","score":5,"author_fullname":"t2_3wi6j7vwh","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"well, indirectly i am? the OP gets notified about each post and i would expect OP to actually read what's posted on their thread.","edited":false,"author_flair_css_class":null,"name":"t1_n4eo9cw","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;well, indirectly i am? the OP gets notified about each post and i would expect OP to actually read what&amp;#39;s posted on their thread.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1m5s6d1","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5s6d1/ai_395_64gb_vs_128gb/n4eo9cw/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753130360,"author_flair_text":null,"collapsed":false,"created_utc":1753130360,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}}],"before":null}},"user_reports":[],"saved":false,"id":"n4em1wv","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AbyssianOne","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4ea7an","score":1,"author_fullname":"t2_1651c3kskq","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"You should probably tell the OP that.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4em1wv","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You should probably tell the OP that.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m5s6d1","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5s6d1/ai_395_64gb_vs_128gb/n4em1wv/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753129730,"author_flair_text":null,"treatment_tags":[],"created_utc":1753129730,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4ea7an","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"LagOps91","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4e938m","score":4,"author_fullname":"t2_3wi6j7vwh","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"for instance, Dots.llm1 and Qwen 3 235b should run at usable speeds if you also have a gpu to offload the tensors used by all experts and for context.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n4ea7an","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;for instance, Dots.llm1 and Qwen 3 235b should run at usable speeds if you also have a gpu to offload the tensors used by all experts and for context.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m5s6d1","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5s6d1/ai_395_64gb_vs_128gb/n4ea7an/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753126343,"author_flair_text":null,"treatment_tags":[],"created_utc":1753126343,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}}],"before":null}},"user_reports":[],"saved":false,"id":"n4e938m","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"LagOps91","can_mod_post":false,"created_utc":1753126021,"send_replies":true,"parent_id":"t1_n4e8bdb","score":-2,"author_fullname":"t2_3wi6j7vwh","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Ram and VRAM are very different things here!\\n\\nThat said, with many MoE models comming out recently, 128GB of ram can allow you to run quite strong models at usable speeds locally. It won't be fast, sure, but if you want to get a new PC anyway, you could spend some of the budget on more fast ram.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4e938m","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Ram and VRAM are very different things here!&lt;/p&gt;\\n\\n&lt;p&gt;That said, with many MoE models comming out recently, 128GB of ram can allow you to run quite strong models at usable speeds locally. It won&amp;#39;t be fast, sure, but if you want to get a new PC anyway, you could spend some of the budget on more fast ram.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m5s6d1","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5s6d1/ai_395_64gb_vs_128gb/n4e938m/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753126021,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":1,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4fsgvb","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"rbit4","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4frwzs","score":1,"author_fullname":"t2_g4rgng0","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Nvme even pcie4 is 50 times faster than hdd and 20 14 times faster than ssd. So its a moot point","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n4fsgvb","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Nvme even pcie4 is 50 times faster than hdd and 20 14 times faster than ssd. So its a moot point&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1m5s6d1","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5s6d1/ai_395_64gb_vs_128gb/n4fsgvb/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753143114,"author_flair_text":null,"treatment_tags":[],"created_utc":1753143114,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":6,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4frwzs","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Unique_Judgment_1304","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4fqlyh","score":2,"author_fullname":"t2_1s9hyoxo94","approved_by":null,"mod_note":null,"all_awardings":[],"body":"Sure, the faster the better. But some people still load models from SATA SSDs or even cheap HDDs. It takes time, but like you said, it's just one time.","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n4frwzs","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Sure, the faster the better. But some people still load models from SATA SSDs or even cheap HDDs. It takes time, but like you said, it&amp;#39;s just one time.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1m5s6d1","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5s6d1/ai_395_64gb_vs_128gb/n4frwzs/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753142922,"author_flair_text":null,"treatment_tags":[],"created_utc":1753142922,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4gjyj7","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"simracerman","can_mod_post":false,"created_utc":1753152843,"send_replies":true,"parent_id":"t1_n4g19x3","score":1,"author_fullname":"t2_vbzgnic","approved_by":null,"mod_note":null,"all_awardings":[],"body":"Again, not in OP's setup. Once you say RAID 0, you're immediately talking lots more $$ and production style machine.","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n4gjyj7","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Again, not in OP&amp;#39;s setup. Once you say RAID 0, you&amp;#39;re immediately talking lots more $$ and production style machine.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1m5s6d1","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5s6d1/ai_395_64gb_vs_128gb/n4gjyj7/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753152843,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":7,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4g19x3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"rbit4","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4g0xh7","score":1,"author_fullname":"t2_g4rgng0","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Raid 0 pcie 5 = 2x14 Gigabytes per s","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n4g19x3","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Raid 0 pcie 5 = 2x14 Gigabytes per s&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1m5s6d1","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5s6d1/ai_395_64gb_vs_128gb/n4g19x3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753146173,"author_flair_text":null,"treatment_tags":[],"created_utc":1753146173,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":6,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4g0xh7","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"simracerman","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4fqlyh","score":1,"author_fullname":"t2_vbzgnic","approved_by":null,"mod_note":null,"all_awardings":[],"body":"Please show me which one can achieve that. OPs mini pc has a max read (real world synthetic test) of just under 7GB. Unless I’m missing something. Here’s a screen grab of a YouTube reviewer showing this:\\n\\nhttps://imgur.com/a/1tvpPTL\\n\\nAbout that 1 time load. You’re thinking about a user buying this general purpose powerful mini PC/laptop to load one large model only? To each their own but I feel like loading and unloading a model is a frequent ordeal every local LLM host deals with daily. We’re not using it for production anyway.","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n4g0xh7","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Please show me which one can achieve that. OPs mini pc has a max read (real world synthetic test) of just under 7GB. Unless I’m missing something. Here’s a screen grab of a YouTube reviewer showing this:&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://imgur.com/a/1tvpPTL\\"&gt;https://imgur.com/a/1tvpPTL&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;About that 1 time load. You’re thinking about a user buying this general purpose powerful mini PC/laptop to load one large model only? To each their own but I feel like loading and unloading a model is a frequent ordeal every local LLM host deals with daily. We’re not using it for production anyway.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1m5s6d1","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5s6d1/ai_395_64gb_vs_128gb/n4g0xh7/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753146052,"author_flair_text":null,"treatment_tags":[],"created_utc":1753146052,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4fqlyh","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"rbit4","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4fqaoh","score":5,"author_fullname":"t2_g4rgng0","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"You can get 28 GBps on pcie5 nvme. But this is one time  before you start chatting. Why is it a problem","edited":false,"author_flair_css_class":null,"name":"t1_n4fqlyh","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You can get 28 GBps on pcie5 nvme. But this is one time  before you start chatting. Why is it a problem&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1m5s6d1","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5s6d1/ai_395_64gb_vs_128gb/n4fqlyh/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753142466,"author_flair_text":null,"collapsed":false,"created_utc":1753142466,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}}],"before":null}},"user_reports":[],"saved":false,"id":"n4fqaoh","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"simracerman","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4fnbai","score":1,"author_fullname":"t2_vbzgnic","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Curious, at 5-6 GB/s NVME--&gt;RAM load. a 512 GB file will take 85-102 seconds to load. That's a really long wait time with no context. Add a few thousands of tokens and you're waiting for minutes to get a first response.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4fqaoh","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Curious, at 5-6 GB/s NVME--&amp;gt;RAM load. a 512 GB file will take 85-102 seconds to load. That&amp;#39;s a really long wait time with no context. Add a few thousands of tokens and you&amp;#39;re waiting for minutes to get a first response.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m5s6d1","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5s6d1/ai_395_64gb_vs_128gb/n4fqaoh/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753142360,"author_flair_text":null,"treatment_tags":[],"created_utc":1753142360,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4fnbai","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Unique_Judgment_1304","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4flgn7","score":2,"author_fullname":"t2_1s9hyoxo94","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"You can't load more than Q3 of Kimi-K2 with 512gb. With 1TB you can get up to Q6.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n4fnbai","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You can&amp;#39;t load more than Q3 of Kimi-K2 with 512gb. With 1TB you can get up to Q6.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m5s6d1","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5s6d1/ai_395_64gb_vs_128gb/n4fnbai/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753141364,"author_flair_text":null,"treatment_tags":[],"created_utc":1753141364,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4fqct9","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"rbit4","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4fn407","score":1,"author_fullname":"t2_g4rgng0","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Lol 1T model at what quant on which machine roflmao","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4fqct9","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Lol 1T model at what quant on which machine roflmao&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m5s6d1","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5s6d1/ai_395_64gb_vs_128gb/n4fqct9/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753142380,"author_flair_text":null,"treatment_tags":[],"created_utc":1753142380,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4fn407","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"emprahsFury","can_mod_post":false,"send_replies":false,"parent_id":"t1_n4flgn7","score":1,"author_fullname":"t2_177r8n","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"oh yeah, I forgot- i dont want to run the new 1T param models. Thanks brosephus","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n4fn407","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;oh yeah, I forgot- i dont want to run the new 1T param models. Thanks brosephus&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m5s6d1","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5s6d1/ai_395_64gb_vs_128gb/n4fn407/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753141298,"author_flair_text":null,"treatment_tags":[],"created_utc":1753141298,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4g50t6","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"rbit4","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4g45v0","score":2,"author_fullname":"t2_g4rgng0","approved_by":null,"mod_note":null,"all_awardings":[],"body":"Go for the 9004 genoa setup, with Turin here you will pay more for Genoa than Milan but you also easily get equivalent or better improvement in performance eg 64c to 64c. Other benefit is you get dual epyc Milan performance is single socket plus 2x in dual socket if you need it. Plus ddr5 and pcie5. I actually have both Milan and genoa systems","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n4g50t6","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Go for the 9004 genoa setup, with Turin here you will pay more for Genoa than Milan but you also easily get equivalent or better improvement in performance eg 64c to 64c. Other benefit is you get dual epyc Milan performance is single socket plus 2x in dual socket if you need it. Plus ddr5 and pcie5. I actually have both Milan and genoa systems&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1m5s6d1","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5s6d1/ai_395_64gb_vs_128gb/n4g50t6/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753147485,"author_flair_text":null,"treatment_tags":[],"created_utc":1753147485,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n4g45v0","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"woahdudee2a","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4fwh7v","score":1,"author_fullname":"t2_o6qm5t0","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"* Broadcom BCM2387 chipset\\n* **1.2GHz Quad-Core ARM Cortex-A53**\\n* **802.11 bgn Wireless LAN and Bluetooth 4.1** (Bluetooth Classic and LE)\\n* **1GB RAM**\\n* **64 Bit CPU**\\n* 4 x USB ports\\n* 4 pole Stereo output and Composite video port\\n* Full size HDMI\\n\\njokes aside im very conflicted at the moment. was planning to build a 6-8 GPU MI50 rig but dual epyc 7003 is more future proof. i just wish real life bandwidth was a little better","edited":false,"author_flair_css_class":null,"name":"t1_n4g45v0","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;ul&gt;\\n&lt;li&gt;Broadcom BCM2387 chipset&lt;/li&gt;\\n&lt;li&gt;&lt;strong&gt;1.2GHz Quad-Core ARM Cortex-A53&lt;/strong&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;strong&gt;802.11 bgn Wireless LAN and Bluetooth 4.1&lt;/strong&gt; (Bluetooth Classic and LE)&lt;/li&gt;\\n&lt;li&gt;&lt;strong&gt;1GB RAM&lt;/strong&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;strong&gt;64 Bit CPU&lt;/strong&gt;&lt;/li&gt;\\n&lt;li&gt;4 x USB ports&lt;/li&gt;\\n&lt;li&gt;4 pole Stereo output and Composite video port&lt;/li&gt;\\n&lt;li&gt;Full size HDMI&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;p&gt;jokes aside im very conflicted at the moment. was planning to build a 6-8 GPU MI50 rig but dual epyc 7003 is more future proof. i just wish real life bandwidth was a little better&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1m5s6d1","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5s6d1/ai_395_64gb_vs_128gb/n4g45v0/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753147184,"author_flair_text":null,"collapsed":false,"created_utc":1753147184,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4fwh7v","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"rbit4","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4fv8y3","score":1,"author_fullname":"t2_g4rgng0","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Got a 96 core epyc system with 512gb ram. 784gb is easy though getting to 1.5tb would need dual socket mb, or multiple dimms per channel. What kinda system do you have","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4fwh7v","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Got a 96 core epyc system with 512gb ram. 784gb is easy though getting to 1.5tb would need dual socket mb, or multiple dimms per channel. What kinda system do you have&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m5s6d1","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5s6d1/ai_395_64gb_vs_128gb/n4fwh7v/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753144511,"author_flair_text":null,"treatment_tags":[],"created_utc":1753144511,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4fv8y3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"woahdudee2a","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4flgn7","score":1,"author_fullname":"t2_o6qm5t0","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"&gt; foreseeable future\\n\\n3 months max","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n4fv8y3","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;foreseeable future&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;3 months max&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m5s6d1","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5s6d1/ai_395_64gb_vs_128gb/n4fv8y3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753144084,"author_flair_text":null,"treatment_tags":[],"created_utc":1753144084,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4ksgi1","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"randomqhacker","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4flgn7","score":1,"author_fullname":"t2_4nw3v","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"ok Bill Gates!","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n4ksgi1","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;ok Bill Gates!&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m5s6d1","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5s6d1/ai_395_64gb_vs_128gb/n4ksgi1/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753210999,"author_flair_text":null,"treatment_tags":[],"created_utc":1753210999,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4flgn7","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":"LOW_SCORE","no_follow":true,"author":"rbit4","can_mod_post":false,"created_utc":1753140753,"send_replies":true,"parent_id":"t1_n4e8bdb","score":-6,"author_fullname":"t2_g4rgng0","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Why would you need 1 TB? 512gb seems to be enough for foreseeable future","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4flgn7","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Why would you need 1 TB? 512gb seems to be enough for foreseeable future&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":"comment score below threshold","distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m5s6d1","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5s6d1/ai_395_64gb_vs_128gb/n4flgn7/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753140753,"author_flair_text":null,"treatment_tags":[],"collapsed":true,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-6}}],"before":null}},"user_reports":[],"saved":false,"id":"n4e8bdb","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"AbyssianOne","can_mod_post":false,"created_utc":1753125799,"send_replies":true,"parent_id":"t3_1m5s6d1","score":38,"author_fullname":"t2_1651c3kskq","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Get 128. You need the memory to even load the models. More is better. I'm wishing I had a bit over 1TB of VRAM these days.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4e8bdb","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Get 128. You need the memory to even load the models. More is better. I&amp;#39;m wishing I had a bit over 1TB of VRAM these days.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5s6d1/ai_395_64gb_vs_128gb/n4e8bdb/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753125799,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m5s6d1","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":38}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4ern2w","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"b3081a","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4eko13","score":4,"author_fullname":"t2_17n5yh7l","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I've seen someone used an M.2 to PCIe extension cable and got a dGPU hooked up to a Chinese 395 minipc so firmware/software wise it would not be a problem, though not a long term solution physically.\\n\\n\\nThe Framework Desktop ITX board has a closed-end PCIe 4.0x4 slot. You can put that into an A4-style ITX case to use a PCIe 4.0 extension cable that is open-end to plug in a x16 device.\\n\\n\\n\\nLater in Q3 there will also be more Chinese boards/minipcs with better I/O capabilities so that's worth keeping an eye on.\\n\\n\\nFor laptops that would be a difficult job though. So maybe going 128GB with q2_k/q3_k is a better choice.","edited":1753171881,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n4ern2w","is_submitter":false,"collapsed":false,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;ve seen someone used an M.2 to PCIe extension cable and got a dGPU hooked up to a Chinese 395 minipc so firmware/software wise it would not be a problem, though not a long term solution physically.&lt;/p&gt;\\n\\n&lt;p&gt;The Framework Desktop ITX board has a closed-end PCIe 4.0x4 slot. You can put that into an A4-style ITX case to use a PCIe 4.0 extension cable that is open-end to plug in a x16 device.&lt;/p&gt;\\n\\n&lt;p&gt;Later in Q3 there will also be more Chinese boards/minipcs with better I/O capabilities so that&amp;#39;s worth keeping an eye on.&lt;/p&gt;\\n\\n&lt;p&gt;For laptops that would be a difficult job though. So maybe going 128GB with q2_k/q3_k is a better choice.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m5s6d1","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5s6d1/ai_395_64gb_vs_128gb/n4ern2w/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753131321,"author_flair_text":"llama.cpp","treatment_tags":[],"created_utc":1753131321,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4hq744","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Rich_Repeat_22","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4h3kr5","score":1,"author_fullname":"t2_viufiki6","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Pretty good. Thank you :)","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n4hq744","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Pretty good. Thank you :)&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1m5s6d1","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5s6d1/ai_395_64gb_vs_128gb/n4hq744/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753173692,"author_flair_text":null,"treatment_tags":[],"created_utc":1753173692,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":6,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4h3kr5","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"federationoffear","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4h28fu","score":2,"author_fullname":"t2_4xa3j","approved_by":null,"mod_note":null,"all_awardings":[],"body":"https://preview.redd.it/yjnx38xe0def1.png?width=1008&amp;format=png&amp;auto=webp&amp;s=02edefa3d033819577a0f77ddc72982e80a70f9f","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n4h3kr5","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;&lt;a href=\\"https://preview.redd.it/yjnx38xe0def1.png?width=1008&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=02edefa3d033819577a0f77ddc72982e80a70f9f\\"&gt;https://preview.redd.it/yjnx38xe0def1.png?width=1008&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=02edefa3d033819577a0f77ddc72982e80a70f9f&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1m5s6d1","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5s6d1/ai_395_64gb_vs_128gb/n4h3kr5/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753161273,"media_metadata":{"yjnx38xe0def1":{"status":"valid","e":"Image","m":"image/png","p":[{"y":36,"x":108,"u":"https://preview.redd.it/yjnx38xe0def1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=28694816e1184d0f97293d6058528b46b2e7113d"},{"y":72,"x":216,"u":"https://preview.redd.it/yjnx38xe0def1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=6cebe05368e426114ba2c32e8e5d147e7c4ba894"},{"y":107,"x":320,"u":"https://preview.redd.it/yjnx38xe0def1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=9bffbac544124ebb6fad547055df0bc69256f38e"},{"y":215,"x":640,"u":"https://preview.redd.it/yjnx38xe0def1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=e6940b8d8e988da2b27c7605aa01e10b2d739ede"},{"y":322,"x":960,"u":"https://preview.redd.it/yjnx38xe0def1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=3a393fb934f1faff5a31cd7cb05c11d0c6ecd51e"}],"s":{"y":339,"x":1008,"u":"https://preview.redd.it/yjnx38xe0def1.png?width=1008&amp;format=png&amp;auto=webp&amp;s=02edefa3d033819577a0f77ddc72982e80a70f9f"},"id":"yjnx38xe0def1"}},"author_flair_text":null,"treatment_tags":[],"created_utc":1753161273,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n4h28fu","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Rich_Repeat_22","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4fpk2v","score":3,"author_fullname":"t2_viufiki6","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Thanks mate. How are the temps btw? Because was thinking about the Thermalright which is same price but watercooled.","edited":false,"author_flair_css_class":null,"name":"t1_n4h28fu","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thanks mate. How are the temps btw? Because was thinking about the Thermalright which is same price but watercooled.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1m5s6d1","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5s6d1/ai_395_64gb_vs_128gb/n4h28fu/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753160617,"author_flair_text":null,"collapsed":false,"created_utc":1753160617,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n4fpk2v","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"federationoffear","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4fdaqp","score":3,"author_fullname":"t2_4xa3j","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"It’s the FEVM FA-EX9, which has an M.2 to PCIe adapter with OCuLink port preinstalled. Works well. Running a 3090 using an AOOSTAR AG02 eGPU dock right now.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4fpk2v","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It’s the FEVM FA-EX9, which has an M.2 to PCIe adapter with OCuLink port preinstalled. Works well. Running a 3090 using an AOOSTAR AG02 eGPU dock right now.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m5s6d1","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5s6d1/ai_395_64gb_vs_128gb/n4fpk2v/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753142110,"author_flair_text":null,"treatment_tags":[],"created_utc":1753142110,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n4fdaqp","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Rich_Repeat_22","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4eko13","score":3,"author_fullname":"t2_viufiki6","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"One of the miniPCs has Oculink instead of 2nd M.2. I don't remember the model (is not the GMK). \\n\\nThat one is the ideal to use external dGPU. Alternative M.2 to oculink adapter and use the 2nd M.2","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n4fdaqp","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;One of the miniPCs has Oculink instead of 2nd M.2. I don&amp;#39;t remember the model (is not the GMK). &lt;/p&gt;\\n\\n&lt;p&gt;That one is the ideal to use external dGPU. Alternative M.2 to oculink adapter and use the 2nd M.2&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m5s6d1","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5s6d1/ai_395_64gb_vs_128gb/n4fdaqp/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753138085,"author_flair_text":null,"treatment_tags":[],"created_utc":1753138085,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n4eko13","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"cfogrady","can_mod_post":false,"created_utc":1753129332,"send_replies":true,"parent_id":"t1_n4efg9y","score":2,"author_fullname":"t2_y0abrfm","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Been thinking of how I could pair with an eGPU... don't want to be hamstrung by USB4 bandwidth... but perhaps that doesn't matter as much for LLMs. I'm mostly looking at a mobile solution (so HP or Asus at the moment), and I don't think there is a mobile solution with Oculink at the moment.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4eko13","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Been thinking of how I could pair with an eGPU... don&amp;#39;t want to be hamstrung by USB4 bandwidth... but perhaps that doesn&amp;#39;t matter as much for LLMs. I&amp;#39;m mostly looking at a mobile solution (so HP or Asus at the moment), and I don&amp;#39;t think there is a mobile solution with Oculink at the moment.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m5s6d1","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5s6d1/ai_395_64gb_vs_128gb/n4eko13/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753129332,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4gvjk4","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"fallingdowndizzyvr","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4eqhuo","score":1,"author_fullname":"t2_o65i6kx","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"The Bosgame comes with Oculink. Officially via a NVME to Oculink adapter.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4gvjk4","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The Bosgame comes with Oculink. Officially via a NVME to Oculink adapter.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m5s6d1","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5s6d1/ai_395_64gb_vs_128gb/n4gvjk4/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753157584,"author_flair_text":null,"treatment_tags":[],"created_utc":1753157584,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4eqhuo","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"b3081a","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4ep7n3","score":1,"author_fullname":"t2_17n5yh7l","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Probably not officially, but I've seen people messing around with M.2 adapters and got a 4090 running on a 395 system. I'd assume it would work on boards like the Framework Desktop.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n4eqhuo","is_submitter":false,"collapsed":false,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Probably not officially, but I&amp;#39;ve seen people messing around with M.2 adapters and got a 4090 running on a 395 system. I&amp;#39;d assume it would work on boards like the Framework Desktop.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m5s6d1","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5s6d1/ai_395_64gb_vs_128gb/n4eqhuo/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753130993,"author_flair_text":"llama.cpp","treatment_tags":[],"created_utc":1753130993,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4fdgbh","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Rich_Repeat_22","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4ep7n3","score":1,"author_fullname":"t2_viufiki6","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yes. There is even a miniPC with Oculink instead of 2nd M.2","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n4fdgbh","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yes. There is even a miniPC with Oculink instead of 2nd M.2&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m5s6d1","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5s6d1/ai_395_64gb_vs_128gb/n4fdgbh/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753138134,"author_flair_text":null,"treatment_tags":[],"created_utc":1753138134,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4ep7n3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"uti24","can_mod_post":false,"created_utc":1753130631,"send_replies":true,"parent_id":"t1_n4efg9y","score":1,"author_fullname":"t2_13hbro","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"&gt;and it's even better paired with a dGPU via oculink or PCIe slot.\\n\\nDoes AI 395 even supports that?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4ep7n3","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;and it&amp;#39;s even better paired with a dGPU via oculink or PCIe slot.&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;Does AI 395 even supports that?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m5s6d1","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5s6d1/ai_395_64gb_vs_128gb/n4ep7n3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753130631,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4hyraf","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"GreenCap49","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4gpfda","score":1,"author_fullname":"t2_5nfdwo7d","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Thank you! My Evo X2 is going to be delivered soon, let's see if I'm happy with the standalone speeds.  The problem with the dGPU is of course that you will add a little space heater to your room...","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4hyraf","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thank you! My Evo X2 is going to be delivered soon, let&amp;#39;s see if I&amp;#39;m happy with the standalone speeds.  The problem with the dGPU is of course that you will add a little space heater to your room...&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m5s6d1","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5s6d1/ai_395_64gb_vs_128gb/n4hyraf/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753178613,"author_flair_text":null,"treatment_tags":[],"created_utc":1753178613,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4gpfda","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"b3081a","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4fcn73","score":2,"author_fullname":"t2_17n5yh7l","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yes, it's using iGPU and dGPU at the same time. The 128GB memory will be used as graphics memory allocated via GTT or BIOS.\\n\\nROCm+SYCL or CUDA+SYCL should work, but ROCm+CUDA doesn't because these two backends shared the same backend registration function and only one will be recognized when both enabled during build. Using Vulkan is an easier approach but that will be sometimes slower than CUDA. It's also possible to use CPU+dGPU instead of iGPU+dGPU.\\n\\nTo configure llama.cpp in this way, the -ot or --override-tensor parameter, assuming ROCm0 is the dGPU and ROCm1 is iGPU, setting -ts 1,0 -ot exps=ROCm1 will put all the dense layers on the dGPU and experts overrided to iGPU. Setting -ot exps=CPU is the more common approach in the community these days and that works well if you have a server CPU with lots of memory channels.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n4gpfda","is_submitter":false,"collapsed":false,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yes, it&amp;#39;s using iGPU and dGPU at the same time. The 128GB memory will be used as graphics memory allocated via GTT or BIOS.&lt;/p&gt;\\n\\n&lt;p&gt;ROCm+SYCL or CUDA+SYCL should work, but ROCm+CUDA doesn&amp;#39;t because these two backends shared the same backend registration function and only one will be recognized when both enabled during build. Using Vulkan is an easier approach but that will be sometimes slower than CUDA. It&amp;#39;s also possible to use CPU+dGPU instead of iGPU+dGPU.&lt;/p&gt;\\n\\n&lt;p&gt;To configure llama.cpp in this way, the -ot or --override-tensor parameter, assuming ROCm0 is the dGPU and ROCm1 is iGPU, setting -ts 1,0 -ot exps=ROCm1 will put all the dense layers on the dGPU and experts overrided to iGPU. Setting -ot exps=CPU is the more common approach in the community these days and that works well if you have a server CPU with lots of memory channels.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m5s6d1","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5s6d1/ai_395_64gb_vs_128gb/n4gpfda/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753155002,"author_flair_text":"llama.cpp","treatment_tags":[],"created_utc":1753155002,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n4fcn73","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"GreenCap49","can_mod_post":false,"created_utc":1753137873,"send_replies":true,"parent_id":"t1_n4efg9y","score":1,"author_fullname":"t2_5nfdwo7d","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"In this scenario can you use the iGPU and the dGPU at the same time time? And the 128GB will not be used as normal RAM?\\n\\nWhat arguments would you have to give llama.cpp to achieve this? I'm thinking about getting an egpu for this exact use case.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4fcn73","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;In this scenario can you use the iGPU and the dGPU at the same time time? And the 128GB will not be used as normal RAM?&lt;/p&gt;\\n\\n&lt;p&gt;What arguments would you have to give llama.cpp to achieve this? I&amp;#39;m thinking about getting an egpu for this exact use case.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m5s6d1","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5s6d1/ai_395_64gb_vs_128gb/n4fcn73/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753137873,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4efg9y","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"b3081a","can_mod_post":false,"created_utc":1753127842,"send_replies":true,"parent_id":"t3_1m5s6d1","score":8,"author_fullname":"t2_17n5yh7l","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"128GB is definitely preferred, and it's even better paired with a dGPU via oculink or PCIe slot.\\n\\n\\n\\nQwen3 235B (especially the latest non-reasoning model) with attention+kvcache on dGPU and 4bit experts offloaded to iGPU fits in perfectly. You'll get ~15 t/s output and plenty of context length, as kvcache stays with attention layers in the extended dGPU VRAM.\\n\\n\\nWithout a dGPU it will not be slower but you'll have to use q3_k or even q2_k. With dynamic quantization the quality wouldn't necessarily be that bad though.","edited":1753128041,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4efg9y","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;128GB is definitely preferred, and it&amp;#39;s even better paired with a dGPU via oculink or PCIe slot.&lt;/p&gt;\\n\\n&lt;p&gt;Qwen3 235B (especially the latest non-reasoning model) with attention+kvcache on dGPU and 4bit experts offloaded to iGPU fits in perfectly. You&amp;#39;ll get ~15 t/s output and plenty of context length, as kvcache stays with attention layers in the extended dGPU VRAM.&lt;/p&gt;\\n\\n&lt;p&gt;Without a dGPU it will not be slower but you&amp;#39;ll have to use q3_k or even q2_k. With dynamic quantization the quality wouldn&amp;#39;t necessarily be that bad though.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5s6d1/ai_395_64gb_vs_128gb/n4efg9y/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753127842,"author_flair_text":"llama.cpp","treatment_tags":[],"link_id":"t3_1m5s6d1","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":8}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4hv1cm","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"SkyFeistyLlama8","can_mod_post":false,"created_utc":1753176526,"send_replies":true,"parent_id":"t1_n4fetg8","score":2,"author_fullname":"t2_1hgbaqgbnq","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I agree, I've only got 64 GB on a unified RAM laptop but I still keep a few models loaded simultaneously. For example, I could have Gemma 4B for quick code completion and classification, some IBM embedding model for RAG, and Mistral 24B or Gemma 27B as the main interpreter LLM. It's also nice to be able to load two 24B or 32B models to compare against each other.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4hv1cm","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I agree, I&amp;#39;ve only got 64 GB on a unified RAM laptop but I still keep a few models loaded simultaneously. For example, I could have Gemma 4B for quick code completion and classification, some IBM embedding model for RAG, and Mistral 24B or Gemma 27B as the main interpreter LLM. It&amp;#39;s also nice to be able to load two 24B or 32B models to compare against each other.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m5s6d1","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5s6d1/ai_395_64gb_vs_128gb/n4hv1cm/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753176526,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n4fetg8","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"RobotRobotWhatDoUSee","can_mod_post":false,"created_utc":1753138578,"send_replies":true,"parent_id":"t3_1m5s6d1","score":6,"author_fullname":"t2_m78cdz1nv","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I have a 128GB RAM in a unified memory machine like you are considering (essentially the previous generation), and I would definitely recommend getting 128GB RAM. Many times lots of RAM in this setup helps me run *small* models better. For example sometimes lots of RAM can solve 'thrashing' problems when a model is loaded, and sometimes you just want more RAM so you can have more context.\\n\\nBut, yes, if you can afford it, get the 128GB. \\n\\nAnd remember to set up the processor so the iGPU can actually use the RAM. There are a couple ways to do this, talk to a frontier LLM about how to do it for your machine.\\n\\n**Edit**: And this may be obvious, but the major use-case for these unified memory machines is mixtures of experts, which tend to have a much bigger footprint in RAM versus equivalent dense models, but are much faster than the dense models, which makes up for the low memory bandwidth in the unified memory. So you want as much RAM as possible, so that you won't be limited when selecting between MoEs.","edited":1753139147,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4fetg8","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I have a 128GB RAM in a unified memory machine like you are considering (essentially the previous generation), and I would definitely recommend getting 128GB RAM. Many times lots of RAM in this setup helps me run &lt;em&gt;small&lt;/em&gt; models better. For example sometimes lots of RAM can solve &amp;#39;thrashing&amp;#39; problems when a model is loaded, and sometimes you just want more RAM so you can have more context.&lt;/p&gt;\\n\\n&lt;p&gt;But, yes, if you can afford it, get the 128GB. &lt;/p&gt;\\n\\n&lt;p&gt;And remember to set up the processor so the iGPU can actually use the RAM. There are a couple ways to do this, talk to a frontier LLM about how to do it for your machine.&lt;/p&gt;\\n\\n&lt;p&gt;&lt;strong&gt;Edit&lt;/strong&gt;: And this may be obvious, but the major use-case for these unified memory machines is mixtures of experts, which tend to have a much bigger footprint in RAM versus equivalent dense models, but are much faster than the dense models, which makes up for the low memory bandwidth in the unified memory. So you want as much RAM as possible, so that you won&amp;#39;t be limited when selecting between MoEs.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5s6d1/ai_395_64gb_vs_128gb/n4fetg8/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753138578,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m5s6d1","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4f0na3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Freonr2","can_mod_post":false,"created_utc":1753134015,"send_replies":true,"parent_id":"t3_1m5s6d1","score":4,"author_fullname":"t2_8xi6x","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I think anyone looking to get a 395+ for local LLMs is going to kick themselves for not getting the 128. The 64GB ones are cheaper, but not by enough to be worth considering IMO.  Just keep in mind it is sort of MOE or bust due to the low mem bandwidth.  \\n\\nAlso when considering 64 v 128, you have to subtract memory for the OS, too, which makes it more substantial of a difference in usable RAM for LLM hosting.  64 is 48GB max anyway, so the $1500 is better spent on two 3090s assuming you have any desktop box you can cram them in.","edited":1753134838,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4f0na3","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I think anyone looking to get a 395+ for local LLMs is going to kick themselves for not getting the 128. The 64GB ones are cheaper, but not by enough to be worth considering IMO.  Just keep in mind it is sort of MOE or bust due to the low mem bandwidth.  &lt;/p&gt;\\n\\n&lt;p&gt;Also when considering 64 v 128, you have to subtract memory for the OS, too, which makes it more substantial of a difference in usable RAM for LLM hosting.  64 is 48GB max anyway, so the $1500 is better spent on two 3090s assuming you have any desktop box you can cram them in.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5s6d1/ai_395_64gb_vs_128gb/n4f0na3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753134015,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m5s6d1","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4excml","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"michaelsoft__binbows","can_mod_post":false,"created_utc":1753133010,"send_replies":true,"parent_id":"t1_n4ep8yi","score":1,"author_fullname":"t2_iifi6ul2l","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yeah i feel like deepseek and full fat qwen3 are the ones to get and they far outstrip 64/128GB setups. Until the pc world catches up you're looking at server platforms (epyc rome for a cheap entry point) and ultra mac studios with 192 or more GB. Meanwhile you can squeeze by fairly well with 24GB on 30B or so class models which to me are punching quite well above their weight.\\n\\nI believe the sweet spot right now is seeing how much useful work you can get out of the 30B models because when they're inferencing efficiently on a 24GB GPU you're talking about over 500 tokens per second (with batching) and it means we should be able to (for example... i'm only tinkering with this and don't have any results to report yet) fire off like 10 prompts constructed in different methods and pick out whatever is successful and still come out ahead of another system capable of providing only 10, 20, 30, 40 tok/s running a larger model\\n\\nthe fact that a 3090 delivers nearly 1TB/s and the amazing capability level of 30B models these days, it leaves me with basically zero desire for a 128gb unified memory system. That's dropping both memory and compute bandwidth by a factor of like 4, that's going to dramatically reduce the throughput. \\n\\nI was excited for getting a 5090 but it turns out in mid 2025 a 3090 already provides way more throughput than i can make use of.","edited":1753133350,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4excml","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yeah i feel like deepseek and full fat qwen3 are the ones to get and they far outstrip 64/128GB setups. Until the pc world catches up you&amp;#39;re looking at server platforms (epyc rome for a cheap entry point) and ultra mac studios with 192 or more GB. Meanwhile you can squeeze by fairly well with 24GB on 30B or so class models which to me are punching quite well above their weight.&lt;/p&gt;\\n\\n&lt;p&gt;I believe the sweet spot right now is seeing how much useful work you can get out of the 30B models because when they&amp;#39;re inferencing efficiently on a 24GB GPU you&amp;#39;re talking about over 500 tokens per second (with batching) and it means we should be able to (for example... i&amp;#39;m only tinkering with this and don&amp;#39;t have any results to report yet) fire off like 10 prompts constructed in different methods and pick out whatever is successful and still come out ahead of another system capable of providing only 10, 20, 30, 40 tok/s running a larger model&lt;/p&gt;\\n\\n&lt;p&gt;the fact that a 3090 delivers nearly 1TB/s and the amazing capability level of 30B models these days, it leaves me with basically zero desire for a 128gb unified memory system. That&amp;#39;s dropping both memory and compute bandwidth by a factor of like 4, that&amp;#39;s going to dramatically reduce the throughput. &lt;/p&gt;\\n\\n&lt;p&gt;I was excited for getting a 5090 but it turns out in mid 2025 a 3090 already provides way more throughput than i can make use of.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m5s6d1","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5s6d1/ai_395_64gb_vs_128gb/n4excml/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753133010,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4f21rs","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Freonr2","can_mod_post":false,"created_utc":1753134444,"send_replies":true,"parent_id":"t1_n4ep8yi","score":1,"author_fullname":"t2_8xi6x","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"50-240B size is at least becoming a bit more more popular recently.  Qwen3 235B, LLama 4 Scout, Hunyuan 80B, all MOEs.  \\n\\nThe MOE aspect is fairly important, too, since dense at that speed would be turtle slow on a 395.  Even 32B dense is going to be a bit on the painful side.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4f21rs","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;50-240B size is at least becoming a bit more more popular recently.  Qwen3 235B, LLama 4 Scout, Hunyuan 80B, all MOEs.  &lt;/p&gt;\\n\\n&lt;p&gt;The MOE aspect is fairly important, too, since dense at that speed would be turtle slow on a 395.  Even 32B dense is going to be a bit on the painful side.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m5s6d1","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5s6d1/ai_395_64gb_vs_128gb/n4f21rs/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753134444,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4ep8yi","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"eloquentemu","can_mod_post":false,"created_utc":1753130641,"send_replies":true,"parent_id":"t3_1m5s6d1","score":7,"author_fullname":"t2_lpdsy","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"128GB is already not enough /s.\\n\\nIn terms of recent models, there's a chasm between ~32B and ~200+B where I think the only things that exist are Llama3.3-70B tunes.  (Some are good, TBF, but not really SOTA.)  But if you look at, say, Qwen3 that everyone is hype about right now, that's 133GB at Q4.  So you could run it at Q3 or something with 128GB but realistically couldn't with 64GB.  Meanwhile for all the 24B-32B models you'd be better off with a 3090 or similar.  So in the end, there isn't really a compelling use-case for 64GB right now.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4ep8yi","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;128GB is already not enough /s.&lt;/p&gt;\\n\\n&lt;p&gt;In terms of recent models, there&amp;#39;s a chasm between ~32B and ~200+B where I think the only things that exist are Llama3.3-70B tunes.  (Some are good, TBF, but not really SOTA.)  But if you look at, say, Qwen3 that everyone is hype about right now, that&amp;#39;s 133GB at Q4.  So you could run it at Q3 or something with 128GB but realistically couldn&amp;#39;t with 64GB.  Meanwhile for all the 24B-32B models you&amp;#39;d be better off with a 3090 or similar.  So in the end, there isn&amp;#39;t really a compelling use-case for 64GB right now.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5s6d1/ai_395_64gb_vs_128gb/n4ep8yi/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753130641,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m5s6d1","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":7}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4e8eha","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"cfogrady","can_mod_post":false,"created_utc":1753125823,"send_replies":true,"parent_id":"t1_n4e81zf","score":3,"author_fullname":"t2_y0abrfm","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"You know the second option of running multiple models quickly isn't one I had considered... but is definitely something I am interested in... Thanks for pointing it out as an option!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4e8eha","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You know the second option of running multiple models quickly isn&amp;#39;t one I had considered... but is definitely something I am interested in... Thanks for pointing it out as an option!&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m5s6d1","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5s6d1/ai_395_64gb_vs_128gb/n4e8eha/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753125823,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n4e81zf","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ElectroSpore","can_mod_post":false,"created_utc":1753125724,"send_replies":true,"parent_id":"t3_1m5s6d1","score":3,"author_fullname":"t2_cb14t","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I think the question is do you want to run one large model VERY slow or do you want to be able to run more than one  small model loaded into vram at a time fast?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4e81zf","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I think the question is do you want to run one large model VERY slow or do you want to be able to run more than one  small model loaded into vram at a time fast?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5s6d1/ai_395_64gb_vs_128gb/n4e81zf/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753125724,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m5s6d1","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4entgg","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"IORelay","can_mod_post":false,"created_utc":1753130233,"send_replies":true,"parent_id":"t3_1m5s6d1","score":2,"author_fullname":"t2_2r6g3a9k","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"It's true that due to bandwidth running large dense models is too slow on strix halo. That said given however expensive the chip is anyway, you may as well go for the 128GB version. ","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4entgg","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It&amp;#39;s true that due to bandwidth running large dense models is too slow on strix halo. That said given however expensive the chip is anyway, you may as well go for the 128GB version. &lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5s6d1/ai_395_64gb_vs_128gb/n4entgg/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753130233,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m5s6d1","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4fytvs","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"cfogrady","can_mod_post":false,"created_utc":1753145322,"send_replies":true,"parent_id":"t1_n4f94zf","score":1,"author_fullname":"t2_y0abrfm","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I did not know that. That's very important information. Thanks for sharing!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4fytvs","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I did not know that. That&amp;#39;s very important information. Thanks for sharing!&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m5s6d1","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5s6d1/ai_395_64gb_vs_128gb/n4fytvs/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753145322,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4f94zf","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Baldur-Norddahl","can_mod_post":false,"created_utc":1753136723,"send_replies":true,"parent_id":"t3_1m5s6d1","score":2,"author_fullname":"t2_bvqb8ng0","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I understand that the 64GB has reduced memory bandwidth. So you need to go for the 128GB in any case to get full speed.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4f94zf","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I understand that the 64GB has reduced memory bandwidth. So you need to go for the 128GB in any case to get full speed.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5s6d1/ai_395_64gb_vs_128gb/n4f94zf/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753136723,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m5s6d1","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4fc1cc","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"fp4guru","can_mod_post":false,"created_utc":1753137674,"send_replies":true,"parent_id":"t3_1m5s6d1","score":2,"author_fullname":"t2_1tp8zldw5g","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"If it can get qwen3 235b q3 to run at more than pp100, generated 10tkps, then it is worth the price.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4fc1cc","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;If it can get qwen3 235b q3 to run at more than pp100, generated 10tkps, then it is worth the price.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5s6d1/ai_395_64gb_vs_128gb/n4fc1cc/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753137674,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m5s6d1","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4gindn","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Trepedation","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4ge7hk","score":1,"author_fullname":"t2_59t4l717","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"From what I remember the performance penalty was about 30% but if portability is important then that’s worth it imo. Even in a desktop it won’t be able to run huge models very fast so as its graphics are not hugely powerful and if the model you’re running is already slow 30% more speed won’t really feel like a lot. I’d go for the laptop if I were choosing and build a desktop if you want to do faster inference in the future as it will be significantly faster even if you have less VRAM.","edited":false,"author_flair_css_class":null,"name":"t1_n4gindn","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;From what I remember the performance penalty was about 30% but if portability is important then that’s worth it imo. Even in a desktop it won’t be able to run huge models very fast so as its graphics are not hugely powerful and if the model you’re running is already slow 30% more speed won’t really feel like a lot. I’d go for the laptop if I were choosing and build a desktop if you want to do faster inference in the future as it will be significantly faster even if you have less VRAM.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1m5s6d1","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5s6d1/ai_395_64gb_vs_128gb/n4gindn/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753152344,"author_flair_text":null,"collapsed":false,"created_utc":1753152344,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4ge7hk","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"cfogrady","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4gd5jv","score":1,"author_fullname":"t2_y0abrfm","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I have... multiple times lol.\\n\\nHe's part of the reason I'm not dead set against the mobile. His graph makes it look to me like the Desktop form factor is only marginally faster as opposed to being fast enough that it would change how quickly I perceived a model to run.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4ge7hk","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I have... multiple times lol.&lt;/p&gt;\\n\\n&lt;p&gt;He&amp;#39;s part of the reason I&amp;#39;m not dead set against the mobile. His graph makes it look to me like the Desktop form factor is only marginally faster as opposed to being fast enough that it would change how quickly I perceived a model to run.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m5s6d1","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5s6d1/ai_395_64gb_vs_128gb/n4ge7hk/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753150706,"author_flair_text":null,"treatment_tags":[],"created_utc":1753150706,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4gd5jv","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Trepedation","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4gaw03","score":1,"author_fullname":"t2_59t4l717","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"It will definitely be able to handle 14 and probably be fine with 32b models but I recommend you watch the YouTube channel Alex Ziskind as he’s done reviews on both the laptop and desktop versions of this chip and tested 14b and 32b models on them so you’ll be able to see what kind of tokens per second you’ll get.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n4gd5jv","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It will definitely be able to handle 14 and probably be fine with 32b models but I recommend you watch the YouTube channel Alex Ziskind as he’s done reviews on both the laptop and desktop versions of this chip and tested 14b and 32b models on them so you’ll be able to see what kind of tokens per second you’ll get.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m5s6d1","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5s6d1/ai_395_64gb_vs_128gb/n4gd5jv/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753150327,"author_flair_text":null,"treatment_tags":[],"created_utc":1753150327,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4gaw03","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"cfogrady","can_mod_post":false,"created_utc":1753149526,"send_replies":true,"parent_id":"t1_n4fhvz8","score":1,"author_fullname":"t2_y0abrfm","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Haven't settled for sure, but leaning towards mobile device. I figure if my 780m can handle 7B models at speeds I consider reasonable, hopefully this thing can handle 14B or 32B models. I want to be able to do couch development while with the family, which is why I'm leaning towards mobile... but an alternative would be desktop if I can reasonably remote to it with a portable device, which is where the mini-ITX Framework might shine, especially if paired with a powerful GPU. I'm not big on Macs... I want to do things other than local AI on this machine, but local AI is a primary motivator for me looking at this chipset specifically.\\n\\nBefore I got onto this, I was looking at a cheap laptop paired with a Desktop with a single powerful gaming GPU. 5080 or maybe the 9070XT... but I'd rather sacrifice the gaming for the ability to do more with local LLMs... and I worry that a single GPU, while fast, will be too limited by lack of VRAM. The budget I'm considering is between 2-3k. Right now I operate solely on a gaming handheld.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4gaw03","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Haven&amp;#39;t settled for sure, but leaning towards mobile device. I figure if my 780m can handle 7B models at speeds I consider reasonable, hopefully this thing can handle 14B or 32B models. I want to be able to do couch development while with the family, which is why I&amp;#39;m leaning towards mobile... but an alternative would be desktop if I can reasonably remote to it with a portable device, which is where the mini-ITX Framework might shine, especially if paired with a powerful GPU. I&amp;#39;m not big on Macs... I want to do things other than local AI on this machine, but local AI is a primary motivator for me looking at this chipset specifically.&lt;/p&gt;\\n\\n&lt;p&gt;Before I got onto this, I was looking at a cheap laptop paired with a Desktop with a single powerful gaming GPU. 5080 or maybe the 9070XT... but I&amp;#39;d rather sacrifice the gaming for the ability to do more with local LLMs... and I worry that a single GPU, while fast, will be too limited by lack of VRAM. The budget I&amp;#39;m considering is between 2-3k. Right now I operate solely on a gaming handheld.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m5s6d1","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5s6d1/ai_395_64gb_vs_128gb/n4gaw03/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753149526,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4fhvz8","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Trepedation","can_mod_post":false,"created_utc":1753139583,"send_replies":true,"parent_id":"t3_1m5s6d1","score":2,"author_fullname":"t2_59t4l717","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"What form factor would you be getting it in? On a mobile device it doesn’t really have enough power to run the side of llms you are talking about at a useful speed and you might even get faster inference running with a decent CPU and system memory. If it’s a desktop it’s a bit more worth it but you it would then be worth considering getting a Mac because even the M1 Max in a MacBook Pro 16 comes close to its speeds in inference. If you do decide to go for it definitely get 128gb of ram since that’s the benefit it has over the Mac setups otherwise you’d be better off with a Mac Studio.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4fhvz8","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;What form factor would you be getting it in? On a mobile device it doesn’t really have enough power to run the side of llms you are talking about at a useful speed and you might even get faster inference running with a decent CPU and system memory. If it’s a desktop it’s a bit more worth it but you it would then be worth considering getting a Mac because even the M1 Max in a MacBook Pro 16 comes close to its speeds in inference. If you do decide to go for it definitely get 128gb of ram since that’s the benefit it has over the Mac setups otherwise you’d be better off with a Mac Studio.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5s6d1/ai_395_64gb_vs_128gb/n4fhvz8/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753139583,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m5s6d1","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4i5x3o","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Much-Farmer-2752","can_mod_post":false,"created_utc":1753182211,"send_replies":true,"parent_id":"t1_n4fngrg","score":1,"author_fullname":"t2_1oyoxw5ijn","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Easy: it's 4x64 (or 8x32b) channels of unbuffered DDR5. 32 gigs per channel is what you can get with the avaliable memory chips - otherwise you'll need buffers, x4 chips and other server stuff Strix does not support.\\n\\nDoubt there are any hardware limits, AMD rarley doing them for mem size - so at least 192 gigs (4x48Gb) configs seems real in the future.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4i5x3o","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Easy: it&amp;#39;s 4x64 (or 8x32b) channels of unbuffered DDR5. 32 gigs per channel is what you can get with the avaliable memory chips - otherwise you&amp;#39;ll need buffers, x4 chips and other server stuff Strix does not support.&lt;/p&gt;\\n\\n&lt;p&gt;Doubt there are any hardware limits, AMD rarley doing them for mem size - so at least 192 gigs (4x48Gb) configs seems real in the future.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m5s6d1","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5s6d1/ai_395_64gb_vs_128gb/n4i5x3o/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753182211,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4fngrg","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"cbeater","can_mod_post":false,"created_utc":1753141415,"send_replies":true,"parent_id":"t3_1m5s6d1","score":2,"author_fullname":"t2_54l30","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Why they cap AMD strix at 128gb? Is it just to release the next version thats larger?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4fngrg","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Why they cap AMD strix at 128gb? Is it just to release the next version thats larger?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5s6d1/ai_395_64gb_vs_128gb/n4fngrg/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753141415,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m5s6d1","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4etq24","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"cfogrady","can_mod_post":false,"created_utc":1753131921,"send_replies":true,"parent_id":"t1_n4etbzd","score":1,"author_fullname":"t2_y0abrfm","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Leaning towards the 128GB... The thought of running multiple small models at the same time appeals to me greatly. I think it's very fair to say it doesn't make sense for larger models, but if I could get 5-10 smaller models working in tandem... or load a larger MoE model... then I think the larger memory size still makes sense.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4etq24","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Leaning towards the 128GB... The thought of running multiple small models at the same time appeals to me greatly. I think it&amp;#39;s very fair to say it doesn&amp;#39;t make sense for larger models, but if I could get 5-10 smaller models working in tandem... or load a larger MoE model... then I think the larger memory size still makes sense.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m5s6d1","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5s6d1/ai_395_64gb_vs_128gb/n4etq24/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753131921,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4etbzd","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Zestyclose-Ad-6147","can_mod_post":false,"created_utc":1753131806,"send_replies":true,"parent_id":"t3_1m5s6d1","score":1,"author_fullname":"t2_87gjhzta","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I've been contemplating this for a while too. I'm curious to see what you'll end up choosing!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4etbzd","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;ve been contemplating this for a while too. I&amp;#39;m curious to see what you&amp;#39;ll end up choosing!&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5s6d1/ai_395_64gb_vs_128gb/n4etbzd/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753131806,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m5s6d1","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4f2o38","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"SmellsLikeAPig","can_mod_post":false,"created_utc":1753134634,"send_replies":true,"parent_id":"t3_1m5s6d1","score":1,"author_fullname":"t2_6aylq","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Imho vastly better results with  big models from big players. Self hosting ai is subpar experience and very expensive","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4f2o38","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Imho vastly better results with  big models from big players. Self hosting ai is subpar experience and very expensive&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5s6d1/ai_395_64gb_vs_128gb/n4f2o38/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753134634,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m5s6d1","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4fm7wu","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"allenasm","can_mod_post":false,"created_utc":1753141002,"send_replies":true,"parent_id":"t3_1m5s6d1","score":1,"author_fullname":"t2_fouwt","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"128 for sure.  The jump in quality for llms in that range is around 70 to 80gb and then you need some ram left over for other things.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4fm7wu","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;128 for sure.  The jump in quality for llms in that range is around 70 to 80gb and then you need some ram left over for other things.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5s6d1/ai_395_64gb_vs_128gb/n4fm7wu/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753141002,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m5s6d1","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4fq5yb","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"triynizzles1","can_mod_post":false,"created_utc":1753142314,"send_replies":true,"parent_id":"t3_1m5s6d1","score":1,"author_fullname":"t2_zr0g49ixt","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"128gb would be best imo. Keep in mind if you are using Windows it’s going to limit you to 96 GB if you use linux then you can use the full 128gb","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4fq5yb","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;128gb would be best imo. Keep in mind if you are using Windows it’s going to limit you to 96 GB if you use linux then you can use the full 128gb&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5s6d1/ai_395_64gb_vs_128gb/n4fq5yb/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753142314,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m5s6d1","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4gve0e","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"fallingdowndizzyvr","can_mod_post":false,"created_utc":1753157516,"send_replies":true,"parent_id":"t3_1m5s6d1","score":1,"author_fullname":"t2_o65i6kx","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I don't even see the point of getting the 64GB. Especially at the prices they go for. The 64GB stubbornly is at $1500. The 128GB has been as low as $1700.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4gve0e","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I don&amp;#39;t even see the point of getting the 64GB. Especially at the prices they go for. The 64GB stubbornly is at $1500. The 128GB has been as low as $1700.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5s6d1/ai_395_64gb_vs_128gb/n4gve0e/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753157516,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m5s6d1","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4i83jk","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"05032-MendicantBias","can_mod_post":false,"created_utc":1753183187,"send_replies":true,"parent_id":"t3_1m5s6d1","score":1,"author_fullname":"t2_6id3lwou","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"You can't expand memory. The only sensible configuration is the one with the most ram, that happens to be 128GB.\\n\\nJumping from 64 to 128 really expands the pool of models you can run. Theoretically around 200B Q4 with the bigger config.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4i83jk","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You can&amp;#39;t expand memory. The only sensible configuration is the one with the most ram, that happens to be 128GB.&lt;/p&gt;\\n\\n&lt;p&gt;Jumping from 64 to 128 really expands the pool of models you can run. Theoretically around 200B Q4 with the bigger config.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5s6d1/ai_395_64gb_vs_128gb/n4i83jk/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753183187,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m5s6d1","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4ilagq","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"spaceman_","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4igzvx","score":1,"author_fullname":"t2_9neub","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I'm hoping to spend tonight getting it set up, so I might post something tomorrow.\\n\\n\\nFor what it's worth, stability and maturity issues in the amdgpu/rocm stack for the Ryzen AI, I'm very pleased with the 64GB model.\\n\\n\\nSure, 128GB is nicer, but I don't feel like I'm missing out on much right now. There have been no models I want to run that I can't fit in ~50GB that would have been OK in 110GB. But that depends on what you want to run and at what speeds. I'd suggest looking at models on HF and setting up hardware specs on you profile to see what would fit.\\n\\n\\nFor me, every model of interest is either &lt;40GB or &gt;170GB,","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n4ilagq","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;m hoping to spend tonight getting it set up, so I might post something tomorrow.&lt;/p&gt;\\n\\n&lt;p&gt;For what it&amp;#39;s worth, stability and maturity issues in the amdgpu/rocm stack for the Ryzen AI, I&amp;#39;m very pleased with the 64GB model.&lt;/p&gt;\\n\\n&lt;p&gt;Sure, 128GB is nicer, but I don&amp;#39;t feel like I&amp;#39;m missing out on much right now. There have been no models I want to run that I can&amp;#39;t fit in ~50GB that would have been OK in 110GB. But that depends on what you want to run and at what speeds. I&amp;#39;d suggest looking at models on HF and setting up hardware specs on you profile to see what would fit.&lt;/p&gt;\\n\\n&lt;p&gt;For me, every model of interest is either &amp;lt;40GB or &amp;gt;170GB,&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m5s6d1","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5s6d1/ai_395_64gb_vs_128gb/n4ilagq/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753188334,"author_flair_text":null,"treatment_tags":[],"created_utc":1753188334,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4igzvx","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"cfogrady","can_mod_post":false,"created_utc":1753186778,"send_replies":true,"parent_id":"t1_n4i9nyh","score":1,"author_fullname":"t2_y0abrfm","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yeah. That's where I was waffling. Whether token speed would even be tolerable by bigger models. I'm curious to hear how the MI50 machine works out for you. I'm also basically looking to replace my laptop, so I was leaning against a desktop system, but if it performs well at that price, I may need to reconsider it as an additional option.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4igzvx","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yeah. That&amp;#39;s where I was waffling. Whether token speed would even be tolerable by bigger models. I&amp;#39;m curious to hear how the MI50 machine works out for you. I&amp;#39;m also basically looking to replace my laptop, so I was leaning against a desktop system, but if it performs well at that price, I may need to reconsider it as an additional option.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m5s6d1","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5s6d1/ai_395_64gb_vs_128gb/n4igzvx/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753186778,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4i9nyh","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"spaceman_","can_mod_post":false,"created_utc":1753183862,"send_replies":true,"parent_id":"t3_1m5s6d1","score":1,"author_fullname":"t2_9neub","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I have the 64GB model and have been using it just fine for Devstral-24B, Gemma3-27B, Qwen3-32B or Qwen3-30B-A3B. I'm running one of these + a few smaller models concurrently, but am working on an OpenAI proxy that loads and unloads models on demand to fit in VRAM.\\n\\nMy thinking was mostly that for the larger models, the latency / token speed would be hard to tolerate with the throughput of the 395+ anyway, but that obviously depends on your use case. How much token speed do you need or latency can you tolerate?\\n\\nIt's also not my only machine, I have a 3x32GB Instinct MI50 machine that I'm setting up for inference as well. That whole machine (3 cards, mobo, cpu, memory, and PSU) cost less than the bump from 64 to 128GB, even before taxes get added on top (64GB vs 128GB was \\\\~800 euro before VAT, the second hand AMD Instinct setup was \\\\~500 euros on the used market all things combined). So if something can't run on my laptop, I will probably try to fit it on there.\\n\\nI've not yet had any case where I couldn't fit a model on this computer that I would have been able to run on the 128GB one. Most of the models I want to run but am not able to are 200GB or more and are outside the realm of possibility for me anyway. Any meaningful quant of a bigger model is going to be bigger than 128GB these days.\\n\\nOf course, it only takes one of these companies to release an AI model that needs 90-100GB for me to regret my choice. Maybe.","edited":1753184084,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4i9nyh","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I have the 64GB model and have been using it just fine for Devstral-24B, Gemma3-27B, Qwen3-32B or Qwen3-30B-A3B. I&amp;#39;m running one of these + a few smaller models concurrently, but am working on an OpenAI proxy that loads and unloads models on demand to fit in VRAM.&lt;/p&gt;\\n\\n&lt;p&gt;My thinking was mostly that for the larger models, the latency / token speed would be hard to tolerate with the throughput of the 395+ anyway, but that obviously depends on your use case. How much token speed do you need or latency can you tolerate?&lt;/p&gt;\\n\\n&lt;p&gt;It&amp;#39;s also not my only machine, I have a 3x32GB Instinct MI50 machine that I&amp;#39;m setting up for inference as well. That whole machine (3 cards, mobo, cpu, memory, and PSU) cost less than the bump from 64 to 128GB, even before taxes get added on top (64GB vs 128GB was ~800 euro before VAT, the second hand AMD Instinct setup was ~500 euros on the used market all things combined). So if something can&amp;#39;t run on my laptop, I will probably try to fit it on there.&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;ve not yet had any case where I couldn&amp;#39;t fit a model on this computer that I would have been able to run on the 128GB one. Most of the models I want to run but am not able to are 200GB or more and are outside the realm of possibility for me anyway. Any meaningful quant of a bigger model is going to be bigger than 128GB these days.&lt;/p&gt;\\n\\n&lt;p&gt;Of course, it only takes one of these companies to release an AI model that needs 90-100GB for me to regret my choice. Maybe.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5s6d1/ai_395_64gb_vs_128gb/n4i9nyh/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753183862,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m5s6d1","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4lahbs","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Swimming-Sky-7025","can_mod_post":false,"created_utc":1753216103,"send_replies":true,"parent_id":"t3_1m5s6d1","score":1,"author_fullname":"t2_1ajivw8k6l","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Keep in mind due to its small memory bandwidth you'll run 70b 4 bit quants at only 4 tokens per second.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4lahbs","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Keep in mind due to its small memory bandwidth you&amp;#39;ll run 70b 4 bit quants at only 4 tokens per second.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5s6d1/ai_395_64gb_vs_128gb/n4lahbs/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753216103,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m5s6d1","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4ehqj6","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"LagOps91","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4edqbf","score":3,"author_fullname":"t2_3wi6j7vwh","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"for large MoE models, you can quant down more aggressively. A larger Q3 quant should work perfectly fine. Unsloth has specifically optimized quants for MoE models. The 235b model should run at a good quality on 128GB ram.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4ehqj6","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;for large MoE models, you can quant down more aggressively. A larger Q3 quant should work perfectly fine. Unsloth has specifically optimized quants for MoE models. The 235b model should run at a good quality on 128GB ram.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m5s6d1","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5s6d1/ai_395_64gb_vs_128gb/n4ehqj6/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753128497,"author_flair_text":null,"treatment_tags":[],"created_utc":1753128497,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4egh1r","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"redoubt515","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4edqbf","score":1,"author_fullname":"t2_kehp8nb59","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"It doesn't, at least not until you get into the lowest quants, but the premise is the same. It's just the example I gave to illustrate the basics of how MoE works.\\n\\nLlama 4 Scout (109B total / 17B active) would be a more relevant example for the specific situation of &gt;64GB of RAM but &lt;128GB of RAM needed to run the model at decent a decent quant. Maybe your Tencent example also, but that is not a model I'm familiar with.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4egh1r","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It doesn&amp;#39;t, at least not until you get into the lowest quants, but the premise is the same. It&amp;#39;s just the example I gave to illustrate the basics of how MoE works.&lt;/p&gt;\\n\\n&lt;p&gt;Llama 4 Scout (109B total / 17B active) would be a more relevant example for the specific situation of &amp;gt;64GB of RAM but &amp;lt;128GB of RAM needed to run the model at decent a decent quant. Maybe your Tencent example also, but that is not a model I&amp;#39;m familiar with.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m5s6d1","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5s6d1/ai_395_64gb_vs_128gb/n4egh1r/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753128135,"author_flair_text":null,"treatment_tags":[],"created_utc":1753128135,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4einm3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Tenzu9","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4edqbf","score":1,"author_fullname":"t2_10wgss","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"you can you its IQ4 quant, i think its around 115 GB","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4einm3","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;you can you its IQ4 quant, i think its around 115 GB&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m5s6d1","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5s6d1/ai_395_64gb_vs_128gb/n4einm3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753128754,"author_flair_text":null,"treatment_tags":[],"created_utc":1753128754,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4edqbf","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Few_Painter_5588","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4ebgpf","score":1,"author_fullname":"t2_uvgafqnfy","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I'm not sure if Qwen3 235B will run on 128GB though, a Q3 quantization could but the perplexity would be harsh. I do think that new Tencent Hunyuan model will be a perfect fit though.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n4edqbf","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;m not sure if Qwen3 235B will run on 128GB though, a Q3 quantization could but the perplexity would be harsh. I do think that new Tencent Hunyuan model will be a perfect fit though.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m5s6d1","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5s6d1/ai_395_64gb_vs_128gb/n4edqbf/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753127353,"author_flair_text":null,"treatment_tags":[],"created_utc":1753127353,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4ebgpf","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"redoubt515","can_mod_post":false,"created_utc":1753126708,"send_replies":true,"parent_id":"t1_n4e8uej","score":8,"author_fullname":"t2_kehp8nb59","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"MOE seems like the other consideration though,\\n\\nLots of RAM is useful even if you don't have the speed to run a large dense model if you might use an MOE model that needs all the RAM just to load the model, only actively uses a subset of parameters actively. E.g. something like Qwen3-235B-A22B (you need enough RAM for 235B parameters in total, but only need enough *bandwidth t*o run 22B active parameters at a decent speed).\\n\\nAs an example, I could *never run* a 32B model (or even a 14B model) at acceptable speeds on my old ass hardware. But I can run a 30B MOE model at very reasonable speeds on that same system.","edited":1753126969,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4ebgpf","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;MOE seems like the other consideration though,&lt;/p&gt;\\n\\n&lt;p&gt;Lots of RAM is useful even if you don&amp;#39;t have the speed to run a large dense model if you might use an MOE model that needs all the RAM just to load the model, only actively uses a subset of parameters actively. E.g. something like Qwen3-235B-A22B (you need enough RAM for 235B parameters in total, but only need enough &lt;em&gt;bandwidth t&lt;/em&gt;o run 22B active parameters at a decent speed).&lt;/p&gt;\\n\\n&lt;p&gt;As an example, I could &lt;em&gt;never run&lt;/em&gt; a 32B model (or even a 14B model) at acceptable speeds on my old ass hardware. But I can run a 30B MOE model at very reasonable speeds on that same system.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m5s6d1","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5s6d1/ai_395_64gb_vs_128gb/n4ebgpf/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753126708,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":8}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4ej0kq","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Tenzu9","can_mod_post":false,"created_utc":1753128854,"send_replies":true,"parent_id":"t1_n4e8uej","score":1,"author_fullname":"t2_10wgss","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"You can a Q4 32B on a single 3090 if you are conservative with context and batch size.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4ej0kq","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You can a Q4 32B on a single 3090 if you are conservative with context and batch size.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m5s6d1","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5s6d1/ai_395_64gb_vs_128gb/n4ej0kq/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753128854,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4fvu0r","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"woahdudee2a","can_mod_post":false,"created_utc":1753144287,"send_replies":true,"parent_id":"t1_n4e8uej","score":1,"author_fullname":"t2_o6qm5t0","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"250 is nothing to scoff at, even if its like 230 in practice. in the server world you'd need to build something with 350-400 gb/s advertised speed to match it","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4fvu0r","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;250 is nothing to scoff at, even if its like 230 in practice. in the server world you&amp;#39;d need to build something with 350-400 gb/s advertised speed to match it&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m5s6d1","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5s6d1/ai_395_64gb_vs_128gb/n4fvu0r/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753144287,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4e8uej","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Few_Painter_5588","can_mod_post":false,"created_utc":1753125951,"send_replies":true,"parent_id":"t3_1m5s6d1","score":-3,"author_fullname":"t2_uvgafqnfy","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"You're only getting around 250GB/s of bandwidth tops from those systems. It's decent if you want to run small LLMs like 32B models, or something tencent's Hunyuan model. But Running medium models that are like 70B will be slow.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4e8uej","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You&amp;#39;re only getting around 250GB/s of bandwidth tops from those systems. It&amp;#39;s decent if you want to run small LLMs like 32B models, or something tencent&amp;#39;s Hunyuan model. But Running medium models that are like 70B will be slow.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5s6d1/ai_395_64gb_vs_128gb/n4e8uej/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753125951,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m5s6d1","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4er6fz","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":"LOW_SCORE","no_follow":true,"author":"SillyLilBear","can_mod_post":false,"created_utc":1753131188,"send_replies":true,"parent_id":"t3_1m5s6d1","score":-5,"author_fullname":"t2_wjjtz","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":true,"body":"No it isn't.  The device is so slow the 128G is pointless.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4er6fz","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;No it isn&amp;#39;t.  The device is so slow the 128G is pointless.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":"comment score below threshold","distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5s6d1/ai_395_64gb_vs_128gb/n4er6fz/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753131188,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m5s6d1","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4elfqf","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":"LOW_SCORE","no_follow":true,"author":"GPTshop_ai","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4ek6mz","score":-9,"author_fullname":"t2_rkmud0isr","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"get rich or die trying. godspeed.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n4elfqf","is_submitter":false,"collapsed":true,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;get rich or die trying. godspeed.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":"comment score below threshold","distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m5s6d1","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5s6d1/ai_395_64gb_vs_128gb/n4elfqf/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753129556,"author_flair_text":null,"treatment_tags":[],"created_utc":1753129556,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-9}}],"before":null}},"user_reports":[],"saved":false,"id":"n4ek6mz","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"cfogrady","can_mod_post":false,"created_utc":1753129190,"send_replies":true,"parent_id":"t1_n4edraa","score":7,"author_fullname":"t2_y0abrfm","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I assume your comment is in jest. I (like most people) don't have 10k to drop on a RTX Pro 6000 system and definitely don't have 40-50k to drop on a GH200 system. To my knowledge DGX spark isn't out yet. From what I've seen as far as system price to performance, the AI 395 is where it's at. Sure I would love the size and memory bandwidth of those options... but until I'm rich or the value is demonstrably beyond hobby level tinkering, I'm not spending a car down payment for a computer system.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4ek6mz","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I assume your comment is in jest. I (like most people) don&amp;#39;t have 10k to drop on a RTX Pro 6000 system and definitely don&amp;#39;t have 40-50k to drop on a GH200 system. To my knowledge DGX spark isn&amp;#39;t out yet. From what I&amp;#39;ve seen as far as system price to performance, the AI 395 is where it&amp;#39;s at. Sure I would love the size and memory bandwidth of those options... but until I&amp;#39;m rich or the value is demonstrably beyond hobby level tinkering, I&amp;#39;m not spending a car down payment for a computer system.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m5s6d1","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5s6d1/ai_395_64gb_vs_128gb/n4ek6mz/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753129190,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":7}}],"before":null}},"user_reports":[],"saved":false,"id":"n4edraa","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"GPTshop_ai","can_mod_post":false,"created_utc":1753127361,"send_replies":true,"parent_id":"t3_1m5s6d1","score":-4,"author_fullname":"t2_rkmud0isr","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Get something better, it wil give you more joy. RTX Pro 6000 or GH200 624GB. Even DGX spark is better.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4edraa","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Get something better, it wil give you more joy. RTX Pro 6000 or GH200 624GB. Even DGX spark is better.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5s6d1/ai_395_64gb_vs_128gb/n4edraa/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753127361,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m5s6d1","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-4}}],"before":null}}]`),n=()=>e.jsx(l,{data:a});export{n as default};
