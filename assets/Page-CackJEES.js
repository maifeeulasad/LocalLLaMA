import{j as e}from"./index-xfnGEtuL.js";import{R as l}from"./RedditPostRenderer-DAZRIZZK.js";import"./index-BaNn5-RR.js";const t=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Whereas prior generations of Granite LLMs utilized a conventional transformer architecture, all models in the Granite 4.0 family utilize a new **hybrid Mamba-2/Transformer architecture,** marrying the speed and efficiency of Mamba with the precision of transformer-based self-attention. \\n\\nGranite 4.0 Tiny-Preview, specifically, is a **fine-grained hybrid** [**mixture of experts (MoE)**](https://www.ibm.com/think/topics/mixture-of-experts) **model,** with 7B total parameters and only 1B active parameters at inference time.\\n\\n[https://huggingface.co/ibm-granite/granite-4.0-tiny-preview](https://huggingface.co/ibm-granite/granite-4.0-tiny-preview)\\n\\n[https://huggingface.co/ibm-granite/granite-4.0-tiny-base-preview](https://huggingface.co/ibm-granite/granite-4.0-tiny-base-preview)\\n\\n[https://huggingface.co/ibm-ai-platform/Bamba-9B-v1](https://huggingface.co/ibm-ai-platform/Bamba-9B-v1)\\n\\n","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Support for the upcoming IBM Granite 4.0 has been merged into llama.cpp","link_flair_richtext":[{"e":"text","t":"New Model"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":70,"top_awarded_type":null,"hide_score":false,"name":"t3_1lwsrx7","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.98,"author_flair_background_color":"#bbbdbf","ups":150,"total_awards_received":0,"media_embed":{},"thumbnail_width":140,"author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","is_original_content":false,"author_fullname":"t2_vqgbql9w","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"New Model","can_mod_post":false,"score":150,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"https://external-preview.redd.it/FNrRGnLNvs7SoS-PWTZeuDoBIeMJrIjippY_Sjx3gVs.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=58c542b2095a081e5dc71e3f0c3caf3b8d97bf2d","edited":false,"author_flair_css_class":null,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"gildings":{},"post_hint":"link","content_categories":null,"is_self":false,"subreddit_type":"public","created":1752193250,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"richtext","domain":"github.com","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Whereas prior generations of Granite LLMs utilized a conventional transformer architecture, all models in the Granite 4.0 family utilize a new &lt;strong&gt;hybrid Mamba-2/Transformer architecture,&lt;/strong&gt; marrying the speed and efficiency of Mamba with the precision of transformer-based self-attention. &lt;/p&gt;\\n\\n&lt;p&gt;Granite 4.0 Tiny-Preview, specifically, is a &lt;strong&gt;fine-grained hybrid&lt;/strong&gt; &lt;a href=\\"https://www.ibm.com/think/topics/mixture-of-experts\\"&gt;&lt;strong&gt;mixture of experts (MoE)&lt;/strong&gt;&lt;/a&gt; &lt;strong&gt;model,&lt;/strong&gt; with 7B total parameters and only 1B active parameters at inference time.&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://huggingface.co/ibm-granite/granite-4.0-tiny-preview\\"&gt;https://huggingface.co/ibm-granite/granite-4.0-tiny-preview&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://huggingface.co/ibm-granite/granite-4.0-tiny-base-preview\\"&gt;https://huggingface.co/ibm-granite/granite-4.0-tiny-base-preview&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://huggingface.co/ibm-ai-platform/Bamba-9B-v1\\"&gt;https://huggingface.co/ibm-ai-platform/Bamba-9B-v1&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"url_overridden_by_dest":"https://github.com/ggml-org/llama.cpp/pull/13550","view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"preview":{"images":[{"source":{"url":"https://external-preview.redd.it/FNrRGnLNvs7SoS-PWTZeuDoBIeMJrIjippY_Sjx3gVs.png?auto=webp&amp;s=599f611a372fbcdb39cff08d5b5708383ec3485e","width":1200,"height":600},"resolutions":[{"url":"https://external-preview.redd.it/FNrRGnLNvs7SoS-PWTZeuDoBIeMJrIjippY_Sjx3gVs.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=4d9dfc6caf6473cddc930c8672b2473ef6a39f9d","width":108,"height":54},{"url":"https://external-preview.redd.it/FNrRGnLNvs7SoS-PWTZeuDoBIeMJrIjippY_Sjx3gVs.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=e20af04b4cab9b55eb315b230cb214fefe9b821d","width":216,"height":108},{"url":"https://external-preview.redd.it/FNrRGnLNvs7SoS-PWTZeuDoBIeMJrIjippY_Sjx3gVs.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=c441275b8459e7c9069b5c09999881a7a3cca8bd","width":320,"height":160},{"url":"https://external-preview.redd.it/FNrRGnLNvs7SoS-PWTZeuDoBIeMJrIjippY_Sjx3gVs.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=be85c7d84d9bd88e720aef5b6d229ca49e85ef9a","width":640,"height":320},{"url":"https://external-preview.redd.it/FNrRGnLNvs7SoS-PWTZeuDoBIeMJrIjippY_Sjx3gVs.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=17ffa9221de2bb975a61c193b0b654fb03fcf6b5","width":960,"height":480},{"url":"https://external-preview.redd.it/FNrRGnLNvs7SoS-PWTZeuDoBIeMJrIjippY_Sjx3gVs.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=c85dbce218541b4a90e6c34de0e348286caf478d","width":1080,"height":540}],"variants":{},"id":"FNrRGnLNvs7SoS-PWTZeuDoBIeMJrIjippY_Sjx3gVs"}],"enabled":false},"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"ced98442-f5d3-11ed-b657-66d3b15490c6","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":"llama.cpp","treatment_tags":[],"visited":false,"removed_by":null,"mod_note":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"num_reports":null,"removal_reason":null,"link_flair_background_color":"#ffb000","id":"1lwsrx7","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"jacek2023","discussion_type":null,"num_comments":19,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":"light","permalink":"/r/LocalLLaMA/comments/1lwsrx7/support_for_the_upcoming_ibm_granite_40_has_been/","stickied":false,"url":"https://github.com/ggml-org/llama.cpp/pull/13550","subreddit_subscribers":497504,"created_utc":1752193250,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2j7dzp","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AaronFeng47","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2j15jv","score":2,"author_fullname":"t2_4gc7hf3m","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"They released some 20B coder models back in the early days, but yeah all of their current gen models are really smallÂ ","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2j7dzp","is_submitter":false,"collapsed":false,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;They released some 20B coder models back in the early days, but yeah all of their current gen models are really smallÂ &lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lwsrx7","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwsrx7/support_for_the_upcoming_ibm_granite_40_has_been/n2j7dzp/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752234457,"author_flair_text":"llama.cpp","treatment_tags":[],"created_utc":1752234457,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n2j15jv","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"vibjelo","can_mod_post":false,"created_utc":1752231749,"send_replies":true,"parent_id":"t1_n2gx6qb","score":2,"author_fullname":"t2_hr2hgnsk","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"That would be a pretty big jump compared to the releases they've done so far. I think 8B is the largest weights they've released up to today, no?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2j15jv","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;That would be a pretty big jump compared to the releases they&amp;#39;ve done so far. I think 8B is the largest weights they&amp;#39;ve released up to today, no?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lwsrx7","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwsrx7/support_for_the_upcoming_ibm_granite_40_has_been/n2j15jv/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752231749,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n2gx6qb","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"AaronFeng47","can_mod_post":false,"created_utc":1752196329,"send_replies":true,"parent_id":"t3_1lwsrx7","score":28,"author_fullname":"t2_4gc7hf3m","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I really hope they can release something around 30B","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2gx6qb","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I really hope they can release something around 30B&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwsrx7/support_for_the_upcoming_ibm_granite_40_has_been/n2gx6qb/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752196329,"author_flair_text":"llama.cpp","treatment_tags":[],"link_id":"t3_1lwsrx7","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":28}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2h6iht","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Dangerous_Fix_5526","can_mod_post":false,"created_utc":1752199580,"send_replies":true,"parent_id":"t3_1lwsrx7","score":16,"author_fullname":"t2_uhummsau","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Granite 4s are MOES ; 128k context, 62 experts (6 active) in under 7B parameters model.  \\n(from config.json file at repo)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2h6iht","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Granite 4s are MOES ; 128k context, 62 experts (6 active) in under 7B parameters model.&lt;br/&gt;\\n(from config.json file at repo)&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwsrx7/support_for_the_upcoming_ibm_granite_40_has_been/n2h6iht/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752199580,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lwsrx7","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":16}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2hzv5l","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"cddelgado","can_mod_post":false,"created_utc":1752211658,"send_replies":true,"parent_id":"t3_1lwsrx7","score":13,"author_fullname":"t2_b68un","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"So....\\n\\n... at what point will llama.cpp get a modular plugin system? Surely we're at the point where such an architecture needs to exist so the core developers don't have to manage the actual Kitchen Sink (220B MoE)â„¢ï¸.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2hzv5l","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;So....&lt;/p&gt;\\n\\n&lt;p&gt;... at what point will llama.cpp get a modular plugin system? Surely we&amp;#39;re at the point where such an architecture needs to exist so the core developers don&amp;#39;t have to manage the actual Kitchen Sink (220B MoE)â„¢ï¸.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwsrx7/support_for_the_upcoming_ibm_granite_40_has_been/n2hzv5l/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752211658,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lwsrx7","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":13}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2i6w4p","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ttkciar","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2gukba","score":1,"author_fullname":"t2_cpegz","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"You and me both.  I use 24B / 25B / 27B models for almost everything, and only switch up to 70B / 72B when necessary.\\n\\nMy Xeon servers *can* infer with Tulu3-405B (Q4_K_M), if barely, and at a painful 0.15 tokens/second, but I almost never take advantage of that.\\n\\nOn the other hand, Strix Halo with 128GB of memory should be able to infer with 105B parameters quantized to Q4_K_M, with enough memory left over for OS and some context.  When that kind of hardware becomes commonplace, I think 105B will be the new sweet spot.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2i6w4p","is_submitter":false,"collapsed":false,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You and me both.  I use 24B / 25B / 27B models for almost everything, and only switch up to 70B / 72B when necessary.&lt;/p&gt;\\n\\n&lt;p&gt;My Xeon servers &lt;em&gt;can&lt;/em&gt; infer with Tulu3-405B (Q4_K_M), if barely, and at a painful 0.15 tokens/second, but I almost never take advantage of that.&lt;/p&gt;\\n\\n&lt;p&gt;On the other hand, Strix Halo with 128GB of memory should be able to infer with 105B parameters quantized to Q4_K_M, with enough memory left over for OS and some context.  When that kind of hardware becomes commonplace, I think 105B will be the new sweet spot.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lwsrx7","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwsrx7/support_for_the_upcoming_ibm_granite_40_has_been/n2i6w4p/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752215231,"author_flair_text":"llama.cpp","treatment_tags":[],"created_utc":1752215231,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2gukba","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"jacek2023","can_mod_post":false,"created_utc":1752195420,"send_replies":true,"parent_id":"t1_n2gtimt","score":23,"author_fullname":"t2_vqgbql9w","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Personally, I don't need a 500B model at all, everything between 24B and 150B is great.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2gukba","is_submitter":true,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Personally, I don&amp;#39;t need a 500B model at all, everything between 24B and 150B is great.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lwsrx7","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwsrx7/support_for_the_upcoming_ibm_granite_40_has_been/n2gukba/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752195420,"author_flair_text":"llama.cpp","treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":23}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2h467p","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Lissanro","can_mod_post":false,"created_utc":1752198766,"send_replies":true,"parent_id":"t1_n2gtimt","score":3,"author_fullname":"t2_fpfao9g","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"500B+ MoE with hybrid architecture would be something great to try for sure! In the meantime, I run mostly IQ4\\\\_K\\\\_M quant of R1 0528, it has 671B so it is not too heavy - 355 GB, and 90GB of VRAM is enough to hold 100K context along with four full layers and common expert tensors.\\n\\nOne thing I hope for in the future models is higher context length and even better context efficiency that maybe Mamba could bring, so higher context cache would still fits in a reasonable amount of memory.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2h467p","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;500B+ MoE with hybrid architecture would be something great to try for sure! In the meantime, I run mostly IQ4_K_M quant of R1 0528, it has 671B so it is not too heavy - 355 GB, and 90GB of VRAM is enough to hold 100K context along with four full layers and common expert tensors.&lt;/p&gt;\\n\\n&lt;p&gt;One thing I hope for in the future models is higher context length and even better context efficiency that maybe Mamba could bring, so higher context cache would still fits in a reasonable amount of memory.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lwsrx7","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwsrx7/support_for_the_upcoming_ibm_granite_40_has_been/n2h467p/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752198766,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2j1c8n","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"vibjelo","can_mod_post":false,"created_utc":1752231837,"send_replies":true,"parent_id":"t1_n2gtimt","score":2,"author_fullname":"t2_hr2hgnsk","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"&gt; tiny models that fly under everyone's radar\\n\\nThere's a reason for that ;) They're tiny enough to not actually be good at anything, and for the classes they're targeting, there is a ton of competition. Personally, I haven't found much use for basically anything for models smaller than ~20B, the accuracy/quality is just too poor to be useful, and since they're only doing &lt;8B models, kind of makes sense they're being kind of ignored.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2j1c8n","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;tiny models that fly under everyone&amp;#39;s radar&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;There&amp;#39;s a reason for that ;) They&amp;#39;re tiny enough to not actually be good at anything, and for the classes they&amp;#39;re targeting, there is a ton of competition. Personally, I haven&amp;#39;t found much use for basically anything for models smaller than ~20B, the accuracy/quality is just too poor to be useful, and since they&amp;#39;re only doing &amp;lt;8B models, kind of makes sense they&amp;#39;re being kind of ignored.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lwsrx7","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwsrx7/support_for_the_upcoming_ibm_granite_40_has_been/n2j1c8n/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752231837,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n2gtimt","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"triynizzles1","can_mod_post":false,"created_utc":1752195060,"send_replies":true,"parent_id":"t3_1lwsrx7","score":29,"author_fullname":"t2_zr0g49ixt","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"IBM has been making a bunch of tiny models that fly under everyone's radar. Every model brings new advancement, modalities, technology and discoveries for their team. One day, they will release a big 500b+ model and leap frog every frontier model out of nowhere.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2gtimt","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;IBM has been making a bunch of tiny models that fly under everyone&amp;#39;s radar. Every model brings new advancement, modalities, technology and discoveries for their team. One day, they will release a big 500b+ model and leap frog every frontier model out of nowhere.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwsrx7/support_for_the_upcoming_ibm_granite_40_has_been/n2gtimt/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752195060,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lwsrx7","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":29}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2i63ag","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"ttkciar","can_mod_post":false,"created_utc":1752214808,"send_replies":true,"parent_id":"t1_n2hg7wn","score":7,"author_fullname":"t2_cpegz","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"When I evaluated Granite-3.1-8B (dense), it hit above its weight at summarization, formal prose analysis, psychology, RAG, self-critique, and function-calling.\\n\\nPretty useless for everything else, at least things for which I tested it.\\n\\nRaw output of my tests (inferred five replies per prompt, 42 prompts): http://ciar.org/h/test.1735587950.gr318.txt","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2i63ag","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;When I evaluated Granite-3.1-8B (dense), it hit above its weight at summarization, formal prose analysis, psychology, RAG, self-critique, and function-calling.&lt;/p&gt;\\n\\n&lt;p&gt;Pretty useless for everything else, at least things for which I tested it.&lt;/p&gt;\\n\\n&lt;p&gt;Raw output of my tests (inferred five replies per prompt, 42 prompts): &lt;a href=\\"http://ciar.org/h/test.1735587950.gr318.txt\\"&gt;http://ciar.org/h/test.1735587950.gr318.txt&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lwsrx7","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwsrx7/support_for_the_upcoming_ibm_granite_40_has_been/n2i63ag/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752214808,"author_flair_text":"llama.cpp","treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":7}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2ilj4x","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"stoppableDissolution","can_mod_post":false,"created_utc":1752223428,"send_replies":true,"parent_id":"t1_n2hg7wn","score":1,"author_fullname":"t2_1n0su21k4z","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"3 has some very good extraction/summarization capabilities. It got insanely big proportion of attention to mlp (32 heads/8 kv for 2b model)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2ilj4x","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;3 has some very good extraction/summarization capabilities. It got insanely big proportion of attention to mlp (32 heads/8 kv for 2b model)&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lwsrx7","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwsrx7/support_for_the_upcoming_ibm_granite_40_has_been/n2ilj4x/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752223428,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2hg7wn","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"condition_oakland","can_mod_post":false,"created_utc":1752203164,"send_replies":true,"parent_id":"t3_1lwsrx7","score":2,"author_fullname":"t2_aoclrow","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"What use cases are granite models known for again?  was it tool usage / agents?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2hg7wn","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;What use cases are granite models known for again?  was it tool usage / agents?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwsrx7/support_for_the_upcoming_ibm_granite_40_has_been/n2hg7wn/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752203164,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lwsrx7","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ef488598-491f-11ef-a847-9a3dd315819c","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2gqhat","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MichaelXie4645","can_mod_post":false,"created_utc":1752193993,"send_replies":true,"parent_id":"t3_1lwsrx7","score":2,"author_fullname":"t2_a06q0mmx","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Why are those not vlms","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2gqhat","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"Llama 405B"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Why are those not vlms&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwsrx7/support_for_the_upcoming_ibm_granite_40_has_been/n2gqhat/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752193993,"author_flair_text":"Llama 405B","treatment_tags":[],"link_id":"t3_1lwsrx7","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2irt6q","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"JLeonsarmiento","can_mod_post":false,"created_utc":1752227033,"send_replies":true,"parent_id":"t3_1lwsrx7","score":1,"author_fullname":"t2_9b9s4a7g","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Oh yes.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2irt6q","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Oh yes.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwsrx7/support_for_the_upcoming_ibm_granite_40_has_been/n2irt6q/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752227033,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lwsrx7","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"50c36eba-fdca-11ee-9735-92a88d7e3b87","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2j1gdw","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"vibjelo","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2hz03q","score":1,"author_fullname":"t2_hr2hgnsk","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"When you're a LLM, anything is possible!","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2j1gdw","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;When you&amp;#39;re a LLM, anything is possible!&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lwsrx7","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwsrx7/support_for_the_upcoming_ibm_granite_40_has_been/n2j1gdw/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752231891,"author_flair_text":null,"treatment_tags":[],"created_utc":1752231891,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2hz03q","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"MidAirRunner","can_mod_post":false,"created_utc":1752211234,"send_replies":true,"parent_id":"t1_n2hsn3b","score":6,"author_fullname":"t2_qwhykwm6l","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Might need to wait for the actual release first lol.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2hz03q","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"Ollama"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Might need to wait for the actual release first lol.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lwsrx7","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwsrx7/support_for_the_upcoming_ibm_granite_40_has_been/n2hz03q/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752211234,"author_flair_text":"Ollama","treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}}],"before":null}},"user_reports":[],"saved":false,"id":"n2hsn3b","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"PaceZealousideal6091","can_mod_post":false,"created_utc":1752208295,"send_replies":true,"parent_id":"t3_1lwsrx7","score":1,"author_fullname":"t2_bpkdm1tk","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Cool. These would be fantastic for Lightweight metadata processing in RAG applications. Waiting for r/unsloth ggufs! u/yoracale u/danielhanchen gguf when? ðŸ˜¬ðŸ™ƒ","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2hsn3b","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Cool. These would be fantastic for Lightweight metadata processing in RAG applications. Waiting for &lt;a href=\\"/r/unsloth\\"&gt;r/unsloth&lt;/a&gt; ggufs! &lt;a href=\\"/u/yoracale\\"&gt;u/yoracale&lt;/a&gt; &lt;a href=\\"/u/danielhanchen\\"&gt;u/danielhanchen&lt;/a&gt; gguf when? ðŸ˜¬ðŸ™ƒ&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwsrx7/support_for_the_upcoming_ibm_granite_40_has_been/n2hsn3b/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752208295,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lwsrx7","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),n=()=>e.jsx(l,{data:t});export{n as default};
