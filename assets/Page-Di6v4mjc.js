import{j as e}from"./index-Cd3v0jxz.js";import{R as l}from"./RedditPostRenderer-cI96YLhy.js";import"./index-DGBoKyQm.js";const t=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Hello everyone,\\n\\nI have just installed Ollama with Llama3:8b, i make prompts via the backend of my website with ajax requests.\\n\\nI have a list of 10000 french words \\"maison, femme, cuisine...\\" and i would like to translate them into 30 other languages, and get declensions (\\"la cuisine, les cuisine, une cuisine...\\") and definitions of these words.\\n\\nI am having a hard time to get what I want, most of the time because llama gives an incorrect translation, and incorrect declension or even gives the word in an incorrect language. Sometimes it give the exact response, as expected, but when i execute the same prompt again I have totally different results.\\n\\nI spent almost 1 week now tweaking the parameters of the prompt, and as a beginner with AI, at this point I am wondering if llama3:8b is the proper tools to achieve my goals  \\n  \\nWould you advise me another tool maybe? Is there a trick to have correct responses with consistency?\\n\\nDo you have other advice for the beginner I am please?\\n\\nAlso, I would like to buy a laptop dedicated to AI, do you think 128GB RAM is enough?","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"beginner with llama3, I cannot get results I want","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1m7cklb","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.5,"author_flair_background_color":null,"subreddit_type":"public","ups":0,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_bwf9p4oxc","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":0,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1753285105,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Hello everyone,&lt;/p&gt;\\n\\n&lt;p&gt;I have just installed Ollama with Llama3:8b, i make prompts via the backend of my website with ajax requests.&lt;/p&gt;\\n\\n&lt;p&gt;I have a list of 10000 french words &amp;quot;maison, femme, cuisine...&amp;quot; and i would like to translate them into 30 other languages, and get declensions (&amp;quot;la cuisine, les cuisine, une cuisine...&amp;quot;) and definitions of these words.&lt;/p&gt;\\n\\n&lt;p&gt;I am having a hard time to get what I want, most of the time because llama gives an incorrect translation, and incorrect declension or even gives the word in an incorrect language. Sometimes it give the exact response, as expected, but when i execute the same prompt again I have totally different results.&lt;/p&gt;\\n\\n&lt;p&gt;I spent almost 1 week now tweaking the parameters of the prompt, and as a beginner with AI, at this point I am wondering if llama3:8b is the proper tools to achieve my goals  &lt;/p&gt;\\n\\n&lt;p&gt;Would you advise me another tool maybe? Is there a trick to have correct responses with consistency?&lt;/p&gt;\\n\\n&lt;p&gt;Do you have other advice for the beginner I am please?&lt;/p&gt;\\n\\n&lt;p&gt;Also, I would like to buy a laptop dedicated to AI, do you think 128GB RAM is enough?&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":true,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1m7cklb","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"FckGAFA","discussion_type":null,"num_comments":17,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1m7cklb/beginner_with_llama3_i_cannot_get_results_i_want/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1m7cklb/beginner_with_llama3_i_cannot_get_results_i_want/","subreddit_subscribers":503758,"created_utc":1753285105,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4rw5gt","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"triynizzles1","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4rdgma","score":2,"author_fullname":"t2_zr0g49ixt","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Try granite 3.3 or qwen 3. Both come in 8 billion parameter models and are good with translation.\\n\\nAs I was thinking about this more, it could be a language barrier issue: some words do not directly translate from one language to another. You might need full sentences so that the AI model can understand the context of the sentence and then convert it to another language.","edited":1753301905,"author_flair_css_class":null,"name":"t1_n4rw5gt","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Try granite 3.3 or qwen 3. Both come in 8 billion parameter models and are good with translation.&lt;/p&gt;\\n\\n&lt;p&gt;As I was thinking about this more, it could be a language barrier issue: some words do not directly translate from one language to another. You might need full sentences so that the AI model can understand the context of the sentence and then convert it to another language.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1m7cklb","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7cklb/beginner_with_llama3_i_cannot_get_results_i_want/n4rw5gt/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753300901,"author_flair_text":null,"collapsed":false,"created_utc":1753300901,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n4rdgma","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"FckGAFA","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4qti3s","score":1,"author_fullname":"t2_bwf9p4oxc","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"i cannot run gemma3:27b , just installed it and i need 20G more RAM\\n\\nMaybe is there a free API for my needs? Or maybe I should subscribe to a paid service, but I saw costs are huge for 10000 words and their declensions","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4rdgma","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;i cannot run gemma3:27b , just installed it and i need 20G more RAM&lt;/p&gt;\\n\\n&lt;p&gt;Maybe is there a free API for my needs? Or maybe I should subscribe to a paid service, but I saw costs are huge for 10000 words and their declensions&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m7cklb","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7cklb/beginner_with_llama3_i_cannot_get_results_i_want/n4rdgma/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753295626,"author_flair_text":null,"treatment_tags":[],"created_utc":1753295626,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4qti3s","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"triynizzles1","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4qn0kd","score":2,"author_fullname":"t2_zr0g49ixt","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"We might be confused by what it is you are doing or what the payload sent to ollama looks like and how it iterates for each word.\\n\\nI would definitely start by trying a different model. Llama 3 8b is great for conversational flow, but it lacks raw intelligence and instruction following. Gemma 3 12b, phi4 and mistral small (if you can run it) are all significantly more capable.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n4qti3s","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;We might be confused by what it is you are doing or what the payload sent to ollama looks like and how it iterates for each word.&lt;/p&gt;\\n\\n&lt;p&gt;I would definitely start by trying a different model. Llama 3 8b is great for conversational flow, but it lacks raw intelligence and instruction following. Gemma 3 12b, phi4 and mistral small (if you can run it) are all significantly more capable.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m7cklb","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7cklb/beginner_with_llama3_i_cannot_get_results_i_want/n4qti3s/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753290215,"author_flair_text":null,"treatment_tags":[],"created_utc":1753290215,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n4qn0kd","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"FckGAFA","can_mod_post":false,"created_utc":1753288450,"send_replies":true,"parent_id":"t1_n4qcgt1","score":1,"author_fullname":"t2_bwf9p4oxc","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"i just send one word at a time, when i click on the \\"populate\\" button, it populates the corresponding input fields for translations in all the different languages","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4qn0kd","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;i just send one word at a time, when i click on the &amp;quot;populate&amp;quot; button, it populates the corresponding input fields for translations in all the different languages&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m7cklb","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7cklb/beginner_with_llama3_i_cannot_get_results_i_want/n4qn0kd/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753288450,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4qcgt1","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"triynizzles1","can_mod_post":false,"created_utc":1753285515,"send_replies":true,"parent_id":"t3_1m7cklb","score":3,"author_fullname":"t2_zr0g49ixt","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Ollama default 4096 as the context window. Try setting it higher.\\n\\nTry out a few different models if there is a difference. Gemma 3 is very good at handling multiple languages.\\n\\nYou could also try changing your script if you are giving it a block of text 10,000 words long and expecting 10,000 words out but in a different language, you likely will not get it. Maybe mistral small could do this but generally, AI’s are trained for long text in - summary out. Break up your list into smaller digestible chunks maybe 300 tokens long and then send it to the AI for translation.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4qcgt1","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Ollama default 4096 as the context window. Try setting it higher.&lt;/p&gt;\\n\\n&lt;p&gt;Try out a few different models if there is a difference. Gemma 3 is very good at handling multiple languages.&lt;/p&gt;\\n\\n&lt;p&gt;You could also try changing your script if you are giving it a block of text 10,000 words long and expecting 10,000 words out but in a different language, you likely will not get it. Maybe mistral small could do this but generally, AI’s are trained for long text in - summary out. Break up your list into smaller digestible chunks maybe 300 tokens long and then send it to the AI for translation.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7cklb/beginner_with_llama3_i_cannot_get_results_i_want/n4qcgt1/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753285515,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m7cklb","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4qpbe5","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"FckGAFA","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4qfsar","score":1,"author_fullname":"t2_bwf9p4oxc","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"i am not sure at all, i installed llama3:8b and made my first prompts straight out of the box with php","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4qpbe5","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;i am not sure at all, i installed llama3:8b and made my first prompts straight out of the box with php&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m7cklb","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7cklb/beginner_with_llama3_i_cannot_get_results_i_want/n4qpbe5/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753289089,"author_flair_text":null,"treatment_tags":[],"created_utc":1753289089,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4qfsar","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"NNN_Throwaway2","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4qf46o","score":2,"author_fullname":"t2_8rrihts9","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"How do you know the backend isn't problematic? I mean, yes, you need a bigger model for something like this, but are you sure you have this one configured properly?","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n4qfsar","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;How do you know the backend isn&amp;#39;t problematic? I mean, yes, you need a bigger model for something like this, but are you sure you have this one configured properly?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m7cklb","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7cklb/beginner_with_llama3_i_cannot_get_results_i_want/n4qfsar/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753286425,"author_flair_text":null,"treatment_tags":[],"created_utc":1753286425,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n4qf46o","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"FckGAFA","can_mod_post":false,"created_utc":1753286241,"send_replies":true,"parent_id":"t1_n4qe6k4","score":1,"author_fullname":"t2_bwf9p4oxc","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"well, i am beginner with AI but I have good skills in IT, the backend interface is not problematic, it's only the results i get are incorrect and inconsistent","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4qf46o","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;well, i am beginner with AI but I have good skills in IT, the backend interface is not problematic, it&amp;#39;s only the results i get are incorrect and inconsistent&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m7cklb","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7cklb/beginner_with_llama3_i_cannot_get_results_i_want/n4qf46o/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753286241,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4qe6k4","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"NNN_Throwaway2","can_mod_post":false,"created_utc":1753285985,"send_replies":true,"parent_id":"t3_1m7cklb","score":2,"author_fullname":"t2_8rrihts9","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"If you're a beginner, why not use a more beginner friendly inference backend?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4qe6k4","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;If you&amp;#39;re a beginner, why not use a more beginner friendly inference backend?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7cklb/beginner_with_llama3_i_cannot_get_results_i_want/n4qe6k4/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753285985,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m7cklb","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4qimbf","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Marksta","can_mod_post":false,"created_utc":1753287223,"send_replies":true,"parent_id":"t3_1m7cklb","score":3,"author_fullname":"t2_559a1","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"You need to use structured output to do this, any other way and you're going to get some \\"Sure, here is the translation: Banana.\\" stuff going on. Use Gemma3 27B.\\n\\n&gt;Also, I would like to buy a laptop dedicated to AI, do you think 128GB RAM is enough?\\n\\nProbably not? Check into other posts with people talking about hardware. Laptop RAM is really going to let you down if you're looking to run good models. Or maybe laptops just in general will let you down, is probably the better answer.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4qimbf","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You need to use structured output to do this, any other way and you&amp;#39;re going to get some &amp;quot;Sure, here is the translation: Banana.&amp;quot; stuff going on. Use Gemma3 27B.&lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;Also, I would like to buy a laptop dedicated to AI, do you think 128GB RAM is enough?&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;Probably not? Check into other posts with people talking about hardware. Laptop RAM is really going to let you down if you&amp;#39;re looking to run good models. Or maybe laptops just in general will let you down, is probably the better answer.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7cklb/beginner_with_llama3_i_cannot_get_results_i_want/n4qimbf/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753287223,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m7cklb","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4reym1","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"FckGAFA","can_mod_post":false,"created_utc":1753296043,"send_replies":true,"parent_id":"t1_n4rcowk","score":1,"author_fullname":"t2_bwf9p4oxc","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Thank you! gonna give a try at Gemini-2.5-Flash-Lite!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4reym1","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thank you! gonna give a try at Gemini-2.5-Flash-Lite!&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m7cklb","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7cklb/beginner_with_llama3_i_cannot_get_results_i_want/n4reym1/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753296043,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4rcowk","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Mysterious_Finish543","can_mod_post":false,"created_utc":1753295411,"send_replies":true,"parent_id":"t3_1m7cklb","score":2,"author_fullname":"t2_gbx2bcdvl","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Llama 3 is quite an old model by now (almost 1 year), so I would suggest using some newer models. I would suggest switching to a newer model like Gemma3-12B-it or Qwen3-8B. Use Qwen3 in particular if some of the 30 languages you'd like to translate into are obscure, multilingual capability was a main focus of the release.\\n\\nIn addition, as other commenters have suggested, it would be a good idea to use structured output to constrain generation, instead of trying to over-engineer your prompt.\\n\\nThat being said, although this is r/LocalLLaMA, it doesn't look like you're dealing with confidential or private information, and you don't seem to be having too much fun with the process, so perhaps you should just use a remote model like Google's \`Gemini-2.5-Flash-Lite\`. This would likely deliver better results at minimal cost.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4rcowk","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Llama 3 is quite an old model by now (almost 1 year), so I would suggest using some newer models. I would suggest switching to a newer model like Gemma3-12B-it or Qwen3-8B. Use Qwen3 in particular if some of the 30 languages you&amp;#39;d like to translate into are obscure, multilingual capability was a main focus of the release.&lt;/p&gt;\\n\\n&lt;p&gt;In addition, as other commenters have suggested, it would be a good idea to use structured output to constrain generation, instead of trying to over-engineer your prompt.&lt;/p&gt;\\n\\n&lt;p&gt;That being said, although this is &lt;a href=\\"/r/LocalLLaMA\\"&gt;r/LocalLLaMA&lt;/a&gt;, it doesn&amp;#39;t look like you&amp;#39;re dealing with confidential or private information, and you don&amp;#39;t seem to be having too much fun with the process, so perhaps you should just use a remote model like Google&amp;#39;s &lt;code&gt;Gemini-2.5-Flash-Lite&lt;/code&gt;. This would likely deliver better results at minimal cost.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7cklb/beginner_with_llama3_i_cannot_get_results_i_want/n4rcowk/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753295411,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m7cklb","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4rikad","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"phree_radical","can_mod_post":false,"created_utc":1753297043,"send_replies":true,"parent_id":"t3_1m7cklb","score":2,"author_fullname":"t2_44nkp","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"&gt; Sometimes it give the exact response, as expected, but when i execute the same prompt again I have totally different results\\n\\nSet temperature = 0, topk = 1","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4rikad","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;Sometimes it give the exact response, as expected, but when i execute the same prompt again I have totally different results&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;Set temperature = 0, topk = 1&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7cklb/beginner_with_llama3_i_cannot_get_results_i_want/n4rikad/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753297043,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m7cklb","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4qfb1v","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"FckGAFA","can_mod_post":false,"created_utc":1753286294,"send_replies":true,"parent_id":"t1_n4qcbvt","score":2,"author_fullname":"t2_bwf9p4oxc","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"i have 10000 words in 30 languages, with all the declensions and variations I would be at 0.001% today","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4qfb1v","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;i have 10000 words in 30 languages, with all the declensions and variations I would be at 0.001% today&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m7cklb","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7cklb/beginner_with_llama3_i_cannot_get_results_i_want/n4qfb1v/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753286294,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n4qcbvt","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"TallComputerDude","can_mod_post":false,"created_utc":1753285478,"send_replies":true,"parent_id":"t3_1m7cklb","score":1,"author_fullname":"t2_1nxmlzxw","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"In all the time you've already spent working on this, could you have completed it already by using your brain?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4qcbvt","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;In all the time you&amp;#39;ve already spent working on this, could you have completed it already by using your brain?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7cklb/beginner_with_llama3_i_cannot_get_results_i_want/n4qcbvt/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753285478,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m7cklb","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4qncyb","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"FckGAFA","can_mod_post":false,"created_utc":1753288546,"send_replies":true,"parent_id":"t1_n4qg3xv","score":2,"author_fullname":"t2_bwf9p4oxc","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Hi, i just send one word at a time, when i click on the \\"populate\\" button, it populates the corresponding input fields with the translations and declensions in all the different languages\\n\\nTo populate the next work, I have to go to the next word page and click populate again\\n\\nIt's the admin backend of my website","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4qncyb","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Hi, i just send one word at a time, when i click on the &amp;quot;populate&amp;quot; button, it populates the corresponding input fields with the translations and declensions in all the different languages&lt;/p&gt;\\n\\n&lt;p&gt;To populate the next work, I have to go to the next word page and click populate again&lt;/p&gt;\\n\\n&lt;p&gt;It&amp;#39;s the admin backend of my website&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m7cklb","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7cklb/beginner_with_llama3_i_cannot_get_results_i_want/n4qncyb/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753288546,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n4qg3xv","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Hanthunius","can_mod_post":false,"created_utc":1753286516,"send_replies":true,"parent_id":"t3_1m7cklb","score":1,"author_fullname":"t2_d2gb9jhgg","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Are you doing it language by language or bulk translating? Break it down to the smallest task first.  \\nI use Gemma3 27B for translations like yours and it does a great job. A laptop with 128GB would be plenty for this kind of work.\\n\\n  \\nedit: also, you need to be REALLY specific with your prompt. It's easy to create prompts with lexical and syntactic ambiguity without even noticing. Don't shy away from being redundant and repeating things in different ways to make sure it gets it.","edited":1753286746,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4qg3xv","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Are you doing it language by language or bulk translating? Break it down to the smallest task first.&lt;br/&gt;\\nI use Gemma3 27B for translations like yours and it does a great job. A laptop with 128GB would be plenty for this kind of work.&lt;/p&gt;\\n\\n&lt;p&gt;edit: also, you need to be REALLY specific with your prompt. It&amp;#39;s easy to create prompts with lexical and syntactic ambiguity without even noticing. Don&amp;#39;t shy away from being redundant and repeating things in different ways to make sure it gets it.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7cklb/beginner_with_llama3_i_cannot_get_results_i_want/n4qg3xv/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753286516,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m7cklb","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),r=()=>e.jsx(l,{data:t});export{r as default};
