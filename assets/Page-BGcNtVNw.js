import{j as t}from"./index-Cd3v0jxz.js";import{R as e}from"./RedditPostRenderer-cI96YLhy.js";import"./index-DGBoKyQm.js";const a=JSON.parse('[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Hi everyone,\\n\\nI\'m a software engineer, but still relatively new to this field.  \\nI’m currently working on a project that extracts data from invoices using structured outputs and a local LLM chat with documents. Everything was working fine with **Gemma 2**, but when I upgraded to **Gemma 3**, things broke.\\n\\n---\\n\\n### Here\'s my setup for structured output:\\n\\n```python\\nclient = instructor.from_openai(\\n    OpenAI(\\n        base_url=\\"http://localhost:11434/v1\\",\\n        api_key=\\"ollama\\",\\n    ),\\n    mode=instructor.Mode.JSON,\\n)\\n```\\n\\nAnd I was using a model like this:\\n\\n```python\\nclass invoiceDetails(BaseModel):\\n    VAT: Optional[float]\\n    adress: Optional[str]\\n```\\n```python\\nresponse = client.chat.completions.create(\\n            model=\\"gemma3:latest\\",\\n            messages=[\\n                {\\"role\\": \\"system\\", \\"content\\": system_prompt},\\n                {\\"role\\": \\"user\\", \\"content\\": full_prompt}],\\n            response_model=invoiceDetails,\\n        )\\n```\\nDespite marking the fields as **Optional**, I\'m now getting this error after upgrading:\\n\\n```\\nraise InstructorRetryException(\\ninstructor.exceptions.InstructorRetryException: RetryError[&lt;Future at 0x7f43c8769790 state=finished raised ValidationError&gt;]\\npydantic_core._pydantic_core.ValidationError: 10 validation errors for invoiceDetails\\nTVA\\n  Field required [type=missing, input_value={}, input_type=dict]\\n  For further information visit https://errors.pydantic.dev/2.11/v/missing\\nadress\\n  Field required...\\n```\\n\\nThis is very confusing to me, because:\\n- The model response does include the required fields.\\n- The fields are marked Optional, so I expected them to bypass strict validation.\\n- It all worked **perfectly with Gemma 2** and i got the JSon answer i expected.\\n\\n---\\n\\nI’ve been stuck for days now \\n\\nIf anyone has encountered this or has experience with `instructor`, `pydantic v2`, and `Ollama`, I’d really appreciate any help.  \\nI also have a few other bugs I’d love to troubleshoot if someone has some time.  \\n**I’m even willing to pay for your time if needed.**\\n\\nI know I may not be super advanced technically, but I’m really trying and learning as I go   \\nThanks so much in advance!\\n","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Structured Output Broken After Upgrade from Gemma2 to Gemma3","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1m87mfd","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.67,"author_flair_background_color":null,"subreddit_type":"public","ups":1,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_mfrg6unhp","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":1,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1753371278,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;m a software engineer, but still relatively new to this field.&lt;br/&gt;\\nI’m currently working on a project that extracts data from invoices using structured outputs and a local LLM chat with documents. Everything was working fine with &lt;strong&gt;Gemma 2&lt;/strong&gt;, but when I upgraded to &lt;strong&gt;Gemma 3&lt;/strong&gt;, things broke.&lt;/p&gt;\\n\\n&lt;hr/&gt;\\n\\n&lt;h3&gt;Here&amp;#39;s my setup for structured output:&lt;/h3&gt;\\n\\n&lt;p&gt;&lt;code&gt;python\\nclient = instructor.from_openai(\\n    OpenAI(\\n        base_url=&amp;quot;http://localhost:11434/v1&amp;quot;,\\n        api_key=&amp;quot;ollama&amp;quot;,\\n    ),\\n    mode=instructor.Mode.JSON,\\n)\\n&lt;/code&gt;&lt;/p&gt;\\n\\n&lt;p&gt;And I was using a model like this:&lt;/p&gt;\\n\\n&lt;p&gt;&lt;code&gt;python\\nclass invoiceDetails(BaseModel):\\n    VAT: Optional[float]\\n    adress: Optional[str]\\n&lt;/code&gt;\\n&lt;code&gt;python\\nresponse = client.chat.completions.create(\\n            model=&amp;quot;gemma3:latest&amp;quot;,\\n            messages=[\\n                {&amp;quot;role&amp;quot;: &amp;quot;system&amp;quot;, &amp;quot;content&amp;quot;: system_prompt},\\n                {&amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, &amp;quot;content&amp;quot;: full_prompt}],\\n            response_model=invoiceDetails,\\n        )\\n&lt;/code&gt;\\nDespite marking the fields as &lt;strong&gt;Optional&lt;/strong&gt;, I&amp;#39;m now getting this error after upgrading:&lt;/p&gt;\\n\\n&lt;p&gt;&lt;code&gt;\\nraise InstructorRetryException(\\ninstructor.exceptions.InstructorRetryException: RetryError[&amp;lt;Future at 0x7f43c8769790 state=finished raised ValidationError&amp;gt;]\\npydantic_core._pydantic_core.ValidationError: 10 validation errors for invoiceDetails\\nTVA\\n  Field required [type=missing, input_value={}, input_type=dict]\\n  For further information visit https://errors.pydantic.dev/2.11/v/missing\\nadress\\n  Field required...\\n&lt;/code&gt;&lt;/p&gt;\\n\\n&lt;p&gt;This is very confusing to me, because:\\n- The model response does include the required fields.\\n- The fields are marked Optional, so I expected them to bypass strict validation.\\n- It all worked &lt;strong&gt;perfectly with Gemma 2&lt;/strong&gt; and i got the JSon answer i expected.&lt;/p&gt;\\n\\n&lt;hr/&gt;\\n\\n&lt;p&gt;I’ve been stuck for days now &lt;/p&gt;\\n\\n&lt;p&gt;If anyone has encountered this or has experience with &lt;code&gt;instructor&lt;/code&gt;, &lt;code&gt;pydantic v2&lt;/code&gt;, and &lt;code&gt;Ollama&lt;/code&gt;, I’d really appreciate any help.&lt;br/&gt;\\nI also have a few other bugs I’d love to troubleshoot if someone has some time.&lt;br/&gt;\\n&lt;strong&gt;I’m even willing to pay for your time if needed.&lt;/strong&gt;&lt;/p&gt;\\n\\n&lt;p&gt;I know I may not be super advanced technically, but I’m really trying and learning as I go&lt;br/&gt;\\nThanks so much in advance!&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":true,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1m87mfd","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"Suppersonic00","discussion_type":null,"num_comments":3,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1m87mfd/structured_output_broken_after_upgrade_from/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1m87mfd/structured_output_broken_after_upgrade_from/","subreddit_subscribers":504023,"created_utc":1753371278,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4xdlhl","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Awwtifishal","can_mod_post":false,"created_utc":1753374775,"send_replies":true,"parent_id":"t3_1m87mfd","score":2,"author_fullname":"t2_1d96a8k10t","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I recommend using llama.cpp instead of ollama because it has support for grammars. You can either make a grammar yourself, or much easier, you can supply a JSON schema, and it will follow it exactly.\\n\\nSearch [here](https://github.com/ggml-org/llama.cpp/blob/master/grammars/README.md) for `json_schema`","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4xdlhl","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I recommend using llama.cpp instead of ollama because it has support for grammars. You can either make a grammar yourself, or much easier, you can supply a JSON schema, and it will follow it exactly.&lt;/p&gt;\\n\\n&lt;p&gt;Search &lt;a href=\\"https://github.com/ggml-org/llama.cpp/blob/master/grammars/README.md\\"&gt;here&lt;/a&gt; for &lt;code&gt;json_schema&lt;/code&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m87mfd/structured_output_broken_after_upgrade_from/n4xdlhl/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753374775,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m87mfd","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4x9ewp","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Ok-Replacement5068","can_mod_post":false,"created_utc":1753373624,"send_replies":true,"parent_id":"t3_1m87mfd","score":1,"author_fullname":"t2_1jc48v81jp","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Hey, ran into a very similar class of problem recently after upgrading models. Super frustrating when a working pipeline breaks. You\'re 99% of the way there, the issue is almost certainly not in your Pydantic model.\\n\\n**The Diagnosis:**\\n\\nThe ValidationError is a symptom. The root cause is that Gemma 3\'s default output is likely \\"dirtier\\" than Gemma 2\'s. It\'s probably wrapping the JSON in conversational text or a markdown block like \\\\`\\\\`\\\\`json ... \\\\`\\\\`\\\\`.\\n\\nWhen the instructor library fails to parse this, it passes an empty dictionary {} to Pydantic, which then correctly flags all the fields as missing.\\n\\n**The Fix (Find the real output first):**\\n\\nBefore you change anything else, you have to see what the model is actually sending back. Bypass instructor for one call and print the raw response.\\n\\n    from openai import OpenAI\\n    \\n    # Use the base client for one raw request\\n    client_raw = OpenAI(base_url=\\"http://localhost:11434/v1\\", api_key=\\"ollama\\")\\n    \\n    response_raw = client_raw.chat.completions.create(\\n        model=\\"gemma3:latest\\",\\n        messages=[{\\"role\\": \\"system\\", \\"content\\": system_prompt}, {\\"role\\": \\"user\\", \\"content\\": full_prompt}],\\n    )\\n    \\n    # See the raw truth:\\n    print(response_raw.choices[0].message.content)\\n\\nMy bet is the output isn\'t clean JSON. Once you see the actual format, the best fix is usually aggressive prompt engineering.\\n\\n**Try adding this to your system prompt:**  \\n\\"IMPORTANT: Your response must be ONLY the raw JSON object, without any markdown formatting, comments, or other text.\\"\\n\\nThis usually forces the model to behave. If it\'s still being stubborn, you might need to do a quick regex clean-up on the string before passing it to instructor, but the prompt fix works 9 times out of 10.\\n\\nHope this helps!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4x9ewp","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Hey, ran into a very similar class of problem recently after upgrading models. Super frustrating when a working pipeline breaks. You&amp;#39;re 99% of the way there, the issue is almost certainly not in your Pydantic model.&lt;/p&gt;\\n\\n&lt;p&gt;&lt;strong&gt;The Diagnosis:&lt;/strong&gt;&lt;/p&gt;\\n\\n&lt;p&gt;The ValidationError is a symptom. The root cause is that Gemma 3&amp;#39;s default output is likely &amp;quot;dirtier&amp;quot; than Gemma 2&amp;#39;s. It&amp;#39;s probably wrapping the JSON in conversational text or a markdown block like ```json ... ```.&lt;/p&gt;\\n\\n&lt;p&gt;When the instructor library fails to parse this, it passes an empty dictionary {} to Pydantic, which then correctly flags all the fields as missing.&lt;/p&gt;\\n\\n&lt;p&gt;&lt;strong&gt;The Fix (Find the real output first):&lt;/strong&gt;&lt;/p&gt;\\n\\n&lt;p&gt;Before you change anything else, you have to see what the model is actually sending back. Bypass instructor for one call and print the raw response.&lt;/p&gt;\\n\\n&lt;pre&gt;&lt;code&gt;from openai import OpenAI\\n\\n# Use the base client for one raw request\\nclient_raw = OpenAI(base_url=&amp;quot;http://localhost:11434/v1&amp;quot;, api_key=&amp;quot;ollama&amp;quot;)\\n\\nresponse_raw = client_raw.chat.completions.create(\\n    model=&amp;quot;gemma3:latest&amp;quot;,\\n    messages=[{&amp;quot;role&amp;quot;: &amp;quot;system&amp;quot;, &amp;quot;content&amp;quot;: system_prompt}, {&amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, &amp;quot;content&amp;quot;: full_prompt}],\\n)\\n\\n# See the raw truth:\\nprint(response_raw.choices[0].message.content)\\n&lt;/code&gt;&lt;/pre&gt;\\n\\n&lt;p&gt;My bet is the output isn&amp;#39;t clean JSON. Once you see the actual format, the best fix is usually aggressive prompt engineering.&lt;/p&gt;\\n\\n&lt;p&gt;&lt;strong&gt;Try adding this to your system prompt:&lt;/strong&gt;&lt;br/&gt;\\n&amp;quot;IMPORTANT: Your response must be ONLY the raw JSON object, without any markdown formatting, comments, or other text.&amp;quot;&lt;/p&gt;\\n\\n&lt;p&gt;This usually forces the model to behave. If it&amp;#39;s still being stubborn, you might need to do a quick regex clean-up on the string before passing it to instructor, but the prompt fix works 9 times out of 10.&lt;/p&gt;\\n\\n&lt;p&gt;Hope this helps!&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m87mfd/structured_output_broken_after_upgrade_from/n4x9ewp/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753373624,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m87mfd","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4ypa35","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Willing_Landscape_61","can_mod_post":false,"created_utc":1753387979,"send_replies":true,"parent_id":"t3_1m87mfd","score":1,"author_fullname":"t2_8lvrytgw","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Structured output should be done with specific tools like outlines.\\n\\n\\nhttps://github.com/dottxt-ai/outlines","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4ypa35","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Structured output should be done with specific tools like outlines.&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://github.com/dottxt-ai/outlines\\"&gt;https://github.com/dottxt-ai/outlines&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m87mfd/structured_output_broken_after_upgrade_from/n4ypa35/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753387979,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m87mfd","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]'),r=()=>t.jsx(e,{data:a});export{r as default};
