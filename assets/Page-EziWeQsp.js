import{j as e}from"./index-BpC9hjVs.js";import{R as t}from"./RedditPostRenderer-BEo6AnSR.js";import"./index-DwkJHX1_.js";const l=JSON.parse('[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Okay, so I have comfyui, works fine with flux, wan and etc. Now I have LM studio and anythingllm. I check the models and there are so many:\\n1) llama\\n2) gemma\\n3) qwen\\n4) hermes\\n5) mistral\\n6).......\\n\\nAnd the different versions, different Q, different parameter set size and etc.\\n\\nI understand quantization levels and parameter set size (b). But is there somewhere an organized place to check which model does something best? I think one could be better at programming, other at emotional support, other at analyzing stuff, but how to know which one? Now I have 200Gb of AI models  and each of them feel very different.\\n\\nMy main purpose is to feed pdfs so it could do QnA based on pdf knowledge. I found hermes 3 llama quite good at this, gemma 3 also, others - not so much.\\n\\nIs there a tier list or something to dive deeper into different model capabilities?\\n\\nPc: 5800x3d, 4070super 12gb vram, 32gb ram.\\n\\nAlso side question: how to increase max avalaible document count and size when trying to do RAG on lmstudio or anythingllm?\\n\\nI tried installing privateGPT, but thats pain in the ass to make it work on gpu. Also it doesnt even work on cpu for me (my skill issue for sure). \\n\\nChatRTX works fast, but problem - no multilanguage support.","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Newbie questions in this world","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1ltm1mp","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":1,"author_flair_background_color":null,"subreddit_type":"public","ups":1,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_2ggbff2e","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":1,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1751866101,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Okay, so I have comfyui, works fine with flux, wan and etc. Now I have LM studio and anythingllm. I check the models and there are so many:\\n1) llama\\n2) gemma\\n3) qwen\\n4) hermes\\n5) mistral\\n6).......&lt;/p&gt;\\n\\n&lt;p&gt;And the different versions, different Q, different parameter set size and etc.&lt;/p&gt;\\n\\n&lt;p&gt;I understand quantization levels and parameter set size (b). But is there somewhere an organized place to check which model does something best? I think one could be better at programming, other at emotional support, other at analyzing stuff, but how to know which one? Now I have 200Gb of AI models  and each of them feel very different.&lt;/p&gt;\\n\\n&lt;p&gt;My main purpose is to feed pdfs so it could do QnA based on pdf knowledge. I found hermes 3 llama quite good at this, gemma 3 also, others - not so much.&lt;/p&gt;\\n\\n&lt;p&gt;Is there a tier list or something to dive deeper into different model capabilities?&lt;/p&gt;\\n\\n&lt;p&gt;Pc: 5800x3d, 4070super 12gb vram, 32gb ram.&lt;/p&gt;\\n\\n&lt;p&gt;Also side question: how to increase max avalaible document count and size when trying to do RAG on lmstudio or anythingllm?&lt;/p&gt;\\n\\n&lt;p&gt;I tried installing privateGPT, but thats pain in the ass to make it work on gpu. Also it doesnt even work on cpu for me (my skill issue for sure). &lt;/p&gt;\\n\\n&lt;p&gt;ChatRTX works fast, but problem - no multilanguage support.&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":true,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1ltm1mp","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"Lxxtsch","discussion_type":null,"num_comments":1,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1ltm1mp/newbie_questions_in_this_world/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1ltm1mp/newbie_questions_in_this_world/","subreddit_subscribers":496034,"created_utc":1751866101,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1ruiy3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AppearanceHeavy6724","can_mod_post":false,"created_utc":1751875425,"send_replies":true,"parent_id":"t3_1ltm1mp","score":2,"author_fullname":"t2_uz37qfx5","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"the benchmarks are scattered - eqbench.com, aider leaderboard, benchmarks by Lech Mazur etc. No centralized registry, unfortunately.\\n\\nEDIT:  4070super 12gb vram is not enough. Add either 3060 if have pocket money or p104-100 if on very tight budget.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1ruiy3","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;the benchmarks are scattered - eqbench.com, aider leaderboard, benchmarks by Lech Mazur etc. No centralized registry, unfortunately.&lt;/p&gt;\\n\\n&lt;p&gt;EDIT:  4070super 12gb vram is not enough. Add either 3060 if have pocket money or p104-100 if on very tight budget.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltm1mp/newbie_questions_in_this_world/n1ruiy3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751875425,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1ltm1mp","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}}]'),r=()=>e.jsx(t,{data:l});export{r as default};
