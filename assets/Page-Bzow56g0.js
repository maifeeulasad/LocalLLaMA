import{j as e}from"./index-DDI5xNtT.js";import{R as l}from"./RedditPostRenderer-D2w0CxE4.js";import"./index-CZ5j6e3h.js";const t=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"If I have several Linux machines with DDR5 ram, 2x3090 on one machine, and a MacBook too does ktransformers or something else allow me to utilize the ram across all the machines for larger context and model sizes? Has anyone done this?","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Anyone used RAM across multiple networked devices?","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lmxhd7","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.38,"author_flair_background_color":null,"subreddit_type":"public","ups":0,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_98un7mie","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":0,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1751145032,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;If I have several Linux machines with DDR5 ram, 2x3090 on one machine, and a MacBook too does ktransformers or something else allow me to utilize the ram across all the machines for larger context and model sizes? Has anyone done this?&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":true,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1lmxhd7","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"bobbiesbottleservice","discussion_type":null,"num_comments":7,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lmxhd7/anyone_used_ram_across_multiple_networked_devices/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lmxhd7/anyone_used_ram_across_multiple_networked_devices/","subreddit_subscribers":492840,"created_utc":1751145032,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0d132z","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Marksta","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0c488f","score":1,"author_fullname":"t2_559a1","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I've used it for GPUs, it works well for combining memory capacity. I've only run it over 1Gb/s networking but the token/s hit is significant in that case. Like, 25 tps on one machine then split the same model and drop to 10 tps. Not so sure how it'd go on CPU only. Or if the config is really supported actually. Give it a little test and see if you already have the computers ready.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0d132z","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;ve used it for GPUs, it works well for combining memory capacity. I&amp;#39;ve only run it over 1Gb/s networking but the token/s hit is significant in that case. Like, 25 tps on one machine then split the same model and drop to 10 tps. Not so sure how it&amp;#39;d go on CPU only. Or if the config is really supported actually. Give it a little test and see if you already have the computers ready.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lmxhd7","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmxhd7/anyone_used_ram_across_multiple_networked_devices/n0d132z/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751174066,"author_flair_text":null,"treatment_tags":[],"created_utc":1751174066,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0c488f","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MatterMean5176","can_mod_post":false,"created_utc":1751159856,"send_replies":true,"parent_id":"t1_n0b1na9","score":1,"author_fullname":"t2_1ju039btvf","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Have you run a model in this manner on CPUs across multiple machines? If so, how did it go?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0c488f","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Have you run a model in this manner on CPUs across multiple machines? If so, how did it go?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lmxhd7","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmxhd7/anyone_used_ram_across_multiple_networked_devices/n0c488f/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751159856,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0b1na9","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Marksta","can_mod_post":false,"created_utc":1751145796,"send_replies":true,"parent_id":"t3_1lmxhd7","score":5,"author_fullname":"t2_559a1","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Llama.cpp RPC is the only solution I know of for CPU inferencing across computers. Check out GPUStack if you want to give it a spin, it packages up llama.cpp RPC in a nice package and orchestrator web server + webgui to manage deploying the models and systems.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0b1na9","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Llama.cpp RPC is the only solution I know of for CPU inferencing across computers. Check out GPUStack if you want to give it a spin, it packages up llama.cpp RPC in a nice package and orchestrator web server + webgui to manage deploying the models and systems.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmxhd7/anyone_used_ram_across_multiple_networked_devices/n0b1na9/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751145796,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lmxhd7","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0bijxv","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"kmouratidis","can_mod_post":false,"created_utc":1751151780,"send_replies":true,"parent_id":"t3_1lmxhd7","score":3,"author_fullname":"t2_k6u7rfxb","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"[petals](https://github.com/bigscience-workshop/petals) can do CPU and GPU over multiple machines. Not sure it supports all possible configurations though. \\n\\nvLLM can run GPU-only and CPU-only, and I know it can run distributed across machines but I don't know if the distributed deployment requires GPU-only or if it can work with different systems.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0bijxv","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;&lt;a href=\\"https://github.com/bigscience-workshop/petals\\"&gt;petals&lt;/a&gt; can do CPU and GPU over multiple machines. Not sure it supports all possible configurations though. &lt;/p&gt;\\n\\n&lt;p&gt;vLLM can run GPU-only and CPU-only, and I know it can run distributed across machines but I don&amp;#39;t know if the distributed deployment requires GPU-only or if it can work with different systems.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmxhd7/anyone_used_ram_across_multiple_networked_devices/n0bijxv/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751151780,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lmxhd7","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0b17yj","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"HypnoDaddy4You","can_mod_post":false,"created_utc":1751145654,"send_replies":true,"parent_id":"t3_1lmxhd7","score":2,"author_fullname":"t2_lb2n7mbsw","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"NVlink is the only networking technology fast enough for it to make a difference. And that's from card to card.\\n\\nFor your setup the best use would be to pick a model that runs on that card and use a load balancer so you can have multiple requests in flight at once.\\n\\nOf course, this is for API use and not interactive, and your application will need to be built to use multiple requests at once...","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0b17yj","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;NVlink is the only networking technology fast enough for it to make a difference. And that&amp;#39;s from card to card.&lt;/p&gt;\\n\\n&lt;p&gt;For your setup the best use would be to pick a model that runs on that card and use a load balancer so you can have multiple requests in flight at once.&lt;/p&gt;\\n\\n&lt;p&gt;Of course, this is for API use and not interactive, and your application will need to be built to use multiple requests at once...&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmxhd7/anyone_used_ram_across_multiple_networked_devices/n0b17yj/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751145654,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lmxhd7","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0b0tcs","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"wadrasil","can_mod_post":false,"created_utc":1751145518,"send_replies":true,"parent_id":"t3_1lmxhd7","score":2,"author_fullname":"t2_6pk6k","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Not going to work because of network speed. On device speeds are multiplies of Gbps;  while most networks are 1 Gpbs. \\n\\nYou can use them as nodes and interact with them sequentially.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0b0tcs","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Not going to work because of network speed. On device speeds are multiplies of Gbps;  while most networks are 1 Gpbs. &lt;/p&gt;\\n\\n&lt;p&gt;You can use them as nodes and interact with them sequentially.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmxhd7/anyone_used_ram_across_multiple_networked_devices/n0b0tcs/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751145518,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lmxhd7","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0ek37c","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"chisleu","can_mod_post":false,"created_utc":1751203468,"send_replies":true,"parent_id":"t3_1lmxhd7","score":1,"author_fullname":"t2_cbxyn","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Oh have I got something awesome for you.\\n\\nAuto discovery. Just start the app and it finds all the other apps running on your local network and auto clusters.\\n\\nAccess any of the endpoints that are running and you get a simple chat interface with model loading. It distributes the model downloading so only parts of the model download to each machine!\\n\\nThen you can use the open ai compatible API or the web interface to chat.\\n\\nhttps://github.com/exo-explore/exo\\n\\nGPU or CPU sharding across everything from RPis to H200s. It's freaking awesome technology.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0ek37c","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Oh have I got something awesome for you.&lt;/p&gt;\\n\\n&lt;p&gt;Auto discovery. Just start the app and it finds all the other apps running on your local network and auto clusters.&lt;/p&gt;\\n\\n&lt;p&gt;Access any of the endpoints that are running and you get a simple chat interface with model loading. It distributes the model downloading so only parts of the model download to each machine!&lt;/p&gt;\\n\\n&lt;p&gt;Then you can use the open ai compatible API or the web interface to chat.&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://github.com/exo-explore/exo\\"&gt;https://github.com/exo-explore/exo&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;GPU or CPU sharding across everything from RPis to H200s. It&amp;#39;s freaking awesome technology.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmxhd7/anyone_used_ram_across_multiple_networked_devices/n0ek37c/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751203468,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lmxhd7","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),s=()=>e.jsx(l,{data:t});export{s as default};
