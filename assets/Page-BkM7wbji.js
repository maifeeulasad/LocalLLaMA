import{j as e}from"./index-BgwOAK4-.js";import{R as t}from"./RedditPostRenderer-BOBjDTFu.js";import"./index-BL22wVg5.js";const a=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"I got an aws activate promo of $1000. I started crunching numbers and decided to train an LLM model.\\n\\n  \\nThe concept a 1.5B model, LLama3 architecture,  with  differential Attention, GaLore , GQA, MoD, and Sink Tokens,. Trained 100% on public domain ( common corpus dataset).  Doing the math I'maiming for 45B tokens, a little over the chinchilla wall. I plan on opensourcing everything.  All training will be done on g5 large single gpu spot instances.\\n\\n  \\n  The stupidest part of the plan, is I don't know python very well. Gemini, Claude, and CHatgpt will write and vet the entire codebase.\\n\\n  \\n  WIsh me luck, or make fun of me. I'm going to do something cool, or waste $1000 in sagemaker credits.\\n\\n  \\nHappy to answer any questions.","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Attempting to train a model from scratch for less than $1000","link_flair_richtext":[{"e":"text","t":"Discussion"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lmbtvg","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.73,"author_flair_background_color":null,"subreddit_type":"public","ups":5,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_i5os0v0","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Discussion","can_mod_post":false,"score":5,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1751077401,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;I got an aws activate promo of $1000. I started crunching numbers and decided to train an LLM model.&lt;/p&gt;\\n\\n&lt;p&gt;The concept a 1.5B model, LLama3 architecture,  with  differential Attention, GaLore , GQA, MoD, and Sink Tokens,. Trained 100% on public domain ( common corpus dataset).  Doing the math I&amp;#39;maiming for 45B tokens, a little over the chinchilla wall. I plan on opensourcing everything.  All training will be done on g5 large single gpu spot instances.&lt;/p&gt;\\n\\n&lt;p&gt;The stupidest part of the plan, is I don&amp;#39;t know python very well. Gemini, Claude, and CHatgpt will write and vet the entire codebase.&lt;/p&gt;\\n\\n&lt;p&gt;WIsh me luck, or make fun of me. I&amp;#39;m going to do something cool, or waste $1000 in sagemaker credits.&lt;/p&gt;\\n\\n&lt;p&gt;Happy to answer any questions.&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"5f921ea4-c7bc-11ed-9c23-3a00622979b4","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#646d73","id":"1lmbtvg","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"thebadslime","discussion_type":null,"num_comments":7,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lmbtvg/attempting_to_train_a_model_from_scratch_for_less/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lmbtvg/attempting_to_train_a_model_from_scratch_for_less/","subreddit_subscribers":492627,"created_utc":1751077401,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n07z58g","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"thecuriousrealbully","can_mod_post":false,"created_utc":1751107646,"send_replies":true,"parent_id":"t3_1lmbtvg","score":5,"author_fullname":"t2_14qm6jnfam","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Do you think that new Gemma 3N architecture would be better for quality as well as performance?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n07z58g","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Do you think that new Gemma 3N architecture would be better for quality as well as performance?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmbtvg/attempting_to_train_a_model_from_scratch_for_less/n07z58g/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751107646,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lmbtvg","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0cjxxl","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"thebadslime","can_mod_post":false,"created_utc":1751166241,"send_replies":true,"parent_id":"t1_n0am1iv","score":1,"author_fullname":"t2_i5os0v0","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Fineweb is scraped web content, I want to use data with a clear public domain provenance.  I think doing it clean is probably the most important goal of the project.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0cjxxl","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Fineweb is scraped web content, I want to use data with a clear public domain provenance.  I think doing it clean is probably the most important goal of the project.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lmbtvg","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmbtvg/attempting_to_train_a_model_from_scratch_for_less/n0cjxxl/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751166241,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0am1iv","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Double_Cause4609","can_mod_post":false,"created_utc":1751140722,"send_replies":true,"parent_id":"t3_1lmbtvg","score":3,"author_fullname":"t2_1kubzxt2ww","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"The Keller Jordan GPT-2 Speedrun basically figured out most of the major efficiency improvements for you.\\n\\nIf you're willing to take some inspiration from their single-file implementations I think they could be adapted for a 1.5B model. I'm not sure of the exact cost, in the sense that while I'd expect it to take maybe 2 hours on an 8xH100 node, I don't know the AWS cost of that hardware off the top of my head. I think 1 H100 should be around $6-12 per hour, typically, so maybe it could be done in two hours, for around $200? Doing it on fewer GPUs will probably result in around the same total training cost, I think.\\n\\nDo note: GaLore is cool (I'd personally take ApolloW of the low rank gradient optimizers, though, but I digress), but Muon is also great, and the only reason IMO to do GaLore is if you were planning to implement Q-Galore (or Q-Apollo) and run on a really cheap single GPU. As soon as you're training in the cloud, though, it's not immediately clear that you gain a lot by fitting the model into such a small GPU, as batching gives you huge efficiency gains in total cost. I'm not saying it's a bad idea, I'm just noting it's not immediately clear that you're actually gaining an advantage.\\n\\nI also think you could replace their Attention improvements with MLA (instead of GQA) which is fairly well documented at this point (lots of people have implemented from scratch) and it performs well on top of being simple in code.\\n\\nIn terms of data, the common corpus is noble as a goal, but fineweb 2 is just significantly better and you'd probably be able to train in 1-10B tokens instead of 40B and probably get similar quality. You may be able to look at the report on fineweb 2 (and perhaps cosmopedia 2), and figure out some ways of generating high quality synthetic data or aggressive filtering to cut down the common corpus size quite a bit.\\n\\nDo note: Chinchilla scaling laws didn't take into account data quality. As you scale in data quality, it makes more sense to spend more compute on the model size than on more tokens.\\n\\nI would highly recommend checking out the Olmo implementations (AllenAI have a core stack that implements all of their training code). It's pretty idiomatic Python and gives you an idea of the syntax to use. Andrej Karpathy's GPT-2 reproduction video is also a great source of stylistic guidelines.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0am1iv","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The Keller Jordan GPT-2 Speedrun basically figured out most of the major efficiency improvements for you.&lt;/p&gt;\\n\\n&lt;p&gt;If you&amp;#39;re willing to take some inspiration from their single-file implementations I think they could be adapted for a 1.5B model. I&amp;#39;m not sure of the exact cost, in the sense that while I&amp;#39;d expect it to take maybe 2 hours on an 8xH100 node, I don&amp;#39;t know the AWS cost of that hardware off the top of my head. I think 1 H100 should be around $6-12 per hour, typically, so maybe it could be done in two hours, for around $200? Doing it on fewer GPUs will probably result in around the same total training cost, I think.&lt;/p&gt;\\n\\n&lt;p&gt;Do note: GaLore is cool (I&amp;#39;d personally take ApolloW of the low rank gradient optimizers, though, but I digress), but Muon is also great, and the only reason IMO to do GaLore is if you were planning to implement Q-Galore (or Q-Apollo) and run on a really cheap single GPU. As soon as you&amp;#39;re training in the cloud, though, it&amp;#39;s not immediately clear that you gain a lot by fitting the model into such a small GPU, as batching gives you huge efficiency gains in total cost. I&amp;#39;m not saying it&amp;#39;s a bad idea, I&amp;#39;m just noting it&amp;#39;s not immediately clear that you&amp;#39;re actually gaining an advantage.&lt;/p&gt;\\n\\n&lt;p&gt;I also think you could replace their Attention improvements with MLA (instead of GQA) which is fairly well documented at this point (lots of people have implemented from scratch) and it performs well on top of being simple in code.&lt;/p&gt;\\n\\n&lt;p&gt;In terms of data, the common corpus is noble as a goal, but fineweb 2 is just significantly better and you&amp;#39;d probably be able to train in 1-10B tokens instead of 40B and probably get similar quality. You may be able to look at the report on fineweb 2 (and perhaps cosmopedia 2), and figure out some ways of generating high quality synthetic data or aggressive filtering to cut down the common corpus size quite a bit.&lt;/p&gt;\\n\\n&lt;p&gt;Do note: Chinchilla scaling laws didn&amp;#39;t take into account data quality. As you scale in data quality, it makes more sense to spend more compute on the model size than on more tokens.&lt;/p&gt;\\n\\n&lt;p&gt;I would highly recommend checking out the Olmo implementations (AllenAI have a core stack that implements all of their training code). It&amp;#39;s pretty idiomatic Python and gives you an idea of the syntax to use. Andrej Karpathy&amp;#39;s GPT-2 reproduction video is also a great source of stylistic guidelines.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmbtvg/attempting_to_train_a_model_from_scratch_for_less/n0am1iv/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751140722,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lmbtvg","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0ajz4c","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"No_Afternoon_4260","can_mod_post":false,"created_utc":1751140047,"send_replies":true,"parent_id":"t3_1lmbtvg","score":2,"author_fullname":"t2_cj9kap4bx","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Good luck! Great poject I'm sure you'll learn a lot!!\\nAre you planning to write your own pytorch code? Because a lot of code base can be found for that, even if it's not exactly what you are aiming for","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0ajz4c","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Good luck! Great poject I&amp;#39;m sure you&amp;#39;ll learn a lot!!\\nAre you planning to write your own pytorch code? Because a lot of code base can be found for that, even if it&amp;#39;s not exactly what you are aiming for&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmbtvg/attempting_to_train_a_model_from_scratch_for_less/n0ajz4c/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751140047,"author_flair_text":"llama.cpp","treatment_tags":[],"link_id":"t3_1lmbtvg","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0c5a9y","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"thebadslime","can_mod_post":false,"created_utc":1751160275,"send_replies":true,"parent_id":"t1_n0c22l6","score":1,"author_fullname":"t2_i5os0v0","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I want to build something new, and I just don't have the skillset for it yet.  I am learning a ton watching them do it TBH","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0c5a9y","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I want to build something new, and I just don&amp;#39;t have the skillset for it yet.  I am learning a ton watching them do it TBH&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lmbtvg","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmbtvg/attempting_to_train_a_model_from_scratch_for_less/n0c5a9y/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751160275,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0c22l6","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"bick_nyers","can_mod_post":false,"created_utc":1751159014,"send_replies":true,"parent_id":"t3_1lmbtvg","score":1,"author_fullname":"t2_6nwld4d3","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I'll offer an alternative path for learning.\\n\\n\\nDo 2 things:\\n\\n\\n1. Karpathy's LLM videos to learn how to build one from scratch\\n\\n\\n2. Take an off the shelf model with an off the shelf training tool (I recommend axolotl) with premade datasets that interest you on HuggingFace and learn the art and science of fine-tuning LLMs.\\n\\n\\nThen you will have all the skills necessary to build something from scratch without relying on Gemini etc. to write the code for you.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0c22l6","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;ll offer an alternative path for learning.&lt;/p&gt;\\n\\n&lt;p&gt;Do 2 things:&lt;/p&gt;\\n\\n&lt;ol&gt;\\n&lt;li&gt;&lt;p&gt;Karpathy&amp;#39;s LLM videos to learn how to build one from scratch&lt;/p&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;p&gt;Take an off the shelf model with an off the shelf training tool (I recommend axolotl) with premade datasets that interest you on HuggingFace and learn the art and science of fine-tuning LLMs.&lt;/p&gt;&lt;/li&gt;\\n&lt;/ol&gt;\\n\\n&lt;p&gt;Then you will have all the skills necessary to build something from scratch without relying on Gemini etc. to write the code for you.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmbtvg/attempting_to_train_a_model_from_scratch_for_less/n0c22l6/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751159014,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lmbtvg","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),r=()=>e.jsx(t,{data:a});export{r as default};
