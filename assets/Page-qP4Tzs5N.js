import{j as e}from"./index-DQXiEb7D.js";import{R as t}from"./RedditPostRenderer-BjndLgq8.js";import"./index-B-ILyjT1.js";const l=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Hi, I’m running a local LLM setup on my Mac Studio (M1 Max, 64GB RAM) using Ollama with the Gemma 3 27B Q4_0 model.\\n\\nOverall, the model is running well and the quality of responses has been great, but I keep running into an issue where the model randomly outputs stop sequence tokens like &lt;/end_of_turn&gt; or &lt;end_of_turn&gt; in its replies, even though I explicitly told it not to in my system prompt.\\n\\nSometimes it even starts simulating the next user message back to itself and gets caught in this weird loop where it keeps writing both sides of the conversation.\\n\\nThings I’ve tried:\\n\\nAdding to the system prompt: “Please DO NOT use any control tokens such as &lt;start_of_turn&gt;, &lt;/end_of_turn&gt;, or simulate user messages.”\\n\\nStarting fresh chats.\\n\\nTweaking other system prompt instructions to clarify roles.\\n\\nContext:\\n\\nI’m using Open WebUI as the frontend.\\n\\nI’ve tried specifying the stop sequences in ollama and in open webui.\\n\\nI’ve seen this issue both in longer chats and in fairly short ones.\\n\\nI’ve also seen similar behavior when asking the model to summarize chats for memory purposes.\\n\\nQuestions:\\n\\nHas anyone else experienced this with Gemma 3 27B Q4_0, or with other models on Ollama?\\n\\nAre there known workarounds? Maybe a better phrasing for the system prompt to prevent this\\n\\nCould this be a model-specific issue, or something about how Ollama handles stop sequences?\\n\\nAny insights, similar experiences, or debugging tips would be super appreciated!","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"gemma3 keeps outputting stop tokens and simulating user responses (using Ollama + Gemma 3 27B Q4_0 + open webui)","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lp7nek","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.5,"author_flair_background_color":null,"subreddit_type":"public","ups":0,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_hket8q","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":0,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1751390543,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Hi, I’m running a local LLM setup on my Mac Studio (M1 Max, 64GB RAM) using Ollama with the Gemma 3 27B Q4_0 model.&lt;/p&gt;\\n\\n&lt;p&gt;Overall, the model is running well and the quality of responses has been great, but I keep running into an issue where the model randomly outputs stop sequence tokens like &amp;lt;/end_of_turn&amp;gt; or &amp;lt;end_of_turn&amp;gt; in its replies, even though I explicitly told it not to in my system prompt.&lt;/p&gt;\\n\\n&lt;p&gt;Sometimes it even starts simulating the next user message back to itself and gets caught in this weird loop where it keeps writing both sides of the conversation.&lt;/p&gt;\\n\\n&lt;p&gt;Things I’ve tried:&lt;/p&gt;\\n\\n&lt;p&gt;Adding to the system prompt: “Please DO NOT use any control tokens such as &amp;lt;start_of_turn&amp;gt;, &amp;lt;/end_of_turn&amp;gt;, or simulate user messages.”&lt;/p&gt;\\n\\n&lt;p&gt;Starting fresh chats.&lt;/p&gt;\\n\\n&lt;p&gt;Tweaking other system prompt instructions to clarify roles.&lt;/p&gt;\\n\\n&lt;p&gt;Context:&lt;/p&gt;\\n\\n&lt;p&gt;I’m using Open WebUI as the frontend.&lt;/p&gt;\\n\\n&lt;p&gt;I’ve tried specifying the stop sequences in ollama and in open webui.&lt;/p&gt;\\n\\n&lt;p&gt;I’ve seen this issue both in longer chats and in fairly short ones.&lt;/p&gt;\\n\\n&lt;p&gt;I’ve also seen similar behavior when asking the model to summarize chats for memory purposes.&lt;/p&gt;\\n\\n&lt;p&gt;Questions:&lt;/p&gt;\\n\\n&lt;p&gt;Has anyone else experienced this with Gemma 3 27B Q4_0, or with other models on Ollama?&lt;/p&gt;\\n\\n&lt;p&gt;Are there known workarounds? Maybe a better phrasing for the system prompt to prevent this&lt;/p&gt;\\n\\n&lt;p&gt;Could this be a model-specific issue, or something about how Ollama handles stop sequences?&lt;/p&gt;\\n\\n&lt;p&gt;Any insights, similar experiences, or debugging tips would be super appreciated!&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":true,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1lp7nek","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"thisisntmethisisme","discussion_type":null,"num_comments":11,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lp7nek/gemma3_keeps_outputting_stop_tokens_and/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lp7nek/gemma3_keeps_outputting_stop_tokens_and/","subreddit_subscribers":493457,"created_utc":1751390543,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0w2mt7","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ttkciar","can_mod_post":false,"created_utc":1751431965,"send_replies":true,"parent_id":"t1_n0szwbj","score":2,"author_fullname":"t2_cpegz","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yep, this.\\n\\nThose problems are *very* typical of badly formatted prompts.  Make sure you are using the right template for the model, and make sure you are using a template at all.\\n\\nPrompting the model with no framing when it expects prompts to be framed will exhibit exactly that kind of behavior.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0w2mt7","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yep, this.&lt;/p&gt;\\n\\n&lt;p&gt;Those problems are &lt;em&gt;very&lt;/em&gt; typical of badly formatted prompts.  Make sure you are using the right template for the model, and make sure you are using a template at all.&lt;/p&gt;\\n\\n&lt;p&gt;Prompting the model with no framing when it expects prompts to be framed will exhibit exactly that kind of behavior.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lp7nek","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp7nek/gemma3_keeps_outputting_stop_tokens_and/n0w2mt7/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751431965,"author_flair_text":"llama.cpp","treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n0szwbj","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"NNN_Throwaway2","can_mod_post":false,"created_utc":1751394907,"send_replies":true,"parent_id":"t3_1lp7nek","score":4,"author_fullname":"t2_8rrihts9","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Check your chat template.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0szwbj","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Check your chat template.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp7nek/gemma3_keeps_outputting_stop_tokens_and/n0szwbj/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751394907,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lp7nek","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0sztsa","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AppearanceHeavy6724","can_mod_post":false,"created_utc":1751394886,"send_replies":true,"parent_id":"t3_1lp7nek","score":4,"author_fullname":"t2_uz37qfx5","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"1. Context too short. Default for Ollama is 2k, too small. 8k is minimum for normal work.\\n\\n2. Sigh. Wrong chat template.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0sztsa","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;ol&gt;\\n&lt;li&gt;&lt;p&gt;Context too short. Default for Ollama is 2k, too small. 8k is minimum for normal work.&lt;/p&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;p&gt;Sigh. Wrong chat template.&lt;/p&gt;&lt;/li&gt;\\n&lt;/ol&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp7nek/gemma3_keeps_outputting_stop_tokens_and/n0sztsa/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751394886,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lp7nek","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0t3k5u","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Total_Activity_7550","can_mod_post":false,"created_utc":1751395937,"send_replies":true,"parent_id":"t3_1lp7nek","score":3,"author_fullname":"t2_nwfj64go","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Use llama.cpp . I use with Gemma 3 27B QAT without issue.\\nOllama is just a wrapper which reduces number of install commands at cost of breaking things randomly.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0t3k5u","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Use llama.cpp . I use with Gemma 3 27B QAT without issue.\\nOllama is just a wrapper which reduces number of install commands at cost of breaking things randomly.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp7nek/gemma3_keeps_outputting_stop_tokens_and/n0t3k5u/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751395937,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lp7nek","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0vz1sg","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"phree_radical","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0ukyy8","score":1,"author_fullname":"t2_44nkp","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"right, but they are never trained to \\"don't output these tags\\"\\n\\nthe chatbot \\"layer\\" as you say, has no \\"knowledge\\" of them (current day)","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0vz1sg","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;right, but they are never trained to &amp;quot;don&amp;#39;t output these tags&amp;quot;&lt;/p&gt;\\n\\n&lt;p&gt;the chatbot &amp;quot;layer&amp;quot; as you say, has no &amp;quot;knowledge&amp;quot; of them (current day)&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lp7nek","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp7nek/gemma3_keeps_outputting_stop_tokens_and/n0vz1sg/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751430305,"author_flair_text":null,"treatment_tags":[],"created_utc":1751430305,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0ukyy8","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Koksny","can_mod_post":false,"created_utc":1751411974,"send_replies":true,"parent_id":"t1_n0ug0ae","score":1,"author_fullname":"t2_olk3n","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"&gt;Tokens used for the chat format are not something you would expect the model to have \\"knowledge\\" of.\\n\\nWhat do you mean? Isn't essentially there a whole 'fine-tune layer' with chat formatting and tokens, just to make the model 'behave' like a chat?\\n\\nI'm fairly sure the models are trained on datasets that contain for example \\"&lt;|start\\\\_of\\\\_turn|&gt; Assistant: Answer. &lt;|start\\\\_of\\\\_turn|&gt; User: \\", people even include different model family templates in fine tunes, to make models more 'compatible'.\\n\\nIf the models weren't trained on tags, i think we wouldn't have to do the template dance with every model family, including the odds like Mistral Tekken.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0ukyy8","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;Tokens used for the chat format are not something you would expect the model to have &amp;quot;knowledge&amp;quot; of.&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;What do you mean? Isn&amp;#39;t essentially there a whole &amp;#39;fine-tune layer&amp;#39; with chat formatting and tokens, just to make the model &amp;#39;behave&amp;#39; like a chat?&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;m fairly sure the models are trained on datasets that contain for example &amp;quot;&amp;lt;|start_of_turn|&amp;gt; Assistant: Answer. &amp;lt;|start_of_turn|&amp;gt; User: &amp;quot;, people even include different model family templates in fine tunes, to make models more &amp;#39;compatible&amp;#39;.&lt;/p&gt;\\n\\n&lt;p&gt;If the models weren&amp;#39;t trained on tags, i think we wouldn&amp;#39;t have to do the template dance with every model family, including the odds like Mistral Tekken.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lp7nek","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp7nek/gemma3_keeps_outputting_stop_tokens_and/n0ukyy8/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751411974,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0ug0ae","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"phree_radical","can_mod_post":false,"created_utc":1751410329,"send_replies":true,"parent_id":"t3_1lp7nek","score":1,"author_fullname":"t2_44nkp","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"These appear to be the correct formatting markers https://ai.google.dev/gemma/docs/core/prompt-structure\\n\\nTokens used for the chat format are not something you would expect the model to have \\"knowledge\\" of.  Further, if you wish to use the model for \\"chat\\" as intended, you or your software must handle those tokens","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0ug0ae","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;These appear to be the correct formatting markers &lt;a href=\\"https://ai.google.dev/gemma/docs/core/prompt-structure\\"&gt;https://ai.google.dev/gemma/docs/core/prompt-structure&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;Tokens used for the chat format are not something you would expect the model to have &amp;quot;knowledge&amp;quot; of.  Further, if you wish to use the model for &amp;quot;chat&amp;quot; as intended, you or your software must handle those tokens&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp7nek/gemma3_keeps_outputting_stop_tokens_and/n0ug0ae/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751410329,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lp7nek","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0t02gs","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Entubulated","can_mod_post":false,"created_utc":1751394955,"send_replies":true,"parent_id":"t3_1lp7nek","score":0,"author_fullname":"t2_1opxde6hyq","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Maybe not the model, but ollama's definition of what is a stop token for the model?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0t02gs","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Maybe not the model, but ollama&amp;#39;s definition of what is a stop token for the model?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp7nek/gemma3_keeps_outputting_stop_tokens_and/n0t02gs/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751394955,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lp7nek","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0souek","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MindOrbits","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0snkkt","score":0,"author_fullname":"t2_v9v8eio1","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I suspect this is a knock on effect of the 'Thinking' stuff. I switched from Ollama to llama.cpp server, if both backends have the same stop sequence token behavior then it could be the model. If not you have your answer and solution.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0souek","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I suspect this is a knock on effect of the &amp;#39;Thinking&amp;#39; stuff. I switched from Ollama to llama.cpp server, if both backends have the same stop sequence token behavior then it could be the model. If not you have your answer and solution.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lp7nek","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp7nek/gemma3_keeps_outputting_stop_tokens_and/n0souek/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751391841,"author_flair_text":null,"treatment_tags":[],"created_utc":1751391841,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}},"user_reports":[],"saved":false,"id":"n0snkkt","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"thisisntmethisisme","can_mod_post":false,"created_utc":1751391499,"send_replies":true,"parent_id":"t1_n0smi41","score":0,"author_fullname":"t2_hket8q","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"it is really interesting, especially bc I use it specifically as a sound board or “supplemental therapy”, so its simulated user responses are sometimes really insightful for me lmao like it’s putting my thoughts into clearer words than I ever could","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0snkkt","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;it is really interesting, especially bc I use it specifically as a sound board or “supplemental therapy”, so its simulated user responses are sometimes really insightful for me lmao like it’s putting my thoughts into clearer words than I ever could&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lp7nek","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp7nek/gemma3_keeps_outputting_stop_tokens_and/n0snkkt/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751391499,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}},"user_reports":[],"saved":false,"id":"n0smi41","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MindOrbits","can_mod_post":false,"created_utc":1751391212,"send_replies":true,"parent_id":"t3_1lp7nek","score":-4,"author_fullname":"t2_v9v8eio1","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Not a fix for you or advice, but I do find it interesting. It's almost like an emergent Ego talking to itself.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0smi41","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Not a fix for you or advice, but I do find it interesting. It&amp;#39;s almost like an emergent Ego talking to itself.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp7nek/gemma3_keeps_outputting_stop_tokens_and/n0smi41/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751391212,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lp7nek","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-4}}],"before":null}}]`),s=()=>e.jsx(t,{data:l});export{s as default};
