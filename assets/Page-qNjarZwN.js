import{j as e}from"./index-Bu7qcPAU.js";import{R as l}from"./RedditPostRenderer-CbHA7O5q.js";import"./index-BKgbfxhf.js";const t=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"I'd like to run models locally (at my workplaces) and also refine models, and fortunately I'm not paying!  I plan to get a Mac Studio with 80 core GPU and 256GB RAM. Is there any strong case that I'm missing for going with 512GB RAM?","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Strong case for a 512GB Mac Studio?","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1m5uu0t","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.67,"author_flair_background_color":null,"subreddit_type":"public","ups":3,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_sk7nmjrs","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":3,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1753131529,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;d like to run models locally (at my workplaces) and also refine models, and fortunately I&amp;#39;m not paying!  I plan to get a Mac Studio with 80 core GPU and 256GB RAM. Is there any strong case that I&amp;#39;m missing for going with 512GB RAM?&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1m5uu0t","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"ChevChance","discussion_type":null,"num_comments":12,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1m5uu0t/strong_case_for_a_512gb_mac_studio/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1m5uu0t/strong_case_for_a_512gb_mac_studio/","subreddit_subscribers":502515,"created_utc":1753131529,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4ffylp","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"ParaboloidalCrest","can_mod_post":false,"created_utc":1753138951,"send_replies":true,"parent_id":"t3_1m5uu0t","score":6,"author_fullname":"t2_nc2u4f7","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Is that a question? Not paying = the biggest you could get.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4ffylp","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Is that a question? Not paying = the biggest you could get.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5uu0t/strong_case_for_a_512gb_mac_studio/n4ffylp/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753138951,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m5uu0t","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4fjdex","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ChevChance","can_mod_post":false,"created_utc":1753140068,"send_replies":true,"parent_id":"t1_n4f81yl","score":1,"author_fullname":"t2_sk7nmjrs","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I'd heard that the larger models like DeepSeek and Kimi ran slow on the studio.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4fjdex","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;d heard that the larger models like DeepSeek and Kimi ran slow on the studio.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m5uu0t","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5uu0t/strong_case_for_a_512gb_mac_studio/n4fjdex/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753140068,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4f81yl","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Baldur-Norddahl","can_mod_post":false,"created_utc":1753136368,"send_replies":true,"parent_id":"t3_1m5uu0t","score":3,"author_fullname":"t2_bvqb8ng0","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"If you are not paying, there are zero reasons for not going with 512GB! :-)\\n\\n512 GB enables running the serious models such as DeepSeek R1 and Kimi K2 (at 4 bit quantization). It is actually a big deal.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4f81yl","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;If you are not paying, there are zero reasons for not going with 512GB! :-)&lt;/p&gt;\\n\\n&lt;p&gt;512 GB enables running the serious models such as DeepSeek R1 and Kimi K2 (at 4 bit quantization). It is actually a big deal.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5uu0t/strong_case_for_a_512gb_mac_studio/n4f81yl/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753136368,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m5uu0t","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4fbrrj","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"HumanAppointment5","can_mod_post":false,"created_utc":1753137587,"send_replies":true,"parent_id":"t3_1m5uu0t","score":3,"author_fullname":"t2_1tt3gtupry","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Currently you will miss out completely on DeepSeek V3 and R1, plus Kimi K2. DeepSeek V3 is really good for translations. R1 for coding.\\n\\nQwen 235B just got a new version today. Where DeepSeek and Kimi K2 were trained at FP8, Qwen 235B was trained at BF16. On the 512GB you can run the full model instead of a quant.\\n\\nThat's right now. Who knows what models are coming out next week where you wish you had \\"just a tad more RAM\\".\\n\\nLike u/Baldur-Norddahl said, \\"If you are not paying, there are zero reasons for not going with 512GB! :-)\\"","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4fbrrj","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Currently you will miss out completely on DeepSeek V3 and R1, plus Kimi K2. DeepSeek V3 is really good for translations. R1 for coding.&lt;/p&gt;\\n\\n&lt;p&gt;Qwen 235B just got a new version today. Where DeepSeek and Kimi K2 were trained at FP8, Qwen 235B was trained at BF16. On the 512GB you can run the full model instead of a quant.&lt;/p&gt;\\n\\n&lt;p&gt;That&amp;#39;s right now. Who knows what models are coming out next week where you wish you had &amp;quot;just a tad more RAM&amp;quot;.&lt;/p&gt;\\n\\n&lt;p&gt;Like &lt;a href=\\"/u/Baldur-Norddahl\\"&gt;u/Baldur-Norddahl&lt;/a&gt; said, &amp;quot;If you are not paying, there are zero reasons for not going with 512GB! :-)&amp;quot;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5uu0t/strong_case_for_a_512gb_mac_studio/n4fbrrj/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753137587,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m5uu0t","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4fknmf","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"chisleu","can_mod_post":false,"created_utc":1753140488,"send_replies":true,"parent_id":"t1_n4fhq6i","score":2,"author_fullname":"t2_cbxyn","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"it's totally a thing, so is distributed llm processing using apple hardware. There are multiple projects going just for that. mx.distributed, exo (exo-labs)... Apple hardware is pretty great for LLMs. You can run the devstral-small bf16 on a macbook pro.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4fknmf","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;it&amp;#39;s totally a thing, so is distributed llm processing using apple hardware. There are multiple projects going just for that. mx.distributed, exo (exo-labs)... Apple hardware is pretty great for LLMs. You can run the devstral-small bf16 on a macbook pro.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m5uu0t","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5uu0t/strong_case_for_a_512gb_mac_studio/n4fknmf/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753140488,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4fscqy","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"chibop1","can_mod_post":false,"created_utc":1753143074,"send_replies":true,"parent_id":"t1_n4fhq6i","score":1,"author_fullname":"t2_e9jh97s","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"You can easily finetune models with MLX.\\n\\nhttps://github.com/ml-explore/mlx-lm/blob/main/mlx_lm/LORA.md","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4fscqy","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You can easily finetune models with MLX.&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://github.com/ml-explore/mlx-lm/blob/main/mlx_lm/LORA.md\\"&gt;https://github.com/ml-explore/mlx-lm/blob/main/mlx_lm/LORA.md&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m5uu0t","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5uu0t/strong_case_for_a_512gb_mac_studio/n4fscqy/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753143074,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4fhq6i","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Willing_Landscape_61","can_mod_post":false,"created_utc":1753139531,"send_replies":true,"parent_id":"t3_1m5uu0t","score":1,"author_fullname":"t2_8lvrytgw","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Never heard of fine tuning on Mac.\\nIs that really a thing?\\nAny benchmark available somewhere?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4fhq6i","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Never heard of fine tuning on Mac.\\nIs that really a thing?\\nAny benchmark available somewhere?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5uu0t/strong_case_for_a_512gb_mac_studio/n4fhq6i/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753139531,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m5uu0t","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4fk6la","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"chisleu","can_mod_post":false,"created_utc":1753140335,"send_replies":true,"parent_id":"t3_1m5uu0t","score":2,"author_fullname":"t2_cbxyn","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I purchased the 512GB model. If you are going to run LLMs, it's a powerful choice. It can run things by itself that would need many times the price in GPUs alone. Not counting the infra to run them.\\n\\nGet the 512GB of RAM version and get the 4TB SSD upgrade. That will max out the SSD throughput.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4fk6la","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I purchased the 512GB model. If you are going to run LLMs, it&amp;#39;s a powerful choice. It can run things by itself that would need many times the price in GPUs alone. Not counting the infra to run them.&lt;/p&gt;\\n\\n&lt;p&gt;Get the 512GB of RAM version and get the 4TB SSD upgrade. That will max out the SSD throughput.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5uu0t/strong_case_for_a_512gb_mac_studio/n4fk6la/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753140335,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m5uu0t","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4f5qlj","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ChevChance","can_mod_post":false,"created_utc":1753135616,"send_replies":true,"parent_id":"t1_n4f069l","score":1,"author_fullname":"t2_sk7nmjrs","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Fair question, I anticipate distillation refinement and running models locally, so that company code never leaves the building.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4f5qlj","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Fair question, I anticipate distillation refinement and running models locally, so that company code never leaves the building.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m5uu0t","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5uu0t/strong_case_for_a_512gb_mac_studio/n4f5qlj/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753135616,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4f069l","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"National_Meeting_749","can_mod_post":false,"created_utc":1753133875,"send_replies":true,"parent_id":"t3_1m5uu0t","score":1,"author_fullname":"t2_drm5tg5d","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"What's your use case? How big of a model do need to run and fine tune?\\n\\nI'm not an expert, but I do know that info will help others help you.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4f069l","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;What&amp;#39;s your use case? How big of a model do need to run and fine tune?&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;m not an expert, but I do know that info will help others help you.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5uu0t/strong_case_for_a_512gb_mac_studio/n4f069l/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753133875,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m5uu0t","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4fnthq","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ChevChance","can_mod_post":false,"created_utc":1753141531,"send_replies":true,"parent_id":"t1_n4f8hi2","score":1,"author_fullname":"t2_sk7nmjrs","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Alex Ziskind suggests that the 96GB RAM is the sweet spot\\n\\n[https://www.youtube.com/watch?v=wzPMdp9Qz6Q&amp;t=366s](https://www.youtube.com/watch?v=wzPMdp9Qz6Q&amp;t=366s)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4fnthq","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Alex Ziskind suggests that the 96GB RAM is the sweet spot&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://www.youtube.com/watch?v=wzPMdp9Qz6Q&amp;amp;t=366s\\"&gt;https://www.youtube.com/watch?v=wzPMdp9Qz6Q&amp;amp;t=366s&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m5uu0t","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5uu0t/strong_case_for_a_512gb_mac_studio/n4fnthq/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753141531,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4f8hi2","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"synn89","can_mod_post":false,"created_utc":1753136508,"send_replies":true,"parent_id":"t3_1m5uu0t","score":1,"author_fullname":"t2_3jm4t","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"While it depends on your use case, today, right now Qwen3-235B runs pretty well on Mac, with reasonable sized input context, and you can look at the file sizes at https://huggingface.co/unsloth/Qwen3-235B-A22B-128K-GGUF/tree/main\\n\\nQ8 may be pushing it for 256GB, but a Q6_K fits pretty well and has very little loss at that quant size in 256GB. Now, if you wanted to run MLX, then I think that skips a 6 bit and leaves you with 4 or 8 bit.\\n\\nBut outside of that, the issue is you don't know what's coming down the pipe. Maybe there's a 400-20 MOE that hits which ends up being perfect for the M3 512GB device. But for now, it feels like we're getting really large SOTA models open providers can run(because China lacks inference compute) or 100-300B models for running on smaller systems.\\n\\nThough, really I feel like 512GB won't be great until we get more memory bandwidth. A M3 chip with 256GB sounds about perfect. I know with my 128GB M1 I'd like just a tad more RAM on it.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4f8hi2","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;While it depends on your use case, today, right now Qwen3-235B runs pretty well on Mac, with reasonable sized input context, and you can look at the file sizes at &lt;a href=\\"https://huggingface.co/unsloth/Qwen3-235B-A22B-128K-GGUF/tree/main\\"&gt;https://huggingface.co/unsloth/Qwen3-235B-A22B-128K-GGUF/tree/main&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;Q8 may be pushing it for 256GB, but a Q6_K fits pretty well and has very little loss at that quant size in 256GB. Now, if you wanted to run MLX, then I think that skips a 6 bit and leaves you with 4 or 8 bit.&lt;/p&gt;\\n\\n&lt;p&gt;But outside of that, the issue is you don&amp;#39;t know what&amp;#39;s coming down the pipe. Maybe there&amp;#39;s a 400-20 MOE that hits which ends up being perfect for the M3 512GB device. But for now, it feels like we&amp;#39;re getting really large SOTA models open providers can run(because China lacks inference compute) or 100-300B models for running on smaller systems.&lt;/p&gt;\\n\\n&lt;p&gt;Though, really I feel like 512GB won&amp;#39;t be great until we get more memory bandwidth. A M3 chip with 256GB sounds about perfect. I know with my 128GB M1 I&amp;#39;d like just a tad more RAM on it.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5uu0t/strong_case_for_a_512gb_mac_studio/n4f8hi2/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753136508,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m5uu0t","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),o=()=>e.jsx(l,{data:t});export{o as default};
