import{j as e}from"./index-DQXiEb7D.js";import{R as l}from"./RedditPostRenderer-BjndLgq8.js";import"./index-B-ILyjT1.js";const t=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Hi all, noob here so forgive the noobitude.\\n\\nRelatively new to the AI coding tool space, started with copilot in VScode, it was OK, then moved to cursor which is/was awesome for a couple months, now it's nerfed get capped even on $200 plan within a couple weeks of the month, auto mode is \\"ok\\". Tried claude code but wasn't really for me, I prefer the IDE interface of cursor or VSCode.\\n\\nI'm now finding that even claude code is constantly timing out, cursor auto just doesn't have the context window for a lot of what I need...\\n\\nI have a 3090, I've been trying to find out if there are any models worth running locally which have tooling agentic capabilities to then run in either cursor or VSCode. From what I've read (not heaps) it sounds like a lot of the open source models that can be run on a 3090 aren't really set up to work with tooling, so won't give a similar experience to cursor or copilot yet. But the space moves so fast so maybe there is something workable now?\\n\\nObviously I'm not expecting Claude level performance, but I wanted to see what's available and give something a try. Even if it's only 70% as good, if it's at least reliable and cheap then it might be good enough for what I am doing.\\n\\nTIA","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Any local models with decent tooling capabilities worth running with 3090?","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1m3nwlf","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.75,"author_flair_background_color":null,"subreddit_type":"public","ups":8,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_1rqkouqs3q","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":8,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1752901601,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Hi all, noob here so forgive the noobitude.&lt;/p&gt;\\n\\n&lt;p&gt;Relatively new to the AI coding tool space, started with copilot in VScode, it was OK, then moved to cursor which is/was awesome for a couple months, now it&amp;#39;s nerfed get capped even on $200 plan within a couple weeks of the month, auto mode is &amp;quot;ok&amp;quot;. Tried claude code but wasn&amp;#39;t really for me, I prefer the IDE interface of cursor or VSCode.&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;m now finding that even claude code is constantly timing out, cursor auto just doesn&amp;#39;t have the context window for a lot of what I need...&lt;/p&gt;\\n\\n&lt;p&gt;I have a 3090, I&amp;#39;ve been trying to find out if there are any models worth running locally which have tooling agentic capabilities to then run in either cursor or VSCode. From what I&amp;#39;ve read (not heaps) it sounds like a lot of the open source models that can be run on a 3090 aren&amp;#39;t really set up to work with tooling, so won&amp;#39;t give a similar experience to cursor or copilot yet. But the space moves so fast so maybe there is something workable now?&lt;/p&gt;\\n\\n&lt;p&gt;Obviously I&amp;#39;m not expecting Claude level performance, but I wanted to see what&amp;#39;s available and give something a try. Even if it&amp;#39;s only 70% as good, if it&amp;#39;s at least reliable and cheap then it might be good enough for what I am doing.&lt;/p&gt;\\n\\n&lt;p&gt;TIA&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1m3nwlf","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"Acceptable_Adagio_91","discussion_type":null,"num_comments":10,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1m3nwlf/any_local_models_with_decent_tooling_capabilities/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1m3nwlf/any_local_models_with_decent_tooling_capabilities/","subreddit_subscribers":501231,"created_utc":1752901601,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3ya5qa","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"dat_cosmo_cat","can_mod_post":false,"created_utc":1752905576,"send_replies":true,"parent_id":"t3_1m3nwlf","score":12,"author_fullname":"t2_2e6gzozr","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"* **Step 1:** raise $80,000 from angel investors. Tell them you are doing an AI startup\\n* **Step 2:** convince Lambda, Exxact Corp, or Supermicro to sell you an underpopulated H200 NVL server\\n* **Step 3:** load giant LLM model into [Open Code](https://github.com/sst/opencode) and type prompt: \\"Create a 1 billion dollar App, make no mistakes\\"\\n* **Step 4:** profit","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3ya5qa","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;ul&gt;\\n&lt;li&gt;&lt;strong&gt;Step 1:&lt;/strong&gt; raise $80,000 from angel investors. Tell them you are doing an AI startup&lt;/li&gt;\\n&lt;li&gt;&lt;strong&gt;Step 2:&lt;/strong&gt; convince Lambda, Exxact Corp, or Supermicro to sell you an underpopulated H200 NVL server&lt;/li&gt;\\n&lt;li&gt;&lt;strong&gt;Step 3:&lt;/strong&gt; load giant LLM model into &lt;a href=\\"https://github.com/sst/opencode\\"&gt;Open Code&lt;/a&gt; and type prompt: &amp;quot;Create a 1 billion dollar App, make no mistakes&amp;quot;&lt;/li&gt;\\n&lt;li&gt;&lt;strong&gt;Step 4:&lt;/strong&gt; profit&lt;/li&gt;\\n&lt;/ul&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":true,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3nwlf/any_local_models_with_decent_tooling_capabilities/n3ya5qa/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752905576,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m3nwlf","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":12}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3yid7p","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"admajic","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3yhqi4","score":1,"author_fullname":"t2_60b9farf","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yes and yes in lmstudio with 128k context window","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3yid7p","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yes and yes in lmstudio with 128k context window&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m3nwlf","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3nwlf/any_local_models_with_decent_tooling_capabilities/n3yid7p/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752910075,"author_flair_text":null,"treatment_tags":[],"created_utc":1752910075,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3yhqi4","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"megadonkeyx","can_mod_post":false,"created_utc":1752909723,"send_replies":true,"parent_id":"t1_n3y6s15","score":1,"author_fullname":"t2_unvzb","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Do you use flash attention and quantized kv to get more memory for context?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3yhqi4","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Do you use flash attention and quantized kv to get more memory for context?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m3nwlf","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3nwlf/any_local_models_with_decent_tooling_capabilities/n3yhqi4/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752909723,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3y6s15","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Physical-Citron5153","can_mod_post":false,"created_utc":1752903814,"send_replies":true,"parent_id":"t3_1m3nwlf","score":3,"author_fullname":"t2_clhgguip","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"The only model that can code to some degree is Devstral combine it with cline and for now this is the best you get, until you could run DeepSeek or other large models… i actually use it for about 70 to 80% of my whole code and it works good","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3y6s15","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The only model that can code to some degree is Devstral combine it with cline and for now this is the best you get, until you could run DeepSeek or other large models… i actually use it for about 70 to 80% of my whole code and it works good&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3nwlf/any_local_models_with_decent_tooling_capabilities/n3y6s15/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752903814,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m3nwlf","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"7dba5c08-72f1-11ee-9b6f-ca195bc297d4","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"7dba5c08-72f1-11ee-9b6f-ca195bc297d4","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3y8is0","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"cbterry","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3y6fat","score":2,"author_fullname":"t2_qfzsn","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I use a discord bot with tool calling. It can read files, search with searx, read local notes, make images, etc. I just convert MCP tools. Anything smaller than 30b and it'll generally loop or fail.\\n\\n\\nOtherwise I use a CLI script that does the same. For one shot coding, it is pretty decent, but an actual coding agent would be a lot better.\\n\\n\\nI also use a slightly modified version of llm-conversation which makes it kind of like any other agent. Create a tech lead, backend, UI and doc personas and they generate then iterate over the code, fixing it, and it comes out pretty well. I haven't tried it for larger projects though, and I haven't added tool calling to it yet.\\n\\n\\nI wrote all of the above except for llm-conversation, mostly because I'm not about to give system/internet access to scripts when I don't know how they work.","edited":1752905651,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3y8is0","is_submitter":false,"collapsed":false,"author_flair_richtext":[{"e":"text","t":"Llama 70B"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I use a discord bot with tool calling. It can read files, search with searx, read local notes, make images, etc. I just convert MCP tools. Anything smaller than 30b and it&amp;#39;ll generally loop or fail.&lt;/p&gt;\\n\\n&lt;p&gt;Otherwise I use a CLI script that does the same. For one shot coding, it is pretty decent, but an actual coding agent would be a lot better.&lt;/p&gt;\\n\\n&lt;p&gt;I also use a slightly modified version of llm-conversation which makes it kind of like any other agent. Create a tech lead, backend, UI and doc personas and they generate then iterate over the code, fixing it, and it comes out pretty well. I haven&amp;#39;t tried it for larger projects though, and I haven&amp;#39;t added tool calling to it yet.&lt;/p&gt;\\n\\n&lt;p&gt;I wrote all of the above except for llm-conversation, mostly because I&amp;#39;m not about to give system/internet access to scripts when I don&amp;#39;t know how they work.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m3nwlf","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3nwlf/any_local_models_with_decent_tooling_capabilities/n3y8is0/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752904718,"author_flair_text":"Llama 70B","treatment_tags":[],"created_utc":1752904718,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n3y6fat","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Acceptable_Adagio_91","can_mod_post":false,"created_utc":1752903631,"send_replies":true,"parent_id":"t1_n3y44vq","score":2,"author_fullname":"t2_1rqkouqs3q","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Cool! Would you mind giving a brief description on your setup?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3y6fat","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Cool! Would you mind giving a brief description on your setup?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m3nwlf","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3nwlf/any_local_models_with_decent_tooling_capabilities/n3y6fat/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752903631,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n3y44vq","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"cbterry","can_mod_post":false,"created_utc":1752902462,"send_replies":true,"parent_id":"t3_1m3nwlf","score":2,"author_fullname":"t2_qfzsn","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Qwen 3 30b-A3B Q4 works fairly consistently for me","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3y44vq","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"Llama 70B"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Qwen 3 30b-A3B Q4 works fairly consistently for me&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3nwlf/any_local_models_with_decent_tooling_capabilities/n3y44vq/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752902462,"author_flair_text":"Llama 70B","treatment_tags":[],"link_id":"t3_1m3nwlf","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3z6fq7","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"HRudy94","can_mod_post":false,"created_utc":1752923371,"send_replies":true,"parent_id":"t3_1m3nwlf","score":2,"author_fullname":"t2_12e33e","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"GLM-4 / Gemma-3 are good local models for code assistance. \\n\\n\\nThat said, LLMs cannot replace humans and so you should only use models as a tool to help you. Forget about vibe-coding as an idea, it doesn't work. They're good for reformatting code, explaining it, finding great templates, parsing the documentation etc though.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3z6fq7","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;GLM-4 / Gemma-3 are good local models for code assistance. &lt;/p&gt;\\n\\n&lt;p&gt;That said, LLMs cannot replace humans and so you should only use models as a tool to help you. Forget about vibe-coding as an idea, it doesn&amp;#39;t work. They&amp;#39;re good for reformatting code, explaining it, finding great templates, parsing the documentation etc though.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3nwlf/any_local_models_with_decent_tooling_capabilities/n3z6fq7/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752923371,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m3nwlf","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}}]`),r=()=>e.jsx(l,{data:t});export{r as default};
