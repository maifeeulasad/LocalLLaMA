import{j as e}from"./index-CWmJdUH_.js";import{R as t}from"./RedditPostRenderer-D2iunoQ9.js";import"./index-BCg9RP6g.js";const l=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"I come to you guys humbly seeking some help...\\n\\nI've been away from the local AI scene for about 6 months - it feels like an eternity - so much has changed!  I decided I wanted to get back into the game by building a new machine.  Because so much has changed, I'm having a hard time gauging what I will actually be able to do and what models I can run with my new specs.  \\nAny ideas?  What would you guys recommend I try that would really take advantage of these specs?\\nI understand that 32GB VRAM is not so special for LLMs, but I'm hoping that 256GB RAM might open up some interesting possibilities.\\n\\n|Category|Component(s)|\\n|:-|:-|\\n|**GPU**|ZOTAC GAMING GeForce RTX 5090 AMP Extreme INFINITY (32GB VRAM)|\\n|**CPU**|AMD Ryzen 9 9950X|\\n|**Memory**|256GB (4×64GB) G.SKILL DDR5 6000MT/s|\\n|**Motherboard**|MSI X870E Tomahawk WiFi|\\n|**Storage**|1× 2TB Samsung 9100 Pro Gen 5 NVMe (OS); 2× 1TB Samsung 990 Pro Gen 4 NVMes *(1 as cache);* 2× 14TB Seagate HDDs|\\n|**PSU**|MSI 1300W Platinum|\\n|**Cooler**|Arctic Freezer III 420 AIO|\\n|**Case**|Geometric Future M5 *(glass panel version)*|\\n|**Fans**|7× Lian Li SL Infinity + 1× TL LCD fan|\\n\\nI am of course interested in LLMs, but I'm also interested in trying out new image, video, music, speech, etc. models as well.  Can I run any of the new agentic models?  In terms of parameters, in general, what's my limit?\\n\\nThank you in advance!","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"I built an AI PC - what should I try out first?","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1m06lrz","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.33,"author_flair_background_color":null,"subreddit_type":"public","ups":0,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_2v0asrl2","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":0,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":1752551531,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1752548155,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;I come to you guys humbly seeking some help...&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;ve been away from the local AI scene for about 6 months - it feels like an eternity - so much has changed!  I decided I wanted to get back into the game by building a new machine.  Because so much has changed, I&amp;#39;m having a hard time gauging what I will actually be able to do and what models I can run with my new specs.&lt;br/&gt;\\nAny ideas?  What would you guys recommend I try that would really take advantage of these specs?\\nI understand that 32GB VRAM is not so special for LLMs, but I&amp;#39;m hoping that 256GB RAM might open up some interesting possibilities.&lt;/p&gt;\\n\\n&lt;table&gt;&lt;thead&gt;\\n&lt;tr&gt;\\n&lt;th align=\\"left\\"&gt;Category&lt;/th&gt;\\n&lt;th align=\\"left\\"&gt;Component(s)&lt;/th&gt;\\n&lt;/tr&gt;\\n&lt;/thead&gt;&lt;tbody&gt;\\n&lt;tr&gt;\\n&lt;td align=\\"left\\"&gt;&lt;strong&gt;GPU&lt;/strong&gt;&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;ZOTAC GAMING GeForce RTX 5090 AMP Extreme INFINITY (32GB VRAM)&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;tr&gt;\\n&lt;td align=\\"left\\"&gt;&lt;strong&gt;CPU&lt;/strong&gt;&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;AMD Ryzen 9 9950X&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;tr&gt;\\n&lt;td align=\\"left\\"&gt;&lt;strong&gt;Memory&lt;/strong&gt;&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;256GB (4×64GB) G.SKILL DDR5 6000MT/s&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;tr&gt;\\n&lt;td align=\\"left\\"&gt;&lt;strong&gt;Motherboard&lt;/strong&gt;&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;MSI X870E Tomahawk WiFi&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;tr&gt;\\n&lt;td align=\\"left\\"&gt;&lt;strong&gt;Storage&lt;/strong&gt;&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;1× 2TB Samsung 9100 Pro Gen 5 NVMe (OS); 2× 1TB Samsung 990 Pro Gen 4 NVMes &lt;em&gt;(1 as cache);&lt;/em&gt; 2× 14TB Seagate HDDs&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;tr&gt;\\n&lt;td align=\\"left\\"&gt;&lt;strong&gt;PSU&lt;/strong&gt;&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;MSI 1300W Platinum&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;tr&gt;\\n&lt;td align=\\"left\\"&gt;&lt;strong&gt;Cooler&lt;/strong&gt;&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;Arctic Freezer III 420 AIO&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;tr&gt;\\n&lt;td align=\\"left\\"&gt;&lt;strong&gt;Case&lt;/strong&gt;&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;Geometric Future M5 &lt;em&gt;(glass panel version)&lt;/em&gt;&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;tr&gt;\\n&lt;td align=\\"left\\"&gt;&lt;strong&gt;Fans&lt;/strong&gt;&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;7× Lian Li SL Infinity + 1× TL LCD fan&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;/tbody&gt;&lt;/table&gt;\\n\\n&lt;p&gt;I am of course interested in LLMs, but I&amp;#39;m also interested in trying out new image, video, music, speech, etc. models as well.  Can I run any of the new agentic models?  In terms of parameters, in general, what&amp;#39;s my limit?&lt;/p&gt;\\n\\n&lt;p&gt;Thank you in advance!&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":true,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1m06lrz","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"wesarnquist","discussion_type":null,"num_comments":14,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1m06lrz/i_built_an_ai_pc_what_should_i_try_out_first/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1m06lrz/i_built_an_ai_pc_what_should_i_try_out_first/","subreddit_subscribers":499294,"created_utc":1752548155,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n37ev4g","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"randomqhacker","can_mod_post":false,"send_replies":true,"parent_id":"t1_n37cl0m","score":1,"author_fullname":"t2_4nw3v","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Probably even Q4, which the CPU side should run a little faster","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n37ev4g","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Probably even Q4, which the CPU side should run a little faster&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m06lrz","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m06lrz/i_built_an_ai_pc_what_should_i_try_out_first/n37ev4g/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752553364,"author_flair_text":null,"treatment_tags":[],"created_utc":1752553364,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n37cl0m","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ForsookComparison","can_mod_post":false,"created_utc":1752552362,"send_replies":true,"parent_id":"t1_n37a21j","score":0,"author_fullname":"t2_on5es7pe3","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"This was my first thought - Qwen3-235b at Q3 should perform decently on this machine.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n37cl0m","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;This was my first thought - Qwen3-235b at Q3 should perform decently on this machine.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m06lrz","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m06lrz/i_built_an_ai_pc_what_should_i_try_out_first/n37cl0m/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752552362,"author_flair_text":"llama.cpp","treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}},"user_reports":[],"saved":false,"id":"n37a21j","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"randomqhacker","can_mod_post":false,"created_utc":1752551299,"send_replies":true,"parent_id":"t3_1m06lrz","score":4,"author_fullname":"t2_4nw3v","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"llama-server is much improved, nice UI\\n\\nFor models that will fit on your card, try the new Devstral 1.1, and GLM4-32B for coding.  Devstral is fine-tuned for agentic use, and GLM-4 is great for code.  Mistral-Small-3.2 is fun to talk to and has great vision support, it can OCR the worst handwriting.  Qwen3-32B is good for code and great for STEM.\\n\\nTry qwen3-235b-a22b MoE with tensor offload like -ot \\"ffn\\\\_.\\\\*=CPU\\" or -ot \\"blk.(\\\\[0|1|2|3|4|5|6|7|8|9\\\\]|\\\\[0-9\\\\]\\\\[0|1|2|3|4|5|6|7|8|9\\\\]).\\\\*=CPU\\" to move some of the experts off the GPU and onto your main memory.  (Play with the pattern matching numbers until you get the right balance.\\n\\nI can't run Deepseek, but the latest release is supposed to be amazing and your system can run it with the right -ot settings.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n37a21j","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;llama-server is much improved, nice UI&lt;/p&gt;\\n\\n&lt;p&gt;For models that will fit on your card, try the new Devstral 1.1, and GLM4-32B for coding.  Devstral is fine-tuned for agentic use, and GLM-4 is great for code.  Mistral-Small-3.2 is fun to talk to and has great vision support, it can OCR the worst handwriting.  Qwen3-32B is good for code and great for STEM.&lt;/p&gt;\\n\\n&lt;p&gt;Try qwen3-235b-a22b MoE with tensor offload like -ot &amp;quot;ffn_.*=CPU&amp;quot; or -ot &amp;quot;blk.([0|1|2|3|4|5|6|7|8|9]|[0-9][0|1|2|3|4|5|6|7|8|9]).*=CPU&amp;quot; to move some of the experts off the GPU and onto your main memory.  (Play with the pattern matching numbers until you get the right balance.&lt;/p&gt;\\n\\n&lt;p&gt;I can&amp;#39;t run Deepseek, but the latest release is supposed to be amazing and your system can run it with the right -ot settings.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m06lrz/i_built_an_ai_pc_what_should_i_try_out_first/n37a21j/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752551299,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m06lrz","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n384arh","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"random-tomato","can_mod_post":false,"created_utc":1752566588,"send_replies":true,"parent_id":"t1_n37a81a","score":0,"author_fullname":"t2_fmd6oq5v6","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"To be a bit more accurate... it's Qwen3 2**3**5B (Active 22B params)\\n\\nSome other good info to know for OP:\\n\\n\\\\- Generally people say it isn't that great, Qwen3 32B is supposed to be a bit better and a lot faster (You can run maybe Q6 or Q5 with good context size)\\n\\n\\\\- If you're set on the 235B A22B, you'll need to look into the llama-server settings for optimizing the offloading, such as \`--no-mmap\`, \`--override-tensors\`, etc. More info [here](https://www.reddit.com/r/LocalLLaMA/comments/1k1rjm1/how_to_run_llama_4_fast_even_though_its_too_big/), but you'll definitely want to run with --no-mmap since you have enough RAM.","edited":1752566973,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n384arh","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;To be a bit more accurate... it&amp;#39;s Qwen3 2&lt;strong&gt;3&lt;/strong&gt;5B (Active 22B params)&lt;/p&gt;\\n\\n&lt;p&gt;Some other good info to know for OP:&lt;/p&gt;\\n\\n&lt;p&gt;- Generally people say it isn&amp;#39;t that great, Qwen3 32B is supposed to be a bit better and a lot faster (You can run maybe Q6 or Q5 with good context size)&lt;/p&gt;\\n\\n&lt;p&gt;- If you&amp;#39;re set on the 235B A22B, you&amp;#39;ll need to look into the llama-server settings for optimizing the offloading, such as &lt;code&gt;--no-mmap&lt;/code&gt;, &lt;code&gt;--override-tensors&lt;/code&gt;, etc. More info &lt;a href=\\"https://www.reddit.com/r/LocalLLaMA/comments/1k1rjm1/how_to_run_llama_4_fast_even_though_its_too_big/\\"&gt;here&lt;/a&gt;, but you&amp;#39;ll definitely want to run with --no-mmap since you have enough RAM.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m06lrz","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m06lrz/i_built_an_ai_pc_what_should_i_try_out_first/n384arh/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752566588,"author_flair_text":"llama.cpp","treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}},"user_reports":[],"saved":false,"id":"n37a81a","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Only-Letterhead-3411","can_mod_post":false,"created_utc":1752551368,"send_replies":true,"parent_id":"t3_1m06lrz","score":3,"author_fullname":"t2_pbfqmgf8","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"&gt;In terms of parameters, in general, what's my limit?\\n\\nBiggest model you can run at usable speeds is probably Qwen3 265B. Since it's MoE, it'll be fine even running offloaded","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n37a81a","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;In terms of parameters, in general, what&amp;#39;s my limit?&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;Biggest model you can run at usable speeds is probably Qwen3 265B. Since it&amp;#39;s MoE, it&amp;#39;ll be fine even running offloaded&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m06lrz/i_built_an_ai_pc_what_should_i_try_out_first/n37a81a/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752551368,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m06lrz","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n38n995","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"fizzy1242","can_mod_post":false,"send_replies":true,"parent_id":"t1_n37le0j","score":1,"author_fullname":"t2_16zcsx","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"absolutely, even for an extra context buffer","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n38n995","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;absolutely, even for an extra context buffer&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m06lrz","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m06lrz/i_built_an_ai_pc_what_should_i_try_out_first/n38n995/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752577146,"author_flair_text":null,"treatment_tags":[],"created_utc":1752577146,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n37le0j","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"wesarnquist","can_mod_post":false,"created_utc":1752556395,"send_replies":true,"parent_id":"t1_n37kljz","score":1,"author_fullname":"t2_2v0asrl2","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I have a 3080 10gb laying around. Do you think it's worth connecting it via egpu?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n37le0j","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I have a 3080 10gb laying around. Do you think it&amp;#39;s worth connecting it via egpu?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m06lrz","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m06lrz/i_built_an_ai_pc_what_should_i_try_out_first/n37le0j/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752556395,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n37kljz","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Delicious-Farmer-234","can_mod_post":false,"created_utc":1752556015,"send_replies":true,"parent_id":"t3_1m06lrz","score":3,"author_fullname":"t2_6x6f3qy5","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Don't rely too much on ram offloading. For small models yes but if you are offloading half of a really big model like R1 you are going to be disappointed. I maxed out my ram too and I have a 14900. I usually run Qwen 32B with max context. I have been playing around with unmute locally with medgemma 27b and created a personal doctor I can talk too. It's pretty cool because the model role plays well and has the knowledge. I have a 5090, 3080ti and 3080 btw.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n37kljz","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Don&amp;#39;t rely too much on ram offloading. For small models yes but if you are offloading half of a really big model like R1 you are going to be disappointed. I maxed out my ram too and I have a 14900. I usually run Qwen 32B with max context. I have been playing around with unmute locally with medgemma 27b and created a personal doctor I can talk too. It&amp;#39;s pretty cool because the model role plays well and has the knowledge. I have a 5090, 3080ti and 3080 btw.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m06lrz/i_built_an_ai_pc_what_should_i_try_out_first/n37kljz/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752556015,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m06lrz","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n37c61s","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"No_Information9314","can_mod_post":false,"created_utc":1752552184,"send_replies":true,"parent_id":"t3_1m06lrz","score":2,"author_fullname":"t2_1losh2aia5","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Qwen3 MoEs","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n37c61s","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Qwen3 MoEs&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m06lrz/i_built_an_ai_pc_what_should_i_try_out_first/n37c61s/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752552184,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m06lrz","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n38j50l","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"un_passant","can_mod_post":false,"created_utc":1752575137,"send_replies":true,"parent_id":"t3_1m06lrz","score":1,"author_fullname":"t2_7rqtc","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"[https://huggingface.co/ubergarm/DeepSeek-R1-0528-GGUF#iq2\\\\_k\\\\_r4-2799-bpw-220gib](https://huggingface.co/ubergarm/DeepSeek-R1-0528-GGUF#iq2_k_r4-2799-bpw-220gib)\\n\\n  \\n\`IQ2_K_R4\` is what you want, after installing ik\\\\_llama.cpp","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n38j50l","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;&lt;a href=\\"https://huggingface.co/ubergarm/DeepSeek-R1-0528-GGUF#iq2_k_r4-2799-bpw-220gib\\"&gt;https://huggingface.co/ubergarm/DeepSeek-R1-0528-GGUF#iq2_k_r4-2799-bpw-220gib&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;&lt;code&gt;IQ2_K_R4&lt;/code&gt; is what you want, after installing ik_llama.cpp&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m06lrz/i_built_an_ai_pc_what_should_i_try_out_first/n38j50l/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752575137,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m06lrz","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n38jk7v","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MelodicRecognition7","can_mod_post":false,"created_utc":1752575356,"send_replies":true,"parent_id":"t3_1m06lrz","score":1,"author_fullname":"t2_1eex9ug5","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"&gt; I'm hoping that 256GB RAM might open up some interesting possibilities.\\n\\n&gt; AMD Ryzen 9 9950X\\n\\n&gt; 2 memory channels\\n\\nno, it will not.\\n\\nhttps://old.reddit.com/r/LocalLLaMA/comments/1lw12gt/beginner_question_entrylevel_hobbyist_build/n2bp37z/","edited":1752576528,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n38jk7v","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;I&amp;#39;m hoping that 256GB RAM might open up some interesting possibilities.&lt;/p&gt;\\n\\n&lt;p&gt;AMD Ryzen 9 9950X&lt;/p&gt;\\n\\n&lt;p&gt;2 memory channels&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;no, it will not.&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://old.reddit.com/r/LocalLLaMA/comments/1lw12gt/beginner_question_entrylevel_hobbyist_build/n2bp37z/\\"&gt;https://old.reddit.com/r/LocalLLaMA/comments/1lw12gt/beginner_question_entrylevel_hobbyist_build/n2bp37z/&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m06lrz/i_built_an_ai_pc_what_should_i_try_out_first/n38jk7v/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752575356,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m06lrz","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n373uyi","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"thecstep","can_mod_post":false,"created_utc":1752548801,"send_replies":true,"parent_id":"t3_1m06lrz","score":1,"author_fullname":"t2_5bva3","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Waifu LLM","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n373uyi","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Waifu LLM&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m06lrz/i_built_an_ai_pc_what_should_i_try_out_first/n373uyi/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752548801,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m06lrz","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n377nmd","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Square-Onion-1825","can_mod_post":false,"created_utc":1752550312,"send_replies":true,"parent_id":"t3_1m06lrz","score":-1,"author_fullname":"t2_1mkh7x2yxn","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I've been asking chatgpt and grok for h/w specs by giving some use cases I'm currently involved with some clients. i suggestion you have a conversation with those bots as they have all the parameters to answer your question. be sure to create a system prompt like \\"You are a \\\\*collaborative reasoning partner\\\\*, not an obedient assistant.\\" so that it doesn't agree with you all the time because that tends to introduce your biases to it.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n377nmd","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;ve been asking chatgpt and grok for h/w specs by giving some use cases I&amp;#39;m currently involved with some clients. i suggestion you have a conversation with those bots as they have all the parameters to answer your question. be sure to create a system prompt like &amp;quot;You are a *collaborative reasoning partner*, not an obedient assistant.&amp;quot; so that it doesn&amp;#39;t agree with you all the time because that tends to introduce your biases to it.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m06lrz/i_built_an_ai_pc_what_should_i_try_out_first/n377nmd/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752550312,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m06lrz","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-1}}],"before":null}}]`),o=()=>e.jsx(t,{data:l});export{o as default};
