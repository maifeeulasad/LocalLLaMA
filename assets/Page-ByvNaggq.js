import{j as e}from"./index-CjwP30j7.js";import{R as t}from"./RedditPostRenderer-BbYuEq_V.js";import"./index-C-yxLSPN.js";const a=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Hey everyone!   \\n  \\nI want to get started with local AI, and I’m looking for advice on where to begin. I'm reading some of the other posts about the same, but seeing how quickly AI advances I figured I'd ask. I’ve been looking at the smaller models like Llama and Deepseek's 8b. Apparently one is as small as 1.5b.... That can be run on some \\\\*very\\\\* modest hardware: [https://martech.org/how-to-run-deepseek-locally-on-your-computer/](https://martech.org/how-to-run-deepseek-locally-on-your-computer/)\\n\\nRight now, I’m working with a laptop with an i9-13980hx, an RTX 4080, 32gb DDR5, and 1tb ssd. I realize that I’m not going to be running a fortune 500 company, solving world hunger, or achieving The Singularity with this setup, but on paper it should be pretty capable for what I’m envisioning.  \\n  \\nThere’s three basic things I’d really like to try with local AI:   \\n  \\n\\\\-Fine-tuning/distilling them for more specific purposes-  \\n  \\nI’m currently using ChatGPT as a day-planner/calendar/to-do list that I can talk to. It’s great that it could also write a comparative essay on the agrarian economies of pre-roman versus post-roman Gaul… but I don’t need my calendar to do that. I need it to accurately follow instructions, keep accurate lists, and answer questions about information it has access to. Sometimes ChatGPT has been surprisingly bad at this, and it’s actually seemed to get worse as the models get “smarter” and “more human”.  \\n  \\n\\\\-Integrating them into larger “digital ecosystems”-\\n\\nThere are some things ChatGPT is too “smart” to do reliably. Like find every mention of a word in a document, or tell me what time it is (try it yourself. 1/3 correct, at best). These sound like tasks for a “dumb” service. Google Assistant will tell me what time it is with 100% accuracy. My 1993 Windows 3.1 finds every mention of a word in a document every time I use “Find”. Getting a local LLM to know when it’s time to offload the work to a different, simpler element would make the whole system much more smooth, reliable, and useful. Bonus points if it can also reach out to more powerful cloud AIs through things like an OpenAI API key.  \\n  \\n\\\\-Image recognition-  \\n  \\nI’ve got some interest in getting a part of that larger system to recognize images I train it for, but this is sort of icing on the cake. I hear things like computervision, resnet, and nyckel thrown around, but I don’t understand enough yet to even know what questions to ask.  \\n  \\nAny tips on where to start?","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Getting started with local AI","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lsyza0","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.73,"author_flair_background_color":null,"subreddit_type":"public","ups":11,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_djjpv9or","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":11,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"post_hint":"self","content_categories":null,"is_self":true,"mod_note":null,"created":1751801281,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Hey everyone!   &lt;/p&gt;\\n\\n&lt;p&gt;I want to get started with local AI, and I’m looking for advice on where to begin. I&amp;#39;m reading some of the other posts about the same, but seeing how quickly AI advances I figured I&amp;#39;d ask. I’ve been looking at the smaller models like Llama and Deepseek&amp;#39;s 8b. Apparently one is as small as 1.5b.... That can be run on some *very* modest hardware: &lt;a href=\\"https://martech.org/how-to-run-deepseek-locally-on-your-computer/\\"&gt;https://martech.org/how-to-run-deepseek-locally-on-your-computer/&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;Right now, I’m working with a laptop with an i9-13980hx, an RTX 4080, 32gb DDR5, and 1tb ssd. I realize that I’m not going to be running a fortune 500 company, solving world hunger, or achieving The Singularity with this setup, but on paper it should be pretty capable for what I’m envisioning.  &lt;/p&gt;\\n\\n&lt;p&gt;There’s three basic things I’d really like to try with local AI:   &lt;/p&gt;\\n\\n&lt;p&gt;-Fine-tuning/distilling them for more specific purposes-  &lt;/p&gt;\\n\\n&lt;p&gt;I’m currently using ChatGPT as a day-planner/calendar/to-do list that I can talk to. It’s great that it could also write a comparative essay on the agrarian economies of pre-roman versus post-roman Gaul… but I don’t need my calendar to do that. I need it to accurately follow instructions, keep accurate lists, and answer questions about information it has access to. Sometimes ChatGPT has been surprisingly bad at this, and it’s actually seemed to get worse as the models get “smarter” and “more human”.  &lt;/p&gt;\\n\\n&lt;p&gt;-Integrating them into larger “digital ecosystems”-&lt;/p&gt;\\n\\n&lt;p&gt;There are some things ChatGPT is too “smart” to do reliably. Like find every mention of a word in a document, or tell me what time it is (try it yourself. 1/3 correct, at best). These sound like tasks for a “dumb” service. Google Assistant will tell me what time it is with 100% accuracy. My 1993 Windows 3.1 finds every mention of a word in a document every time I use “Find”. Getting a local LLM to know when it’s time to offload the work to a different, simpler element would make the whole system much more smooth, reliable, and useful. Bonus points if it can also reach out to more powerful cloud AIs through things like an OpenAI API key.  &lt;/p&gt;\\n\\n&lt;p&gt;-Image recognition-  &lt;/p&gt;\\n\\n&lt;p&gt;I’ve got some interest in getting a part of that larger system to recognize images I train it for, but this is sort of icing on the cake. I hear things like computervision, resnet, and nyckel thrown around, but I don’t understand enough yet to even know what questions to ask.  &lt;/p&gt;\\n\\n&lt;p&gt;Any tips on where to start?&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"preview":{"images":[{"source":{"url":"https://external-preview.redd.it/L0IbFLU4mAFgCcd4enGCZMd9nPmPHGGAMFM5XYUSqpU.png?auto=webp&amp;s=cfd5ee4ad43b989a84c4caf381b22afd8e6d8ea5","width":1920,"height":1080},"resolutions":[{"url":"https://external-preview.redd.it/L0IbFLU4mAFgCcd4enGCZMd9nPmPHGGAMFM5XYUSqpU.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=0bec92da8b7c865fcbfd67b314e99e5bd33f293c","width":108,"height":60},{"url":"https://external-preview.redd.it/L0IbFLU4mAFgCcd4enGCZMd9nPmPHGGAMFM5XYUSqpU.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=92f38610799c0911650ee9f180e5cf9faece431a","width":216,"height":121},{"url":"https://external-preview.redd.it/L0IbFLU4mAFgCcd4enGCZMd9nPmPHGGAMFM5XYUSqpU.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=0318c23e2f3071fd16fe8b65515c943a4755552b","width":320,"height":180},{"url":"https://external-preview.redd.it/L0IbFLU4mAFgCcd4enGCZMd9nPmPHGGAMFM5XYUSqpU.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=32a7352c8d588e3d37b875c01718a2a4450d0aa5","width":640,"height":360},{"url":"https://external-preview.redd.it/L0IbFLU4mAFgCcd4enGCZMd9nPmPHGGAMFM5XYUSqpU.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=4230117da66d05bc3179272583d4bdb1aeae14b6","width":960,"height":540},{"url":"https://external-preview.redd.it/L0IbFLU4mAFgCcd4enGCZMd9nPmPHGGAMFM5XYUSqpU.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=bef8acd875dec557d098ec5142079141b3d083eb","width":1080,"height":607}],"variants":{},"id":"L0IbFLU4mAFgCcd4enGCZMd9nPmPHGGAMFM5XYUSqpU"}],"enabled":false},"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1lsyza0","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"Bitter-Ad640","discussion_type":null,"num_comments":10,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lsyza0/getting_started_with_local_ai/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lsyza0/getting_started_with_local_ai/","subreddit_subscribers":495650,"created_utc":1751801281,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1mieul","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"indicava","can_mod_post":false,"created_utc":1751805193,"send_replies":true,"parent_id":"t3_1lsyza0","score":4,"author_fullname":"t2_4dvff","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"It will be quite difficult to train the model to count occurrences of words accurately 100% of the time. It would be better to train it on tool use and provide it with a tool that counts word occurrences and then the LLM can use the result in its response.\\n\\nYou could definitely run and fine tune (using optimizations like PEFT) a 3B parameter model on your hardware and for the use case you describe, with good fine tuning it should be more than up to the task. Look into the Qwen (currently Qwen3) family of models.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1mieul","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It will be quite difficult to train the model to count occurrences of words accurately 100% of the time. It would be better to train it on tool use and provide it with a tool that counts word occurrences and then the LLM can use the result in its response.&lt;/p&gt;\\n\\n&lt;p&gt;You could definitely run and fine tune (using optimizations like PEFT) a 3B parameter model on your hardware and for the use case you describe, with good fine tuning it should be more than up to the task. Look into the Qwen (currently Qwen3) family of models.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":true,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lsyza0/getting_started_with_local_ai/n1mieul/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751805193,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lsyza0","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1nxypj","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MelodicRecognition7","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1nv7g0","score":1,"author_fullname":"t2_1eex9ug5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"&gt; this is called \\"function calling\\", google for \\"Model Context Protocol\\"","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n1nxypj","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;this is called &amp;quot;function calling&amp;quot;, google for &amp;quot;Model Context Protocol&amp;quot;&lt;/p&gt;\\n&lt;/blockquote&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lsyza0","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lsyza0/getting_started_with_local_ai/n1nxypj/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751822118,"author_flair_text":null,"treatment_tags":[],"created_utc":1751822118,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1nv7g0","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Bitter-Ad640","can_mod_post":false,"created_utc":1751821287,"send_replies":true,"parent_id":"t1_n1mk6hi","score":1,"author_fullname":"t2_djjpv9or","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I'm not sure its fair to call ChatGPT just a \\"text generator\\". There seems to be quite a lot of workflow going on in the back behind the OpenAI ChatGPT interface. Searching the internet, making sense of images, making images, formatting and converting documents, some sort of logic system, tts, stt.\\n\\nThis is why I'd like to set up workflows that call on different things for different purposes. That aside, I also don't think \\"It's a text generator\\" would explain away its poor ability to tell time. Something that can understand and perform \\"search the internet\\", \\"start a new document\\", \\"save this in the memories\\" and \\"make an image\\" should also be able to search for the time.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1nv7g0","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;m not sure its fair to call ChatGPT just a &amp;quot;text generator&amp;quot;. There seems to be quite a lot of workflow going on in the back behind the OpenAI ChatGPT interface. Searching the internet, making sense of images, making images, formatting and converting documents, some sort of logic system, tts, stt.&lt;/p&gt;\\n\\n&lt;p&gt;This is why I&amp;#39;d like to set up workflows that call on different things for different purposes. That aside, I also don&amp;#39;t think &amp;quot;It&amp;#39;s a text generator&amp;quot; would explain away its poor ability to tell time. Something that can understand and perform &amp;quot;search the internet&amp;quot;, &amp;quot;start a new document&amp;quot;, &amp;quot;save this in the memories&amp;quot; and &amp;quot;make an image&amp;quot; should also be able to search for the time.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lsyza0","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lsyza0/getting_started_with_local_ai/n1nv7g0/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751821287,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1mk6hi","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MelodicRecognition7","can_mod_post":false,"created_utc":1751805929,"send_replies":true,"parent_id":"t3_1lsyza0","score":2,"author_fullname":"t2_1eex9ug5","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"&gt; -Fine-tuning/distilling them for more specific purposes- \\n\\nnot possible with your hardware\\n\\n&gt; ChatGPT has been surprisingly bad at this\\n\\nbecause it is a text generator, not a program\\n\\n&gt; Google Assistant will tell me what time it is with 100% accuracy. My 1993 Windows 3.1 finds every mention of a word in a document every time I use “Find”.\\n\\nbecause these are programs, not a text generators.\\n\\n&gt; -Integrating them into larger “digital ecosystems”-\\n\\nthis is called \\"function calling\\", google for \\"Model Context Protocol\\"","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1mk6hi","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;-Fine-tuning/distilling them for more specific purposes- &lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;not possible with your hardware&lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;ChatGPT has been surprisingly bad at this&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;because it is a text generator, not a program&lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;Google Assistant will tell me what time it is with 100% accuracy. My 1993 Windows 3.1 finds every mention of a word in a document every time I use “Find”.&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;because these are programs, not a text generators.&lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;-Integrating them into larger “digital ecosystems”-&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;this is called &amp;quot;function calling&amp;quot;, google for &amp;quot;Model Context Protocol&amp;quot;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lsyza0/getting_started_with_local_ai/n1mk6hi/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751805929,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lsyza0","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1miibn","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"godndiogoat","can_mod_post":false,"created_utc":1751805233,"send_replies":true,"parent_id":"t3_1lsyza0","score":1,"author_fullname":"t2_1o8b7or53v","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Build a minimal end-to-end loop first-spin up a 7B Llama in Ollama, plug it into a simple LangChain agent, and route tasks to python functions for things like regex search or datetime. Once that’s stable, swap the base model with QLoRA-tuned checkpoints you make in BitsAndBytes on your 4080; 16-bit fits fine, 4-bit fits with headroom for context windows. For task routing, LangChain’s StructuredTool decorator lets the model decide when to call external code-works great for “what time is it” style queries. Feed it long-term memory through a local vector store (Chroma or Milvus) so your day-planner stays factual without ballooning the prompt. For vision, start with CLIP or LLaVA; both run under 12 GB vRAM and you can fine-tune new classes with a few dozen labeled shots using LoRA. I tried Ollama and LangChain together, but APIWrapper.ai glued the local stack to cloud GPT endpoints without me hand-rolling REST calls. Build the loop small, then iterate; that pattern scales.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1miibn","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Build a minimal end-to-end loop first-spin up a 7B Llama in Ollama, plug it into a simple LangChain agent, and route tasks to python functions for things like regex search or datetime. Once that’s stable, swap the base model with QLoRA-tuned checkpoints you make in BitsAndBytes on your 4080; 16-bit fits fine, 4-bit fits with headroom for context windows. For task routing, LangChain’s StructuredTool decorator lets the model decide when to call external code-works great for “what time is it” style queries. Feed it long-term memory through a local vector store (Chroma or Milvus) so your day-planner stays factual without ballooning the prompt. For vision, start with CLIP or LLaVA; both run under 12 GB vRAM and you can fine-tune new classes with a few dozen labeled shots using LoRA. I tried Ollama and LangChain together, but APIWrapper.ai glued the local stack to cloud GPT endpoints without me hand-rolling REST calls. Build the loop small, then iterate; that pattern scales.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lsyza0/getting_started_with_local_ai/n1miibn/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751805233,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lsyza0","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1mjn81","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Cergorach","can_mod_post":false,"created_utc":1751805710,"send_replies":true,"parent_id":"t3_1lsyza0","score":1,"author_fullname":"t2_cs4w88d2l","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Just start with installing Ollama and LM Studio, start downloading some LLMs and try running them. When you have understand how that works, work from there.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1mjn81","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Just start with installing Ollama and LM Studio, start downloading some LLMs and try running them. When you have understand how that works, work from there.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lsyza0/getting_started_with_local_ai/n1mjn81/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751805710,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lsyza0","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1mm3md","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"BidWestern1056","can_mod_post":false,"created_utc":1751806702,"send_replies":true,"parent_id":"t1_n1mlrjf","score":2,"author_fullname":"t2_uzxql7po","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"and as others have said, LLMs arent good at counting so i wouldnt rely on them for such things. instead build a tool that the LLM can call on your word doc. also you can fine tune tinyllama, llama3.2, gemma3:1b,4b on your hardware most likely. \\nhttps://huggingface.co/npc-worldwide/tinytim for an example w tiny llama. im working on upgrading this to instruction tuning with a gemma3 model.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1mm3md","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;and as others have said, LLMs arent good at counting so i wouldnt rely on them for such things. instead build a tool that the LLM can call on your word doc. also you can fine tune tinyllama, llama3.2, gemma3:1b,4b on your hardware most likely. \\n&lt;a href=\\"https://huggingface.co/npc-worldwide/tinytim\\"&gt;https://huggingface.co/npc-worldwide/tinytim&lt;/a&gt; for an example w tiny llama. im working on upgrading this to instruction tuning with a gemma3 model.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lsyza0","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lsyza0/getting_started_with_local_ai/n1mm3md/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751806702,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n1mlrjf","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"BidWestern1056","can_mod_post":false,"created_utc":1751806568,"send_replies":true,"parent_id":"t3_1lsyza0","score":1,"author_fullname":"t2_uzxql7po","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"try out npcpy \\nhttps://github.com/NPC-Worldwide/npcpy\\nand the npc shell has a lot of useful commands for you. \\ngemma:4b is prolly your best model for lightness and reliability that can also handle images. llava image stuff kinda mid. small local thinking models also quite mid imo","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1mlrjf","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;try out npcpy \\n&lt;a href=\\"https://github.com/NPC-Worldwide/npcpy\\"&gt;https://github.com/NPC-Worldwide/npcpy&lt;/a&gt;\\nand the npc shell has a lot of useful commands for you. \\ngemma:4b is prolly your best model for lightness and reliability that can also handle images. llava image stuff kinda mid. small local thinking models also quite mid imo&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lsyza0/getting_started_with_local_ai/n1mlrjf/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751806568,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lsyza0","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1ntev3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Just-Syllabub-2194","can_mod_post":false,"created_utc":1751820742,"send_replies":true,"parent_id":"t3_1lsyza0","score":1,"author_fullname":"t2_hockwoffv","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"For chat minimal requirements are 2CPUs with 4GRAM memory, no GPUs required, models working are Qwen3 0.6b , TinyLlama and Deepseek-r1:1.5b.\\n\\nFor image recognition requirements are 4-6CPUs with 16GRAM,  no GPUs required, models working are [llava](https://ollama.com/library/llava).\\n\\nEverything works in docker, either pull directly ollama docker image or use debian image and install ollama in debian container.\\n\\n[https://hub.docker.com/r/ollama/ollama](https://hub.docker.com/r/ollama/ollama)\\n\\nSpace required for Ollama + Qwen3 0.6b + TinyLlama = approx. 4GB\\n\\nSpace required for Ollama + LLava = approx. 10GB","edited":1751820955,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1ntev3","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;For chat minimal requirements are 2CPUs with 4GRAM memory, no GPUs required, models working are Qwen3 0.6b , TinyLlama and Deepseek-r1:1.5b.&lt;/p&gt;\\n\\n&lt;p&gt;For image recognition requirements are 4-6CPUs with 16GRAM,  no GPUs required, models working are &lt;a href=\\"https://ollama.com/library/llava\\"&gt;llava&lt;/a&gt;.&lt;/p&gt;\\n\\n&lt;p&gt;Everything works in docker, either pull directly ollama docker image or use debian image and install ollama in debian container.&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://hub.docker.com/r/ollama/ollama\\"&gt;https://hub.docker.com/r/ollama/ollama&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;Space required for Ollama + Qwen3 0.6b + TinyLlama = approx. 4GB&lt;/p&gt;\\n\\n&lt;p&gt;Space required for Ollama + LLava = approx. 10GB&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lsyza0/getting_started_with_local_ai/n1ntev3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751820742,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lsyza0","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1p898w","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"gopietz","can_mod_post":false,"created_utc":1751836341,"send_replies":true,"parent_id":"t3_1lsyza0","score":1,"author_fullname":"t2_16en7b","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Honestly brother, read a book. What you're writing and planning doesn't make any sense. You clearly have no idea what you're talking about and you will not achieve what you're planning.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1p898w","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Honestly brother, read a book. What you&amp;#39;re writing and planning doesn&amp;#39;t make any sense. You clearly have no idea what you&amp;#39;re talking about and you will not achieve what you&amp;#39;re planning.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lsyza0/getting_started_with_local_ai/n1p898w/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751836341,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lsyza0","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),r=()=>e.jsx(t,{data:a});export{r as default};
