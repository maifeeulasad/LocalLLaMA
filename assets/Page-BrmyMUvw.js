import{j as e}from"./index-Ce60G8KS.js";import{R as t}from"./RedditPostRenderer-Dxoi04TZ.js";import"./index-D3PeynhN.js";const l=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"The more LLMs think, the worse they translate","link_flair_richtext":[{"e":"text","t":"Resources"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":76,"top_awarded_type":null,"hide_score":false,"name":"t3_1llqp0a","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.94,"author_flair_background_color":null,"ups":125,"total_awards_received":0,"media_embed":{},"thumbnail_width":140,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_14v7epz03x","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Resources","can_mod_post":false,"score":125,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"https://external-preview.redd.it/sl5AWBXJbnd8seHGhHam-my2xN8-2MTiLgaFFv_9VgQ.jpeg?width=140&amp;height=76&amp;crop=140:76,smart&amp;auto=webp&amp;s=84dd7def76caec8257a73592762d6dd1fc89bc30","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"post_hint":"link","content_categories":null,"is_self":false,"subreddit_type":"public","created":1751020900,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"nuenki.app","allow_live_comments":false,"selftext_html":null,"likes":null,"suggested_sort":null,"banned_at_utc":null,"url_overridden_by_dest":"https://nuenki.app/blog/the_more_llms_think_the_worse_they_translate","view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"preview":{"images":[{"source":{"url":"https://external-preview.redd.it/sl5AWBXJbnd8seHGhHam-my2xN8-2MTiLgaFFv_9VgQ.jpeg?auto=webp&amp;s=8a28c599f7c03d7569c67da276ce2b976ab8d771","width":3811,"height":2096},"resolutions":[{"url":"https://external-preview.redd.it/sl5AWBXJbnd8seHGhHam-my2xN8-2MTiLgaFFv_9VgQ.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=2d0e312ff46b334fd90a3a7e76ccc030f2a17c7c","width":108,"height":59},{"url":"https://external-preview.redd.it/sl5AWBXJbnd8seHGhHam-my2xN8-2MTiLgaFFv_9VgQ.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=66560eb22bd2f2916f1afd88638271e1b8b96ca5","width":216,"height":118},{"url":"https://external-preview.redd.it/sl5AWBXJbnd8seHGhHam-my2xN8-2MTiLgaFFv_9VgQ.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=d30cb01c71b0a5ebe51ccda76bb7ee6c9b91b025","width":320,"height":175},{"url":"https://external-preview.redd.it/sl5AWBXJbnd8seHGhHam-my2xN8-2MTiLgaFFv_9VgQ.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=c116c7e6295d776b6382e425434256d0d8559943","width":640,"height":351},{"url":"https://external-preview.redd.it/sl5AWBXJbnd8seHGhHam-my2xN8-2MTiLgaFFv_9VgQ.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=06e53490e36528e0977a21e03221cf462a99f9a9","width":960,"height":527},{"url":"https://external-preview.redd.it/sl5AWBXJbnd8seHGhHam-my2xN8-2MTiLgaFFv_9VgQ.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=4e0749d24e4d075d2a9e19313c35d51775ca4918","width":1080,"height":593}],"variants":{},"id":"sl5AWBXJbnd8seHGhHam-my2xN8-2MTiLgaFFv_9VgQ"}],"enabled":false},"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"mod_note":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"num_reports":null,"removal_reason":null,"link_flair_background_color":"#ccac2b","id":"1llqp0a","is_robot_indexable":true,"num_duplicates":1,"report_reasons":null,"author":"Nuenki","discussion_type":null,"num_comments":35,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1llqp0a/the_more_llms_think_the_worse_they_translate/","stickied":false,"url":"https://nuenki.app/blog/the_more_llms_think_the_worse_they_translate","subreddit_subscribers":492232,"created_utc":1751020900,"num_crossposts":1,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0326h8","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Nuenki","can_mod_post":false,"send_replies":true,"parent_id":"t1_n030r9s","score":4,"author_fullname":"t2_14v7epz03x","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Interesting hypothesis. I'll include it in the next big test.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0326h8","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Interesting hypothesis. I&amp;#39;ll include it in the next big test.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1llqp0a","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1llqp0a/the_more_llms_think_the_worse_they_translate/n0326h8/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751039660,"author_flair_text":null,"treatment_tags":[],"created_utc":1751039660,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n05ktjw","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"mpasila","can_mod_post":false,"send_replies":true,"parent_id":"t1_n030r9s","score":1,"author_fullname":"t2_lhhagpdw","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I tried the new R1 that can also think in different languages and it butchered the translation much worse than V3.1.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n05ktjw","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I tried the new R1 that can also think in different languages and it butchered the translation much worse than V3.1.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1llqp0a","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1llqp0a/the_more_llms_think_the_worse_they_translate/n05ktjw/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751067072,"author_flair_text":null,"treatment_tags":[],"created_utc":1751067072,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n030r9s","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"stddealer","can_mod_post":false,"send_replies":true,"parent_id":"t1_n01m0lv","score":3,"author_fullname":"t2_5gk3j2hj","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yes but R1 (not zero) was taught to stick to English only for the reasoning. My hypothesis is that this may hurt its translation abilities?","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n030r9s","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yes but R1 (not zero) was taught to stick to English only for the reasoning. My hypothesis is that this may hurt its translation abilities?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1llqp0a","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1llqp0a/the_more_llms_think_the_worse_they_translate/n030r9s/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751039268,"author_flair_text":null,"treatment_tags":[],"created_utc":1751039268,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n01m0lv","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Nuenki","can_mod_post":false,"created_utc":1751022306,"send_replies":true,"parent_id":"t1_n01kepk","score":15,"author_fullname":"t2_14v7epz03x","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I've tested R1 (though not R1 zero) in my broader tests:\\n\\n[https://nuenki.app/blog/claude\\\\_4\\\\_is\\\\_good\\\\_at\\\\_translation\\\\_but\\\\_nothing\\\\_special](https://nuenki.app/blog/claude_4_is_good_at_translation_but_nothing_special)\\n\\nIt's... fine. In between old Deepseek V3 and new Deepseek V3 (which performs worse, as an interesting quirk).","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n01m0lv","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;ve tested R1 (though not R1 zero) in my broader tests:&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://nuenki.app/blog/claude_4_is_good_at_translation_but_nothing_special\\"&gt;https://nuenki.app/blog/claude_4_is_good_at_translation_but_nothing_special&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;It&amp;#39;s... fine. In between old Deepseek V3 and new Deepseek V3 (which performs worse, as an interesting quirk).&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1llqp0a","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1llqp0a/the_more_llms_think_the_worse_they_translate/n01m0lv/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751022306,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":15}}],"before":null}},"user_reports":[],"saved":false,"id":"n01kepk","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"stddealer","can_mod_post":false,"created_utc":1751021556,"send_replies":true,"parent_id":"t3_1llqp0a","score":30,"author_fullname":"t2_5gk3j2hj","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I wonder if the results would be the same for a model like R1 zero, which can mix languages in the chain of thought.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n01kepk","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I wonder if the results would be the same for a model like R1 zero, which can mix languages in the chain of thought.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1llqp0a/the_more_llms_think_the_worse_they_translate/n01kepk/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751021556,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1llqp0a","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":30}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n02bzdn","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"JustImmunity","can_mod_post":false,"created_utc":1751032056,"send_replies":true,"parent_id":"t1_n01o2gj","score":5,"author_fullname":"t2_c4pwgz16","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"heck qwen3 with think hasn’t given me bad results either, q5kxl","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n02bzdn","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;heck qwen3 with think hasn’t given me bad results either, q5kxl&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1llqp0a","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1llqp0a/the_more_llms_think_the_worse_they_translate/n02bzdn/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751032056,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n04sszl","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"IrisColt","can_mod_post":false,"created_utc":1751057770,"send_replies":true,"parent_id":"t1_n01o2gj","score":2,"author_fullname":"t2_c2f558x","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Thanks for the insight!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n04sszl","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thanks for the insight!&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1llqp0a","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1llqp0a/the_more_llms_think_the_worse_they_translate/n04sszl/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751057770,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n05kzyf","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"mpasila","can_mod_post":false,"created_utc":1751067133,"send_replies":true,"parent_id":"t1_n01o2gj","score":1,"author_fullname":"t2_lhhagpdw","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Which languages does Qwen 3 support?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n05kzyf","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Which languages does Qwen 3 support?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1llqp0a","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1llqp0a/the_more_llms_think_the_worse_they_translate/n05kzyf/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751067133,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n01o2gj","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"datbackup","can_mod_post":false,"created_utc":1751023237,"send_replies":true,"parent_id":"t3_1llqp0a","score":14,"author_fullname":"t2_ielo6","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Guess this explains why v3 0324 has become my goto for translating. Qwen3 with nothink is good too though","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n01o2gj","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Guess this explains why v3 0324 has become my goto for translating. Qwen3 with nothink is good too though&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1llqp0a/the_more_llms_think_the_worse_they_translate/n01o2gj/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751023237,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1llqp0a","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":14}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n021gbw","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"bones10145","can_mod_post":false,"created_utc":1751028544,"send_replies":true,"parent_id":"t3_1llqp0a","score":13,"author_fullname":"t2_4wqt1e9c","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Kinda like people. Ever overthink something and make it worse? Lol","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n021gbw","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Kinda like people. Ever overthink something and make it worse? Lol&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1llqp0a/the_more_llms_think_the_worse_they_translate/n021gbw/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751028544,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1llqp0a","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":13}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n02fsmi","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Nuenki","can_mod_post":false,"created_utc":1751033230,"send_replies":true,"parent_id":"t1_n02d6kj","score":5,"author_fullname":"t2_14v7epz03x","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Thanks for the link to the paper; I hadn't read that, and it seems quite relevant!\\n\\nI used non-reasoning models with reasoning instructions for this test, because I wanted to control for the variable of different RL techniques etc. However, I have tested reasoning vs non reasoning before, here:\\n\\n[https://nuenki.app/blog/claude\\\\_4\\\\_is\\\\_good\\\\_at\\\\_translation\\\\_but\\\\_nothing\\\\_special](https://nuenki.app/blog/claude_4_is_good_at_translation_but_nothing_special)\\n\\nIt shows that Gemini 2.5 flash is better with reasoning off, and R1 is slightly worse than V3.\\n\\nIt is weird - I had the same assumption, that self-critique would help. But apparently not!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n02fsmi","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thanks for the link to the paper; I hadn&amp;#39;t read that, and it seems quite relevant!&lt;/p&gt;\\n\\n&lt;p&gt;I used non-reasoning models with reasoning instructions for this test, because I wanted to control for the variable of different RL techniques etc. However, I have tested reasoning vs non reasoning before, here:&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://nuenki.app/blog/claude_4_is_good_at_translation_but_nothing_special\\"&gt;https://nuenki.app/blog/claude_4_is_good_at_translation_but_nothing_special&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;It shows that Gemini 2.5 flash is better with reasoning off, and R1 is slightly worse than V3.&lt;/p&gt;\\n\\n&lt;p&gt;It is weird - I had the same assumption, that self-critique would help. But apparently not!&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1llqp0a","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1llqp0a/the_more_llms_think_the_worse_they_translate/n02fsmi/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751033230,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n05rz7x","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"llmentry","can_mod_post":false,"send_replies":true,"parent_id":"t1_n03z8cw","score":1,"author_fullname":"t2_1lufy6yx6z","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Based on the published literature - real-time translation / bilingualism isn't slow language-based reasoning.  It's rapid, conceptual, and high-level.\\n\\n\\nSure, if you're agonising over the perfect word to match the exact language, Pevear and Volokhonsky style, yes CoT might be useful in some instances.  But these edge cases. For most uses of LLM translation, imagine forcing an internal monologue about the best use of language, in one language only, on yourself as you translate between two individuals!?  That's simply not going to help.\\n\\n\\nAnd as the OP demonstrates, it doesn't help.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n05rz7x","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Based on the published literature - real-time translation / bilingualism isn&amp;#39;t slow language-based reasoning.  It&amp;#39;s rapid, conceptual, and high-level.&lt;/p&gt;\\n\\n&lt;p&gt;Sure, if you&amp;#39;re agonising over the perfect word to match the exact language, Pevear and Volokhonsky style, yes CoT might be useful in some instances.  But these edge cases. For most uses of LLM translation, imagine forcing an internal monologue about the best use of language, in one language only, on yourself as you translate between two individuals!?  That&amp;#39;s simply not going to help.&lt;/p&gt;\\n\\n&lt;p&gt;And as the OP demonstrates, it doesn&amp;#39;t help.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1llqp0a","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1llqp0a/the_more_llms_think_the_worse_they_translate/n05rz7x/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751069599,"author_flair_text":null,"treatment_tags":[],"created_utc":1751069599,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n03z8cw","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"himself_v","can_mod_post":false,"send_replies":true,"parent_id":"t1_n02rcpp","score":2,"author_fullname":"t2_3tsyg","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Translators think about things all the time. Sometimes you have to iterate until you find the perfect words, write essays on what's happening in the scene to figure out what's this subtle intonation which the source has and your version misses.\\n\\nSometimes it just works, and sure, when it does it does.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n03z8cw","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Translators think about things all the time. Sometimes you have to iterate until you find the perfect words, write essays on what&amp;#39;s happening in the scene to figure out what&amp;#39;s this subtle intonation which the source has and your version misses.&lt;/p&gt;\\n\\n&lt;p&gt;Sometimes it just works, and sure, when it does it does.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1llqp0a","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1llqp0a/the_more_llms_think_the_worse_they_translate/n03z8cw/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751048941,"author_flair_text":null,"treatment_tags":[],"created_utc":1751048941,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n02rcpp","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"llmentry","can_mod_post":false,"created_utc":1751036604,"send_replies":true,"parent_id":"t1_n02d6kj","score":3,"author_fullname":"t2_1lufy6yx6z","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"&gt;I think translation should actually benefit form the pre-response reasoning chain, given that it would allow for self-critique to happen.\\n\\nI don't think this follows at all.  Translation in bilingual individuals does not involve language-mediated reasoning, but rather high-level conceptual reasoning.  Forcing a model to use a language-based CoT for translation is actually a terrible idea, and if LLMs work anything like our own brains it's guaranteed to be counter-productive.\\n\\nThis fits perfectly with the paper you cited, btw -- the idea that CoT reasoning is only useful for tasks where we find it useful ourselves.  (If only model creators could take note of this!  CoT reasoning is not a one-size-fits-all solution.)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n02rcpp","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;I think translation should actually benefit form the pre-response reasoning chain, given that it would allow for self-critique to happen.&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;I don&amp;#39;t think this follows at all.  Translation in bilingual individuals does not involve language-mediated reasoning, but rather high-level conceptual reasoning.  Forcing a model to use a language-based CoT for translation is actually a terrible idea, and if LLMs work anything like our own brains it&amp;#39;s guaranteed to be counter-productive.&lt;/p&gt;\\n\\n&lt;p&gt;This fits perfectly with the paper you cited, btw -- the idea that CoT reasoning is only useful for tasks where we find it useful ourselves.  (If only model creators could take note of this!  CoT reasoning is not a one-size-fits-all solution.)&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1llqp0a","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1llqp0a/the_more_llms_think_the_worse_they_translate/n02rcpp/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751036604,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n035vh1","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"abreakfromlurking","can_mod_post":false,"created_utc":1751040698,"send_replies":true,"parent_id":"t1_n02d6kj","score":2,"author_fullname":"t2_9mylt6k7","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"&gt; I think translation should actually benefit form the pre-response reasoning chain, given that it would allow for self-critique to happen.\\n\\nDid my own testing recently and that's what I've observed as well. However, the results of the tests were actually quite more nuanced than I had anticipated. Published the translation analysis [here](https://elusivewordsmith.com/reflections/crosslinguistic-musings/#the-translation-analysis) and if you don't feel like reading through all of that, here's the [reasoning section](https://elusivewordsmith.com/reflections/crosslinguistic-musings/#the-reasoning-comparison). Tldr: Just a bit of casual research comparing how LLMs handle a small syntactic challenge and a pun (source language: English; target languages: German and French).","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n035vh1","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;I think translation should actually benefit form the pre-response reasoning chain, given that it would allow for self-critique to happen.&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;Did my own testing recently and that&amp;#39;s what I&amp;#39;ve observed as well. However, the results of the tests were actually quite more nuanced than I had anticipated. Published the translation analysis &lt;a href=\\"https://elusivewordsmith.com/reflections/crosslinguistic-musings/#the-translation-analysis\\"&gt;here&lt;/a&gt; and if you don&amp;#39;t feel like reading through all of that, here&amp;#39;s the &lt;a href=\\"https://elusivewordsmith.com/reflections/crosslinguistic-musings/#the-reasoning-comparison\\"&gt;reasoning section&lt;/a&gt;. Tldr: Just a bit of casual research comparing how LLMs handle a small syntactic challenge and a pun (source language: English; target languages: German and French).&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1llqp0a","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1llqp0a/the_more_llms_think_the_worse_they_translate/n035vh1/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751040698,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n02d6kj","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"FullOf_Bad_Ideas","can_mod_post":false,"created_utc":1751032428,"send_replies":true,"parent_id":"t3_1llqp0a","score":11,"author_fullname":"t2_9s7pmakgx","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Read this if you haven't - https://arxiv.org/abs/2410.21333\\n\\nIt looks like you're also mostly testing non-reasoning models and asking them to reason, that's substantially different than using models specifically trained to reason before answering.\\n\\nI think translation should actually benefit form the pre-response reasoning chain, given that it would allow for self-critique to happen.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n02d6kj","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Read this if you haven&amp;#39;t - &lt;a href=\\"https://arxiv.org/abs/2410.21333\\"&gt;https://arxiv.org/abs/2410.21333&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;It looks like you&amp;#39;re also mostly testing non-reasoning models and asking them to reason, that&amp;#39;s substantially different than using models specifically trained to reason before answering.&lt;/p&gt;\\n\\n&lt;p&gt;I think translation should actually benefit form the pre-response reasoning chain, given that it would allow for self-critique to happen.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1llqp0a/the_more_llms_think_the_worse_they_translate/n02d6kj/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751032428,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1llqp0a","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":11}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n02xxdn","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"s101c","can_mod_post":false,"send_replies":true,"parent_id":"t1_n02swx0","score":2,"author_fullname":"t2_rg6hb6my5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I have been using it in aistudio.google.com, and back then it was showing the entire thinking process.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n02xxdn","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I have been using it in aistudio.google.com, and back then it was showing the entire thinking process.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1llqp0a","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1llqp0a/the_more_llms_think_the_worse_they_translate/n02xxdn/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751038476,"author_flair_text":null,"treatment_tags":[],"created_utc":1751038476,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n02swx0","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"llmentry","can_mod_post":false,"created_utc":1751037049,"send_replies":true,"parent_id":"t1_n01yi8j","score":2,"author_fullname":"t2_1lufy6yx6z","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"&gt;Gemini 2.5 succeeded **because** it was thinking, it was selecting the contextually correct translation inside the thinking process, word-by-word.\\n\\nBut Gemini doesn't reveal it's CoT tokens (the models only output a very-high-level summary of the CoT) -- so how can you be sure this is what it was doing?   Languages generally don't perfectly map 1:1 token:token, and grammatical structure is often very different, so I'd also be surprised if a word-by-word translation process could work at all ...","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n02swx0","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;Gemini 2.5 succeeded &lt;strong&gt;because&lt;/strong&gt; it was thinking, it was selecting the contextually correct translation inside the thinking process, word-by-word.&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;But Gemini doesn&amp;#39;t reveal it&amp;#39;s CoT tokens (the models only output a very-high-level summary of the CoT) -- so how can you be sure this is what it was doing?   Languages generally don&amp;#39;t perfectly map 1:1 token:token, and grammatical structure is often very different, so I&amp;#39;d also be surprised if a word-by-word translation process could work at all ...&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1llqp0a","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1llqp0a/the_more_llms_think_the_worse_they_translate/n02swx0/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751037049,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n023b3a","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AppearanceHeavy6724","can_mod_post":false,"created_utc":1751029193,"send_replies":true,"parent_id":"t1_n01yi8j","score":3,"author_fullname":"t2_uz37qfx5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"&gt;  Gemini 2.5 succeeded because it was thinking\\n\\nWe are not talking about non-local though. In most cases with local models natural text processing tasks suffer with CoT.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n023b3a","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;Gemini 2.5 succeeded because it was thinking&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;We are not talking about non-local though. In most cases with local models natural text processing tasks suffer with CoT.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1llqp0a","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1llqp0a/the_more_llms_think_the_worse_they_translate/n023b3a/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751029193,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n01yi8j","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"s101c","can_mod_post":false,"created_utc":1751027470,"send_replies":true,"parent_id":"t3_1llqp0a","score":5,"author_fullname":"t2_rg6hb6my5","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Not true in my tests. Gemini 2.5 Experimental has provided the most correct, contextually-aware translation.\\n\\nThe original text was a one-page document from our contractor with specific terminology which translates differently if mentioned in a general conversation.\\n\\nClaude, R1, Mistral Le Chat, GPT-4o all failed and provided vague or incorrect bits in the translation. Gemini 2.5 succeeded **because** it was thinking, it was selecting the contextually correct translation inside the thinking process, word-by-word.\\n\\nThe only downside is that Gemini 2.5 was not able to translate long texts, this worked only with texts the size of a long e-mail.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n01yi8j","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Not true in my tests. Gemini 2.5 Experimental has provided the most correct, contextually-aware translation.&lt;/p&gt;\\n\\n&lt;p&gt;The original text was a one-page document from our contractor with specific terminology which translates differently if mentioned in a general conversation.&lt;/p&gt;\\n\\n&lt;p&gt;Claude, R1, Mistral Le Chat, GPT-4o all failed and provided vague or incorrect bits in the translation. Gemini 2.5 succeeded &lt;strong&gt;because&lt;/strong&gt; it was thinking, it was selecting the contextually correct translation inside the thinking process, word-by-word.&lt;/p&gt;\\n\\n&lt;p&gt;The only downside is that Gemini 2.5 was not able to translate long texts, this worked only with texts the size of a long e-mail.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1llqp0a/the_more_llms_think_the_worse_they_translate/n01yi8j/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751027470,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1llqp0a","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0258o3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"davidgutierrezpalma","can_mod_post":false,"send_replies":true,"parent_id":"t1_n01xail","score":2,"author_fullname":"t2_u88u3","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Thank you for the info. It is really really useful.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0258o3","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thank you for the info. It is really really useful.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1llqp0a","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1llqp0a/the_more_llms_think_the_worse_they_translate/n0258o3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751029858,"author_flair_text":null,"treatment_tags":[],"created_utc":1751029858,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n01xail","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Nuenki","can_mod_post":false,"created_utc":1751027012,"send_replies":true,"parent_id":"t1_n01vlgz","score":8,"author_fullname":"t2_14v7epz03x","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yeah, so\\n\\n\\\\- A translation generated from the combined outputs of several non-thinking models beats a single model\\n\\n\\\\- If you use a single model, telling it to think beforehand makes it perform worse.\\n\\n\\\\- If you use a single model, passing a new instance of the model its earlier translation and asking it to critique and make a new one makes it perform much worse. Interestingly this is despite the fact that LLMs are pretty decent at evaluating translations, with high agreement with other metrics and a good ability to discern differences - just not acting on them.\\n\\n*-* Doing both makes it even worse than that.\\n\\nI didn't use RLd thinking models because it's another variable in the test, but I have some data on them here\\\\[0\\\\] and it gives a similar picture. I also fairly frequently talk with other people who are doing this kind of testing, and they've anecdotally agreed that thinking doesn't seem to help.\\n\\n\\\\[0\\\\] [https://nuenki.app/blog/claude\\\\_4\\\\_is\\\\_good\\\\_at\\\\_translation\\\\_but\\\\_nothing\\\\_special](https://nuenki.app/blog/claude_4_is_good_at_translation_but_nothing_special)\\n\\nEdit: Oh and I also tested this using a more academic-standard route, using a model that's finetuned to evaluate translations against a base reference, and it agreed with the data - I just stuck with the current visualisations for the sake of the blog post.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n01xail","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yeah, so&lt;/p&gt;\\n\\n&lt;p&gt;- A translation generated from the combined outputs of several non-thinking models beats a single model&lt;/p&gt;\\n\\n&lt;p&gt;- If you use a single model, telling it to think beforehand makes it perform worse.&lt;/p&gt;\\n\\n&lt;p&gt;- If you use a single model, passing a new instance of the model its earlier translation and asking it to critique and make a new one makes it perform much worse. Interestingly this is despite the fact that LLMs are pretty decent at evaluating translations, with high agreement with other metrics and a good ability to discern differences - just not acting on them.&lt;/p&gt;\\n\\n&lt;p&gt;&lt;em&gt;-&lt;/em&gt; Doing both makes it even worse than that.&lt;/p&gt;\\n\\n&lt;p&gt;I didn&amp;#39;t use RLd thinking models because it&amp;#39;s another variable in the test, but I have some data on them here[0] and it gives a similar picture. I also fairly frequently talk with other people who are doing this kind of testing, and they&amp;#39;ve anecdotally agreed that thinking doesn&amp;#39;t seem to help.&lt;/p&gt;\\n\\n&lt;p&gt;[0] &lt;a href=\\"https://nuenki.app/blog/claude_4_is_good_at_translation_but_nothing_special\\"&gt;https://nuenki.app/blog/claude_4_is_good_at_translation_but_nothing_special&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;Edit: Oh and I also tested this using a more academic-standard route, using a model that&amp;#39;s finetuned to evaluate translations against a base reference, and it agreed with the data - I just stuck with the current visualisations for the sake of the blog post.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1llqp0a","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1llqp0a/the_more_llms_think_the_worse_they_translate/n01xail/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751027012,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":8}}],"before":null}},"user_reports":[],"saved":false,"id":"n01vlgz","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"davidgutierrezpalma","can_mod_post":false,"created_utc":1751026365,"send_replies":true,"parent_id":"t3_1llqp0a","score":3,"author_fullname":"t2_u88u3","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I'm not sure if I'm understanding it correctly and I haven't looked at the source code at the repository yet, but...\\n\\nDoes this article mean \\"a translation generated from the combined outputs of several non-thinking models\\" is better than the translation generated by a single model... but if you can only use a single model, it's better to use a non-thinking model than a thinking model?\\n\\nCan anybody confirm if I have understood it correctly?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n01vlgz","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;m not sure if I&amp;#39;m understanding it correctly and I haven&amp;#39;t looked at the source code at the repository yet, but...&lt;/p&gt;\\n\\n&lt;p&gt;Does this article mean &amp;quot;a translation generated from the combined outputs of several non-thinking models&amp;quot; is better than the translation generated by a single model... but if you can only use a single model, it&amp;#39;s better to use a non-thinking model than a thinking model?&lt;/p&gt;\\n\\n&lt;p&gt;Can anybody confirm if I have understood it correctly?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1llqp0a/the_more_llms_think_the_worse_they_translate/n01vlgz/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751026365,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1llqp0a","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n029z5v","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Nuenki","can_mod_post":false,"created_utc":1751031423,"send_replies":true,"parent_id":"t1_n027sou","score":2,"author_fullname":"t2_14v7epz03x","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"They're tiny models. The trend of AI research over the last year or so has been to apply reinforcement learning to small models so that they can reason through problems systematically. That works well for most tasks, but translation really benefits from better base models, increased parameters, and more \\"world knowledge\\", rather than reinforcement learning. They need to know what's correct in order to apply it!\\n\\nAnd yeah, everyone I've spoken to about it has observed similar effects. Of course thinking helps in some cases - there's an anecdote in this thread about gemini 2.5 - but, in aggregate, it doesn't work very well. You *can* beat simply asking a large model for a one-shot translation, but you need to be cleverer about it than just asking them to reason!\\n\\nI think it's also quite interesting that thinking tends to dramatically increase variance, rather than just decreasing the mean.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n029z5v","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;They&amp;#39;re tiny models. The trend of AI research over the last year or so has been to apply reinforcement learning to small models so that they can reason through problems systematically. That works well for most tasks, but translation really benefits from better base models, increased parameters, and more &amp;quot;world knowledge&amp;quot;, rather than reinforcement learning. They need to know what&amp;#39;s correct in order to apply it!&lt;/p&gt;\\n\\n&lt;p&gt;And yeah, everyone I&amp;#39;ve spoken to about it has observed similar effects. Of course thinking helps in some cases - there&amp;#39;s an anecdote in this thread about gemini 2.5 - but, in aggregate, it doesn&amp;#39;t work very well. You &lt;em&gt;can&lt;/em&gt; beat simply asking a large model for a one-shot translation, but you need to be cleverer about it than just asking them to reason!&lt;/p&gt;\\n\\n&lt;p&gt;I think it&amp;#39;s also quite interesting that thinking tends to dramatically increase variance, rather than just decreasing the mean.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1llqp0a","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1llqp0a/the_more_llms_think_the_worse_they_translate/n029z5v/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751031423,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n027sou","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Quagmirable","can_mod_post":false,"created_utc":1751030707,"send_replies":true,"parent_id":"t3_1llqp0a","score":3,"author_fullname":"t2_17i8f5bprh","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Interesting, that's exactly what I observed in these two recent posts as well:\\n\\n- https://www.reddit.com/r/LocalLLaMA/comments/1lkls2v/has_anybody_else_found_deepseek_r1_0528_qwen3_8b/\\n- https://www.reddit.com/r/LocalLLaMA/comments/1lkls2v/has_anybody_else_found_deepseek_r1_0528_qwen3_8b/mztntnu/","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n027sou","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Interesting, that&amp;#39;s exactly what I observed in these two recent posts as well:&lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;&lt;a href=\\"https://www.reddit.com/r/LocalLLaMA/comments/1lkls2v/has_anybody_else_found_deepseek_r1_0528_qwen3_8b/\\"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1lkls2v/has_anybody_else_found_deepseek_r1_0528_qwen3_8b/&lt;/a&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;a href=\\"https://www.reddit.com/r/LocalLLaMA/comments/1lkls2v/has_anybody_else_found_deepseek_r1_0528_qwen3_8b/mztntnu/\\"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1lkls2v/has_anybody_else_found_deepseek_r1_0528_qwen3_8b/mztntnu/&lt;/a&gt;&lt;/li&gt;\\n&lt;/ul&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1llqp0a/the_more_llms_think_the_worse_they_translate/n027sou/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751030707,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1llqp0a","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n025d0j","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ahmetegesel","can_mod_post":false,"created_utc":1751029897,"send_replies":true,"parent_id":"t3_1llqp0a","score":2,"author_fullname":"t2_69skhb61","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Not really sure, R1 and Qwen3 were better with reasoning in English-Finnish translation. Isn’t it also depending on prompting, models own capabilities, training set etc?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n025d0j","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Not really sure, R1 and Qwen3 were better with reasoning in English-Finnish translation. Isn’t it also depending on prompting, models own capabilities, training set etc?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1llqp0a/the_more_llms_think_the_worse_they_translate/n025d0j/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751029897,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1llqp0a","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n02pfn2","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"viag","can_mod_post":false,"send_replies":true,"parent_id":"t1_n02ll9g","score":2,"author_fullname":"t2_i3e2x","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Ok, thanks for the clarification, it's really nice to see that you're experimenting with your evaluation process and taking a hands-on approach to the subject! So, good job on the methodology ;)\\n\\nI'm doing research in NLP but translation isn't my field at all, so I honestly don't know which metrics are currently used. I think it would also be interesting to define multiple evaluation dimensions (such as preservation of tone, cultural nuances, etc.) instead of just a global \\"quality\\" metric. This could provide a more fine-grained view of the differences between the various models.\\n\\nThanks for taking the time to answer and good luck with your app!","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n02pfn2","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Ok, thanks for the clarification, it&amp;#39;s really nice to see that you&amp;#39;re experimenting with your evaluation process and taking a hands-on approach to the subject! So, good job on the methodology ;)&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;m doing research in NLP but translation isn&amp;#39;t my field at all, so I honestly don&amp;#39;t know which metrics are currently used. I think it would also be interesting to define multiple evaluation dimensions (such as preservation of tone, cultural nuances, etc.) instead of just a global &amp;quot;quality&amp;quot; metric. This could provide a more fine-grained view of the differences between the various models.&lt;/p&gt;\\n\\n&lt;p&gt;Thanks for taking the time to answer and good luck with your app!&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1llqp0a","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1llqp0a/the_more_llms_think_the_worse_they_translate/n02pfn2/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751036056,"author_flair_text":null,"treatment_tags":[],"created_utc":1751036056,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n02ll9g","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Nuenki","can_mod_post":false,"created_utc":1751034936,"send_replies":true,"parent_id":"t1_n02k43x","score":1,"author_fullname":"t2_14v7epz03x","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"LLM-as-a-judge. For this test I just used one LLM; for the broader ones I tend to use a corpus of them, and you can turn them on and off to compare them. Here's the latest big model comparison:\\n\\n[https://nuenki.app/blog/claude\\\\_4\\\\_is\\\\_good\\\\_at\\\\_translation\\\\_but\\\\_nothing\\\\_special](https://nuenki.app/blog/claude_4_is_good_at_translation_but_nothing_special)\\n\\nI've experimented with various metrics, including semantic distance, \\"coherence\\" (translate back and forth a few times, then take semantic distance), and the ones academics like (sadly I accidentally deleted that code while clearing out my hard drive... I was trying to get rid of the cached model, not the code!), and they all correlate quite closely with LLM evaluation.\\n\\nThere also isn't much bias between LLMs, as you can see if you mess with the blog post above, which was a pleasant surprise. So that's what my current pipeline prefers. Some of the older blogs have a slightly different approach.\\n\\nI also messed with pairwise evaluations over two different experiments, and after £150 in openrouter credits with zero usable results (I'm 19 and this tool doesn't make much money, so that's quite a lot for me) I wrote a blog post about why I was abandoning that:\\n\\n[https://nuenki.app/blog/experimentation\\\\_matters\\\\_why\\\\_we\\\\_arent\\\\_using\\\\_pairwise](https://nuenki.app/blog/experimentation_matters_why_we_arent_using_pairwise)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n02ll9g","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;LLM-as-a-judge. For this test I just used one LLM; for the broader ones I tend to use a corpus of them, and you can turn them on and off to compare them. Here&amp;#39;s the latest big model comparison:&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://nuenki.app/blog/claude_4_is_good_at_translation_but_nothing_special\\"&gt;https://nuenki.app/blog/claude_4_is_good_at_translation_but_nothing_special&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;ve experimented with various metrics, including semantic distance, &amp;quot;coherence&amp;quot; (translate back and forth a few times, then take semantic distance), and the ones academics like (sadly I accidentally deleted that code while clearing out my hard drive... I was trying to get rid of the cached model, not the code!), and they all correlate quite closely with LLM evaluation.&lt;/p&gt;\\n\\n&lt;p&gt;There also isn&amp;#39;t much bias between LLMs, as you can see if you mess with the blog post above, which was a pleasant surprise. So that&amp;#39;s what my current pipeline prefers. Some of the older blogs have a slightly different approach.&lt;/p&gt;\\n\\n&lt;p&gt;I also messed with pairwise evaluations over two different experiments, and after £150 in openrouter credits with zero usable results (I&amp;#39;m 19 and this tool doesn&amp;#39;t make much money, so that&amp;#39;s quite a lot for me) I wrote a blog post about why I was abandoning that:&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://nuenki.app/blog/experimentation_matters_why_we_arent_using_pairwise\\"&gt;https://nuenki.app/blog/experimentation_matters_why_we_arent_using_pairwise&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1llqp0a","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1llqp0a/the_more_llms_think_the_worse_they_translate/n02ll9g/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751034936,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n02k43x","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"viag","can_mod_post":false,"created_utc":1751034502,"send_replies":true,"parent_id":"t3_1llqp0a","score":2,"author_fullname":"t2_i3e2x","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"It's great to see people actually evaluating models! Maybe I read through your blog a bit too quickly, but I can't seem to find which metric you used to evaluate the translation quality? Is it a LLM-as-a-judge ? (and the judge  would be google/gemini-2.5-flash-preview ?) Or is it something like BLEU ?\\n\\nIt would be interesting to check with various metrics, because each one might bias the results a certain way..","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n02k43x","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It&amp;#39;s great to see people actually evaluating models! Maybe I read through your blog a bit too quickly, but I can&amp;#39;t seem to find which metric you used to evaluate the translation quality? Is it a LLM-as-a-judge ? (and the judge  would be google/gemini-2.5-flash-preview ?) Or is it something like BLEU ?&lt;/p&gt;\\n\\n&lt;p&gt;It would be interesting to check with various metrics, because each one might bias the results a certain way..&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1llqp0a/the_more_llms_think_the_worse_they_translate/n02k43x/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751034502,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1llqp0a","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n04vj04","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"BidWestern1056","can_mod_post":false,"created_utc":1751058611,"send_replies":true,"parent_id":"t3_1llqp0a","score":2,"author_fullname":"t2_uzxql7po","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"we touch on this a bit in this paper: [https://arxiv.org/pdf/2506.10077](https://arxiv.org/pdf/2506.10077)\\n\\nessentially any such natural language translation task is beleaguered by these fundamental limitations inherent to natural language itself. it is non-algorithmic, it cannot be \\"encoded\\" in a truly meaningful way with the current ways we are doing things, and it will always fail at these edge cases when complexity gets too high for it to manage all the potential dependencies.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n04vj04","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;we touch on this a bit in this paper: &lt;a href=\\"https://arxiv.org/pdf/2506.10077\\"&gt;https://arxiv.org/pdf/2506.10077&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;essentially any such natural language translation task is beleaguered by these fundamental limitations inherent to natural language itself. it is non-algorithmic, it cannot be &amp;quot;encoded&amp;quot; in a truly meaningful way with the current ways we are doing things, and it will always fail at these edge cases when complexity gets too high for it to manage all the potential dependencies.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1llqp0a/the_more_llms_think_the_worse_they_translate/n04vj04/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751058611,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1llqp0a","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n05o3gy","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Possible-Moment-6313","can_mod_post":false,"created_utc":1751068202,"send_replies":true,"parent_id":"t3_1llqp0a","score":2,"author_fullname":"t2_9djkbrte","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Right tool for the job.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n05o3gy","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Right tool for the job.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1llqp0a/the_more_llms_think_the_worse_they_translate/n05o3gy/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751068202,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1llqp0a","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n06yhn3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Kooky-Net784","can_mod_post":false,"created_utc":1751087041,"send_replies":true,"parent_id":"t3_1llqp0a","score":1,"author_fullname":"t2_1c7x793800","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"What's your favorite multi-language open-source LLM?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n06yhn3","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;What&amp;#39;s your favorite multi-language open-source LLM?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1llqp0a/the_more_llms_think_the_worse_they_translate/n06yhn3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751087041,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1llqp0a","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n079vwe","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Sicarius_The_First","can_mod_post":false,"created_utc":1751093006,"send_replies":true,"parent_id":"t3_1llqp0a","score":0,"author_fullname":"t2_ik8czvp65","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I said it when refelction 70b was released. thinking is a meme. stop with this nonsense.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n079vwe","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I said it when refelction 70b was released. thinking is a meme. stop with this nonsense.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1llqp0a/the_more_llms_think_the_worse_they_translate/n079vwe/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751093006,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1llqp0a","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n07acva","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"kumonovel","can_mod_post":false,"created_utc":1751093260,"send_replies":true,"parent_id":"t3_1llqp0a","score":1,"author_fullname":"t2_lqps7j7f","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I don't think broad claims like these can be based on the evaluations you provided. After reading your comments saying you used LLM as a judge... That is basically just a very tiny indicator.\\n\\nI'm not saying it is useless as one of many indicators sure, but currently I have not seen any automatic evaluation, neither model nor statisical based, that gives an acurate indication of GOOD translations. Even comet is flawed tremendously, favoring accuracy over readability every day of the weak. Good translation is not a word by word translation, but a conversion of language.\\n\\nIn some regards lower scores could mean that the translation became better, cause the models stop adhering to literal translations and moving to more infered/meaning based translation which automatic systems penalize heavily.\\n\\nGood work, but maybe stop with clickbaity headlines?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n07acva","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I don&amp;#39;t think broad claims like these can be based on the evaluations you provided. After reading your comments saying you used LLM as a judge... That is basically just a very tiny indicator.&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;m not saying it is useless as one of many indicators sure, but currently I have not seen any automatic evaluation, neither model nor statisical based, that gives an acurate indication of GOOD translations. Even comet is flawed tremendously, favoring accuracy over readability every day of the weak. Good translation is not a word by word translation, but a conversion of language.&lt;/p&gt;\\n\\n&lt;p&gt;In some regards lower scores could mean that the translation became better, cause the models stop adhering to literal translations and moving to more infered/meaning based translation which automatic systems penalize heavily.&lt;/p&gt;\\n\\n&lt;p&gt;Good work, but maybe stop with clickbaity headlines?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1llqp0a/the_more_llms_think_the_worse_they_translate/n07acva/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751093260,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1llqp0a","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0212rz","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Kooky-Somewhere-2883","can_mod_post":false,"created_utc":1751028411,"send_replies":true,"parent_id":"t3_1llqp0a","score":1,"author_fullname":"t2_16kjuck66n","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"We have an overthinking section in Jan-nano technical report coming soon\\n\\nI’m Alan author of Jan-nano","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0212rz","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;We have an overthinking section in Jan-nano technical report coming soon&lt;/p&gt;\\n\\n&lt;p&gt;I’m Alan author of Jan-nano&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1llqp0a/the_more_llms_think_the_worse_they_translate/n0212rz/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751028411,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1llqp0a","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),o=()=>e.jsx(t,{data:l});export{o as default};
