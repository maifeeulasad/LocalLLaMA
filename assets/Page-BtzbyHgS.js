import{j as e}from"./index-cvG704yx.js";import{R as l}from"./RedditPostRenderer-CBthLTAH.js";import"./index-D-GavSZU.js";const t=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Came across a discussion in ik\\\\_llama.cpp by accident where the main developer (ikawrakow) is soliciting feedback about whether they should focus on improving the performance of the Vulkan backend on ik\\\\_llama.cpp.\\n\\nThe discussion is 2 weeks old, but hasn't garnered much attention until now.\\n\\nI think improved Vulkan performance in this project will benefit the community a lot. As I commented in that discussion, these are my arguments in favor of ikawrakow giving the Vulkan backend more attention:\\n\\n* This project doesn't get that much attention on reddit, etc compared to llama.cpp. So, he current userbase is a lot smaller. Having this question in the discussions, while appropriate, won't attract that much attention.\\n* Vulkan is the only backend that's not tied to a specific vendor. Any optimization you make there will be useful on all GPUs, discrete or otherwise. If you can bring Vulkan close to parity with CUDA, it will be a huge win for any device that supports Vulkan, including older GPUs from Nvidia and AMD.\\n* As firecoperana noted, not all quants need to be supported. A handful of the recent IQs used in recent MoE's like Qwen3-235B, DeepSeek-671B, and Kimi-K2 are more than enough. I'd even argue for supporting only power of two IQ quants only initially to limit scope and effort.\\n* Inte's A770 is now arguably the cheapest 16GB GPU with decent compute and memory bandwidth, but it doesn't get much attention in the community. Vulkan support would benefit those of us running Arcs, and free us from having to fiddle with OneAPI.\\n\\nIf you own AMD or Intel GPUs, I'd urge you to check this discussion and vote in favor of improving Vulkan performance.\\n\\n[Link to the discussion](https://github.com/ikawrakow/ik_llama.cpp/discussions/590)","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Help vote for improved Vulkan performance in ik_llama.cpp","link_flair_richtext":[{"e":"text","t":"Discussion"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1m2o3ht","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.88,"author_flair_background_color":null,"subreddit_type":"public","ups":40,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_17n3nqtj56","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Discussion","can_mod_post":false,"score":40,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"post_hint":"self","content_categories":null,"is_self":true,"mod_note":null,"created":1752798542,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Came across a discussion in ik_llama.cpp by accident where the main developer (ikawrakow) is soliciting feedback about whether they should focus on improving the performance of the Vulkan backend on ik_llama.cpp.&lt;/p&gt;\\n\\n&lt;p&gt;The discussion is 2 weeks old, but hasn&amp;#39;t garnered much attention until now.&lt;/p&gt;\\n\\n&lt;p&gt;I think improved Vulkan performance in this project will benefit the community a lot. As I commented in that discussion, these are my arguments in favor of ikawrakow giving the Vulkan backend more attention:&lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;This project doesn&amp;#39;t get that much attention on reddit, etc compared to llama.cpp. So, he current userbase is a lot smaller. Having this question in the discussions, while appropriate, won&amp;#39;t attract that much attention.&lt;/li&gt;\\n&lt;li&gt;Vulkan is the only backend that&amp;#39;s not tied to a specific vendor. Any optimization you make there will be useful on all GPUs, discrete or otherwise. If you can bring Vulkan close to parity with CUDA, it will be a huge win for any device that supports Vulkan, including older GPUs from Nvidia and AMD.&lt;/li&gt;\\n&lt;li&gt;As firecoperana noted, not all quants need to be supported. A handful of the recent IQs used in recent MoE&amp;#39;s like Qwen3-235B, DeepSeek-671B, and Kimi-K2 are more than enough. I&amp;#39;d even argue for supporting only power of two IQ quants only initially to limit scope and effort.&lt;/li&gt;\\n&lt;li&gt;Inte&amp;#39;s A770 is now arguably the cheapest 16GB GPU with decent compute and memory bandwidth, but it doesn&amp;#39;t get much attention in the community. Vulkan support would benefit those of us running Arcs, and free us from having to fiddle with OneAPI.&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;p&gt;If you own AMD or Intel GPUs, I&amp;#39;d urge you to check this discussion and vote in favor of improving Vulkan performance.&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://github.com/ikawrakow/ik_llama.cpp/discussions/590\\"&gt;Link to the discussion&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"preview":{"images":[{"source":{"url":"https://external-preview.redd.it/uuVIObmdWTAFZrRZFFKEQ5BD6wxuznomv2LWwczGh00.png?auto=webp&amp;s=c2cdcc28984c0c84b65c406f3b359b0160900ffb","width":1200,"height":600},"resolutions":[{"url":"https://external-preview.redd.it/uuVIObmdWTAFZrRZFFKEQ5BD6wxuznomv2LWwczGh00.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=023fad5bb3042f9c25f3691bf539604b94e0d923","width":108,"height":54},{"url":"https://external-preview.redd.it/uuVIObmdWTAFZrRZFFKEQ5BD6wxuznomv2LWwczGh00.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=b33d52b746a327e6046fb93dee2227878f4154a2","width":216,"height":108},{"url":"https://external-preview.redd.it/uuVIObmdWTAFZrRZFFKEQ5BD6wxuznomv2LWwczGh00.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=cb59aba7ddedfdce7ccae33ef43118527650465f","width":320,"height":160},{"url":"https://external-preview.redd.it/uuVIObmdWTAFZrRZFFKEQ5BD6wxuznomv2LWwczGh00.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=59120906a80c077456d9f83087a3ef84c4a3e7b8","width":640,"height":320},{"url":"https://external-preview.redd.it/uuVIObmdWTAFZrRZFFKEQ5BD6wxuznomv2LWwczGh00.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=ea6eced43229461188a9533a589ce5bb252bb21d","width":960,"height":480},{"url":"https://external-preview.redd.it/uuVIObmdWTAFZrRZFFKEQ5BD6wxuznomv2LWwczGh00.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=9c4a0ec54efb9faa1182d4949448afe98ce51536","width":1080,"height":540}],"variants":{},"id":"uuVIObmdWTAFZrRZFFKEQ5BD6wxuznomv2LWwczGh00"}],"enabled":false},"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"5f921ea4-c7bc-11ed-9c23-3a00622979b4","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#646d73","id":"1m2o3ht","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"FullstackSensei","discussion_type":null,"num_comments":12,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1m2o3ht/help_vote_for_improved_vulkan_performance_in_ik/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1m2o3ht/help_vote_for_improved_vulkan_performance_in_ik/","subreddit_subscribers":501077,"created_utc":1752798542,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3qlvo2","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"No_Efficiency_1144","can_mod_post":false,"created_utc":1752801775,"send_replies":true,"parent_id":"t3_1m2o3ht","score":12,"author_fullname":"t2_1nkj9l14b0","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Better Vulkan performance is always nice yeah","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3qlvo2","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Better Vulkan performance is always nice yeah&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2o3ht/help_vote_for_improved_vulkan_performance_in_ik/n3qlvo2/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752801775,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m2o3ht","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":12}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3s9orx","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"FullstackSensei","can_mod_post":false,"created_utc":1752829426,"send_replies":true,"parent_id":"t1_n3quc0h","score":1,"author_fullname":"t2_17n3nqtj56","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yeah, took me a couple of hours to figure how to setup SYCL after downloading and installing OneAPI. Trying to compile ik_llama.cpp against SYCL was how I found that discussion","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3s9orx","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yeah, took me a couple of hours to figure how to setup SYCL after downloading and installing OneAPI. Trying to compile ik_llama.cpp against SYCL was how I found that discussion&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2o3ht","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2o3ht/help_vote_for_improved_vulkan_performance_in_ik/n3s9orx/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752829426,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3quc0h","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"fallingdowndizzyvr","can_mod_post":false,"created_utc":1752804856,"send_replies":true,"parent_id":"t3_1m2o3ht","score":9,"author_fullname":"t2_o65i6kx","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I fully support more Vulkan anywhere.\\n\\n&gt; Inte's A770 is now arguably the cheapest 16GB GPU with decent compute and memory bandwidth, but it doesn't get much attention in the community. Vulkan support would benefit those of us running Arcs, and free us from having to fiddle with OneAPI.\\n\\nThat's how I run my A770s. Vulkan is faster than SYCL and way easier. As in there is no setup other than installing the Intel driver and downloading/compiling llama.cpp with the Vulkan backend. It just works.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3quc0h","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I fully support more Vulkan anywhere.&lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;Inte&amp;#39;s A770 is now arguably the cheapest 16GB GPU with decent compute and memory bandwidth, but it doesn&amp;#39;t get much attention in the community. Vulkan support would benefit those of us running Arcs, and free us from having to fiddle with OneAPI.&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;That&amp;#39;s how I run my A770s. Vulkan is faster than SYCL and way easier. As in there is no setup other than installing the Intel driver and downloading/compiling llama.cpp with the Vulkan backend. It just works.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2o3ht/help_vote_for_improved_vulkan_performance_in_ik/n3quc0h/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752804856,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m2o3ht","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":9}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3sadmk","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Glittering-Call8746","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3s9e6w","score":1,"author_fullname":"t2_tqwl6sawb","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I did cuda ik_llama.cpp docker . Not easy compilation error. Took me a week. I guess here's to vulkan now..","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3sadmk","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I did cuda ik_llama.cpp docker . Not easy compilation error. Took me a week. I guess here&amp;#39;s to vulkan now..&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2o3ht","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2o3ht/help_vote_for_improved_vulkan_performance_in_ik/n3sadmk/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752829828,"author_flair_text":null,"treatment_tags":[],"created_utc":1752829828,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3s9e6w","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"FullstackSensei","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3rq4k8","score":1,"author_fullname":"t2_17n3nqtj56","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I think you need to compile it with the Vulkan backend.\\n\\nCompilation flags seem to be mostly the same as llama.cpp.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3s9e6w","is_submitter":true,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I think you need to compile it with the Vulkan backend.&lt;/p&gt;\\n\\n&lt;p&gt;Compilation flags seem to be mostly the same as llama.cpp.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2o3ht","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2o3ht/help_vote_for_improved_vulkan_performance_in_ik/n3s9e6w/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752829255,"author_flair_text":null,"treatment_tags":[],"created_utc":1752829255,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3rq4k8","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Glittering-Call8746","can_mod_post":false,"created_utc":1752818611,"send_replies":true,"parent_id":"t1_n3rifi1","score":0,"author_fullname":"t2_tqwl6sawb","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Ok so any pointers how to run this on docker? I'm on amd 7900xtx","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3rq4k8","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Ok so any pointers how to run this on docker? I&amp;#39;m on amd 7900xtx&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2o3ht","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2o3ht/help_vote_for_improved_vulkan_performance_in_ik/n3rq4k8/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752818611,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}},"user_reports":[],"saved":false,"id":"n3rifi1","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Marksta","can_mod_post":false,"created_utc":1752814788,"send_replies":true,"parent_id":"t3_1m2o3ht","score":8,"author_fullname":"t2_559a1","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Check latest commits, Ik made a fancy one called [Vulkan: a fresh start](https://github.com/ikawrakow/ik_llama.cpp/commit/2081b3fccb9923699bf4d5e926d8719fc1d12c39) so I think he's already ahead of you but more feedback can't hurt. Looking forward to it, I haven't had any luck with it just yet with CUDA and AMD mixing.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3rifi1","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Check latest commits, Ik made a fancy one called &lt;a href=\\"https://github.com/ikawrakow/ik_llama.cpp/commit/2081b3fccb9923699bf4d5e926d8719fc1d12c39\\"&gt;Vulkan: a fresh start&lt;/a&gt; so I think he&amp;#39;s already ahead of you but more feedback can&amp;#39;t hurt. Looking forward to it, I haven&amp;#39;t had any luck with it just yet with CUDA and AMD mixing.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2o3ht/help_vote_for_improved_vulkan_performance_in_ik/n3rifi1/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752814788,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m2o3ht","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":8}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3rc4k2","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Glittering-Call8746","can_mod_post":false,"created_utc":1752811932,"send_replies":true,"parent_id":"t3_1m2o3ht","score":6,"author_fullname":"t2_tqwl6sawb","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Yes vote for vulkan. I want to run a single nvidia gpu for pp alongside amd for vram..","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3rc4k2","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yes vote for vulkan. I want to run a single nvidia gpu for pp alongside amd for vram..&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2o3ht/help_vote_for_improved_vulkan_performance_in_ik/n3rc4k2/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752811932,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m2o3ht","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3stn47","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Glittering-Call8746","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3ryf35","score":1,"author_fullname":"t2_tqwl6sawb","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"My bad.. yeah..  48gb.. looking at 64gb dimms all day.. lol","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3stn47","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;My bad.. yeah..  48gb.. looking at 64gb dimms all day.. lol&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2o3ht","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2o3ht/help_vote_for_improved_vulkan_performance_in_ik/n3stn47/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752839356,"author_flair_text":null,"treatment_tags":[],"created_utc":1752839356,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3ryf35","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"fallingdowndizzyvr","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3rq18i","score":3,"author_fullname":"t2_o65i6kx","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"&gt; Get 64gb intel dual gpu on one pcb first.\\n\\nDon't you mean 48GB?","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3ryf35","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;Get 64gb intel dual gpu on one pcb first.&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;Don&amp;#39;t you mean 48GB?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2o3ht","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2o3ht/help_vote_for_improved_vulkan_performance_in_ik/n3ryf35/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752823031,"author_flair_text":null,"treatment_tags":[],"created_utc":1752823031,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n3rq18i","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Glittering-Call8746","can_mod_post":false,"created_utc":1752818562,"send_replies":true,"parent_id":"t1_n3qjvgt","score":1,"author_fullname":"t2_tqwl6sawb","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Get 64gb intel dual gpu on one pcb first. Then decide if u want to get dual cards..","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3rq18i","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Get 64gb intel dual gpu on one pcb first. Then decide if u want to get dual cards..&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2o3ht","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2o3ht/help_vote_for_improved_vulkan_performance_in_ik/n3rq18i/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752818562,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3qjvgt","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"No_Afternoon_4260","can_mod_post":false,"created_utc":1752801049,"send_replies":true,"parent_id":"t3_1m2o3ht","score":1,"author_fullname":"t2_cj9kap4bx","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I vote for vulkan, but if I buy 16gig cards I want to use them by 4 with tensor parral","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3qjvgt","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I vote for vulkan, but if I buy 16gig cards I want to use them by 4 with tensor parral&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2o3ht/help_vote_for_improved_vulkan_performance_in_ik/n3qjvgt/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752801049,"author_flair_text":"llama.cpp","treatment_tags":[],"link_id":"t3_1m2o3ht","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),r=()=>e.jsx(l,{data:t});export{r as default};
