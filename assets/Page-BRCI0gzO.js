import{j as e}from"./index-BOnf-UhU.js";import{R as t}from"./RedditPostRenderer-Ce39qICS.js";import"./index-CDase6VD.js";const a=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"**TLDR**: Anyone has infographics/doc/dashboard for this? Please share. Thanks.\\n\\n\\n\\n^(I'm talking about stuff like Temperature, TopK, TopP, MinP, etc., values for all models. Though advanced users can apply these values with their experience, newbies like me need some kind of dashboard or list or repo with such details so we could open that before using models.)\\n\\n^(Currently my system has 20+ tiny models(Llama, Gemma, Qwen, Deepseek, Granite, etc.,). Even though I take settings for particular model from HF page before using, some models don't have the settings there.) \\n\\n^(Also I need to enter the values of those settings again whenever I open New chat. Accidentally I deleted some chat histories multiple times in past. So going to HF page again &amp; again just for this is too repetitive &amp; boring for me.)  ","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Recommended Settings ( Temperature, TopK, TopP, MinP, etc., ) for All models","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1m7k50u","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":1,"author_flair_background_color":null,"subreddit_type":"public","ups":5,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_1deiadfhb1","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":5,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1753302214,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;&lt;strong&gt;TLDR&lt;/strong&gt;: Anyone has infographics/doc/dashboard for this? Please share. Thanks.&lt;/p&gt;\\n\\n&lt;p&gt;&lt;sup&gt;I&amp;#39;m talking about stuff like Temperature, TopK, TopP, MinP, etc., values for all models. Though advanced users can apply these values with their experience, newbies like me need some kind of dashboard or list or repo with such details so we could open that before using models.&lt;/sup&gt;&lt;/p&gt;\\n\\n&lt;p&gt;&lt;sup&gt;Currently my system has 20+ tiny models(Llama, Gemma, Qwen, Deepseek, Granite, etc.,&lt;/sup&gt;. Even though I take settings for particular model from HF page before using, some models don&amp;#39;t have the settings there.) &lt;/p&gt;\\n\\n&lt;p&gt;&lt;sup&gt;Also I need to enter the values of those settings again whenever I open New chat. Accidentally I deleted some chat histories multiple times in past. So going to HF page again &amp;amp; again just for this is too repetitive &amp;amp; boring for me.&lt;/sup&gt;  &lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1m7k50u","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"pmttyji","discussion_type":null,"num_comments":3,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1m7k50u/recommended_settings_temperature_topk_topp_minp/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1m7k50u/recommended_settings_temperature_topk_topp_minp/","subreddit_subscribers":503517,"created_utc":1753302214,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4s3rnm","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"No_Efficiency_1144","can_mod_post":false,"created_utc":1753303025,"send_replies":true,"parent_id":"t3_1m7k50u","score":3,"author_fullname":"t2_1nkj9l14b0","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"There is an entire subfield of machine learning called hyper parameter optimisation for tasks like this. You can also train a small model to sample the logits instead.\\n\\n\\nIt’s AI era so I can’t suggest doing something by hand.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4s3rnm","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;There is an entire subfield of machine learning called hyper parameter optimisation for tasks like this. You can also train a small model to sample the logits instead.&lt;/p&gt;\\n\\n&lt;p&gt;It’s AI era so I can’t suggest doing something by hand.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7k50u/recommended_settings_temperature_topk_topp_minp/n4s3rnm/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753303025,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m7k50u","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4shczl","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"segmond","can_mod_post":false,"created_utc":1753306939,"send_replies":true,"parent_id":"t3_1m7k50u","score":1,"author_fullname":"t2_ah13x","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"use a search engine, even the same model could have difference suggestions based on what the chat is about.  you might need something different for coding vs creative writing.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4shczl","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;use a search engine, even the same model could have difference suggestions based on what the chat is about.  you might need something different for coding vs creative writing.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7k50u/recommended_settings_temperature_topk_topp_minp/n4shczl/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753306939,"author_flair_text":"llama.cpp","treatment_tags":[],"link_id":"t3_1m7k50u","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4tewcp","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ttkciar","can_mod_post":false,"created_utc":1753317917,"send_replies":true,"parent_id":"t3_1m7k50u","score":1,"author_fullname":"t2_cpegz","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Sometimes model authors will recommend hyperparameters in their model card (if there even is a model card).\\n\\nLacking such recommendations, my habit is to start with a temperature of 0.7 and increase it if needed when its responses seem too unvaried or formulaic during evaluation, usually in increments of 0.2 or 0.3.\\n\\nFor other hyperparameters, I almost always stick with llama.cpp's defaults, which you can find from the output of \`llama-cli --help\`.  Here's a copy of that, so you don't have to configure llama.cpp yourself:  http://ciar.org/h/llama-cli-help.txt\\n\\nOnce I figure out what hyperparameters to use, I encode them in a wrapper script, so I don't have to remember or type them all out every time I use the model.\\n\\nFor example, my Gemma3-27B wrapper script:  http://ciar.org/h/g3","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4tewcp","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Sometimes model authors will recommend hyperparameters in their model card (if there even is a model card).&lt;/p&gt;\\n\\n&lt;p&gt;Lacking such recommendations, my habit is to start with a temperature of 0.7 and increase it if needed when its responses seem too unvaried or formulaic during evaluation, usually in increments of 0.2 or 0.3.&lt;/p&gt;\\n\\n&lt;p&gt;For other hyperparameters, I almost always stick with llama.cpp&amp;#39;s defaults, which you can find from the output of &lt;code&gt;llama-cli --help&lt;/code&gt;.  Here&amp;#39;s a copy of that, so you don&amp;#39;t have to configure llama.cpp yourself:  &lt;a href=\\"http://ciar.org/h/llama-cli-help.txt\\"&gt;http://ciar.org/h/llama-cli-help.txt&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;Once I figure out what hyperparameters to use, I encode them in a wrapper script, so I don&amp;#39;t have to remember or type them all out every time I use the model.&lt;/p&gt;\\n\\n&lt;p&gt;For example, my Gemma3-27B wrapper script:  &lt;a href=\\"http://ciar.org/h/g3\\"&gt;http://ciar.org/h/g3&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7k50u/recommended_settings_temperature_topk_topp_minp/n4tewcp/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753317917,"author_flair_text":"llama.cpp","treatment_tags":[],"link_id":"t3_1m7k50u","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),o=()=>e.jsx(t,{data:a});export{o as default};
