import{j as e}from"./index-CmSyeZDT.js";import{R as l}from"./RedditPostRenderer-C2Zg39IK.js";import"./index-CiTZuv6Z.js";const t=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Hi everyone,\\n\\nI’m planning to build a **private, on-premise infrastructure** to serve **Llama 3 70B** for my office (about 30 users, possibly with a few remote users via VPN).  \\n**No data or files should leave our local network** – security and privacy are key. All inference and data processing must stay entirely within our private servers.\\n\\nMy requirements:\\n\\n* Serve Llama 3 70B (chat/inference, not training) to up to 30 simultaneous users (browser chat interface and API endpoints).\\n* Support file uploads and interaction with the model (docs, pdfs, txt, etc.), again, strictly within our own storage/network.\\n* I want to allow remote use for staff working from home, but only via VPN and under full company control.\\n* I want a **detailed, complete list** of what to buy (hardware, GPUs, server specs, network, power, backup, etc.) and recommended open-source software stack for this use-case.\\n* Budget is flexible, but I want the best price/performance/capacity ratio and a future-proof build.\\n\\nThanks in advance for your help and expertise!","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Help me design a robust on-prem Llama 3 70B infrastructure for 30 users – Complete hardware/software list wanted","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lnt6yj","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.41,"author_flair_background_color":null,"subreddit_type":"public","ups":0,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_1isohezaad","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":0,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1751240843,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;\\n\\n&lt;p&gt;I’m planning to build a &lt;strong&gt;private, on-premise infrastructure&lt;/strong&gt; to serve &lt;strong&gt;Llama 3 70B&lt;/strong&gt; for my office (about 30 users, possibly with a few remote users via VPN).&lt;br/&gt;\\n&lt;strong&gt;No data or files should leave our local network&lt;/strong&gt; – security and privacy are key. All inference and data processing must stay entirely within our private servers.&lt;/p&gt;\\n\\n&lt;p&gt;My requirements:&lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;Serve Llama 3 70B (chat/inference, not training) to up to 30 simultaneous users (browser chat interface and API endpoints).&lt;/li&gt;\\n&lt;li&gt;Support file uploads and interaction with the model (docs, pdfs, txt, etc.), again, strictly within our own storage/network.&lt;/li&gt;\\n&lt;li&gt;I want to allow remote use for staff working from home, but only via VPN and under full company control.&lt;/li&gt;\\n&lt;li&gt;I want a &lt;strong&gt;detailed, complete list&lt;/strong&gt; of what to buy (hardware, GPUs, server specs, network, power, backup, etc.) and recommended open-source software stack for this use-case.&lt;/li&gt;\\n&lt;li&gt;Budget is flexible, but I want the best price/performance/capacity ratio and a future-proof build.&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;p&gt;Thanks in advance for your help and expertise!&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":true,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1lnt6yj","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"Routine_Fail_2255","discussion_type":null,"num_comments":19,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lnt6yj/help_me_design_a_robust_onprem_llama_3_70b/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lnt6yj/help_me_design_a_robust_onprem_llama_3_70b/","subreddit_subscribers":492929,"created_utc":1751240843,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0iih8a","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"MDT-49","can_mod_post":false,"created_utc":1751251494,"send_replies":true,"parent_id":"t3_1lnt6yj","score":34,"author_fullname":"t2_h8yrica5","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"&gt;I want a **detailed, complete list** of what to buy (hardware, GPUs, server specs, network, power, backup, etc.) and recommended open-source software stack for this use-case.\\n\\nHave you mistaken Reddit for an AI assistant, or is using prompts the new norm for human interaction?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0iih8a","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;I want a &lt;strong&gt;detailed, complete list&lt;/strong&gt; of what to buy (hardware, GPUs, server specs, network, power, backup, etc.) and recommended open-source software stack for this use-case.&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;Have you mistaken Reddit for an AI assistant, or is using prompts the new norm for human interaction?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnt6yj/help_me_design_a_robust_onprem_llama_3_70b/n0iih8a/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751251494,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnt6yj","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":34}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0i1vm0","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"SashaUsesReddit","can_mod_post":false,"created_utc":1751245141,"send_replies":true,"parent_id":"t1_n0hzcgb","score":0,"author_fullname":"t2_57wafqev","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"This","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0i1vm0","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;This&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnt6yj","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnt6yj/help_me_design_a_robust_onprem_llama_3_70b/n0i1vm0/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751245141,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}},"user_reports":[],"saved":false,"id":"n0hzcgb","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Conscious_Cut_6144","can_mod_post":false,"created_utc":1751244192,"send_replies":true,"parent_id":"t3_1lnt6yj","score":17,"author_fullname":"t2_9hl4ymvj","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Questions you need to answer:  \\nFP8 or something else? (BF16 or FP4/AWQ4)  \\nHow many T/s does it need to do with 30 concurrent users?  \\nHow much total context is needed across those 30 users?\\n\\nThis sounds like a pretty standard VLLM + OpenWebUI setup.  \\nBut we would need the specifics before knowing what gpu you need.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0hzcgb","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Questions you need to answer:&lt;br/&gt;\\nFP8 or something else? (BF16 or FP4/AWQ4)&lt;br/&gt;\\nHow many T/s does it need to do with 30 concurrent users?&lt;br/&gt;\\nHow much total context is needed across those 30 users?&lt;/p&gt;\\n\\n&lt;p&gt;This sounds like a pretty standard VLLM + OpenWebUI setup.&lt;br/&gt;\\nBut we would need the specifics before knowing what gpu you need.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnt6yj/help_me_design_a_robust_onprem_llama_3_70b/n0hzcgb/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751244192,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnt6yj","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":17}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0hxsmn","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"SkyFeistyLlama8","can_mod_post":false,"created_utc":1751243614,"send_replies":true,"parent_id":"t3_1lnt6yj","score":20,"author_fullname":"t2_1hgbaqgbnq","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I can help you... for a price LOL","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0hxsmn","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I can help you... for a price LOL&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnt6yj/help_me_design_a_robust_onprem_llama_3_70b/n0hxsmn/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751243614,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnt6yj","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":20}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0idtxf","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"TheGABB","can_mod_post":false,"created_utc":1751249659,"send_replies":true,"parent_id":"t3_1lnt6yj","score":7,"author_fullname":"t2_lxuvm","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Sure. What’s your budget to get some consulting advice on hardware purchases …","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0idtxf","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Sure. What’s your budget to get some consulting advice on hardware purchases …&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnt6yj/help_me_design_a_robust_onprem_llama_3_70b/n0idtxf/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751249659,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnt6yj","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":7}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0ilz3l","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"xoexohexox","can_mod_post":false,"created_utc":1751252916,"send_replies":true,"parent_id":"t3_1lnt6yj","score":3,"author_fullname":"t2_323db","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Sure, I'll help you design it for 1000 dollars.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0ilz3l","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Sure, I&amp;#39;ll help you design it for 1000 dollars.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnt6yj/help_me_design_a_robust_onprem_llama_3_70b/n0ilz3l/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751252916,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnt6yj","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0jxyyj","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"SkyFeistyLlama8","can_mod_post":false,"created_utc":1751278914,"send_replies":true,"parent_id":"t1_n0jpa6l","score":1,"author_fullname":"t2_1hgbaqgbnq","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"When it's down to my AI talking to your AI, weirder things have happened.\\n\\nJob applicants are using LLMs to create resumes and companies are using LLMs to evaluate those resumes. Fun times.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0jxyyj","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;When it&amp;#39;s down to my AI talking to your AI, weirder things have happened.&lt;/p&gt;\\n\\n&lt;p&gt;Job applicants are using LLMs to create resumes and companies are using LLMs to evaluate those resumes. Fun times.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnt6yj","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnt6yj/help_me_design_a_robust_onprem_llama_3_70b/n0jxyyj/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751278914,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0jpa6l","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MelodicRecognition7","can_mod_post":false,"created_utc":1751273762,"send_replies":true,"parent_id":"t3_1lnt6yj","score":2,"author_fullname":"t2_1eex9ug5","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"lol I understand questions about GPU but this\\n\\n&gt;  network, power, backup, etc.\\n\\nraises some questions. Are you a vibe sysadmin that accidentally passed the tech interview?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0jpa6l","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;lol I understand questions about GPU but this&lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;network, power, backup, etc.&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;raises some questions. Are you a vibe sysadmin that accidentally passed the tech interview?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnt6yj/help_me_design_a_robust_onprem_llama_3_70b/n0jpa6l/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751273762,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnt6yj","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0huxau","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"raiffuvar","can_mod_post":false,"created_utc":1751242524,"send_replies":true,"parent_id":"t3_1lnt6yj","score":2,"author_fullname":"t2_4a6mfq67","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Buy MLE. \\n... or rather hire MLE","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0huxau","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Buy MLE. \\n... or rather hire MLE&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnt6yj/help_me_design_a_robust_onprem_llama_3_70b/n0huxau/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751242524,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnt6yj","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0idzdl","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"jain-nivedit","can_mod_post":false,"created_utc":1751249718,"send_replies":true,"parent_id":"t3_1lnt6yj","score":1,"author_fullname":"t2_1nb525ak26","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"You can use exosphere.host to come up with the orchestrator\\n\\n- built in queues\\n- file management\\n- infra management \\n- state management \\n- failovers\\n- completely open source: can be entirely deployed in your network \\n- comes with many implemented integrations to run on your own infra\\n- plug your code out of the box\\n\\nI'm building this, would be happy to share more details!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0idzdl","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You can use exosphere.host to come up with the orchestrator&lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;built in queues&lt;/li&gt;\\n&lt;li&gt;file management&lt;/li&gt;\\n&lt;li&gt;infra management &lt;/li&gt;\\n&lt;li&gt;state management &lt;/li&gt;\\n&lt;li&gt;failovers&lt;/li&gt;\\n&lt;li&gt;completely open source: can be entirely deployed in your network &lt;/li&gt;\\n&lt;li&gt;comes with many implemented integrations to run on your own infra&lt;/li&gt;\\n&lt;li&gt;plug your code out of the box&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;p&gt;I&amp;#39;m building this, would be happy to share more details!&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnt6yj/help_me_design_a_robust_onprem_llama_3_70b/n0idzdl/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751249718,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnt6yj","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0jzeza","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"JustImmunity","can_mod_post":false,"created_utc":1751279706,"send_replies":true,"parent_id":"t3_1lnt6yj","score":1,"author_fullname":"t2_c4pwgz16","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"buy a ups, buy a rtx6000 pro blackwell, the one with 96 gb of vram, and send it with vllm. buy two rtx6000 pros if you dont want to deal with running out of kv cache with vllm, or stick with one and move to LM cache if you want to cheap out.\\n\\nyou dont need networking, unless you have zero networking. Anything that predates your system is probably going to be fine\\n\\nand backups? your buying the tech for one singular use case, im sure you can figure out how to use a hard drive to do image backups.\\n\\nand just use tailscale and bind the access point through it as an exit node or something, no clue there. your company likely already has a preexisting solution for remote access, i recommend you get with somebody who knows about it and work with them.\\n\\nliterally everything can be consumer with a setup like this minus the gpu, you dont need to pay commercial or enterprice prices for things that will not affect throughput. You cannot future proof something like this. you can only go cheaper, in which case, buy 4 3090's for 3 grand, run them in tensor parallel and profit","edited":1751279951,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0jzeza","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;buy a ups, buy a rtx6000 pro blackwell, the one with 96 gb of vram, and send it with vllm. buy two rtx6000 pros if you dont want to deal with running out of kv cache with vllm, or stick with one and move to LM cache if you want to cheap out.&lt;/p&gt;\\n\\n&lt;p&gt;you dont need networking, unless you have zero networking. Anything that predates your system is probably going to be fine&lt;/p&gt;\\n\\n&lt;p&gt;and backups? your buying the tech for one singular use case, im sure you can figure out how to use a hard drive to do image backups.&lt;/p&gt;\\n\\n&lt;p&gt;and just use tailscale and bind the access point through it as an exit node or something, no clue there. your company likely already has a preexisting solution for remote access, i recommend you get with somebody who knows about it and work with them.&lt;/p&gt;\\n\\n&lt;p&gt;literally everything can be consumer with a setup like this minus the gpu, you dont need to pay commercial or enterprice prices for things that will not affect throughput. You cannot future proof something like this. you can only go cheaper, in which case, buy 4 3090&amp;#39;s for 3 grand, run them in tensor parallel and profit&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnt6yj/help_me_design_a_robust_onprem_llama_3_70b/n0jzeza/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751279706,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnt6yj","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0i047m","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"SashaUsesReddit","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0hwsqi","score":-4,"author_fullname":"t2_57wafqev","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"That model hallucinates like crazy","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0i047m","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;That model hallucinates like crazy&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnt6yj","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnt6yj/help_me_design_a_robust_onprem_llama_3_70b/n0i047m/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751244482,"author_flair_text":null,"treatment_tags":[],"created_utc":1751244482,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-4}}],"before":null}},"user_reports":[],"saved":false,"id":"n0hwsqi","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"invent-wander","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0hrhcd","score":-2,"author_fullname":"t2_1r03vqxdkn","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Would recommend Gemma 3 27B","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0hwsqi","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Would recommend Gemma 3 27B&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnt6yj","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnt6yj/help_me_design_a_robust_onprem_llama_3_70b/n0hwsqi/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751243233,"author_flair_text":null,"treatment_tags":[],"created_utc":1751243233,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-2}}],"before":null}},"user_reports":[],"saved":false,"id":"n0hrhcd","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Routine_Fail_2255","can_mod_post":false,"created_utc":1751241243,"send_replies":true,"parent_id":"t1_n0hqngy","score":-1,"author_fullname":"t2_1isohezaad","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"which model do you suggest?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0hrhcd","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;which model do you suggest?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnt6yj","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnt6yj/help_me_design_a_robust_onprem_llama_3_70b/n0hrhcd/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751241243,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0hqngy","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Healthy-Nebula-3603","can_mod_post":false,"created_utc":1751240942,"send_replies":true,"parent_id":"t3_1lnt6yj","score":-3,"author_fullname":"t2_ogjj6ebj","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"why so obsolete model?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0hqngy","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;why so obsolete model?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnt6yj/help_me_design_a_robust_onprem_llama_3_70b/n0hqngy/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751240942,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnt6yj","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-3}},{"kind":"t1","data":{"total_awards_received":0,"approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"ups":-5,"removal_reason":null,"link_id":"t3_1lnt6yj","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0ieewh","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"sleeping-in-crypto","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0idv4g","score":1,"author_fullname":"t2_by2pkeg5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Thank you!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0ieewh","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thank you!&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnt6yj","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnt6yj/help_me_design_a_robust_onprem_llama_3_70b/n0ieewh/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751249886,"author_flair_text":null,"treatment_tags":[],"created_utc":1751249886,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0idv4g","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Conscious_Cut_6144","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0ibizv","score":6,"author_fullname":"t2_9hl4ymvj","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Ollama is not appropriate for 30 concurrent users.\\nNeed a proper inference tool like vllm, sglang or trt-llm","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0idv4g","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Ollama is not appropriate for 30 concurrent users.\\nNeed a proper inference tool like vllm, sglang or trt-llm&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnt6yj","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnt6yj/help_me_design_a_robust_onprem_llama_3_70b/n0idv4g/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751249672,"author_flair_text":null,"treatment_tags":[],"created_utc":1751249672,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}}],"before":null}},"user_reports":[],"saved":false,"id":"n0ibizv","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"sleeping-in-crypto","can_mod_post":false,"created_utc":1751248779,"send_replies":true,"parent_id":"t1_n0i43my","score":1,"author_fullname":"t2_by2pkeg5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Why is this downvoted? It seems very helpful.\\n\\nFWIW I also want to get into self hosting but haven’t yet so I really don’t know what is good vs bad advice.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0ibizv","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Why is this downvoted? It seems very helpful.&lt;/p&gt;\\n\\n&lt;p&gt;FWIW I also want to get into self hosting but haven’t yet so I really don’t know what is good vs bad advice.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnt6yj","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnt6yj/help_me_design_a_robust_onprem_llama_3_70b/n0ibizv/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751248779,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0i43my","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":"DELETED","no_follow":true,"author":"[deleted]","can_mod_post":false,"send_replies":true,"parent_id":"t3_1lnt6yj","score":-5,"approved_by":null,"report_reasons":null,"all_awardings":[],"subreddit_id":"t5_81eyvm","body":"[deleted]","edited":1751252718,"downs":0,"author_flair_css_class":null,"collapsed":true,"is_submitter":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;[deleted]&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"associated_award":null,"stickied":false,"subreddit_type":"public","can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":"dark","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnt6yj/help_me_design_a_robust_onprem_llama_3_70b/n0i43my/","num_reports":null,"locked":false,"name":"t1_n0i43my","created":1751245974,"subreddit":"LocalLLaMA","author_flair_text":null,"treatment_tags":[],"created_utc":1751245974,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"","collapsed_because_crowd_control":null,"mod_reports":[],"mod_note":null,"distinguished":null}}],"before":null}}]`),o=()=>e.jsx(l,{data:t});export{o as default};
