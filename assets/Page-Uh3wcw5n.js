import{j as e}from"./index-CNyNkRpk.js";import{R as l}from"./RedditPostRenderer-Dza0u9i2.js";import"./index-BUchu_-K.js";const t=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Hey everyone,\\nI’m interested in running a self-hosted local LLM for coding assistance—something similar to what Cursor offers, but fully local for privacy and experimentation. Ideally, I’d like it to support code completion, inline suggestions, and maybe even multi-file context.\\n\\nWhat kind of hardware would I realistically need to run this smoothly?\\nSome specific questions:\\n\\t•\\tIs a consumer-grade GPU (like an RTX 4070/4080) enough for models like Code Llama or Phi-3?\\n\\t•\\tHow much RAM is recommended for practical use?\\n\\t•\\tAre there any CPU-only setups that work decently, or is GPU basically required for real-time performance?\\n\\t•\\tAny tips for keeping power consumption/noise low while running this 24/7?\\n\\nWould love to hear from anyone who’s running something like this already—what’s your setup and experience been like?\\n\\nThanks in advance!","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"What kind of hardware would I need to self-host a local LLM for coding (like Cursor)?","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lyyelr","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.6,"author_flair_background_color":null,"subreddit_type":"public","ups":1,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_37p33r","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":1,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1752427438,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Hey everyone,\\nI’m interested in running a self-hosted local LLM for coding assistance—something similar to what Cursor offers, but fully local for privacy and experimentation. Ideally, I’d like it to support code completion, inline suggestions, and maybe even multi-file context.&lt;/p&gt;\\n\\n&lt;p&gt;What kind of hardware would I realistically need to run this smoothly?\\nSome specific questions:\\n    • Is a consumer-grade GPU (like an RTX 4070/4080) enough for models like Code Llama or Phi-3?\\n    • How much RAM is recommended for practical use?\\n    • Are there any CPU-only setups that work decently, or is GPU basically required for real-time performance?\\n    • Any tips for keeping power consumption/noise low while running this 24/7?&lt;/p&gt;\\n\\n&lt;p&gt;Would love to hear from anyone who’s running something like this already—what’s your setup and experience been like?&lt;/p&gt;\\n\\n&lt;p&gt;Thanks in advance!&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":true,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1lyyelr","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"ClassicHabit","discussion_type":null,"num_comments":3,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lyyelr/what_kind_of_hardware_would_i_need_to_selfhost_a/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lyyelr/what_kind_of_hardware_would_i_need_to_selfhost_a/","subreddit_subscribers":498850,"created_utc":1752427438,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2xijqc","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Acrobatic_Cat_3448","can_mod_post":false,"created_utc":1752427705,"send_replies":true,"parent_id":"t3_1lyyelr","score":2,"author_fullname":"t2_133m0xy6vg","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Cursor is not a LLM but an IDE, using powerful LLMs with long and prompts prompts. It's doubtful if it can be recreated locally. Other than that, Macbook with 96GB RAM should let you use some 32B models.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2xijqc","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Cursor is not a LLM but an IDE, using powerful LLMs with long and prompts prompts. It&amp;#39;s doubtful if it can be recreated locally. Other than that, Macbook with 96GB RAM should let you use some 32B models.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyyelr/what_kind_of_hardware_would_i_need_to_selfhost_a/n2xijqc/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752427705,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lyyelr","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2y7fbi","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MelodicRecognition7","can_mod_post":false,"created_utc":1752435077,"send_replies":true,"parent_id":"t3_1lyyelr","score":2,"author_fullname":"t2_1eex9ug5","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Yes, GPU is required. RTX Pro 6000 96GB will let you run Kimi-Dev-72B but it will be very far from Claude. There is no way to keep it low power/low noise.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2y7fbi","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yes, GPU is required. RTX Pro 6000 96GB will let you run Kimi-Dev-72B but it will be very far from Claude. There is no way to keep it low power/low noise.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyyelr/what_kind_of_hardware_would_i_need_to_selfhost_a/n2y7fbi/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752435077,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lyyelr","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2yav6g","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DAlmighty","can_mod_post":false,"created_utc":1752436107,"send_replies":true,"parent_id":"t3_1lyyelr","score":2,"author_fullname":"t2_a04uj","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"It’s very possible to do what OP is asking. OP also didn’t say Cursor was an LLM. \\n\\nOP: All you need is a PC with as much VRAM as you can afford. The tried and true budget champ is an RTX 3090, but there are also other options that are either more expensive or more work to get going. The problem with going with 24-32GB of VRAM is the abilities of the models are limited. 96 GB of VRAM is a sweet spot in my opinion, but let it be known that it is VERY EXPENSIVE.  \\n\\nThe moral of the story is, if you don’t need the privacy use an online provider. If you need to run offline, prepare yourself for some financial pain. Oh and even if you spend the money, you will very likely NOT get a result as good as Claude or Chat GPT.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2yav6g","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It’s very possible to do what OP is asking. OP also didn’t say Cursor was an LLM. &lt;/p&gt;\\n\\n&lt;p&gt;OP: All you need is a PC with as much VRAM as you can afford. The tried and true budget champ is an RTX 3090, but there are also other options that are either more expensive or more work to get going. The problem with going with 24-32GB of VRAM is the abilities of the models are limited. 96 GB of VRAM is a sweet spot in my opinion, but let it be known that it is VERY EXPENSIVE.  &lt;/p&gt;\\n\\n&lt;p&gt;The moral of the story is, if you don’t need the privacy use an online provider. If you need to run offline, prepare yourself for some financial pain. Oh and even if you spend the money, you will very likely NOT get a result as good as Claude or Chat GPT.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyyelr/what_kind_of_hardware_would_i_need_to_selfhost_a/n2yav6g/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752436107,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lyyelr","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}}]`),n=()=>e.jsx(l,{data:t});export{n as default};
