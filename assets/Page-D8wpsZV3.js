import{j as e}from"./index-CNyNkRpk.js";import{R as t}from"./RedditPostRenderer-Dza0u9i2.js";import"./index-BUchu_-K.js";const l=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"I’ve been experimenting with local LLMs, and while I’ve had success running some models, I’m overwhelmed by the sheer number of options. I’d love some advice on how to narrow things down:\\n\\n* **What should I look for** in a model (e.g., size, architecture, benchmarks)?\\n* **Where’s the best place to find** reliable models (HF, GGUF repos, etc.)?\\n* **How can I estimate performance** on my 20GB VRAM GPU without downloading dozens of models?\\n\\nI’d prefer not to waste time and storage testing models blindly, so any tips on evaluating them beforehand would be hugely appreciated!\\n\\n*(Bonus: If you have personal favorites for my setup, I’m open to suggestions—but I’m mostly interested in learning how to decide.)*\\n\\n*EDIT:*  \\n*My primary use cases:*\\n\\n1. *Brainstorming (big part of my job—needs creative, coherent output).*\\n2. *Summarizing long texts (documents, articles, etc.).*\\n\\n","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Help im lost","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lvyqvq","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.73,"author_flair_background_color":null,"subreddit_type":"public","ups":5,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_13xhzq","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":5,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":1752106852,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1752106603,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;I’ve been experimenting with local LLMs, and while I’ve had success running some models, I’m overwhelmed by the sheer number of options. I’d love some advice on how to narrow things down:&lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;&lt;strong&gt;What should I look for&lt;/strong&gt; in a model (e.g., size, architecture, benchmarks)?&lt;/li&gt;\\n&lt;li&gt;&lt;strong&gt;Where’s the best place to find&lt;/strong&gt; reliable models (HF, GGUF repos, etc.)?&lt;/li&gt;\\n&lt;li&gt;&lt;strong&gt;How can I estimate performance&lt;/strong&gt; on my 20GB VRAM GPU without downloading dozens of models?&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;p&gt;I’d prefer not to waste time and storage testing models blindly, so any tips on evaluating them beforehand would be hugely appreciated!&lt;/p&gt;\\n\\n&lt;p&gt;&lt;em&gt;(Bonus: If you have personal favorites for my setup, I’m open to suggestions—but I’m mostly interested in learning how to decide.)&lt;/em&gt;&lt;/p&gt;\\n\\n&lt;p&gt;&lt;em&gt;EDIT:&lt;/em&gt;&lt;br/&gt;\\n&lt;em&gt;My primary use cases:&lt;/em&gt;&lt;/p&gt;\\n\\n&lt;ol&gt;\\n&lt;li&gt;&lt;em&gt;Brainstorming (big part of my job—needs creative, coherent output).&lt;/em&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;em&gt;Summarizing long texts (documents, articles, etc.).&lt;/em&gt;&lt;/li&gt;\\n&lt;/ol&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1lvyqvq","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"DaBe99","discussion_type":null,"num_comments":13,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lvyqvq/help_im_lost/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lvyqvq/help_im_lost/","subreddit_subscribers":497354,"created_utc":1752106603,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2a5xy8","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DaBe99","can_mod_post":false,"created_utc":1752109074,"send_replies":true,"parent_id":"t1_n2a3kat","score":2,"author_fullname":"t2_13xhzq","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Thanks for the suggestion. I totall agree that sticking with established models like Gemma3 is the pragmatic approach, especially for reliability.\\n\\nThat said, part of me still obsesses over whether I'm leaving performance on the table by not min-maxing (I know it's Irrational... but i can't help it lol).","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2a5xy8","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thanks for the suggestion. I totall agree that sticking with established models like Gemma3 is the pragmatic approach, especially for reliability.&lt;/p&gt;\\n\\n&lt;p&gt;That said, part of me still obsesses over whether I&amp;#39;m leaving performance on the table by not min-maxing (I know it&amp;#39;s Irrational... but i can&amp;#39;t help it lol).&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lvyqvq","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lvyqvq/help_im_lost/n2a5xy8/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752109074,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n2a3kat","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"pkdc0001","can_mod_post":false,"created_utc":1752108258,"send_replies":true,"parent_id":"t3_1lvyqvq","score":3,"author_fullname":"t2_yh1y8mfje","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"If you don't want to play with unknowns stay with the brand names, for what you mention Gemma 3 12B should be extremely capable, easy to run. If you download LM Studio, you can talk to it with chat interface or https request easily \\n\\nhttps://deepmind.google/models/gemma/gemma-3/","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2a3kat","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;If you don&amp;#39;t want to play with unknowns stay with the brand names, for what you mention Gemma 3 12B should be extremely capable, easy to run. If you download LM Studio, you can talk to it with chat interface or https request easily &lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://deepmind.google/models/gemma/gemma-3/\\"&gt;https://deepmind.google/models/gemma/gemma-3/&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lvyqvq/help_im_lost/n2a3kat/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752108258,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lvyqvq","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2icovp","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DaBe99","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2gi80x","score":1,"author_fullname":"t2_13xhzq","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"First off: that GitHub page you shared gave me a much clearer picture of how things work – thanks for that!\\n\\nThe difference in outputs between the 27B model and the 13B model is honestly **insane** to me; the 27B performs so much better.\\n\\n\\n\\n&gt;If you don't mind, I'd love to know how that works for you. I'm mulling over a GPU upgrade if it's worth it.\\n\\nHappy to share my experience! I've been testing the **Gemma 3 27B Q4 model** on my setup (AMD RX 7900XTX, Ryzen 9 5900X, 32GB DDR4 2600MT/s RAM – RAM upgrade planned!).\\n\\nI was really hyped with the initial speeds and outputs from the model. However, as the **10K context window** slowly filled up, generations started to slow down significantly:\\n\\n* **First generation:** around **30 tok/s**\\n* **Fourth generation:** dropped to around **18 tok/s**\\n\\nIt then settled at about **15 tok/s**. This speed is still great, and I'm happy with it, but the slowdown is definitely worth mentioning. I strongly suspect this is due to a **RAM bottleneck**, and I'm hoping to see even better results once I upgrade to DDR5.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2icovp","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;First off: that GitHub page you shared gave me a much clearer picture of how things work – thanks for that!&lt;/p&gt;\\n\\n&lt;p&gt;The difference in outputs between the 27B model and the 13B model is honestly &lt;strong&gt;insane&lt;/strong&gt; to me; the 27B performs so much better.&lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;If you don&amp;#39;t mind, I&amp;#39;d love to know how that works for you. I&amp;#39;m mulling over a GPU upgrade if it&amp;#39;s worth it.&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;Happy to share my experience! I&amp;#39;ve been testing the &lt;strong&gt;Gemma 3 27B Q4 model&lt;/strong&gt; on my setup (AMD RX 7900XTX, Ryzen 9 5900X, 32GB DDR4 2600MT/s RAM – RAM upgrade planned!).&lt;/p&gt;\\n\\n&lt;p&gt;I was really hyped with the initial speeds and outputs from the model. However, as the &lt;strong&gt;10K context window&lt;/strong&gt; slowly filled up, generations started to slow down significantly:&lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;&lt;strong&gt;First generation:&lt;/strong&gt; around &lt;strong&gt;30 tok/s&lt;/strong&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;strong&gt;Fourth generation:&lt;/strong&gt; dropped to around &lt;strong&gt;18 tok/s&lt;/strong&gt;&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;p&gt;It then settled at about &lt;strong&gt;15 tok/s&lt;/strong&gt;. This speed is still great, and I&amp;#39;m happy with it, but the slowdown is definitely worth mentioning. I strongly suspect this is due to a &lt;strong&gt;RAM bottleneck&lt;/strong&gt;, and I&amp;#39;m hoping to see even better results once I upgrade to DDR5.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lvyqvq","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lvyqvq/help_im_lost/n2icovp/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752218355,"author_flair_text":null,"treatment_tags":[],"created_utc":1752218355,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2gi80x","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DarthFluttershy_","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2e57g7","score":1,"author_fullname":"t2_d52if","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"&gt;about the Q4/5 quants... is that mainly to balance speed/quality with my 20GB VRAM setup\\n\\nYes, precisely, though also in general I don't find it usually worthwhile to run anything more than a Q6 even if you do have the space, because the loss from f16 to q6 is pretty small. Usually the significant drop happens betweenQ4 and Q3, sometimes Q5 and Q4 (see [here ](https://gist.github.com/Artefact2/b5f810600771265fc1e39442288e8ec9)for example), though this is model-specific. So, typically, I say Q4 is your baseline for being able to fit in a bigger model that still mostly operates as a bigger model should. If it's a smaller model, maybe bump it up (though it will slow it down), and if dropping to Q3 is the difference maker in fitting a bigger model, maybe try it... but honestly, much below that is usually worthless. I will say bigger models do quantize better, so maybe you could barely squeeze in a Q2XSS of a 70B and find it's better... but I doubt it.\\n\\n&gt;I'll give the 27B model a shot and compare a little.\\n\\nIf you don't mind, I'd love to know how that works for you. I'm mulling over a GPU upgrade if it's worth it.\\n\\n&gt;\\n\\nFor summaries, my texts are usually under 10k tokens\\n\\nWell, with the 12B model you should be able to fit that into context easily. But LM Studio does have a RAG feature... I just haven't played with it much.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2gi80x","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;about the Q4/5 quants... is that mainly to balance speed/quality with my 20GB VRAM setup&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;Yes, precisely, though also in general I don&amp;#39;t find it usually worthwhile to run anything more than a Q6 even if you do have the space, because the loss from f16 to q6 is pretty small. Usually the significant drop happens betweenQ4 and Q3, sometimes Q5 and Q4 (see &lt;a href=\\"https://gist.github.com/Artefact2/b5f810600771265fc1e39442288e8ec9\\"&gt;here &lt;/a&gt;for example), though this is model-specific. So, typically, I say Q4 is your baseline for being able to fit in a bigger model that still mostly operates as a bigger model should. If it&amp;#39;s a smaller model, maybe bump it up (though it will slow it down), and if dropping to Q3 is the difference maker in fitting a bigger model, maybe try it... but honestly, much below that is usually worthless. I will say bigger models do quantize better, so maybe you could barely squeeze in a Q2XSS of a 70B and find it&amp;#39;s better... but I doubt it.&lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;I&amp;#39;ll give the 27B model a shot and compare a little.&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;If you don&amp;#39;t mind, I&amp;#39;d love to know how that works for you. I&amp;#39;m mulling over a GPU upgrade if it&amp;#39;s worth it.&lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;For summaries, my texts are usually under 10k tokens&lt;/p&gt;\\n\\n&lt;p&gt;Well, with the 12B model you should be able to fit that into context easily. But LM Studio does have a RAG feature... I just haven&amp;#39;t played with it much.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lvyqvq","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lvyqvq/help_im_lost/n2gi80x/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752191120,"author_flair_text":null,"treatment_tags":[],"created_utc":1752191120,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2e57g7","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DaBe99","can_mod_post":false,"created_utc":1752166142,"send_replies":true,"parent_id":"t1_n2auh21","score":1,"author_fullname":"t2_13xhzq","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"just wow. this is *exactly* the kind of breakdown i needed. Thank you for taking the time to write this all out!\\n\\nabout the Q4/5 quants... is that mainly to balance speed/quality with my 20GB VRAM setup?\\n\\nI already started using LMStudio and played a little with the Gemma 3 12B model. Performance is great (\\\\~40 tok/s out of the box), tho I don't mind slower speeds if the quality improves - I'll give the 27B model a shot  and compare a little.  \\n  \\nFor summaries, my texts are usually under 10k tokens, so I’ll test a little and see if it works out for me or not.\\n\\n**Seriously, this comment is a goldmine. Grateful for the expertise!**","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2e57g7","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;just wow. this is &lt;em&gt;exactly&lt;/em&gt; the kind of breakdown i needed. Thank you for taking the time to write this all out!&lt;/p&gt;\\n\\n&lt;p&gt;about the Q4/5 quants... is that mainly to balance speed/quality with my 20GB VRAM setup?&lt;/p&gt;\\n\\n&lt;p&gt;I already started using LMStudio and played a little with the Gemma 3 12B model. Performance is great (~40 tok/s out of the box), tho I don&amp;#39;t mind slower speeds if the quality improves - I&amp;#39;ll give the 27B model a shot  and compare a little.  &lt;/p&gt;\\n\\n&lt;p&gt;For summaries, my texts are usually under 10k tokens, so I’ll test a little and see if it works out for me or not.&lt;/p&gt;\\n\\n&lt;p&gt;&lt;strong&gt;Seriously, this comment is a goldmine. Grateful for the expertise!&lt;/strong&gt;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lvyqvq","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lvyqvq/help_im_lost/n2e57g7/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752166142,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2auh21","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DarthFluttershy_","can_mod_post":false,"created_utc":1752117945,"send_replies":true,"parent_id":"t3_1lvyqvq","score":2,"author_fullname":"t2_d52if","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"1. For \\"learning how to decide,\\" maybe just put $5 into an openrouter account and use their chat or a frontend with their API to just try some models out. They have a good selection of the local models hosted there.\\n\\n2. LM Studio is the user-friendliest local LLM platform, IMO, great for getting started. Others give you a bit more ability to optimize, though. LM Studio will pull from HF for you as an integrated feature, but HF is the best place regardless.  \\n  \\n3. 20 GB of RAM will run any reasonable quant of 12B easily, but you can probably get decent speeds on 27B models or so with some offloading. As a general rule, the size of the model is the amount of VRAM you'll need to fully offload it before any context, so expect to need 1.2-1.5x the model size for a full offload.  \\n  \\n4. What speed do you consider acceptable? If you don't mind slower tokens, you can do partial VRAM offloads for smarter models. I have 12GB of VRAM and can get about 17-20 tokens per second on a 30B-A3B model (because it's a mixture of experts and doesn't use all the memory at once), but only 3 tokens/s on Gemma3 27B, wereas with a full offload of Gemma 3 12B I get 30 tokesn/s. You might squeeze the 27B in completely though. Aim for Q4 or Q5 quants generally.\\n\\n5. How long of texts do you want to summarize? Sumamrizing long texts might be tricky locally, since it's gonna eat memory... so you might need a smaller LLM to expand context if RAG isn't doing it for you (I have very little experience with local RAG)\\n\\n6. I'd suggest look at Mistral 24B, Gemma 3 27B (or 12B if you want a it really fast), and Qwen3 30B A3B, which gives you a lot of bang for you VRAM, but I've had mixed results in following instructions.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2auh21","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;ol&gt;\\n&lt;li&gt;&lt;p&gt;For &amp;quot;learning how to decide,&amp;quot; maybe just put $5 into an openrouter account and use their chat or a frontend with their API to just try some models out. They have a good selection of the local models hosted there.&lt;/p&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;p&gt;LM Studio is the user-friendliest local LLM platform, IMO, great for getting started. Others give you a bit more ability to optimize, though. LM Studio will pull from HF for you as an integrated feature, but HF is the best place regardless.  &lt;/p&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;p&gt;20 GB of RAM will run any reasonable quant of 12B easily, but you can probably get decent speeds on 27B models or so with some offloading. As a general rule, the size of the model is the amount of VRAM you&amp;#39;ll need to fully offload it before any context, so expect to need 1.2-1.5x the model size for a full offload.  &lt;/p&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;p&gt;What speed do you consider acceptable? If you don&amp;#39;t mind slower tokens, you can do partial VRAM offloads for smarter models. I have 12GB of VRAM and can get about 17-20 tokens per second on a 30B-A3B model (because it&amp;#39;s a mixture of experts and doesn&amp;#39;t use all the memory at once), but only 3 tokens/s on Gemma3 27B, wereas with a full offload of Gemma 3 12B I get 30 tokesn/s. You might squeeze the 27B in completely though. Aim for Q4 or Q5 quants generally.&lt;/p&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;p&gt;How long of texts do you want to summarize? Sumamrizing long texts might be tricky locally, since it&amp;#39;s gonna eat memory... so you might need a smaller LLM to expand context if RAG isn&amp;#39;t doing it for you (I have very little experience with local RAG)&lt;/p&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;p&gt;I&amp;#39;d suggest look at Mistral 24B, Gemma 3 27B (or 12B if you want a it really fast), and Qwen3 30B A3B, which gives you a lot of bang for you VRAM, but I&amp;#39;ve had mixed results in following instructions.&lt;/p&gt;&lt;/li&gt;\\n&lt;/ol&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lvyqvq/help_im_lost/n2auh21/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752117945,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lvyqvq","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2c2zmk","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Corporate_Drone31","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2a6fiu","score":1,"author_fullname":"t2_32o8hu91","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"FYI, Hugging Face now has a built-in GGUF metadata reader. You can access it from the model card page from the right-hand bar - just click the \\"button\\" corresponding to the quant level and it'll show up.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2c2zmk","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;FYI, Hugging Face now has a built-in GGUF metadata reader. You can access it from the model card page from the right-hand bar - just click the &amp;quot;button&amp;quot; corresponding to the quant level and it&amp;#39;ll show up.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lvyqvq","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lvyqvq/help_im_lost/n2c2zmk/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752140659,"author_flair_text":null,"treatment_tags":[],"created_utc":1752140659,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2a6fiu","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DaBe99","can_mod_post":false,"created_utc":1752109242,"send_replies":true,"parent_id":"t1_n2a40z8","score":1,"author_fullname":"t2_13xhzq","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Thanks for the tip! I'll see if I can find the tool you're referring to.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2a6fiu","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thanks for the tip! I&amp;#39;ll see if I can find the tool you&amp;#39;re referring to.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lvyqvq","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lvyqvq/help_im_lost/n2a6fiu/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752109242,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2a40z8","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"RhubarbSimilar1683","can_mod_post":false,"created_utc":1752108417,"send_replies":true,"parent_id":"t3_1lvyqvq","score":1,"author_fullname":"t2_1k4sjdwzk2","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"HF is ideal as a source but they might not always work for you. As for estimating performance there is a tool that can read gguf metadata or extract metadata from gguf models but I don't remember its name anymore. It's a command line tool on github","edited":1752108775,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2a40z8","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;HF is ideal as a source but they might not always work for you. As for estimating performance there is a tool that can read gguf metadata or extract metadata from gguf models but I don&amp;#39;t remember its name anymore. It&amp;#39;s a command line tool on github&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lvyqvq/help_im_lost/n2a40z8/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752108417,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lvyqvq","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2ai1va","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"techmago","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2a8vud","score":1,"author_fullname":"t2_azy5rpp","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Ah, notes:\\n\\ni did saw a expressive difference between mistral q4 from mistral q8.\\n\\nIf oyu are using ollama... mistral mem usage is fucked up.   \\nI'm running ollama from a exerimental branch because of that\\n\\n[https://github.com/ollama/ollama/pull/11090](https://github.com/ollama/ollama/pull/11090)  \\nthe new memory allocation system from this guy help a lot\\n\\n  \\nAlso, if oyu are using ollama, use q8 insted the default (fp16) for the context.  \\nMake a huge diference in context memory size.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2ai1va","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Ah, notes:&lt;/p&gt;\\n\\n&lt;p&gt;i did saw a expressive difference between mistral q4 from mistral q8.&lt;/p&gt;\\n\\n&lt;p&gt;If oyu are using ollama... mistral mem usage is fucked up.&lt;br/&gt;\\nI&amp;#39;m running ollama from a exerimental branch because of that&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://github.com/ollama/ollama/pull/11090\\"&gt;https://github.com/ollama/ollama/pull/11090&lt;/a&gt;&lt;br/&gt;\\nthe new memory allocation system from this guy help a lot&lt;/p&gt;\\n\\n&lt;p&gt;Also, if oyu are using ollama, use q8 insted the default (fp16) for the context.&lt;br/&gt;\\nMake a huge diference in context memory size.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lvyqvq","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lvyqvq/help_im_lost/n2ai1va/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752113295,"author_flair_text":null,"treatment_tags":[],"created_utc":1752113295,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2a8vud","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DaBe99","can_mod_post":false,"created_utc":1752110089,"send_replies":true,"parent_id":"t1_n2a7l89","score":1,"author_fullname":"t2_13xhzq","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yeah, Gemini Pro is honestly in its own tier. I use it daily and just love it. Sadly, work rules mean I’m stuck with local setups for privacy, so I’m looking for the next best thing.  \\nThis sounds like a solid setup. Really appreciate the tip... gonna try this asap,","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2a8vud","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yeah, Gemini Pro is honestly in its own tier. I use it daily and just love it. Sadly, work rules mean I’m stuck with local setups for privacy, so I’m looking for the next best thing.&lt;br/&gt;\\nThis sounds like a solid setup. Really appreciate the tip... gonna try this asap,&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lvyqvq","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lvyqvq/help_im_lost/n2a8vud/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752110089,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2a7l89","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"techmago","can_mod_post":false,"created_utc":1752109643,"send_replies":true,"parent_id":"t3_1lvyqvq","score":1,"author_fullname":"t2_azy5rpp","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"20 GB ram you can use mistral:24 and quen3:32b\\n\\nuse openwebui as frontend and use the 2 models at the same time.  \\nGreat for brainstorms.\\n\\nPlus, for sumary, mistral is the best model i tested so far (the best is gemini-pro)\\n\\n(yes, i was too lazy to make a for)\\n\\nhttps://preview.redd.it/ggfdb5ti5ybf1.png?width=2323&amp;format=png&amp;auto=webp&amp;s=5a3452d8db5cc6d3e088eb325c95236166fe1bbe","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2a7l89","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;20 GB ram you can use mistral:24 and quen3:32b&lt;/p&gt;\\n\\n&lt;p&gt;use openwebui as frontend and use the 2 models at the same time.&lt;br/&gt;\\nGreat for brainstorms.&lt;/p&gt;\\n\\n&lt;p&gt;Plus, for sumary, mistral is the best model i tested so far (the best is gemini-pro)&lt;/p&gt;\\n\\n&lt;p&gt;(yes, i was too lazy to make a for)&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://preview.redd.it/ggfdb5ti5ybf1.png?width=2323&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5a3452d8db5cc6d3e088eb325c95236166fe1bbe\\"&gt;https://preview.redd.it/ggfdb5ti5ybf1.png?width=2323&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5a3452d8db5cc6d3e088eb325c95236166fe1bbe&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lvyqvq/help_im_lost/n2a7l89/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752109643,"media_metadata":{"ggfdb5ti5ybf1":{"status":"valid","e":"Image","m":"image/png","p":[{"y":52,"x":108,"u":"https://preview.redd.it/ggfdb5ti5ybf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=46b3f03849691d469b74bc8ce41e6ff0b9ae4666"},{"y":105,"x":216,"u":"https://preview.redd.it/ggfdb5ti5ybf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=925c0f303e2b593f68f6af4b0af283fc0accefbb"},{"y":156,"x":320,"u":"https://preview.redd.it/ggfdb5ti5ybf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=65dc2d00b89afafbf55c5a0763d3a9f5e5025c2f"},{"y":313,"x":640,"u":"https://preview.redd.it/ggfdb5ti5ybf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=fbe646fd69f9818c129fc52d953661f49c6ddbf8"},{"y":470,"x":960,"u":"https://preview.redd.it/ggfdb5ti5ybf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=d53f2504fb6ba8775d7938d0d78b138037f55615"},{"y":529,"x":1080,"u":"https://preview.redd.it/ggfdb5ti5ybf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=bf1219cb71bf2f25e4f0c5188e0e93e00483c054"}],"s":{"y":1139,"x":2323,"u":"https://preview.redd.it/ggfdb5ti5ybf1.png?width=2323&amp;format=png&amp;auto=webp&amp;s=5a3452d8db5cc6d3e088eb325c95236166fe1bbe"},"id":"ggfdb5ti5ybf1"}},"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lvyqvq","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),r=()=>e.jsx(t,{data:l});export{r as default};
