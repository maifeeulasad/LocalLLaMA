import{j as e}from"./index-DACS7Nh6.js";import{R as t}from"./RedditPostRenderer-Dqa1NZuX.js";import"./index-DiMIVQx4.js";const a=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"https://preview.redd.it/thvbs96odfbf1.png?width=1080&amp;format=png&amp;auto=webp&amp;s=46a8ee47ee075177a59c3b692c3caa87f220f9e6\\n\\nGPU is not being utilized as much as my CPU on the KDE Neon distribution I'm currently using. On my previous Ubuntu distribution, my GPU usage was around 90%, compared to my CPU. I'm not sure what went wrong. I added the following options to /etc/modprobe.d/nvidia-power-management.conf to address wake-up issues with the GPU not functioning after sleep:\\n\\n    Code\\n    \\n    options nvidia NVreg_PreserveVideoMemoryAllocations=1\\n    options nvidia NVreg_TemporaryFilePath=/tmp\\n\\nSince then, Ollama has been using my GPU less than my CPU. I've been searching for answers for a week.\\n\\ni am running llama3.1 8b model. i used same models on both distros.\\n\\nhelp me guys.............  \\nMy GPU is not being utilized as much as my CPU on the KDE Neon   \\ndistribution I'm currently using. On my previous Ubuntu distribution, my  \\n GPU usage was around 90%, compared to my CPU. I'm not sure what went   \\nwrong. I added the following options to   \\n/etc/modprobe.d/nvidia-power-management.conf to address wake-up issues   \\nwith the GPU not functioning after sleep:  \\nCode  \\n  \\noptions nvidia NVreg\\\\_PreserveVideoMemoryAllocations=1  \\noptions nvidia NVreg\\\\_TemporaryFilePath=/tmp  \\nSince then, Ollama has been using my GPU less than my CPU. I've been searching for answers for a week.  \\n  \\ni am running llama3.1 8b model. i used same models on both distros.  \\n  \\nhelp me guys.............  \\n","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"GUYS HELP ME : Ollama is utilizing my CPU more than my GPU.","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":78,"top_awarded_type":null,"hide_score":false,"media_metadata":{"thvbs96odfbf1":{"status":"valid","e":"Image","m":"image/png","p":[{"y":60,"x":108,"u":"https://preview.redd.it/thvbs96odfbf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=801563c8628a3ce7cd6274f622cca598acd5fa9d"},{"y":121,"x":216,"u":"https://preview.redd.it/thvbs96odfbf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=d7c49426dee2dacc8df00bdd276c886acc22934a"},{"y":179,"x":320,"u":"https://preview.redd.it/thvbs96odfbf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=6f7baf9108e48546f960b8e68f5e0c9b15cefa17"},{"y":359,"x":640,"u":"https://preview.redd.it/thvbs96odfbf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=3b89d4fbf38157c32537ab3ee0c2d7903092bc1b"},{"y":539,"x":960,"u":"https://preview.redd.it/thvbs96odfbf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=d833a6c195790473fe734e8cd3bab8e2497cf182"},{"y":607,"x":1080,"u":"https://preview.redd.it/thvbs96odfbf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=ec2c8df10b23d579084f18d71d350d476bc14023"}],"s":{"y":607,"x":1080,"u":"https://preview.redd.it/thvbs96odfbf1.png?width=1080&amp;format=png&amp;auto=webp&amp;s=46a8ee47ee075177a59c3b692c3caa87f220f9e6"},"id":"thvbs96odfbf1"}},"name":"t3_1ltq7n9","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.33,"author_flair_background_color":null,"subreddit_type":"public","ups":0,"total_awards_received":0,"media_embed":{},"thumbnail_width":140,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_qfs9po80","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":0,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"https://b.thumbs.redditmedia.com/qi3rNesyzjBggKIOUmxBw702YnmfNg6NNPcHjz3zflI.jpg","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1751882414,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;&lt;a href=\\"https://preview.redd.it/thvbs96odfbf1.png?width=1080&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=46a8ee47ee075177a59c3b692c3caa87f220f9e6\\"&gt;https://preview.redd.it/thvbs96odfbf1.png?width=1080&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=46a8ee47ee075177a59c3b692c3caa87f220f9e6&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;GPU is not being utilized as much as my CPU on the KDE Neon distribution I&amp;#39;m currently using. On my previous Ubuntu distribution, my GPU usage was around 90%, compared to my CPU. I&amp;#39;m not sure what went wrong. I added the following options to /etc/modprobe.d/nvidia-power-management.conf to address wake-up issues with the GPU not functioning after sleep:&lt;/p&gt;\\n\\n&lt;pre&gt;&lt;code&gt;Code\\n\\noptions nvidia NVreg_PreserveVideoMemoryAllocations=1\\noptions nvidia NVreg_TemporaryFilePath=/tmp\\n&lt;/code&gt;&lt;/pre&gt;\\n\\n&lt;p&gt;Since then, Ollama has been using my GPU less than my CPU. I&amp;#39;ve been searching for answers for a week.&lt;/p&gt;\\n\\n&lt;p&gt;i am running llama3.1 8b model. i used same models on both distros.&lt;/p&gt;\\n\\n&lt;p&gt;help me guys.............&lt;br/&gt;\\nMy GPU is not being utilized as much as my CPU on the KDE Neon&lt;br/&gt;\\ndistribution I&amp;#39;m currently using. On my previous Ubuntu distribution, my&lt;br/&gt;\\n GPU usage was around 90%, compared to my CPU. I&amp;#39;m not sure what went&lt;br/&gt;\\nwrong. I added the following options to&lt;br/&gt;\\n/etc/modprobe.d/nvidia-power-management.conf to address wake-up issues&lt;br/&gt;\\nwith the GPU not functioning after sleep:&lt;br/&gt;\\nCode  &lt;/p&gt;\\n\\n&lt;p&gt;options nvidia NVreg_PreserveVideoMemoryAllocations=1&lt;br/&gt;\\noptions nvidia NVreg_TemporaryFilePath=/tmp&lt;br/&gt;\\nSince then, Ollama has been using my GPU less than my CPU. I&amp;#39;ve been searching for answers for a week.  &lt;/p&gt;\\n\\n&lt;p&gt;i am running llama3.1 8b model. i used same models on both distros.  &lt;/p&gt;\\n\\n&lt;p&gt;help me guys.............  &lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":true,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1ltq7n9","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"HighlightPrudent554","discussion_type":null,"num_comments":8,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1ltq7n9/guys_help_me_ollama_is_utilizing_my_cpu_more_than/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1ltq7n9/guys_help_me_ollama_is_utilizing_my_cpu_more_than/","subreddit_subscribers":496034,"created_utc":1751882414,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1sfqru","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"chibop1","can_mod_post":false,"created_utc":1751887348,"send_replies":true,"parent_id":"t3_1ltq7n9","score":6,"author_fullname":"t2_e9jh97s","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"type ollama ps and see if layers are offloaded to cpu. If the model is  offloaded to cpu, you need to reduce context size or use smaller model.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1sfqru","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;type ollama ps and see if layers are offloaded to cpu. If the model is  offloaded to cpu, you need to reduce context size or use smaller model.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltq7n9/guys_help_me_ollama_is_utilizing_my_cpu_more_than/n1sfqru/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751887348,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1ltq7n9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1sryl4","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"LA_rent_Aficionado","can_mod_post":false,"created_utc":1751892471,"send_replies":true,"parent_id":"t1_n1sni3s","score":2,"author_fullname":"t2_t8zbiflk","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Probably worth clarifying the ~2x speed boost is PP not Gen","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1sryl4","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Probably worth clarifying the ~2x speed boost is PP not Gen&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ltq7n9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltq7n9/guys_help_me_ollama_is_utilizing_my_cpu_more_than/n1sryl4/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751892471,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1suomu","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"HighlightPrudent554","can_mod_post":false,"created_utc":1751893470,"send_replies":true,"parent_id":"t1_n1sni3s","score":1,"author_fullname":"t2_qfs9po80","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I will try that man..","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1suomu","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I will try that man..&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ltq7n9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltq7n9/guys_help_me_ollama_is_utilizing_my_cpu_more_than/n1suomu/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751893470,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1sni3s","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Lissanro","can_mod_post":false,"created_utc":1751890727,"send_replies":true,"parent_id":"t3_1ltq7n9","score":2,"author_fullname":"t2_fpfao9g","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"For GPU+CPU inference it is a good idea to use ik\\\\_llama.cpp (I shared [how to setup and run it](https://www.reddit.com/r/LocalLLaMA/comments/1jtx05j/comment/mlyf0ux/?utm_source=share&amp;utm_medium=web3x&amp;utm_name=web3xcss&amp;utm_term=1&amp;utm_content=share_button) in case you are interested to give it a try), it is about twice as fast as llama.cpp that Ollama is based on, at least with MoE models I tried that used both RAM and VRAM.","edited":1751891605,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1sni3s","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;For GPU+CPU inference it is a good idea to use ik_llama.cpp (I shared &lt;a href=\\"https://www.reddit.com/r/LocalLLaMA/comments/1jtx05j/comment/mlyf0ux/?utm_source=share&amp;amp;utm_medium=web3x&amp;amp;utm_name=web3xcss&amp;amp;utm_term=1&amp;amp;utm_content=share_button\\"&gt;how to setup and run it&lt;/a&gt; in case you are interested to give it a try), it is about twice as fast as llama.cpp that Ollama is based on, at least with MoE models I tried that used both RAM and VRAM.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltq7n9/guys_help_me_ollama_is_utilizing_my_cpu_more_than/n1sni3s/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751890727,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1ltq7n9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1skqjj","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"HighlightPrudent554","can_mod_post":false,"created_utc":1751889573,"send_replies":true,"parent_id":"t1_n1sfoak","score":1,"author_fullname":"t2_qfs9po80","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Thanks man","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1skqjj","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thanks man&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ltq7n9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltq7n9/guys_help_me_ollama_is_utilizing_my_cpu_more_than/n1skqjj/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751889573,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1sfoak","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Nepherpitu","can_mod_post":false,"created_utc":1751887316,"send_replies":true,"parent_id":"t3_1ltq7n9","score":6,"author_fullname":"t2_plp1w","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Just skip this debug quest entirely and go for llama.cpp instead of ollama wrapper. You can add llama-swap for model switching. As a result you will get more control over model loading and serving. Expect no major performance uplift if limited by gpu, but earlier models support and better performance when ollama estimations and settings are wrong.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1sfoak","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Just skip this debug quest entirely and go for llama.cpp instead of ollama wrapper. You can add llama-swap for model switching. As a result you will get more control over model loading and serving. Expect no major performance uplift if limited by gpu, but earlier models support and better performance when ollama estimations and settings are wrong.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltq7n9/guys_help_me_ollama_is_utilizing_my_cpu_more_than/n1sfoak/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751887316,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1ltq7n9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1sqa08","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Marksta","can_mod_post":false,"created_utc":1751891831,"send_replies":true,"parent_id":"t3_1ltq7n9","score":2,"author_fullname":"t2_559a1","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Don't use llama.cpp wrapper is the answer, it's obfuscating you from the answer for a week already, that's crazy!\\n\\nBut the using CPU more thing isn't about using it more, it's about the slowness of CPU vs. GPU. This could be using 95% of model loaded into GPU VRAM and 5% loaded to CPU and the scenario would look like this with slow enough cpu/ram. The GPU completes its work load each cycle nearly instantly then needs to await on the CPU. Once the CPU completes it's step, it again has its next step already ready for it so it sits at 100% forever and GPU looks like it's not being used much of any. Basically, any hybrid GPU-CPU usage looks like this scenario, CPU threads in use go to 100% and GPU performance is crippled just awaiting CPU to do its part.\\n\\nWith this sort of setup, you need to make sure you never overflow out of GPU VRAM limit or you see the performance tanks really hard. No way to do this in Ollama, it overflows silently. Llama.cpp will do an out of memory self-crash when you launch it with params that overflows so you can know to dial the settings back a bit or that you need a smaller model.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1sqa08","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Don&amp;#39;t use llama.cpp wrapper is the answer, it&amp;#39;s obfuscating you from the answer for a week already, that&amp;#39;s crazy!&lt;/p&gt;\\n\\n&lt;p&gt;But the using CPU more thing isn&amp;#39;t about using it more, it&amp;#39;s about the slowness of CPU vs. GPU. This could be using 95% of model loaded into GPU VRAM and 5% loaded to CPU and the scenario would look like this with slow enough cpu/ram. The GPU completes its work load each cycle nearly instantly then needs to await on the CPU. Once the CPU completes it&amp;#39;s step, it again has its next step already ready for it so it sits at 100% forever and GPU looks like it&amp;#39;s not being used much of any. Basically, any hybrid GPU-CPU usage looks like this scenario, CPU threads in use go to 100% and GPU performance is crippled just awaiting CPU to do its part.&lt;/p&gt;\\n\\n&lt;p&gt;With this sort of setup, you need to make sure you never overflow out of GPU VRAM limit or you see the performance tanks really hard. No way to do this in Ollama, it overflows silently. Llama.cpp will do an out of memory self-crash when you launch it with params that overflows so you can know to dial the settings back a bit or that you need a smaller model.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltq7n9/guys_help_me_ollama_is_utilizing_my_cpu_more_than/n1sqa08/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751891831,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1ltq7n9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}}]`),s=()=>e.jsx(t,{data:a});export{s as default};
