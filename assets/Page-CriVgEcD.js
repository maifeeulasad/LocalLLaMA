import{j as e}from"./index-CjwP30j7.js";import{R as a}from"./RedditPostRenderer-BbYuEq_V.js";import"./index-C-yxLSPN.js";const l=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"I have 30 years in IT but new to AI, and I'd like to run Ollama locally. To save $$ I'd like to repurpose an older machine with max hardware: KGPE-D16 mobo, dual Opteron 6380's, 128GB ECC RAM and 8TB SSD storage.\\n\\nResearch indicates the best solution is to get a solid GPU only for the VRAM. Best value GPU is currently Tesla K80 24gb card, but apparently requires a BIOS setting called 'Enable Above 4G Decoding' which this BIOS does not have; I checked every setting I could find. Best available GPU for this board is NVIDIA Quadro K6000.\\n\\nNo problem getting the Quadro, but will it (or any other GPU) work without that BIOS setting? Any guidance is much appreciated.\\n\\n","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Local AI platform on older machine","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lpbamg","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.29,"author_flair_background_color":null,"subreddit_type":"public","ups":0,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_1sld4v4omz","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":0,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1751398877,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;I have 30 years in IT but new to AI, and I&amp;#39;d like to run Ollama locally. To save $$ I&amp;#39;d like to repurpose an older machine with max hardware: KGPE-D16 mobo, dual Opteron 6380&amp;#39;s, 128GB ECC RAM and 8TB SSD storage.&lt;/p&gt;\\n\\n&lt;p&gt;Research indicates the best solution is to get a solid GPU only for the VRAM. Best value GPU is currently Tesla K80 24gb card, but apparently requires a BIOS setting called &amp;#39;Enable Above 4G Decoding&amp;#39; which this BIOS does not have; I checked every setting I could find. Best available GPU for this board is NVIDIA Quadro K6000.&lt;/p&gt;\\n\\n&lt;p&gt;No problem getting the Quadro, but will it (or any other GPU) work without that BIOS setting? Any guidance is much appreciated.&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":true,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1lpbamg","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"zearo_kool","discussion_type":null,"num_comments":5,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lpbamg/local_ai_platform_on_older_machine/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lpbamg/local_ai_platform_on_older_machine/","subreddit_subscribers":493457,"created_utc":1751398877,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0tlmzt","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"FullstackSensei","can_mod_post":false,"created_utc":1751401148,"send_replies":true,"parent_id":"t3_1lpbamg","score":2,"author_fullname":"t2_17n3nqtj56","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Work is a very relative word here. What are your expectations of speed? What do you expect 5o do with the models?\\n\\nThose Opteron are so old that more recent DDR4 desktop platforms will perform faster. They're also PCIe Gen 2, which will make things slower if you run models that don't fit on a single GPU. The Kepler based Tesla or Quadro cards you looked at aren't true 24GB cards. They're dual 12GB GPUs on one card. Kepler is also so old that it's not much faster than said more recent desktop CPU.\\n\\nRather than spending money on this, and assuming you have a relatively recent desktop, you could upgrade said desktop RAM to 64GB to get your feet wet.\\n\\nOllama will be fine for the first week or two. You'll quickly outgrow it if you're experimenting. It's based on llama.cpp, so you might as well skip it and go straight to learning how to use llama.cpp. Ollama also fornicates with model names, which can lead to a lot of frustration and disappointment. So, again you might just as well skip it and download your models from HuggingFace. You'll end up there anyway after a couple of weeks.\\n\\nDon't spend on buying very old hardware if you're just starting. If you have 16GB RAM on your desktop you can already play with 7-8B parameters to get your feet wet, find and learn how to  use the myriad of available frameworks and UIs, and find your favorites.\\n\\nOnce you really know what you're doing, you can look at buying hardware based on the use cases you have in mind, and your expectations or needs for performance.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0tlmzt","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Work is a very relative word here. What are your expectations of speed? What do you expect 5o do with the models?&lt;/p&gt;\\n\\n&lt;p&gt;Those Opteron are so old that more recent DDR4 desktop platforms will perform faster. They&amp;#39;re also PCIe Gen 2, which will make things slower if you run models that don&amp;#39;t fit on a single GPU. The Kepler based Tesla or Quadro cards you looked at aren&amp;#39;t true 24GB cards. They&amp;#39;re dual 12GB GPUs on one card. Kepler is also so old that it&amp;#39;s not much faster than said more recent desktop CPU.&lt;/p&gt;\\n\\n&lt;p&gt;Rather than spending money on this, and assuming you have a relatively recent desktop, you could upgrade said desktop RAM to 64GB to get your feet wet.&lt;/p&gt;\\n\\n&lt;p&gt;Ollama will be fine for the first week or two. You&amp;#39;ll quickly outgrow it if you&amp;#39;re experimenting. It&amp;#39;s based on llama.cpp, so you might as well skip it and go straight to learning how to use llama.cpp. Ollama also fornicates with model names, which can lead to a lot of frustration and disappointment. So, again you might just as well skip it and download your models from HuggingFace. You&amp;#39;ll end up there anyway after a couple of weeks.&lt;/p&gt;\\n\\n&lt;p&gt;Don&amp;#39;t spend on buying very old hardware if you&amp;#39;re just starting. If you have 16GB RAM on your desktop you can already play with 7-8B parameters to get your feet wet, find and learn how to  use the myriad of available frameworks and UIs, and find your favorites.&lt;/p&gt;\\n\\n&lt;p&gt;Once you really know what you&amp;#39;re doing, you can look at buying hardware based on the use cases you have in mind, and your expectations or needs for performance.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpbamg/local_ai_platform_on_older_machine/n0tlmzt/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751401148,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lpbamg","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0udyzc","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Cergorach","can_mod_post":false,"created_utc":1751409656,"send_replies":true,"parent_id":"t3_1lpbamg","score":1,"author_fullname":"t2_cs4w88d2l","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"How are you saving costs with running your own LLM? It's not just parts, but also power running and if you go budget you'll be using the lobotomized models at very slow speeds.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0udyzc","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;How are you saving costs with running your own LLM? It&amp;#39;s not just parts, but also power running and if you go budget you&amp;#39;ll be using the lobotomized models at very slow speeds.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpbamg/local_ai_platform_on_older_machine/n0udyzc/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751409656,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lpbamg","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0va7tt","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"jsconiers","can_mod_post":false,"created_utc":1751420801,"send_replies":true,"parent_id":"t3_1lpbamg","score":1,"author_fullname":"t2_913y7e69","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Similar situation as an IT professional who wanted run a local LLM.  Use an old desktop that you can upgrade as needed and then if needed build a machine.  I ran an i5 desktop with 16gb of memory and a 1650 graphics card.  Then upgraded to more memory.  Slighter better graphics card .  Then upgraded it again before i went all out on a local LLM server build.  You can temporarily use cloud based LLMs (AWS) for free or get a small account with a provider that you can use to see the differences, performance, etc.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0va7tt","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Similar situation as an IT professional who wanted run a local LLM.  Use an old desktop that you can upgrade as needed and then if needed build a machine.  I ran an i5 desktop with 16gb of memory and a 1650 graphics card.  Then upgraded to more memory.  Slighter better graphics card .  Then upgraded it again before i went all out on a local LLM server build.  You can temporarily use cloud based LLMs (AWS) for free or get a small account with a provider that you can use to see the differences, performance, etc.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpbamg/local_ai_platform_on_older_machine/n0va7tt/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751420801,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lpbamg","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0vt2ty","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"My_Unbiased_Opinion","can_mod_post":false,"created_utc":1751427756,"send_replies":true,"parent_id":"t3_1lpbamg","score":1,"author_fullname":"t2_esiyl0yb","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I am HUGE on budget inference. Old beast used to be P40s, but those skyrocket in price. Then the M40s, but those skyrocketed in price as well. The BEST budget card right now IMHO is the Nvidia P102-100 10GB cards. They are 60 bucks a pop. For 120$ you can get 20gb and it's a Pascal card so well supported on Ollama and llama.cpp. It can even use flash attention. ","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0vt2ty","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I am HUGE on budget inference. Old beast used to be P40s, but those skyrocket in price. Then the M40s, but those skyrocketed in price as well. The BEST budget card right now IMHO is the Nvidia P102-100 10GB cards. They are 60 bucks a pop. For 120$ you can get 20gb and it&amp;#39;s a Pascal card so well supported on Ollama and llama.cpp. It can even use flash attention. &lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpbamg/local_ai_platform_on_older_machine/n0vt2ty/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751427756,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lpbamg","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0tg7ff","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"fizzy1242","can_mod_post":false,"created_utc":1751399591,"send_replies":true,"parent_id":"t3_1lpbamg","score":1,"author_fullname":"t2_16zcsx","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"i would advise against old kepler and maxwell gpus, or any gpus without tensor cores. you wont get very fast inference with those.\\n\\npascal cards seem to be \\"ok\\" with llama.cpp, but they can get quite hot and aren't the fastest either\\n\\n3060 is solid for getting your feet wet, but it's not very fast either especially on larger models. in the end, used 3090s still hold up the best in my opinion, but their prices have gone slightly up recently.","edited":1751399851,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0tg7ff","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;i would advise against old kepler and maxwell gpus, or any gpus without tensor cores. you wont get very fast inference with those.&lt;/p&gt;\\n\\n&lt;p&gt;pascal cards seem to be &amp;quot;ok&amp;quot; with llama.cpp, but they can get quite hot and aren&amp;#39;t the fastest either&lt;/p&gt;\\n\\n&lt;p&gt;3060 is solid for getting your feet wet, but it&amp;#39;s not very fast either especially on larger models. in the end, used 3090s still hold up the best in my opinion, but their prices have gone slightly up recently.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpbamg/local_ai_platform_on_older_machine/n0tg7ff/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751399591,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lpbamg","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),n=()=>e.jsx(a,{data:l});export{n as default};
