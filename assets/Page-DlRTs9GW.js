import{j as e}from"./index-BpC9hjVs.js";import{R as t}from"./RedditPostRenderer-BEo6AnSR.js";import"./index-DwkJHX1_.js";const l=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Hey guys,\\n\\nWe're the startup team behind some of the projects you might be familiar with, including **PowerInfer (https://github.com/SJTU-IPADS/PowerInfer)** and **SmallThinker (https://huggingface.co/PowerInfer/SmallThinker-3B-Preview)**. The feedback from this community has been crucial, and we're excited to give you a heads-up on our next open-source release coming in **late July**.\\n\\nWe're releasing two new MoE models, both of which we have **pre-trained from scratch** with a structure specifically optimized for efficient inference on edge devices:\\n\\n* **A new 4B Reasoning Model:** An evolution of SmallThinker with significantly improved logic capabilities.\\n* **A 20B Model:** Designed for high performance in a local-first environment.\\n\\nWe'll be releasing the **full weights, a technical report, and parts of the training dataset** for both.\\n\\nOur core focus is achieving high performance on low-power, compact hardware. To push this to the limit, we've also been developing a dedicated edge device. It's a small, self-contained unit (**around 10x7x1.5 cm**) capable of running the 20B model completely offline with a power draw of **around 30W**.\\n\\nThis is still a work in progress, but it proves what's possible with full-stack optimization. We'd love to get your feedback on this direction:\\n\\n1. For a compact, private device like this, what are the most compelling use cases you can imagine?\\n2. For developers, what kind of APIs or hardware interfaces would you want on such a device to make it truly useful for your own projects?\\n3. Any thoughts on the power/performance trade-off? Is a 30W power envelope for a 20B model something that excites you?\\n\\nWe'll be in the comments to answer questions. We're incredibly excited to share our work and believe local AI is the future we're all building together","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"[Upcoming Release &amp; Feedback] A new 4B &amp; 20B model, building on our SmallThinker work. Plus, a new hardware device to run them locally.","link_flair_richtext":[{"e":"text","t":"Discussion"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lqpm60","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.87,"author_flair_background_color":null,"subreddit_type":"public","ups":28,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_1qznwxvu","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Discussion","can_mod_post":false,"score":28,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"post_hint":"self","content_categories":null,"is_self":true,"mod_note":null,"created":1751549323,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Hey guys,&lt;/p&gt;\\n\\n&lt;p&gt;We&amp;#39;re the startup team behind some of the projects you might be familiar with, including &lt;strong&gt;PowerInfer (&lt;a href=\\"https://github.com/SJTU-IPADS/PowerInfer\\"&gt;https://github.com/SJTU-IPADS/PowerInfer&lt;/a&gt;)&lt;/strong&gt; and &lt;strong&gt;SmallThinker (&lt;a href=\\"https://huggingface.co/PowerInfer/SmallThinker-3B-Preview\\"&gt;https://huggingface.co/PowerInfer/SmallThinker-3B-Preview&lt;/a&gt;)&lt;/strong&gt;. The feedback from this community has been crucial, and we&amp;#39;re excited to give you a heads-up on our next open-source release coming in &lt;strong&gt;late July&lt;/strong&gt;.&lt;/p&gt;\\n\\n&lt;p&gt;We&amp;#39;re releasing two new MoE models, both of which we have &lt;strong&gt;pre-trained from scratch&lt;/strong&gt; with a structure specifically optimized for efficient inference on edge devices:&lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;&lt;strong&gt;A new 4B Reasoning Model:&lt;/strong&gt; An evolution of SmallThinker with significantly improved logic capabilities.&lt;/li&gt;\\n&lt;li&gt;&lt;strong&gt;A 20B Model:&lt;/strong&gt; Designed for high performance in a local-first environment.&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;p&gt;We&amp;#39;ll be releasing the &lt;strong&gt;full weights, a technical report, and parts of the training dataset&lt;/strong&gt; for both.&lt;/p&gt;\\n\\n&lt;p&gt;Our core focus is achieving high performance on low-power, compact hardware. To push this to the limit, we&amp;#39;ve also been developing a dedicated edge device. It&amp;#39;s a small, self-contained unit (&lt;strong&gt;around 10x7x1.5 cm&lt;/strong&gt;) capable of running the 20B model completely offline with a power draw of &lt;strong&gt;around 30W&lt;/strong&gt;.&lt;/p&gt;\\n\\n&lt;p&gt;This is still a work in progress, but it proves what&amp;#39;s possible with full-stack optimization. We&amp;#39;d love to get your feedback on this direction:&lt;/p&gt;\\n\\n&lt;ol&gt;\\n&lt;li&gt;For a compact, private device like this, what are the most compelling use cases you can imagine?&lt;/li&gt;\\n&lt;li&gt;For developers, what kind of APIs or hardware interfaces would you want on such a device to make it truly useful for your own projects?&lt;/li&gt;\\n&lt;li&gt;Any thoughts on the power/performance trade-off? Is a 30W power envelope for a 20B model something that excites you?&lt;/li&gt;\\n&lt;/ol&gt;\\n\\n&lt;p&gt;We&amp;#39;ll be in the comments to answer questions. We&amp;#39;re incredibly excited to share our work and believe local AI is the future we&amp;#39;re all building together&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"preview":{"images":[{"source":{"url":"https://external-preview.redd.it/NTcsdPEUrzmYyre3A2GnLmnyWG2Gi3Ui77PBSAG39aI.png?auto=webp&amp;s=1aec8de7771a3a6bf2fb7294273c490dc06dcabd","width":1200,"height":600},"resolutions":[{"url":"https://external-preview.redd.it/NTcsdPEUrzmYyre3A2GnLmnyWG2Gi3Ui77PBSAG39aI.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=035b7f53a7b18dd7b7ecb29766539170f3263cdd","width":108,"height":54},{"url":"https://external-preview.redd.it/NTcsdPEUrzmYyre3A2GnLmnyWG2Gi3Ui77PBSAG39aI.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=886c0dc66edad038eb63775c7c89f33b970808be","width":216,"height":108},{"url":"https://external-preview.redd.it/NTcsdPEUrzmYyre3A2GnLmnyWG2Gi3Ui77PBSAG39aI.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=e6893b211bf4a8cff73e02050ba60d32bb76b4e7","width":320,"height":160},{"url":"https://external-preview.redd.it/NTcsdPEUrzmYyre3A2GnLmnyWG2Gi3Ui77PBSAG39aI.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=45900a8c745f7b4a70676f45ecc603e4d0d5b769","width":640,"height":320},{"url":"https://external-preview.redd.it/NTcsdPEUrzmYyre3A2GnLmnyWG2Gi3Ui77PBSAG39aI.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=d93656512791ac056fda1d75e5b42132e5505b99","width":960,"height":480},{"url":"https://external-preview.redd.it/NTcsdPEUrzmYyre3A2GnLmnyWG2Gi3Ui77PBSAG39aI.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=043adc578549f1f078c89c4ba97448d3830f71d3","width":1080,"height":540}],"variants":{},"id":"NTcsdPEUrzmYyre3A2GnLmnyWG2Gi3Ui77PBSAG39aI"}],"enabled":false},"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"5f921ea4-c7bc-11ed-9c23-3a00622979b4","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#646d73","id":"1lqpm60","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"yzmizeyu","discussion_type":null,"num_comments":17,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lqpm60/upcoming_release_feedback_a_new_4b_20b_model/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lqpm60/upcoming_release_feedback_a_new_4b_20b_model/","subreddit_subscribers":494198,"created_utc":1751549323,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n14qxhc","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"yzmizeyu","can_mod_post":false,"created_utc":1751552564,"send_replies":true,"parent_id":"t1_n14k0do","score":3,"author_fullname":"t2_1qznwxvu","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"This is super valuable feedback for our team. Thank you!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n14qxhc","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;This is super valuable feedback for our team. Thank you!&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lqpm60","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqpm60/upcoming_release_feedback_a_new_4b_20b_model/n14qxhc/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751552564,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n14k0do","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"No-Refrigerator-1672","can_mod_post":false,"created_utc":1751550449,"send_replies":true,"parent_id":"t3_1lqpm60","score":11,"author_fullname":"t2_baavelp5","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I think that your listed device spec is a perfect fit for HomeAssistant, which is self-hosted smart home OS. It has a built-in voice assistant plug in that can interface with AI through OpenAI API, allowing them to take control of your house. However, this requires quite a hefty context length: like 8k at least; ideally more, cause the HA lists all the available devices and their actions in the prompt (basically tool calling). Also ideally you'd want to have double that capacity, so that HA prompt can sit in the cache and not require re-processing, allowing low-latency interactions in cases when your small processor serves both HA and something else, i.e. OpenWebUI.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n14k0do","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I think that your listed device spec is a perfect fit for HomeAssistant, which is self-hosted smart home OS. It has a built-in voice assistant plug in that can interface with AI through OpenAI API, allowing them to take control of your house. However, this requires quite a hefty context length: like 8k at least; ideally more, cause the HA lists all the available devices and their actions in the prompt (basically tool calling). Also ideally you&amp;#39;d want to have double that capacity, so that HA prompt can sit in the cache and not require re-processing, allowing low-latency interactions in cases when your small processor serves both HA and something else, i.e. OpenWebUI.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqpm60/upcoming_release_feedback_a_new_4b_20b_model/n14k0do/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751550449,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lqpm60","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":11}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n14ozbh","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"yzmizeyu","can_mod_post":false,"created_utc":1751551977,"send_replies":true,"parent_id":"t1_n14i9oj","score":5,"author_fullname":"t2_1qznwxvu","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Thanks so much for the kind words and for following our work! It's super motivating. The \\"automation\\" use case is exactly what we're targeting with our agent framework.\\n\\nCould you give me an example of the kind of automation you're thinking of? Is it for personal workflows, smart home tasks, or something else?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n14ozbh","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thanks so much for the kind words and for following our work! It&amp;#39;s super motivating. The &amp;quot;automation&amp;quot; use case is exactly what we&amp;#39;re targeting with our agent framework.&lt;/p&gt;\\n\\n&lt;p&gt;Could you give me an example of the kind of automation you&amp;#39;re thinking of? Is it for personal workflows, smart home tasks, or something else?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lqpm60","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqpm60/upcoming_release_feedback_a_new_4b_20b_model/n14ozbh/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751551977,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}}],"before":null}},"user_reports":[],"saved":false,"id":"n14i9oj","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Felladrin","can_mod_post":false,"created_utc":1751549890,"send_replies":true,"parent_id":"t3_1lqpm60","score":7,"author_fullname":"t2_wp9mv","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I admire your work on powerinfer and smallthinker! And would interested in an edge device with low consumption; mainly for automation where a sequence of single-shot answers from small models is all I need. 20B is good enough!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n14i9oj","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I admire your work on powerinfer and smallthinker! And would interested in an edge device with low consumption; mainly for automation where a sequence of single-shot answers from small models is all I need. 20B is good enough!&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqpm60/upcoming_release_feedback_a_new_4b_20b_model/n14i9oj/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751549890,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lqpm60","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":7}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n14khfo","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"idesireawill","can_mod_post":false,"created_utc":1751550598,"send_replies":true,"parent_id":"t1_n14hn2c","score":3,"author_fullname":"t2_60p5qnji","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Lets me elaborate on few points,\\n\\nFor maximal use case, the best device would run at least 20 k Token on a 70 B model with 20 tk/s prompt generation, portability would be a better benefit for me rather than power, because then i can use it both at home and in business setting.  Maybe it can come with an additional software so that i can embed and store my documents on my local computer and when i plug the device i can directly run a predefined RAG with it, but when i choose not to i can use it as an llm.\\n\\nIdeally you should aim for 30 B model and 10 k content length for QWEN and simple coding\\n\\nIf you can make it a portable handheld that runs simple linux, few agents/ workflows with langgraph or n8n, with tethered internet provided and wifi and monitor pluggable , this would be a nice device.\\n\\nIf you can make them stackable with an affordable price maybe different people with different needs can buy different amounts","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n14khfo","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Lets me elaborate on few points,&lt;/p&gt;\\n\\n&lt;p&gt;For maximal use case, the best device would run at least 20 k Token on a 70 B model with 20 tk/s prompt generation, portability would be a better benefit for me rather than power, because then i can use it both at home and in business setting.  Maybe it can come with an additional software so that i can embed and store my documents on my local computer and when i plug the device i can directly run a predefined RAG with it, but when i choose not to i can use it as an llm.&lt;/p&gt;\\n\\n&lt;p&gt;Ideally you should aim for 30 B model and 10 k content length for QWEN and simple coding&lt;/p&gt;\\n\\n&lt;p&gt;If you can make it a portable handheld that runs simple linux, few agents/ workflows with langgraph or n8n, with tethered internet provided and wifi and monitor pluggable , this would be a nice device.&lt;/p&gt;\\n\\n&lt;p&gt;If you can make them stackable with an affordable price maybe different people with different needs can buy different amounts&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lqpm60","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqpm60/upcoming_release_feedback_a_new_4b_20b_model/n14khfo/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751550598,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n14olt2","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"idesireawill","can_mod_post":false,"send_replies":true,"parent_id":"t1_n14o447","score":2,"author_fullname":"t2_60p5qnji","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"The benefit of 70B models are obvious otherwise. Larger context and more cohesive output","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n14olt2","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The benefit of 70B models are obvious otherwise. Larger context and more cohesive output&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lqpm60","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqpm60/upcoming_release_feedback_a_new_4b_20b_model/n14olt2/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751551863,"author_flair_text":null,"treatment_tags":[],"created_utc":1751551863,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n14o447","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"idesireawill","can_mod_post":false,"send_replies":true,"parent_id":"t1_n14km9x","score":3,"author_fullname":"t2_60p5qnji","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I phrased that wrong, a tamagotchi wouldnt be my first target if i can run 70B model locally with that speed. It was just an idea that i believed to make the product sell more. The content size can allow more creative interactions with the tamagotchi.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n14o447","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I phrased that wrong, a tamagotchi wouldnt be my first target if i can run 70B model locally with that speed. It was just an idea that i believed to make the product sell more. The content size can allow more creative interactions with the tamagotchi.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lqpm60","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqpm60/upcoming_release_feedback_a_new_4b_20b_model/n14o447/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751551715,"author_flair_text":null,"treatment_tags":[],"created_utc":1751551715,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n14km9x","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"yzmizeyu","can_mod_post":false,"created_utc":1751550640,"send_replies":true,"parent_id":"t1_n14hn2c","score":2,"author_fullname":"t2_1qznwxvu","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Great point, and thanks! A 70B device is the dream. We're starting with 20B to balance performance and power on our hardware.\\nFor your \\"modern tamagotchi\\" use case, what's the one key capability that makes 70B is necessary\\"? Really curious about the specific scenarios.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n14km9x","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Great point, and thanks! A 70B device is the dream. We&amp;#39;re starting with 20B to balance performance and power on our hardware.\\nFor your &amp;quot;modern tamagotchi&amp;quot; use case, what&amp;#39;s the one key capability that makes 70B is necessary&amp;quot;? Really curious about the specific scenarios.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lqpm60","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqpm60/upcoming_release_feedback_a_new_4b_20b_model/n14km9x/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751550640,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n14hn2c","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"idesireawill","can_mod_post":false,"created_utc":1751549684,"send_replies":true,"parent_id":"t3_1lqpm60","score":2,"author_fullname":"t2_60p5qnji","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I dont see a use for a 20B model, but i would seriously consider having an phone sized device to run 70 B model in 15 tk/s or more. With a decent battery and an average screen you have a modern tamagotchi :)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n14hn2c","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I dont see a use for a 20B model, but i would seriously consider having an phone sized device to run 70 B model in 15 tk/s or more. With a decent battery and an average screen you have a modern tamagotchi :)&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqpm60/upcoming_release_feedback_a_new_4b_20b_model/n14hn2c/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751549684,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lqpm60","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n14mj4o","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"generaluser123","can_mod_post":false,"created_utc":1751551227,"send_replies":true,"parent_id":"t3_1lqpm60","score":2,"author_fullname":"t2_127304","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"We are working on ai agent in healthcare setting to answer simple questions. We could use a 8b model with 0.5b parakeet asr and tts","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n14mj4o","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;We are working on ai agent in healthcare setting to answer simple questions. We could use a 8b model with 0.5b parakeet asr and tts&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqpm60/upcoming_release_feedback_a_new_4b_20b_model/n14mj4o/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751551227,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lqpm60","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n14rxvl","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"JohnTheNerd3","can_mod_post":false,"created_utc":1751552859,"send_replies":true,"parent_id":"t3_1lqpm60","score":2,"author_fullname":"t2_ctm6fsu7","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I feel like aiming for the ~32b range may be better since most models created nowadays have that parameter count. I would also be curious to know whether it can only run _that_ 20b?\\n\\n\\nalso, echoing the Home Assistant message above, that would be a very interesting usecase. it currently relies on tool calling and has a very prefill-heavy workflow of huge contexts (8k+ is typical). decode speed is not as crucial since we stream everything other tool calls (consider that it's a voice assistant, therefore you just need to be faster than the _speech_) and the responses tend to be fairly short compared to the context.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n14rxvl","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I feel like aiming for the ~32b range may be better since most models created nowadays have that parameter count. I would also be curious to know whether it can only run &lt;em&gt;that&lt;/em&gt; 20b?&lt;/p&gt;\\n\\n&lt;p&gt;also, echoing the Home Assistant message above, that would be a very interesting usecase. it currently relies on tool calling and has a very prefill-heavy workflow of huge contexts (8k+ is typical). decode speed is not as crucial since we stream everything other tool calls (consider that it&amp;#39;s a voice assistant, therefore you just need to be faster than the &lt;em&gt;speech&lt;/em&gt;) and the responses tend to be fairly short compared to the context.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqpm60/upcoming_release_feedback_a_new_4b_20b_model/n14rxvl/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751552859,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lqpm60","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n14le7j","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Felladrin","can_mod_post":false,"created_utc":1751550877,"send_replies":true,"parent_id":"t1_n14ivuf","score":5,"author_fullname":"t2_wp9mv","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Could you share the tokens/second for this case?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n14le7j","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Could you share the tokens/second for this case?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lqpm60","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqpm60/upcoming_release_feedback_a_new_4b_20b_model/n14le7j/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751550877,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}}],"before":null}},"user_reports":[],"saved":false,"id":"n14ivuf","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"emprahsFury","can_mod_post":false,"created_utc":1751550089,"send_replies":false,"parent_id":"t3_1lqpm60","score":2,"author_fullname":"t2_177r8n","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"A raspberry pi can run a 20b model in 8.5 x 5.6 cm too, at 20w @$150. What are the performance characteristics of the proposed box?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n14ivuf","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;A raspberry pi can run a 20b model in 8.5 x 5.6 cm too, at 20w @$150. What are the performance characteristics of the proposed box?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqpm60/upcoming_release_feedback_a_new_4b_20b_model/n14ivuf/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751550089,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lqpm60","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n14qn0o","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"RemindMeBot","can_mod_post":false,"created_utc":1751552477,"send_replies":true,"parent_id":"t1_n14qgx2","score":1,"author_fullname":"t2_gbm4p","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I will be messaging you in 7 days on [**2025-07-10 14:20:27 UTC**](http://www.wolframalpha.com/input/?i=2025-07-10%2014:20:27%20UTC%20To%20Local%20Time) to remind you of [**this link**](https://www.reddit.com/r/LocalLLaMA/comments/1lqpm60/upcoming_release_feedback_a_new_4b_20b_model/n14qgx2/?context=3)\\n\\n[**1 OTHERS CLICKED THIS LINK**](https://www.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2FLocalLLaMA%2Fcomments%2F1lqpm60%2Fupcoming_release_feedback_a_new_4b_20b_model%2Fn14qgx2%2F%5D%0A%0ARemindMe%21%202025-07-10%2014%3A20%3A27%20UTC) to send a PM to also be reminded and to reduce spam.\\n\\n^(Parent commenter can ) [^(delete this message to hide from others.)](https://www.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Delete%20Comment&amp;message=Delete%21%201lqpm60)\\n\\n*****\\n\\n|[^(Info)](https://www.reddit.com/r/RemindMeBot/comments/e1bko7/remindmebot_info_v21/)|[^(Custom)](https://www.reddit.com/message/compose/?to=RemindMeBot&amp;subject=Reminder&amp;message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here)|[^(Your Reminders)](https://www.reddit.com/message/compose/?to=RemindMeBot&amp;subject=List%20Of%20Reminders&amp;message=MyReminders%21)|[^(Feedback)](https://www.reddit.com/message/compose/?to=Watchful1&amp;subject=RemindMeBot%20Feedback)|\\n|-|-|-|-|","edited":1751560421,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n14qn0o","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I will be messaging you in 7 days on &lt;a href=\\"http://www.wolframalpha.com/input/?i=2025-07-10%2014:20:27%20UTC%20To%20Local%20Time\\"&gt;&lt;strong&gt;2025-07-10 14:20:27 UTC&lt;/strong&gt;&lt;/a&gt; to remind you of &lt;a href=\\"https://www.reddit.com/r/LocalLLaMA/comments/1lqpm60/upcoming_release_feedback_a_new_4b_20b_model/n14qgx2/?context=3\\"&gt;&lt;strong&gt;this link&lt;/strong&gt;&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://www.reddit.com/message/compose/?to=RemindMeBot&amp;amp;subject=Reminder&amp;amp;message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2FLocalLLaMA%2Fcomments%2F1lqpm60%2Fupcoming_release_feedback_a_new_4b_20b_model%2Fn14qgx2%2F%5D%0A%0ARemindMe%21%202025-07-10%2014%3A20%3A27%20UTC\\"&gt;&lt;strong&gt;1 OTHERS CLICKED THIS LINK&lt;/strong&gt;&lt;/a&gt; to send a PM to also be reminded and to reduce spam.&lt;/p&gt;\\n\\n&lt;p&gt;&lt;sup&gt;Parent commenter can &lt;/sup&gt; &lt;a href=\\"https://www.reddit.com/message/compose/?to=RemindMeBot&amp;amp;subject=Delete%20Comment&amp;amp;message=Delete%21%201lqpm60\\"&gt;&lt;sup&gt;delete this message to hide from others.&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;hr/&gt;\\n\\n&lt;table&gt;&lt;thead&gt;\\n&lt;tr&gt;\\n&lt;th&gt;&lt;a href=\\"https://www.reddit.com/r/RemindMeBot/comments/e1bko7/remindmebot_info_v21/\\"&gt;&lt;sup&gt;Info&lt;/sup&gt;&lt;/a&gt;&lt;/th&gt;\\n&lt;th&gt;&lt;a href=\\"https://www.reddit.com/message/compose/?to=RemindMeBot&amp;amp;subject=Reminder&amp;amp;message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here\\"&gt;&lt;sup&gt;Custom&lt;/sup&gt;&lt;/a&gt;&lt;/th&gt;\\n&lt;th&gt;&lt;a href=\\"https://www.reddit.com/message/compose/?to=RemindMeBot&amp;amp;subject=List%20Of%20Reminders&amp;amp;message=MyReminders%21\\"&gt;&lt;sup&gt;Your Reminders&lt;/sup&gt;&lt;/a&gt;&lt;/th&gt;\\n&lt;th&gt;&lt;a href=\\"https://www.reddit.com/message/compose/?to=Watchful1&amp;amp;subject=RemindMeBot%20Feedback\\"&gt;&lt;sup&gt;Feedback&lt;/sup&gt;&lt;/a&gt;&lt;/th&gt;\\n&lt;/tr&gt;\\n&lt;/thead&gt;&lt;tbody&gt;\\n&lt;/tbody&gt;&lt;/table&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lqpm60","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqpm60/upcoming_release_feedback_a_new_4b_20b_model/n14qn0o/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751552477,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n14qgx2","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Competitive_Ad_5515","can_mod_post":false,"created_utc":1751552427,"send_replies":true,"parent_id":"t3_1lqpm60","score":1,"author_fullname":"t2_4iz2agd1","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"!remindme 1 week","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n14qgx2","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;!remindme 1 week&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqpm60/upcoming_release_feedback_a_new_4b_20b_model/n14qgx2/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751552427,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lqpm60","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n15kqan","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Pogo4Fufu","can_mod_post":false,"created_utc":1751561050,"send_replies":true,"parent_id":"t3_1lqpm60","score":1,"author_fullname":"t2_3q3msk1c","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Well, it must beat a Mini PC in price and performance significantly. Those small Mini PC with Ryzen 5/7/9 and 64GB RAM are quite nice for home usage. A3B and MoE make them quite useful - and they draw also not really that much power. Even my older one with a AMD Ryzen 7 PRO 5875U and slow DDR4 is fine for home usage.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n15kqan","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Well, it must beat a Mini PC in price and performance significantly. Those small Mini PC with Ryzen 5/7/9 and 64GB RAM are quite nice for home usage. A3B and MoE make them quite useful - and they draw also not really that much power. Even my older one with a AMD Ryzen 7 PRO 5875U and slow DDR4 is fine for home usage.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqpm60/upcoming_release_feedback_a_new_4b_20b_model/n15kqan/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751561050,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lqpm60","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),n=()=>e.jsx(t,{data:l});export{n as default};
