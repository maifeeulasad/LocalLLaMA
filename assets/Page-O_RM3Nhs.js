import{j as e}from"./index-Cd3v0jxz.js";import{R as t}from"./RedditPostRenderer-cI96YLhy.js";import"./index-DGBoKyQm.js";const a=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Source: [https://www.kaggle.com/whitepaper-prompt-engineering](https://www.kaggle.com/whitepaper-prompt-engineering)","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Should I really always set temperature to 0 with reasoning models?","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":85,"top_awarded_type":null,"hide_score":false,"name":"t3_1m82rai","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.45,"author_flair_background_color":null,"ups":0,"total_awards_received":0,"media_embed":{},"thumbnail_width":140,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_gm504","secure_media":null,"is_reddit_media_domain":true,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":0,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"https://b.thumbs.redditmedia.com/YhFZIjkjsb-4XKaiV4ZRlNZQXf-cA5YA0jzoX-CSzHU.jpg","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"post_hint":"image","content_categories":null,"is_self":false,"subreddit_type":"public","created":1753359252,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"i.redd.it","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Source: &lt;a href=\\"https://www.kaggle.com/whitepaper-prompt-engineering\\"&gt;https://www.kaggle.com/whitepaper-prompt-engineering&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"url_overridden_by_dest":"https://i.redd.it/frmtfk84dtef1.png","view_count":null,"archived":false,"no_follow":true,"is_crosspostable":false,"pinned":false,"over_18":false,"preview":{"images":[{"source":{"url":"https://preview.redd.it/frmtfk84dtef1.png?auto=webp&amp;s=87587b3ac28a3c5a40e79b94697358ba56f930ec","width":1088,"height":664},"resolutions":[{"url":"https://preview.redd.it/frmtfk84dtef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=7a182307404ac5bbcd60d8f87ed73c52e1967522","width":108,"height":65},{"url":"https://preview.redd.it/frmtfk84dtef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=f6e6e7953a005b6e57493881dd1ce8aabc96e7fd","width":216,"height":131},{"url":"https://preview.redd.it/frmtfk84dtef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=4474981eb1093d15657b37d636345d44b0c18a6a","width":320,"height":195},{"url":"https://preview.redd.it/frmtfk84dtef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=231ffe96efd02ba4bc1e23701e27c87733679715","width":640,"height":390},{"url":"https://preview.redd.it/frmtfk84dtef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=aa7a22d2357d9c8c7db13292105e4c759ddd204f","width":960,"height":585},{"url":"https://preview.redd.it/frmtfk84dtef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=31dcaff3b6efa21b108a1e14f2b637bed8a2599c","width":1080,"height":659}],"variants":{},"id":"aDhAraT9ozyppqMWq0DYLEBY-gQpRb-l0vXjZ6GxNbc"}],"enabled":true},"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"mod_note":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"num_reports":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1m82rai","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"robertpiosik","discussion_type":null,"num_comments":5,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1m82rai/should_i_really_always_set_temperature_to_0_with/","stickied":false,"url":"https://i.redd.it/frmtfk84dtef1.png","subreddit_subscribers":504023,"created_utc":1753359252,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4vzoxj","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"NNN_Throwaway2","can_mod_post":false,"created_utc":1753360057,"send_replies":true,"parent_id":"t3_1m82rai","score":15,"author_fullname":"t2_8rrihts9","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"No. Using greedy decoding with reasoning models can cause endless repetition. Also, CoT is different than reasoning.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4vzoxj","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;No. Using greedy decoding with reasoning models can cause endless repetition. Also, CoT is different than reasoning.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m82rai/should_i_really_always_set_temperature_to_0_with/n4vzoxj/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753360057,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m82rai","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":15}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4w1fl0","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Entubulated","can_mod_post":false,"created_utc":1753360677,"send_replies":true,"parent_id":"t3_1m82rai","score":3,"author_fullname":"t2_1opxde6hyq","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"This advice is situational. For recommended inferencing settings, look for documentation from the model's publisher first, experiment later. Might save yourself some time and aggravation.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4w1fl0","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;This advice is situational. For recommended inferencing settings, look for documentation from the model&amp;#39;s publisher first, experiment later. Might save yourself some time and aggravation.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m82rai/should_i_really_always_set_temperature_to_0_with/n4w1fl0/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753360677,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m82rai","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4vynsy","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"AppearanceHeavy6724","can_mod_post":false,"created_utc":1753359681,"send_replies":true,"parent_id":"t3_1m82rai","score":7,"author_fullname":"t2_uz37qfx5","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"From what I understant it is about CoT prompting of non-reasoning models. I never heard though about temp=0 in tat case.\\n\\nWith proper reasoning models you should  _never_ set T=0, normal T for such model is around 0.6.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4vynsy","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;From what I understant it is about CoT prompting of non-reasoning models. I never heard though about temp=0 in tat case.&lt;/p&gt;\\n\\n&lt;p&gt;With proper reasoning models you should  &lt;em&gt;never&lt;/em&gt; set T=0, normal T for such model is around 0.6.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m82rai/should_i_really_always_set_temperature_to_0_with/n4vynsy/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753359681,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m82rai","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":7}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4w1avp","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"milo-75","can_mod_post":false,"created_utc":1753360631,"send_replies":true,"parent_id":"t3_1m82rai","score":2,"author_fullname":"t2_txewhr0q","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"This is talking about CoT promoting, and not reasoning models, but still setting temp to zero is probably not what you want. Sure, if the model happens to get the answer right for the question asked, temp zero will make it possible to consistently get the same answer across multiple attempts. However, if the model comes up with the wrong answer, you’ll be sure to get the wrong answer every time. With CoT, you likely want to set a non-zero temp, then ask the model the same question a few times (say, 5), then have the model look at all five answers and give you a final answer based on the consensus of the different answers. \\n\\nWith reasoning models that were trained with RL to refine thought traces, you will want to use the same temp the model was trained with(ie the temp that was used to generate the thought traces in the first place), or you’ll get degraded reasoning performance.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4w1avp","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;This is talking about CoT promoting, and not reasoning models, but still setting temp to zero is probably not what you want. Sure, if the model happens to get the answer right for the question asked, temp zero will make it possible to consistently get the same answer across multiple attempts. However, if the model comes up with the wrong answer, you’ll be sure to get the wrong answer every time. With CoT, you likely want to set a non-zero temp, then ask the model the same question a few times (say, 5), then have the model look at all five answers and give you a final answer based on the consensus of the different answers. &lt;/p&gt;\\n\\n&lt;p&gt;With reasoning models that were trained with RL to refine thought traces, you will want to use the same temp the model was trained with(ie the temp that was used to generate the thought traces in the first place), or you’ll get degraded reasoning performance.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m82rai/should_i_really_always_set_temperature_to_0_with/n4w1avp/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753360631,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m82rai","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4wafq8","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Thomas-Lore","can_mod_post":false,"created_utc":1753363632,"send_replies":true,"parent_id":"t3_1m82rai","score":2,"author_fullname":"t2_5hobp6m4","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Definitely no. Gemini Pro 2.5 was shown in some use cases to work best at 0.7 for example.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4wafq8","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Definitely no. Gemini Pro 2.5 was shown in some use cases to work best at 0.7 for example.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m82rai/should_i_really_always_set_temperature_to_0_with/n4wafq8/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753363632,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m82rai","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}}]`),s=()=>e.jsx(t,{data:a});export{s as default};
