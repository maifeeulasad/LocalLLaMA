import{j as e}from"./index-DACS7Nh6.js";import{R as l}from"./RedditPostRenderer-Dqa1NZuX.js";import"./index-DiMIVQx4.js";const t=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Hi all, noob here so forgive the noobitude.\\n\\nRelatively new to the AI coding tool space, started with copilot in VScode, it was OK, then moved to cursor which is/was awesome for a couple months, now it's nerfed get capped even on $200 plan within a couple weeks of the month, auto mode is \\"ok\\". Tried claude code but wasn't really for me, I prefer the IDE interface of cursor or VSCode.\\n\\nI'm now finding that even claude code is constantly timing out, cursor auto just doesn't have the context window for a lot of what I need...\\n\\nI have a 3090, I've been trying to find out if there are any models worth running locally which have tooling agentic capabilities to then run in either cursor or VSCode. From what I've read (not heaps) it sounds like a lot of the open source models that can be run on a 3090 aren't really set up to work with tooling, so won't give a similar experience to cursor or copilot yet. But the space moves so fast so maybe there is something workable now?\\n\\nObviously I'm not expecting Claude level performance, but I wanted to see what's available and give something a try. Even if it's only 70% as good, if it's at least reliable and cheap then it might be good enough for what I am doing.\\n\\nTIA","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Any local models with decent tooling capabilities worth running with 3090?","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1m3nwlf","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.79,"author_flair_background_color":null,"subreddit_type":"public","ups":11,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_1rqkouqs3q","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":11,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1752901601,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Hi all, noob here so forgive the noobitude.&lt;/p&gt;\\n\\n&lt;p&gt;Relatively new to the AI coding tool space, started with copilot in VScode, it was OK, then moved to cursor which is/was awesome for a couple months, now it&amp;#39;s nerfed get capped even on $200 plan within a couple weeks of the month, auto mode is &amp;quot;ok&amp;quot;. Tried claude code but wasn&amp;#39;t really for me, I prefer the IDE interface of cursor or VSCode.&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;m now finding that even claude code is constantly timing out, cursor auto just doesn&amp;#39;t have the context window for a lot of what I need...&lt;/p&gt;\\n\\n&lt;p&gt;I have a 3090, I&amp;#39;ve been trying to find out if there are any models worth running locally which have tooling agentic capabilities to then run in either cursor or VSCode. From what I&amp;#39;ve read (not heaps) it sounds like a lot of the open source models that can be run on a 3090 aren&amp;#39;t really set up to work with tooling, so won&amp;#39;t give a similar experience to cursor or copilot yet. But the space moves so fast so maybe there is something workable now?&lt;/p&gt;\\n\\n&lt;p&gt;Obviously I&amp;#39;m not expecting Claude level performance, but I wanted to see what&amp;#39;s available and give something a try. Even if it&amp;#39;s only 70% as good, if it&amp;#39;s at least reliable and cheap then it might be good enough for what I am doing.&lt;/p&gt;\\n\\n&lt;p&gt;TIA&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1m3nwlf","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"Acceptable_Adagio_91","discussion_type":null,"num_comments":27,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1m3nwlf/any_local_models_with_decent_tooling_capabilities/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1m3nwlf/any_local_models_with_decent_tooling_capabilities/","subreddit_subscribers":501527,"created_utc":1752901601,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3ya5qa","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"dat_cosmo_cat","can_mod_post":false,"created_utc":1752905576,"send_replies":true,"parent_id":"t3_1m3nwlf","score":19,"author_fullname":"t2_2e6gzozr","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"* **Step 1:** raise $80,000 from angel investors. Tell them you are doing an AI startup\\n* **Step 2:** convince Lambda, Exxact Corp, or Supermicro to sell you an underpopulated H200 NVL server\\n* **Step 3:** load giant LLM model into [Open Code](https://github.com/sst/opencode) and type prompt: \\"Create a 1 billion dollar App, make no mistakes\\"\\n* **Step 4:** profit","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3ya5qa","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;ul&gt;\\n&lt;li&gt;&lt;strong&gt;Step 1:&lt;/strong&gt; raise $80,000 from angel investors. Tell them you are doing an AI startup&lt;/li&gt;\\n&lt;li&gt;&lt;strong&gt;Step 2:&lt;/strong&gt; convince Lambda, Exxact Corp, or Supermicro to sell you an underpopulated H200 NVL server&lt;/li&gt;\\n&lt;li&gt;&lt;strong&gt;Step 3:&lt;/strong&gt; load giant LLM model into &lt;a href=\\"https://github.com/sst/opencode\\"&gt;Open Code&lt;/a&gt; and type prompt: &amp;quot;Create a 1 billion dollar App, make no mistakes&amp;quot;&lt;/li&gt;\\n&lt;li&gt;&lt;strong&gt;Step 4:&lt;/strong&gt; profit&lt;/li&gt;\\n&lt;/ul&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":true,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3nwlf/any_local_models_with_decent_tooling_capabilities/n3ya5qa/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752905576,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m3nwlf","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":19}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"7dba5c08-72f1-11ee-9b6f-ca195bc297d4","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"7dba5c08-72f1-11ee-9b6f-ca195bc297d4","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3y8is0","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"cbterry","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3y6fat","score":3,"author_fullname":"t2_qfzsn","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I use a discord bot with tool calling. It can read files, search with searx, read local notes, make images, etc. I just convert MCP tools. Anything smaller than 30b and it'll generally loop or fail.\\n\\n\\nOtherwise I use a CLI script that does the same. For one shot coding, it is pretty decent, but an actual coding agent would be a lot better.\\n\\n\\nI also use a slightly modified version of llm-conversation which makes it kind of like any other agent. Create a tech lead, backend, UI and doc personas and they generate then iterate over the code, fixing it, and it comes out pretty well. I haven't tried it for larger projects though, and I haven't added tool calling to it yet.\\n\\n\\nI wrote all of the above except for llm-conversation, mostly because I'm not about to give system/internet access to scripts when I don't know how they work.","edited":1752905651,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3y8is0","is_submitter":false,"collapsed":false,"author_flair_richtext":[{"e":"text","t":"Llama 70B"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I use a discord bot with tool calling. It can read files, search with searx, read local notes, make images, etc. I just convert MCP tools. Anything smaller than 30b and it&amp;#39;ll generally loop or fail.&lt;/p&gt;\\n\\n&lt;p&gt;Otherwise I use a CLI script that does the same. For one shot coding, it is pretty decent, but an actual coding agent would be a lot better.&lt;/p&gt;\\n\\n&lt;p&gt;I also use a slightly modified version of llm-conversation which makes it kind of like any other agent. Create a tech lead, backend, UI and doc personas and they generate then iterate over the code, fixing it, and it comes out pretty well. I haven&amp;#39;t tried it for larger projects though, and I haven&amp;#39;t added tool calling to it yet.&lt;/p&gt;\\n\\n&lt;p&gt;I wrote all of the above except for llm-conversation, mostly because I&amp;#39;m not about to give system/internet access to scripts when I don&amp;#39;t know how they work.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m3nwlf","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3nwlf/any_local_models_with_decent_tooling_capabilities/n3y8is0/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752904718,"author_flair_text":"Llama 70B","treatment_tags":[],"created_utc":1752904718,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3ztm06","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"IONaut","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3y6fat","score":3,"author_fullname":"t2_6cr22","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I'm using a 3090 as well and Qwen 3 30b-A3b is the only model I've found so far that works consistently with things like Kilo code VS code extension that leverages a lot of automated steps and tool use. For things like autocomplete I use the 'Continue' extension with Qwen 2.5 7b for speed.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3ztm06","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;m using a 3090 as well and Qwen 3 30b-A3b is the only model I&amp;#39;ve found so far that works consistently with things like Kilo code VS code extension that leverages a lot of automated steps and tool use. For things like autocomplete I use the &amp;#39;Continue&amp;#39; extension with Qwen 2.5 7b for speed.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m3nwlf","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3nwlf/any_local_models_with_decent_tooling_capabilities/n3ztm06/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752932654,"author_flair_text":null,"treatment_tags":[],"created_utc":1752932654,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n3y6fat","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Acceptable_Adagio_91","can_mod_post":false,"created_utc":1752903631,"send_replies":true,"parent_id":"t1_n3y44vq","score":2,"author_fullname":"t2_1rqkouqs3q","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Cool! Would you mind giving a brief description on your setup?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3y6fat","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Cool! Would you mind giving a brief description on your setup?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m3nwlf","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3nwlf/any_local_models_with_decent_tooling_capabilities/n3y6fat/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752903631,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n3y44vq","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"cbterry","can_mod_post":false,"created_utc":1752902462,"send_replies":true,"parent_id":"t3_1m3nwlf","score":8,"author_fullname":"t2_qfzsn","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Qwen 3 30b-A3B Q4 works fairly consistently for me","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3y44vq","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"Llama 70B"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Qwen 3 30b-A3B Q4 works fairly consistently for me&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3nwlf/any_local_models_with_decent_tooling_capabilities/n3y44vq/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752902462,"author_flair_text":"Llama 70B","treatment_tags":[],"link_id":"t3_1m3nwlf","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":8}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n401tlr","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"YouDontSeemRight","can_mod_post":false,"send_replies":true,"parent_id":"t1_n401g3b","score":3,"author_fullname":"t2_1b7gjxtue9","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Same, I keep falling back to it. It's just so damn solid and does exactly what you ask.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n401tlr","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Same, I keep falling back to it. It&amp;#39;s just so damn solid and does exactly what you ask.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m3nwlf","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3nwlf/any_local_models_with_decent_tooling_capabilities/n401tlr/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752935377,"author_flair_text":null,"treatment_tags":[],"created_utc":1752935377,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n40rzob","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"sixx7","can_mod_post":false,"send_replies":true,"parent_id":"t1_n401g3b","score":1,"author_fullname":"t2_jxjl6u","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"+1 a lot of new models get hyped up and I get really excited to try them.  They all fall short of qwen3","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n40rzob","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;+1 a lot of new models get hyped up and I get really excited to try them.  They all fall short of qwen3&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m3nwlf","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3nwlf/any_local_models_with_decent_tooling_capabilities/n40rzob/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752943554,"author_flair_text":null,"treatment_tags":[],"created_utc":1752943554,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n401g3b","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"OGScottingham","can_mod_post":false,"created_utc":1752935258,"send_replies":true,"parent_id":"t1_n3zw9rq","score":4,"author_fullname":"t2_5p0rvz5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Qwen3 32b has been top dog and unbeatable for me since it came out.\\n\\nI'm hoping granite 4 will also do one about the same size that will take top spot.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n401g3b","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Qwen3 32b has been top dog and unbeatable for me since it came out.&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;m hoping granite 4 will also do one about the same size that will take top spot.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m3nwlf","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3nwlf/any_local_models_with_decent_tooling_capabilities/n401g3b/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752935258,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}}],"before":null}},"user_reports":[],"saved":false,"id":"n3zw9rq","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"YouDontSeemRight","can_mod_post":false,"created_utc":1752933566,"send_replies":true,"parent_id":"t3_1m3nwlf","score":3,"author_fullname":"t2_1b7gjxtue9","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"There's Devstral which is mistral fine tuned for agentic dev work. Ollama has a Qwen2.5 Code 32B model fine tuned for tool calling from a user named hao that works well. https://ollama.com/hhao/qwen2.5-coder-tools\\n\\nOf course Qwen3 32B is a great option.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3zw9rq","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;There&amp;#39;s Devstral which is mistral fine tuned for agentic dev work. Ollama has a Qwen2.5 Code 32B model fine tuned for tool calling from a user named hao that works well. &lt;a href=\\"https://ollama.com/hhao/qwen2.5-coder-tools\\"&gt;https://ollama.com/hhao/qwen2.5-coder-tools&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;Of course Qwen3 32B is a great option.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3nwlf/any_local_models_with_decent_tooling_capabilities/n3zw9rq/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752933566,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m3nwlf","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n400jh9","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"admajic","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3zz8xw","score":1,"author_fullname":"t2_60b9farf","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Q4 k m and the q4 cache","edited":false,"author_flair_css_class":null,"name":"t1_n400jh9","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Q4 k m and the q4 cache&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1m3nwlf","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3nwlf/any_local_models_with_decent_tooling_capabilities/n400jh9/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752934970,"author_flair_text":null,"collapsed":false,"created_utc":1752934970,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3zz8xw","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"vibjelo","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3yid7p","score":1,"author_fullname":"t2_hr2hgnsk","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"What quant you run that with if you're fitting that on a 3090?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3zz8xw","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;What quant you run that with if you&amp;#39;re fitting that on a 3090?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m3nwlf","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3nwlf/any_local_models_with_decent_tooling_capabilities/n3zz8xw/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752934550,"author_flair_text":null,"treatment_tags":[],"created_utc":1752934550,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3yid7p","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"admajic","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3yhqi4","score":1,"author_fullname":"t2_60b9farf","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yes and yes in lmstudio with 128k context window","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3yid7p","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yes and yes in lmstudio with 128k context window&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m3nwlf","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3nwlf/any_local_models_with_decent_tooling_capabilities/n3yid7p/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752910075,"author_flair_text":null,"treatment_tags":[],"created_utc":1752910075,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n42y80k","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"admajic","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3zocuu","score":1,"author_fullname":"t2_60b9farf","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Get the unsloth ver it has the 128k context window.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n42y80k","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Get the unsloth ver it has the 128k context window.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m3nwlf","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3nwlf/any_local_models_with_decent_tooling_capabilities/n42y80k/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752969184,"author_flair_text":null,"treatment_tags":[],"created_utc":1752969184,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3zocuu","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Physical-Citron5153","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3yhqi4","score":1,"author_fullname":"t2_clhgguip","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Definitely use self attention, and yes tune your model to get more context,","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3zocuu","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Definitely use self attention, and yes tune your model to get more context,&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m3nwlf","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3nwlf/any_local_models_with_decent_tooling_capabilities/n3zocuu/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752930768,"author_flair_text":null,"treatment_tags":[],"created_utc":1752930768,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3yhqi4","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"megadonkeyx","can_mod_post":false,"created_utc":1752909723,"send_replies":true,"parent_id":"t1_n3y6s15","score":1,"author_fullname":"t2_unvzb","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Do you use flash attention and quantized kv to get more memory for context?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3yhqi4","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Do you use flash attention and quantized kv to get more memory for context?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m3nwlf","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3nwlf/any_local_models_with_decent_tooling_capabilities/n3yhqi4/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752909723,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3y6s15","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Physical-Citron5153","can_mod_post":false,"created_utc":1752903814,"send_replies":true,"parent_id":"t3_1m3nwlf","score":3,"author_fullname":"t2_clhgguip","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"The only model that can code to some degree is Devstral combine it with cline and for now this is the best you get, until you could run DeepSeek or other large models… i actually use it for about 70 to 80% of my whole code and it works good","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3y6s15","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The only model that can code to some degree is Devstral combine it with cline and for now this is the best you get, until you could run DeepSeek or other large models… i actually use it for about 70 to 80% of my whole code and it works good&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3nwlf/any_local_models_with_decent_tooling_capabilities/n3y6s15/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752903814,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m3nwlf","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n405qxa","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"LostHisDog","can_mod_post":false,"send_replies":true,"parent_id":"t1_n401vyi","score":1,"author_fullname":"t2_x5jky","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Sure - [https://filebin.net/8c036kd50dpmscgb](https://filebin.net/8c036kd50dpmscgb) \\\\- WIP obviously and not exposed to the greater world yet - Basically a python project running a flask server to track medication use across multiple dosing schedules with working google OAuth for integration with google ecosystem. I'm working on a separate module ATM for local LLM integration to scrape medications and appointments off mychart that's not in here yet. There's some spare pages and unused code I am sure just as part of the process so far. \\n\\nI have not yet done even the most basic of housekeeping to clean up any of this code. The most LLM thing about it, outside of the slope code I am sure it's put together to accomplish it's task, is the egregious commenting that LLM's love to do. At some point I'll run dueling AI's to sweep out as much bull as possible and start looking for security issues long before exposing the code to the wider internet to use remotely. But, for now, it works and functions better than a spreadsheet to provide a quick glancable weather, task, medication appointment dashboard built to my needs. \\n\\nVibe-coding might suck, but it's not not working IMO. This thing works for what I need to the extent that I've used it so far.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n405qxa","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Sure - &lt;a href=\\"https://filebin.net/8c036kd50dpmscgb\\"&gt;https://filebin.net/8c036kd50dpmscgb&lt;/a&gt; - WIP obviously and not exposed to the greater world yet - Basically a python project running a flask server to track medication use across multiple dosing schedules with working google OAuth for integration with google ecosystem. I&amp;#39;m working on a separate module ATM for local LLM integration to scrape medications and appointments off mychart that&amp;#39;s not in here yet. There&amp;#39;s some spare pages and unused code I am sure just as part of the process so far. &lt;/p&gt;\\n\\n&lt;p&gt;I have not yet done even the most basic of housekeeping to clean up any of this code. The most LLM thing about it, outside of the slope code I am sure it&amp;#39;s put together to accomplish it&amp;#39;s task, is the egregious commenting that LLM&amp;#39;s love to do. At some point I&amp;#39;ll run dueling AI&amp;#39;s to sweep out as much bull as possible and start looking for security issues long before exposing the code to the wider internet to use remotely. But, for now, it works and functions better than a spreadsheet to provide a quick glancable weather, task, medication appointment dashboard built to my needs. &lt;/p&gt;\\n\\n&lt;p&gt;Vibe-coding might suck, but it&amp;#39;s not not working IMO. This thing works for what I need to the extent that I&amp;#39;ve used it so far.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m3nwlf","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3nwlf/any_local_models_with_decent_tooling_capabilities/n405qxa/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752936618,"author_flair_text":null,"treatment_tags":[],"created_utc":1752936618,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n401vyi","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"vibjelo","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3zs7oh","score":1,"author_fullname":"t2_hr2hgnsk","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Care to show the code for this? Always curious to see how the final source code looks for people who go days without reviewing it and just accepting what LLMs give you.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n401vyi","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Care to show the code for this? Always curious to see how the final source code looks for people who go days without reviewing it and just accepting what LLMs give you.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m3nwlf","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3nwlf/any_local_models_with_decent_tooling_capabilities/n401vyi/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752935398,"author_flair_text":null,"treatment_tags":[],"created_utc":1752935398,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n420e9b","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"HRudy94","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4188ps","score":0,"author_fullname":"t2_12e33e","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Wrong for a single thing, AIs don't understand anything they write. They just use probabilities to decide the next token. \\n\\nNow yes they've replaced the random samples from stackoverflow, but that's about it, you're not gonna have any good code (not in terms of how beautiful it is but how it actually functions) out of vibe-coding, unless your need is mostly from samples. \\n\\nAI isn't gonna replace any proper dev anytime soon (just like it's not gonna replace artists).  \\nIt's just a tool that if used well, can help gather docs and do the boring tasks but it cannot be trusted all on its own.","edited":false,"author_flair_css_class":null,"name":"t1_n420e9b","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Wrong for a single thing, AIs don&amp;#39;t understand anything they write. They just use probabilities to decide the next token. &lt;/p&gt;\\n\\n&lt;p&gt;Now yes they&amp;#39;ve replaced the random samples from stackoverflow, but that&amp;#39;s about it, you&amp;#39;re not gonna have any good code (not in terms of how beautiful it is but how it actually functions) out of vibe-coding, unless your need is mostly from samples. &lt;/p&gt;\\n\\n&lt;p&gt;AI isn&amp;#39;t gonna replace any proper dev anytime soon (just like it&amp;#39;s not gonna replace artists).&lt;br/&gt;\\nIt&amp;#39;s just a tool that if used well, can help gather docs and do the boring tasks but it cannot be trusted all on its own.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1m3nwlf","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3nwlf/any_local_models_with_decent_tooling_capabilities/n420e9b/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752957617,"author_flair_text":null,"collapsed":false,"created_utc":1752957617,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}},"user_reports":[],"saved":false,"id":"n4188ps","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"LostHisDog","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4153ko","score":0,"author_fullname":"t2_x5jky","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I get where you are coming from but a year ago I couldn't ask for and eventually receive a working personal dashboard tailored to my specific requirements. Of course most of the code is ripped willy nilly from other code repositories but let's be real honest with ourselves here and admit that stackoverflow was used in much the same way before AI. AI's have just democratized that process of finding the right bits of code to cut and paste to a whole nother level of technical (in)competence. \\n\\nRight now the code it creates is mostly functional but it's not especially beautiful. That's today though. I can EASILY imagine a world where AI's don't need all these fancy human level languages we've invented because keeping track of registers and memory addresses is cumbersome. There's no reason at all AI's won't eventually spit out the absolutely most beautiful code written directly in binary. \\n\\nAnyway, LLM's have already replaced humans in development if in no other spot then in my personal development needs. I don't think I'm alone here though and the shift is eventually going to, IMO of course, tilt all the way over to AI's doing ALL the programing simply because they can be trained to speak fully and completely in binary much more quickly than any human can and that knowledge is transportable, reproducable and upgradable all via a flash drive vs a human being with all of our messy needs and individual complications.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4188ps","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I get where you are coming from but a year ago I couldn&amp;#39;t ask for and eventually receive a working personal dashboard tailored to my specific requirements. Of course most of the code is ripped willy nilly from other code repositories but let&amp;#39;s be real honest with ourselves here and admit that stackoverflow was used in much the same way before AI. AI&amp;#39;s have just democratized that process of finding the right bits of code to cut and paste to a whole nother level of technical (in)competence. &lt;/p&gt;\\n\\n&lt;p&gt;Right now the code it creates is mostly functional but it&amp;#39;s not especially beautiful. That&amp;#39;s today though. I can EASILY imagine a world where AI&amp;#39;s don&amp;#39;t need all these fancy human level languages we&amp;#39;ve invented because keeping track of registers and memory addresses is cumbersome. There&amp;#39;s no reason at all AI&amp;#39;s won&amp;#39;t eventually spit out the absolutely most beautiful code written directly in binary. &lt;/p&gt;\\n\\n&lt;p&gt;Anyway, LLM&amp;#39;s have already replaced humans in development if in no other spot then in my personal development needs. I don&amp;#39;t think I&amp;#39;m alone here though and the shift is eventually going to, IMO of course, tilt all the way over to AI&amp;#39;s doing ALL the programing simply because they can be trained to speak fully and completely in binary much more quickly than any human can and that knowledge is transportable, reproducable and upgradable all via a flash drive vs a human being with all of our messy needs and individual complications.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m3nwlf","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3nwlf/any_local_models_with_decent_tooling_capabilities/n4188ps/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752948523,"author_flair_text":null,"treatment_tags":[],"created_utc":1752948523,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}},"user_reports":[],"saved":false,"id":"n4153ko","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"HRudy94","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3zs7oh","score":0,"author_fullname":"t2_12e33e","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"LLMs only combine patterns together, it can slightly customize them but chances are most of the code were generic components that were slightly adapted for the job. It also tends to generate a lot of wanky code that won't work, won't compile or will be filled with security or performance issues.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n4153ko","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;LLMs only combine patterns together, it can slightly customize them but chances are most of the code were generic components that were slightly adapted for the job. It also tends to generate a lot of wanky code that won&amp;#39;t work, won&amp;#39;t compile or will be filled with security or performance issues.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m3nwlf","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3nwlf/any_local_models_with_decent_tooling_capabilities/n4153ko/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752947535,"author_flair_text":null,"treatment_tags":[],"created_utc":1752947535,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}},"user_reports":[],"saved":false,"id":"n3zs7oh","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"LostHisDog","can_mod_post":false,"created_utc":1752932169,"send_replies":true,"parent_id":"t1_n3z6fq7","score":2,"author_fullname":"t2_x5jky","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I don't know about the vibe-coding not working. I wanted a personal dashboard that tracks unique medical data and spent a few days negotiating with AI's to build one which works great now. It's not something that existed in the training data and is 100% a thing in my head I asked an AI to build up. I had tried this a year ago and failed with the tools available then. Now it's just working and in a year I suspect the few days of back and forth it took will be down to a few minutes if the trends continue.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3zs7oh","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I don&amp;#39;t know about the vibe-coding not working. I wanted a personal dashboard that tracks unique medical data and spent a few days negotiating with AI&amp;#39;s to build one which works great now. It&amp;#39;s not something that existed in the training data and is 100% a thing in my head I asked an AI to build up. I had tried this a year ago and failed with the tools available then. Now it&amp;#39;s just working and in a year I suspect the few days of back and forth it took will be down to a few minutes if the trends continue.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m3nwlf","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3nwlf/any_local_models_with_decent_tooling_capabilities/n3zs7oh/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752932169,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n42ztej","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Physical-Citron5153","can_mod_post":false,"created_utc":1752969768,"send_replies":true,"parent_id":"t1_n3z6fq7","score":1,"author_fullname":"t2_clhgguip","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Did glm4 really worked for you in cline and other tool calling environment? It's just a mess for me, so i want to know if you did some system prompts or anything special that makes it better? It was just pure garbage compared to qwen 3 32B and devstral at least for me, i just use it for one shot ui components, and even in that, at least 50% of the functionality wont work","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n42ztej","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Did glm4 really worked for you in cline and other tool calling environment? It&amp;#39;s just a mess for me, so i want to know if you did some system prompts or anything special that makes it better? It was just pure garbage compared to qwen 3 32B and devstral at least for me, i just use it for one shot ui components, and even in that, at least 50% of the functionality wont work&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m3nwlf","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3nwlf/any_local_models_with_decent_tooling_capabilities/n42ztej/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752969768,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3z6fq7","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"HRudy94","can_mod_post":false,"created_utc":1752923371,"send_replies":true,"parent_id":"t3_1m3nwlf","score":3,"author_fullname":"t2_12e33e","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"GLM-4 / Gemma-3 are good local models for code assistance. \\n\\n\\nThat said, LLMs cannot replace humans and so you should only use models as a tool to help you. Forget about vibe-coding as an idea, it doesn't work. They're good for reformatting code, explaining it, finding great templates, parsing the documentation etc though.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3z6fq7","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;GLM-4 / Gemma-3 are good local models for code assistance. &lt;/p&gt;\\n\\n&lt;p&gt;That said, LLMs cannot replace humans and so you should only use models as a tool to help you. Forget about vibe-coding as an idea, it doesn&amp;#39;t work. They&amp;#39;re good for reformatting code, explaining it, finding great templates, parsing the documentation etc though.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3nwlf/any_local_models_with_decent_tooling_capabilities/n3z6fq7/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752923371,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m3nwlf","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n40015z","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"getpodapp","can_mod_post":false,"created_utc":1752934805,"send_replies":true,"parent_id":"t3_1m3nwlf","score":1,"author_fullname":"t2_v2x4wxet","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I was very happy to see qwen3 14b performed relatively well for this task.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n40015z","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I was very happy to see qwen3 14b performed relatively well for this task.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3nwlf/any_local_models_with_decent_tooling_capabilities/n40015z/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752934805,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m3nwlf","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),r=()=>e.jsx(l,{data:t});export{r as default};
