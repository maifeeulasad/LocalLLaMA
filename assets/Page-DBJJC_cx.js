import{j as e}from"./index-F0NXdzZX.js";import{R as l}from"./RedditPostRenderer-CoEZB1dt.js";import"./index-DrtravXm.js";const a=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"It's a been a great 6 months to be using local AI as the performance delta has, on average, been very low for classic LLMs, with R1 typically being at or near SOTA, and smaller models consistently getting better and better benchmarks.  \\n  \\nHowever, the below are all things where there has been a surprising lag between closed systems' release dates and the availability of high quality local alternatives  \\n  \\n1. A voice mode that is on par with Chat Gpt. Most all the pieces are in place to have something akin to 4o with voice. Sesame, Kyutai, or Chatterbox for TTS, any local model for the LLM, decent STT is, I think, also a thing already. We just need the parts put together in a fairly user-friendly, fast streaming package.  \\n  \\n2. Local deep research on the level of o3's web search. o3 is quite amazing now in its ability to rapidly search several web pages to answer questions. There are some solutions for local llms but none that I've tried seem to be fulfilling the potential of web search agents with clever and easily customizable workflows. I would be fine with a much slower process if the answers were as good. Something like Qwen 235b I believe could do a great job of being the foundation of such an agent.  \\n  \\n3. A local visual llm that can reliably read any human-legible document. Maverick is quite good but not nearly as good as Gemini Pro or Chat GPT at this.\\n\\nWhat else am I forgetting about?\\n\\n","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Where local is lagging behind... Wish lists for the rest of 2025","link_flair_richtext":[{"e":"text","t":"Discussion"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lxn8ry","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.85,"author_flair_background_color":null,"subreddit_type":"public","ups":13,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_syq52","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Discussion","can_mod_post":false,"score":13,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1752280957,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;It&amp;#39;s a been a great 6 months to be using local AI as the performance delta has, on average, been very low for classic LLMs, with R1 typically being at or near SOTA, and smaller models consistently getting better and better benchmarks.  &lt;/p&gt;\\n\\n&lt;p&gt;However, the below are all things where there has been a surprising lag between closed systems&amp;#39; release dates and the availability of high quality local alternatives  &lt;/p&gt;\\n\\n&lt;ol&gt;\\n&lt;li&gt;&lt;p&gt;A voice mode that is on par with Chat Gpt. Most all the pieces are in place to have something akin to 4o with voice. Sesame, Kyutai, or Chatterbox for TTS, any local model for the LLM, decent STT is, I think, also a thing already. We just need the parts put together in a fairly user-friendly, fast streaming package.  &lt;/p&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;p&gt;Local deep research on the level of o3&amp;#39;s web search. o3 is quite amazing now in its ability to rapidly search several web pages to answer questions. There are some solutions for local llms but none that I&amp;#39;ve tried seem to be fulfilling the potential of web search agents with clever and easily customizable workflows. I would be fine with a much slower process if the answers were as good. Something like Qwen 235b I believe could do a great job of being the foundation of such an agent.  &lt;/p&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;p&gt;A local visual llm that can reliably read any human-legible document. Maverick is quite good but not nearly as good as Gemini Pro or Chat GPT at this.&lt;/p&gt;&lt;/li&gt;\\n&lt;/ol&gt;\\n\\n&lt;p&gt;What else am I forgetting about?&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"5f921ea4-c7bc-11ed-9c23-3a00622979b4","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#646d73","id":"1lxn8ry","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"nomorebuttsplz","discussion_type":null,"num_comments":24,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lxn8ry/where_local_is_lagging_behind_wish_lists_for_the/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lxn8ry/where_local_is_lagging_behind_wish_lists_for_the/","subreddit_subscribers":498115,"created_utc":1752280957,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2nopqk","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Secure_Reflection409","can_mod_post":false,"created_utc":1752284894,"send_replies":true,"parent_id":"t3_1lxn8ry","score":17,"author_fullname":"t2_by77ogdhr","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Hardware is our biggest issue.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2nopqk","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Hardware is our biggest issue.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxn8ry/where_local_is_lagging_behind_wish_lists_for_the/n2nopqk/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752284894,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lxn8ry","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":17}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2nmxfx","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"dinerburgeryum","can_mod_post":false,"created_utc":1752284238,"send_replies":true,"parent_id":"t3_1lxn8ry","score":5,"author_fullname":"t2_1j53p3yv3e","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Seconding real-time voice mode. I kind of agree with your point that all the pieces are there; we have Qwen Omni, which absolutely supports streaming ingest and generation, but none of the code released demonstrates its usage. This is unfortunately a software problem, and not one that's easy to overcome.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2nmxfx","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Seconding real-time voice mode. I kind of agree with your point that all the pieces are there; we have Qwen Omni, which absolutely supports streaming ingest and generation, but none of the code released demonstrates its usage. This is unfortunately a software problem, and not one that&amp;#39;s easy to overcome.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxn8ry/where_local_is_lagging_behind_wish_lists_for_the/n2nmxfx/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752284238,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lxn8ry","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2tbeyu","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"SlowFail2433","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2r4aa2","score":1,"author_fullname":"t2_131eezppgs","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"IDK if the way the closed source deep research does it is even the best.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2tbeyu","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;IDK if the way the closed source deep research does it is even the best.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lxn8ry","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxn8ry/where_local_is_lagging_behind_wish_lists_for_the/n2tbeyu/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752364532,"author_flair_text":null,"treatment_tags":[],"created_utc":1752364532,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2r4aa2","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"nomorebuttsplz","can_mod_post":false,"created_utc":1752338735,"send_replies":true,"parent_id":"t1_n2p6g88","score":2,"author_fullname":"t2_syq52","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I know there's no purely technical reason why we can't have a good local deep research workflow, but I don't know what setup is the best.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2r4aa2","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I know there&amp;#39;s no purely technical reason why we can&amp;#39;t have a good local deep research workflow, but I don&amp;#39;t know what setup is the best.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lxn8ry","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxn8ry/where_local_is_lagging_behind_wish_lists_for_the/n2r4aa2/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752338735,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n2p6g88","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"LevianMcBirdo","can_mod_post":false,"created_utc":1752310655,"send_replies":true,"parent_id":"t3_1lxn8ry","score":5,"author_fullname":"t2_cw9f6o0r","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"The deep research part is probably mostly a setup problem. Perplexity uses R1 for its own deep research and it's at least on par with o3 web search, probably better. Also their labs isn't bad either and probably uses mostly R1","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2p6g88","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The deep research part is probably mostly a setup problem. Perplexity uses R1 for its own deep research and it&amp;#39;s at least on par with o3 web search, probably better. Also their labs isn&amp;#39;t bad either and probably uses mostly R1&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxn8ry/where_local_is_lagging_behind_wish_lists_for_the/n2p6g88/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752310655,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lxn8ry","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2qnulw","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"pumukidelfuturo","can_mod_post":false,"created_utc":1752333612,"send_replies":true,"parent_id":"t1_n2pyp5e","score":2,"author_fullname":"t2_tw7dirj","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"this is all that matters.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2qnulw","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;this is all that matters.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lxn8ry","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxn8ry/where_local_is_lagging_behind_wish_lists_for_the/n2qnulw/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752333612,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n2pyp5e","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Lorian0x7","can_mod_post":false,"created_utc":1752324987,"send_replies":true,"parent_id":"t3_1lxn8ry","score":3,"author_fullname":"t2_9ulpagz7","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I wish for a local model that can run with 8 gb vram, or locally  on smartphone and it's as good a 4o","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2pyp5e","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I wish for a local model that can run with 8 gb vram, or locally  on smartphone and it&amp;#39;s as good a 4o&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxn8ry/where_local_is_lagging_behind_wish_lists_for_the/n2pyp5e/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752324987,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lxn8ry","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2nmhy0","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"SlowFail2433","can_mod_post":false,"created_utc":1752284082,"send_replies":true,"parent_id":"t3_1lxn8ry","score":2,"author_fullname":"t2_131eezppgs","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"For LLMs vision abilities and long context abilities\\nAlso image and audio generation by LLMs\\n\\n\\nFor diffusion we are just not close to parity at all","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2nmhy0","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;For LLMs vision abilities and long context abilities\\nAlso image and audio generation by LLMs&lt;/p&gt;\\n\\n&lt;p&gt;For diffusion we are just not close to parity at all&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxn8ry/where_local_is_lagging_behind_wish_lists_for_the/n2nmhy0/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752284082,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lxn8ry","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2u5svq","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"SlowFail2433","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2tj1hq","score":1,"author_fullname":"t2_131eezppgs","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Some future better version could be good yeah","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2u5svq","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Some future better version could be good yeah&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lxn8ry","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxn8ry/where_local_is_lagging_behind_wish_lists_for_the/n2u5svq/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752376110,"author_flair_text":null,"treatment_tags":[],"created_utc":1752376110,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2tj1hq","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"offlinesir","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2tddf9","score":1,"author_fullname":"t2_jn5ft2le","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yeah, it kinda sucks. I turned it off. But looking towards the future, personalization is likely going to be more important. Hopefully, not for advertising.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2tj1hq","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yeah, it kinda sucks. I turned it off. But looking towards the future, personalization is likely going to be more important. Hopefully, not for advertising.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lxn8ry","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxn8ry/where_local_is_lagging_behind_wish_lists_for_the/n2tj1hq/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752367373,"author_flair_text":null,"treatment_tags":[],"created_utc":1752367373,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2tddf9","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"SlowFail2433","can_mod_post":false,"created_utc":1752365253,"send_replies":true,"parent_id":"t1_n2no7uz","score":1,"author_fullname":"t2_131eezppgs","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I found OpenAI’s usage of the memory feature crude but maybe I was unlucky. It would shoe-horn it in too much.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2tddf9","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I found OpenAI’s usage of the memory feature crude but maybe I was unlucky. It would shoe-horn it in too much.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lxn8ry","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxn8ry/where_local_is_lagging_behind_wish_lists_for_the/n2tddf9/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752365253,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2no7uz","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"offlinesir","can_mod_post":false,"created_utc":1752284712,"send_replies":true,"parent_id":"t3_1lxn8ry","score":2,"author_fullname":"t2_jn5ft2le","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"We are missing text to video at the level of Veo 2, 3, or even Sora. We are also missing personalized LLM's at the level of OpenAI's memory feature.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2no7uz","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;We are missing text to video at the level of Veo 2, 3, or even Sora. We are also missing personalized LLM&amp;#39;s at the level of OpenAI&amp;#39;s memory feature.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxn8ry/where_local_is_lagging_behind_wish_lists_for_the/n2no7uz/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752284712,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lxn8ry","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2tcla8","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"nomorebuttsplz","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2ozgod","score":1,"author_fullname":"t2_syq52","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Thanks, I will check out 2 and 3.\\n\\nQwen omni may be decent if you can get it running, which seems like a pain.\\n\\nBut what is needed is something modular so you can use a more intelligent model, as that one is about the level of an average 4b text only llm","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2tcla8","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thanks, I will check out 2 and 3.&lt;/p&gt;\\n\\n&lt;p&gt;Qwen omni may be decent if you can get it running, which seems like a pain.&lt;/p&gt;\\n\\n&lt;p&gt;But what is needed is something modular so you can use a more intelligent model, as that one is about the level of an average 4b text only llm&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lxn8ry","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxn8ry/where_local_is_lagging_behind_wish_lists_for_the/n2tcla8/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752364967,"author_flair_text":null,"treatment_tags":[],"created_utc":1752364967,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2ozgod","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Mkengine","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2oqraj","score":1,"author_fullname":"t2_9p2xe","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I don't want to say I have a solution to your problems, but here are some links that may help for them:\\n\\n1. https://github.com/QwenLM/Qwen2.5-Omni\\n\\n2. https://github.com/DavidZWZ/Awesome-Deep-Research\\n\\n3. https://github.com/GiftMungmeeprued/document-parsers-list\\n\\nwhat are your thoughts?","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2ozgod","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I don&amp;#39;t want to say I have a solution to your problems, but here are some links that may help for them:&lt;/p&gt;\\n\\n&lt;ol&gt;\\n&lt;li&gt;&lt;p&gt;&lt;a href=\\"https://github.com/QwenLM/Qwen2.5-Omni\\"&gt;https://github.com/QwenLM/Qwen2.5-Omni&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;p&gt;&lt;a href=\\"https://github.com/DavidZWZ/Awesome-Deep-Research\\"&gt;https://github.com/DavidZWZ/Awesome-Deep-Research&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;p&gt;&lt;a href=\\"https://github.com/GiftMungmeeprued/document-parsers-list\\"&gt;https://github.com/GiftMungmeeprued/document-parsers-list&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;\\n&lt;/ol&gt;\\n\\n&lt;p&gt;what are your thoughts?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lxn8ry","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxn8ry/where_local_is_lagging_behind_wish_lists_for_the/n2ozgod/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752306496,"author_flair_text":null,"treatment_tags":[],"created_utc":1752306496,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2oqraj","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"nomorebuttsplz","can_mod_post":false,"created_utc":1752301534,"send_replies":true,"parent_id":"t1_n2oa9na","score":3,"author_fullname":"t2_syq52","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I have a mac studio 512 gb","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2oqraj","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I have a mac studio 512 gb&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lxn8ry","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxn8ry/where_local_is_lagging_behind_wish_lists_for_the/n2oqraj/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752301534,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n2oa9na","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Square-Onion-1825","can_mod_post":false,"created_utc":1752293382,"send_replies":true,"parent_id":"t3_1lxn8ry","score":1,"author_fullname":"t2_1mkh7x2yxn","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"i'm curious, what is your h/w set up? i'm buying a system that i will dedicate to AI dev and RAG work, but i'm kinda new at this...","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2oa9na","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;i&amp;#39;m curious, what is your h/w set up? i&amp;#39;m buying a system that i will dedicate to AI dev and RAG work, but i&amp;#39;m kinda new at this...&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxn8ry/where_local_is_lagging_behind_wish_lists_for_the/n2oa9na/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752293382,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lxn8ry","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2ojz09","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"My_Unbiased_Opinion","can_mod_post":false,"created_utc":1752297930,"send_replies":true,"parent_id":"t3_1lxn8ry","score":1,"author_fullname":"t2_esiyl0yb","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"What I want is tool use web search inside the reasoning chain so the model can get information directly from the web while it reasons. Should massively increase quality without needing any more VRAM. ","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2ojz09","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;What I want is tool use web search inside the reasoning chain so the model can get information directly from the web while it reasons. Should massively increase quality without needing any more VRAM. &lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxn8ry/where_local_is_lagging_behind_wish_lists_for_the/n2ojz09/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752297930,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lxn8ry","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2teeip","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"SlowFail2433","can_mod_post":false,"created_utc":1752365627,"send_replies":true,"parent_id":"t1_n2rhspa","score":1,"author_fullname":"t2_131eezppgs","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"People care enormously more about throughput than latency, which only dictates time to first token","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2teeip","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;People care enormously more about throughput than latency, which only dictates time to first token&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lxn8ry","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxn8ry/where_local_is_lagging_behind_wish_lists_for_the/n2teeip/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752365627,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2rhspa","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"05032-MendicantBias","can_mod_post":false,"created_utc":1752342812,"send_replies":true,"parent_id":"t3_1lxn8ry","score":1,"author_fullname":"t2_6id3lwou","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Local is lagging behind in high RAM size and bandwidth hardware.\\n\\nIt's cool that researchers are trying to push performance up, but it's more important for real world applications to push hardware requirements down.\\n\\nE.g. Where are the sparce models that take advantage of CPU low latency random access execution and run with regular RAM?\\n\\nThe Playstation gets to use GDDR with the APU, we still have APU that only use DDR. Even AMD 395 and Nvidia Digit work with DDR and not GDDR. What gives?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2rhspa","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Local is lagging behind in high RAM size and bandwidth hardware.&lt;/p&gt;\\n\\n&lt;p&gt;It&amp;#39;s cool that researchers are trying to push performance up, but it&amp;#39;s more important for real world applications to push hardware requirements down.&lt;/p&gt;\\n\\n&lt;p&gt;E.g. Where are the sparce models that take advantage of CPU low latency random access execution and run with regular RAM?&lt;/p&gt;\\n\\n&lt;p&gt;The Playstation gets to use GDDR with the APU, we still have APU that only use DDR. Even AMD 395 and Nvidia Digit work with DDR and not GDDR. What gives?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxn8ry/where_local_is_lagging_behind_wish_lists_for_the/n2rhspa/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752342812,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lxn8ry","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2u67aa","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"SlowFail2433","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2tzw0r","score":1,"author_fullname":"t2_131eezppgs","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Sorry I misunderstood \\n\\n\\nOCR isn’t an area I follow, its interesting that Qwen is doing so well there","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2u67aa","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Sorry I misunderstood &lt;/p&gt;\\n\\n&lt;p&gt;OCR isn’t an area I follow, its interesting that Qwen is doing so well there&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lxn8ry","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxn8ry/where_local_is_lagging_behind_wish_lists_for_the/n2u67aa/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752376275,"author_flair_text":null,"treatment_tags":[],"created_utc":1752376275,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2tzw0r","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ArsNeph","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2tei2e","score":1,"author_fullname":"t2_vt0xkv60d","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"No, I mean specifically OCR tasks, because that's specifically what OP is referring to. If you look at VLM leaderboards and sort by things like OCR bench, you'll see that it's one of the top","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2tzw0r","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;No, I mean specifically OCR tasks, because that&amp;#39;s specifically what OP is referring to. If you look at VLM leaderboards and sort by things like OCR bench, you&amp;#39;ll see that it&amp;#39;s one of the top&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lxn8ry","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxn8ry/where_local_is_lagging_behind_wish_lists_for_the/n2tzw0r/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752373747,"author_flair_text":null,"treatment_tags":[],"created_utc":1752373747,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2tei2e","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"SlowFail2433","can_mod_post":false,"created_utc":1752365664,"send_replies":true,"parent_id":"t1_n2rq4s1","score":1,"author_fullname":"t2_131eezppgs","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Its Gemini that leads in Vision","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2tei2e","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Its Gemini that leads in Vision&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lxn8ry","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxn8ry/where_local_is_lagging_behind_wish_lists_for_the/n2tei2e/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752365664,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2rq4s1","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ArsNeph","can_mod_post":false,"created_utc":1752345430,"send_replies":true,"parent_id":"t3_1lxn8ry","score":1,"author_fullname":"t2_vt0xkv60d","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Are you sure about the vision part? Qwen 2.5 VL 72B according to most benchmarks is better than ChatGPT 4o in ocr tasks, and even in real world usage seems to be around on par. Have you tried it?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2rq4s1","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Are you sure about the vision part? Qwen 2.5 VL 72B according to most benchmarks is better than ChatGPT 4o in ocr tasks, and even in real world usage seems to be around on par. Have you tried it?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxn8ry/where_local_is_lagging_behind_wish_lists_for_the/n2rq4s1/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752345430,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lxn8ry","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2svbyd","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Uncle___Marty","can_mod_post":false,"created_utc":1752358891,"send_replies":true,"parent_id":"t3_1lxn8ry","score":1,"author_fullname":"t2_75p7pgz3","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Honestly agree. I've been REALLY looking forward to finding a smallish model that I can just use voice with. I get bored typing and reading when I know I don't \\\\*really\\\\* need to. There are amazing options out there but to cobble them together is a PITA. Lets hope someone makes a mini chatGPT with proper TTS/STT built in natively.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2svbyd","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Honestly agree. I&amp;#39;ve been REALLY looking forward to finding a smallish model that I can just use voice with. I get bored typing and reading when I know I don&amp;#39;t *really* need to. There are amazing options out there but to cobble them together is a PITA. Lets hope someone makes a mini chatGPT with proper TTS/STT built in natively.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxn8ry/where_local_is_lagging_behind_wish_lists_for_the/n2svbyd/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752358891,"author_flair_text":"llama.cpp","treatment_tags":[],"link_id":"t3_1lxn8ry","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),n=()=>e.jsx(l,{data:a});export{n as default};
