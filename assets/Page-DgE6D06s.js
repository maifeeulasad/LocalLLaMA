import{j as t}from"./index-BlGsFJYy.js";import{R as e}from"./RedditPostRenderer-B6uvq_Zl.js";import"./index-DDvVtNwD.js";const a=JSON.parse('[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"I\'m trying to get MistralThinker to... think. According to discussion on the model page (https://huggingface.co/Undi95/MistralThinker-v1.1/discussions/1) it is necessary to encourage the model to use reasoning with some structured output or otherwise prefixes. But I\'m not using SillyTavern so the suggestions in the thread don\'t seem applicable for me. Instead I\'m using LM studio for out of the box ROCm support. \\n\\nI\'ve never made a json schema before so I tried generating a structured output, but I\'m not entirely sure what the structure is supposed to look like, as I found the LM Studio documentation unclear with poor examples. Here\'s where I\'m at:\\n\\n    {\\n      \\"type\\": \\"object\\",\\n      \\"properties\\": {\\n        \\"reasoning_prefix\\": {\\n          \\"type\\": \\"string\\",\\n          \\"enum\\": [\\"&lt;think&gt;\\"],\\n          \\"description\\": \\"Prefix indicating the model is thinking\\"\\n        },\\n        \\"reasoning\\": {\\n          \\"type\\": \\"string\\",\\n          \\"description\\": \\"The model\'s internal reasoning and thought process\\"\\n        },\\n        \\"reasoning_suffix\\": {\\n          \\"type\\": \\"string\\",\\n          \\"enum\\": [\\"&lt;/think&gt;\\"],\\n          \\"description\\": \\"Suffix marking the end of the thinking phase\\"\\n        },\\n        \\"reply\\": {\\n          \\"type\\": \\"string\\",\\n          \\"description\\": \\"Final response to the user after reasoning\\"\\n        }\\n      },\\n      \\"required\\": [\\n        \\"reasoning_prefix\\",\\n        \\"reasoning\\",\\n        \\"reasoning_suffix\\",\\n        \\"reply\\"\\n      ]\\n    }\\n\\nThis *sort of works* in that it does in fact cause the model to perform reasoning, but some bits of undesired json are being included in the output. Such as:\\n\\n&gt; { \\"thinking_prefix\\": \\"\\n&gt; \\n&gt; &lt;think&gt;\\",\\n&gt; \\"thoughts\\": \\"The user is asking for a simple test. I need to respond positively and confirm functionality. Maybe add a playful emoji.\\"\\n&gt; , \\"thinking_suffix\\": \\"&lt;/think&gt;\\n&gt; \\n&gt; \\",\\n&gt; \\"reply\\": \\"Testing successful! ðŸ˜Š Everything seems to be working smoothly. How can I assist you today?\\" } \\n\\nI assume I\'ve done something wrong. Can anyone help me understand how to format the schema correctly for this purpose?\\n\\nOn an unrelated note, if anyone can tell me where to find or modify more llama.cpp sampler settings I\'d love to know about it. Otherwise it seems like I can only change Temperature, TopK, Rep. Pen., MinP, and TopP...","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Structured output help (LM Studio)","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1m3s01i","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.67,"author_flair_background_color":null,"subreddit_type":"public","ups":1,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_gebwv","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":1,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"post_hint":"self","content_categories":null,"is_self":true,"mod_note":null,"created":1752917148,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;m trying to get MistralThinker to... think. According to discussion on the model page (&lt;a href=\\"https://huggingface.co/Undi95/MistralThinker-v1.1/discussions/1\\"&gt;https://huggingface.co/Undi95/MistralThinker-v1.1/discussions/1&lt;/a&gt;) it is necessary to encourage the model to use reasoning with some structured output or otherwise prefixes. But I&amp;#39;m not using SillyTavern so the suggestions in the thread don&amp;#39;t seem applicable for me. Instead I&amp;#39;m using LM studio for out of the box ROCm support. &lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;ve never made a json schema before so I tried generating a structured output, but I&amp;#39;m not entirely sure what the structure is supposed to look like, as I found the LM Studio documentation unclear with poor examples. Here&amp;#39;s where I&amp;#39;m at:&lt;/p&gt;\\n\\n&lt;pre&gt;&lt;code&gt;{\\n  &amp;quot;type&amp;quot;: &amp;quot;object&amp;quot;,\\n  &amp;quot;properties&amp;quot;: {\\n    &amp;quot;reasoning_prefix&amp;quot;: {\\n      &amp;quot;type&amp;quot;: &amp;quot;string&amp;quot;,\\n      &amp;quot;enum&amp;quot;: [&amp;quot;&amp;lt;think&amp;gt;&amp;quot;],\\n      &amp;quot;description&amp;quot;: &amp;quot;Prefix indicating the model is thinking&amp;quot;\\n    },\\n    &amp;quot;reasoning&amp;quot;: {\\n      &amp;quot;type&amp;quot;: &amp;quot;string&amp;quot;,\\n      &amp;quot;description&amp;quot;: &amp;quot;The model&amp;#39;s internal reasoning and thought process&amp;quot;\\n    },\\n    &amp;quot;reasoning_suffix&amp;quot;: {\\n      &amp;quot;type&amp;quot;: &amp;quot;string&amp;quot;,\\n      &amp;quot;enum&amp;quot;: [&amp;quot;&amp;lt;/think&amp;gt;&amp;quot;],\\n      &amp;quot;description&amp;quot;: &amp;quot;Suffix marking the end of the thinking phase&amp;quot;\\n    },\\n    &amp;quot;reply&amp;quot;: {\\n      &amp;quot;type&amp;quot;: &amp;quot;string&amp;quot;,\\n      &amp;quot;description&amp;quot;: &amp;quot;Final response to the user after reasoning&amp;quot;\\n    }\\n  },\\n  &amp;quot;required&amp;quot;: [\\n    &amp;quot;reasoning_prefix&amp;quot;,\\n    &amp;quot;reasoning&amp;quot;,\\n    &amp;quot;reasoning_suffix&amp;quot;,\\n    &amp;quot;reply&amp;quot;\\n  ]\\n}\\n&lt;/code&gt;&lt;/pre&gt;\\n\\n&lt;p&gt;This &lt;em&gt;sort of works&lt;/em&gt; in that it does in fact cause the model to perform reasoning, but some bits of undesired json are being included in the output. Such as:&lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;{ &amp;quot;thinking_prefix&amp;quot;: &amp;quot;&lt;/p&gt;\\n\\n&lt;p&gt;&amp;lt;think&amp;gt;&amp;quot;,\\n&amp;quot;thoughts&amp;quot;: &amp;quot;The user is asking for a simple test. I need to respond positively and confirm functionality. Maybe add a playful emoji.&amp;quot;\\n, &amp;quot;thinking_suffix&amp;quot;: &amp;quot;&amp;lt;/think&amp;gt;&lt;/p&gt;\\n\\n&lt;p&gt;&amp;quot;,\\n&amp;quot;reply&amp;quot;: &amp;quot;Testing successful! ðŸ˜Š Everything seems to be working smoothly. How can I assist you today?&amp;quot; } &lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;I assume I&amp;#39;ve done something wrong. Can anyone help me understand how to format the schema correctly for this purpose?&lt;/p&gt;\\n\\n&lt;p&gt;On an unrelated note, if anyone can tell me where to find or modify more llama.cpp sampler settings I&amp;#39;d love to know about it. Otherwise it seems like I can only change Temperature, TopK, Rep. Pen., MinP, and TopP...&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":true,"is_crosspostable":false,"pinned":false,"over_18":false,"preview":{"images":[{"source":{"url":"https://external-preview.redd.it/ytmT6spTi9ZIVCmBZ4Xdi8PdPPKQAGuJJuT6HnMnshk.png?auto=webp&amp;s=4f9186aabac5847d8bb8cba8a974ad27cd3964d3","width":1200,"height":648},"resolutions":[{"url":"https://external-preview.redd.it/ytmT6spTi9ZIVCmBZ4Xdi8PdPPKQAGuJJuT6HnMnshk.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=965bf8099f34d87a454fb8dd1f546be43d598fb7","width":108,"height":58},{"url":"https://external-preview.redd.it/ytmT6spTi9ZIVCmBZ4Xdi8PdPPKQAGuJJuT6HnMnshk.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=03851a7f7e0086b37a428817e7f3ddf5dd74354e","width":216,"height":116},{"url":"https://external-preview.redd.it/ytmT6spTi9ZIVCmBZ4Xdi8PdPPKQAGuJJuT6HnMnshk.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=4aaa81559ba0c45f7b6fe03da92d3652c0837843","width":320,"height":172},{"url":"https://external-preview.redd.it/ytmT6spTi9ZIVCmBZ4Xdi8PdPPKQAGuJJuT6HnMnshk.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=69d5a1db9f762af62c26cda7748ec1a3b24b1d37","width":640,"height":345},{"url":"https://external-preview.redd.it/ytmT6spTi9ZIVCmBZ4Xdi8PdPPKQAGuJJuT6HnMnshk.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=db6631de9e38e7205dc68f64e424d22140dc4387","width":960,"height":518},{"url":"https://external-preview.redd.it/ytmT6spTi9ZIVCmBZ4Xdi8PdPPKQAGuJJuT6HnMnshk.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b01269a9d5f541a8510da2f205dd43b2628558ef","width":1080,"height":583}],"variants":{},"id":"ytmT6spTi9ZIVCmBZ4Xdi8PdPPKQAGuJJuT6HnMnshk"}],"enabled":false},"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1m3s01i","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"Jawzper","discussion_type":null,"num_comments":3,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1m3s01i/structured_output_help_lm_studio/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1m3s01i/structured_output_help_lm_studio/","subreddit_subscribers":501753,"created_utc":1752917148,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3zp401","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Jawzper","can_mod_post":false,"created_utc":1752931044,"send_replies":true,"parent_id":"t1_n3zo6f7","score":1,"author_fullname":"t2_gebwv","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Oh, is that how it works? I suppose I misunderstood the purpose of structured output then.\\n\\nThese jinja templates look complicated, but you might be right that this is where I should be looking. Thanks.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3zp401","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Oh, is that how it works? I suppose I misunderstood the purpose of structured output then.&lt;/p&gt;\\n\\n&lt;p&gt;These jinja templates look complicated, but you might be right that this is where I should be looking. Thanks.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m3s01i","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3s01i/structured_output_help_lm_studio/n3zp401/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752931044,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3zo6f7","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Gregory-Wolf","can_mod_post":false,"created_utc":1752930703,"send_replies":true,"parent_id":"t3_1m3s01i","score":1,"author_fullname":"t2_gethr3mh","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"nothing\'s wrong. structured output is for json output - you provide schema and model is forced to generate a json.\\n\\nyou sure it\'s structured output you need and not jinja template? I\'m not much into templates though, not sure","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3zo6f7","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;nothing&amp;#39;s wrong. structured output is for json output - you provide schema and model is forced to generate a json.&lt;/p&gt;\\n\\n&lt;p&gt;you sure it&amp;#39;s structured output you need and not jinja template? I&amp;#39;m not much into templates though, not sure&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3s01i/structured_output_help_lm_studio/n3zo6f7/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752930703,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m3s01i","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4213dy","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"No-Mountain3817","can_mod_post":false,"created_utc":1752957844,"send_replies":true,"parent_id":"t3_1m3s01i","score":1,"author_fullname":"t2_hylfch6q5","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"`{` Â  `\\"type\\": \\"object\\",` Â   `\\"properties\\": {` `\\"first_name\\": {` `\\"type\\": \\"string\\"` `},` `\\"last_name\\": {` `\\"type\\": \\"string\\"` `},` `\\"age\\": {` `\\"type\\": \\"integer\\"` `}` Â  `},` Â  `\\"required\\": [` `\\"first_name\\",` `\\"last_name\\",` `\\"age\\"` Â  `]` `}`\\n\\ne.g. output\\n\\nhttps://preview.redd.it/r12nr68a8wdf1.png?width=1132&amp;format=png&amp;auto=webp&amp;s=f611323f0bf81c36040641f5d6a1fe828a549005","edited":1752958053,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4213dy","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;&lt;code&gt;{&lt;/code&gt; Â  &lt;code&gt;&amp;quot;type&amp;quot;: &amp;quot;object&amp;quot;,&lt;/code&gt; Â   &lt;code&gt;&amp;quot;properties&amp;quot;: {&lt;/code&gt; &lt;code&gt;&amp;quot;first_name&amp;quot;: {&lt;/code&gt; &lt;code&gt;&amp;quot;type&amp;quot;: &amp;quot;string&amp;quot;&lt;/code&gt; &lt;code&gt;},&lt;/code&gt; &lt;code&gt;&amp;quot;last_name&amp;quot;: {&lt;/code&gt; &lt;code&gt;&amp;quot;type&amp;quot;: &amp;quot;string&amp;quot;&lt;/code&gt; &lt;code&gt;},&lt;/code&gt; &lt;code&gt;&amp;quot;age&amp;quot;: {&lt;/code&gt; &lt;code&gt;&amp;quot;type&amp;quot;: &amp;quot;integer&amp;quot;&lt;/code&gt; &lt;code&gt;}&lt;/code&gt; Â  &lt;code&gt;},&lt;/code&gt; Â  &lt;code&gt;&amp;quot;required&amp;quot;: [&lt;/code&gt; &lt;code&gt;&amp;quot;first_name&amp;quot;,&lt;/code&gt; &lt;code&gt;&amp;quot;last_name&amp;quot;,&lt;/code&gt; &lt;code&gt;&amp;quot;age&amp;quot;&lt;/code&gt; Â  &lt;code&gt;]&lt;/code&gt; &lt;code&gt;}&lt;/code&gt;&lt;/p&gt;\\n\\n&lt;p&gt;e.g. output&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://preview.redd.it/r12nr68a8wdf1.png?width=1132&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f611323f0bf81c36040641f5d6a1fe828a549005\\"&gt;https://preview.redd.it/r12nr68a8wdf1.png?width=1132&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f611323f0bf81c36040641f5d6a1fe828a549005&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3s01i/structured_output_help_lm_studio/n4213dy/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752957844,"media_metadata":{"r12nr68a8wdf1":{"status":"valid","e":"Image","m":"image/png","p":[{"y":88,"x":108,"u":"https://preview.redd.it/r12nr68a8wdf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=b78c9a9c99d4ca6c1843e6092ab01e5ec4e29c21"},{"y":176,"x":216,"u":"https://preview.redd.it/r12nr68a8wdf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=a191fe9162edc79c071574fa63bccd25d87618e8"},{"y":261,"x":320,"u":"https://preview.redd.it/r12nr68a8wdf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=9d2e25decf0b5fb4dde8f6f5a836c8d581f42d4b"},{"y":522,"x":640,"u":"https://preview.redd.it/r12nr68a8wdf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=0448d36dcc64a7e817d6d744fee1493818ff97d0"},{"y":783,"x":960,"u":"https://preview.redd.it/r12nr68a8wdf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=ad5274af9373e84f286514b6926563173df35851"},{"y":881,"x":1080,"u":"https://preview.redd.it/r12nr68a8wdf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=000c9952d66c72b8a66a30297ef64a8938d93944"}],"s":{"y":924,"x":1132,"u":"https://preview.redd.it/r12nr68a8wdf1.png?width=1132&amp;format=png&amp;auto=webp&amp;s=f611323f0bf81c36040641f5d6a1fe828a549005"},"id":"r12nr68a8wdf1"}},"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m3s01i","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]'),r=()=>t.jsx(e,{data:a});export{r as default};
