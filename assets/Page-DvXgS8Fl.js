import{j as e}from"./index-CWmJdUH_.js";import{R as l}from"./RedditPostRenderer-D2iunoQ9.js";import"./index-BCg9RP6g.js";const a=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"I’m currently using ChatGPT 4o, and I’d like to explore the possibility of running a local LLM on my home server. I know VRAM is a really big factor and I’m considering purchasing two RTX 3090s for running a local LLM. What models would compete with GPT 4o?","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Looking to possibly replace my ChatGPT subscription with running a local LLM. What local models match/rival 4o?","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1m5pmox","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.33,"author_flair_background_color":null,"subreddit_type":"public","ups":0,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_5j33g3s4","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":0,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1753119827,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;I’m currently using ChatGPT 4o, and I’d like to explore the possibility of running a local LLM on my home server. I know VRAM is a really big factor and I’m considering purchasing two RTX 3090s for running a local LLM. What models would compete with GPT 4o?&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":true,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1m5pmox","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"ActuallyGeyzer","discussion_type":null,"num_comments":9,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1m5pmox/looking_to_possibly_replace_my_chatgpt/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1m5pmox/looking_to_possibly_replace_my_chatgpt/","subreddit_subscribers":502721,"created_utc":1753119827,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"d2642412-d9ce-11ed-ae30-32b11309f5bd","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4dqliq","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ActuallyGeyzer","can_mod_post":false,"created_utc":1753120803,"send_replies":true,"parent_id":"t1_n4dq6bp","score":2,"author_fullname":"t2_5j33g3s4","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"My plan is to rent some GPU hours to test, but I don’t really know what models in that range to test out. But you are absolutely right.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4dqliq","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;My plan is to rent some GPU hours to test, but I don’t really know what models in that range to test out. But you are absolutely right.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m5pmox","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5pmox/looking_to_possibly_replace_my_chatgpt/n4dqliq/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753120803,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n4dq6bp","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"harrro","can_mod_post":false,"created_utc":1753120689,"send_replies":true,"parent_id":"t3_1m5pmox","score":19,"author_fullname":"t2_4axt7","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Before you start throwing money into hardware, first put a few dollars into an Openrouter account and try out the hundreds of models available there.\\n\\nYou'll then get an idea of which type/size of models you're interested in (some people get away with 32B models or smaller while others prefer 70B+ models).\\n\\nSample a few then based on the models you like, build a system around that.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4dq6bp","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"Alpaca"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Before you start throwing money into hardware, first put a few dollars into an Openrouter account and try out the hundreds of models available there.&lt;/p&gt;\\n\\n&lt;p&gt;You&amp;#39;ll then get an idea of which type/size of models you&amp;#39;re interested in (some people get away with 32B models or smaller while others prefer 70B+ models).&lt;/p&gt;\\n\\n&lt;p&gt;Sample a few then based on the models you like, build a system around that.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5pmox/looking_to_possibly_replace_my_chatgpt/n4dq6bp/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753120689,"author_flair_text":"Alpaca","treatment_tags":[],"link_id":"t3_1m5pmox","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bd9e9e","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":19}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4dwn5q","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"itroot","can_mod_post":false,"created_utc":1753122498,"send_replies":true,"parent_id":"t3_1m5pmox","score":7,"author_fullname":"t2_59hyp5s9","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I would suggest running MoE models like Qwen3 30B-A3B. You can test it even on CPU.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4dwn5q","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I would suggest running MoE models like Qwen3 30B-A3B. You can test it even on CPU.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5pmox/looking_to_possibly_replace_my_chatgpt/n4dwn5q/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753122498,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m5pmox","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":7}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4elteo","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"FlamaVadim","can_mod_post":false,"created_utc":1753129664,"send_replies":true,"parent_id":"t3_1m5pmox","score":7,"author_fullname":"t2_5yjg4ml2m","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"https://preview.redd.it/kq85mrdkeaef1.png?width=630&amp;format=png&amp;auto=webp&amp;s=843bb745107482c2c7815900becacb22d4d278c4","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4elteo","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;&lt;a href=\\"https://preview.redd.it/kq85mrdkeaef1.png?width=630&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=843bb745107482c2c7815900becacb22d4d278c4\\"&gt;https://preview.redd.it/kq85mrdkeaef1.png?width=630&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=843bb745107482c2c7815900becacb22d4d278c4&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5pmox/looking_to_possibly_replace_my_chatgpt/n4elteo/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753129664,"media_metadata":{"kq85mrdkeaef1":{"status":"valid","e":"Image","m":"image/png","p":[{"y":123,"x":108,"u":"https://preview.redd.it/kq85mrdkeaef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=144744990facf6fa8980bc6cd6b0a71f1294434f"},{"y":246,"x":216,"u":"https://preview.redd.it/kq85mrdkeaef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=0b2a7acc9a81bb949d0520c67ae456073dcd95d0"},{"y":365,"x":320,"u":"https://preview.redd.it/kq85mrdkeaef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=abbfba739897427194922d99bdd50b9273785bbe"}],"s":{"y":720,"x":630,"u":"https://preview.redd.it/kq85mrdkeaef1.png?width=630&amp;format=png&amp;auto=webp&amp;s=843bb745107482c2c7815900becacb22d4d278c4"},"id":"kq85mrdkeaef1"}},"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m5pmox","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":7}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4ec4ug","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"kevin_1994","can_mod_post":false,"created_utc":1753126902,"send_replies":true,"parent_id":"t3_1m5pmox","score":3,"author_fullname":"t2_o015g","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"according to https://livebench.ai then qwen3 series is better than chatgpt 4o, specifically:\\n\\n- Qwen3 30BA3\\n- Qwen3 32B\\n\\nNow, this benchmark seems far too optimistic about Qwen3 30BA3. But Qwen3 32B is roughly equivalent to me.\\n\\nYou can run these on 2x3090s no problem. Other open models you'll need 4+ 3090s lol","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4ec4ug","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;according to &lt;a href=\\"https://livebench.ai\\"&gt;https://livebench.ai&lt;/a&gt; then qwen3 series is better than chatgpt 4o, specifically:&lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;Qwen3 30BA3&lt;/li&gt;\\n&lt;li&gt;Qwen3 32B&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;p&gt;Now, this benchmark seems far too optimistic about Qwen3 30BA3. But Qwen3 32B is roughly equivalent to me.&lt;/p&gt;\\n\\n&lt;p&gt;You can run these on 2x3090s no problem. Other open models you&amp;#39;ll need 4+ 3090s lol&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":true,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5pmox/looking_to_possibly_replace_my_chatgpt/n4ec4ug/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753126902,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m5pmox","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4dsu1p","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Natejka7273","can_mod_post":false,"created_utc":1753121429,"send_replies":true,"parent_id":"t3_1m5pmox","score":4,"author_fullname":"t2_536mw","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"While truly answering this does depend somewhat on your use case, overall there is nothing that you can run locally with two 3090s that will match or exceed 4o or realistically come that close. However, that doesn't mean you shouldn't look into running llms locally as there are many advantages beyond raw power. Kimi K2 is arguably the most powerful open-weight model you can run right now...with 32 H100s...","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4dsu1p","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;While truly answering this does depend somewhat on your use case, overall there is nothing that you can run locally with two 3090s that will match or exceed 4o or realistically come that close. However, that doesn&amp;#39;t mean you shouldn&amp;#39;t look into running llms locally as there are many advantages beyond raw power. Kimi K2 is arguably the most powerful open-weight model you can run right now...with 32 H100s...&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5pmox/looking_to_possibly_replace_my_chatgpt/n4dsu1p/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753121429,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m5pmox","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4dxurw","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"__JockY__","can_mod_post":false,"created_utc":1753122838,"send_replies":true,"parent_id":"t3_1m5pmox","score":4,"author_fullname":"t2_qf8h7ka8","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"What local models rival 4o…. For what use case? \\n\\nCoding? Kimi K2 perhaps. The new Qwen3 235B released today looks very promising. Anything else… we’d need more details about your planned use cases.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4dxurw","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;What local models rival 4o…. For what use case? &lt;/p&gt;\\n\\n&lt;p&gt;Coding? Kimi K2 perhaps. The new Qwen3 235B released today looks very promising. Anything else… we’d need more details about your planned use cases.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5pmox/looking_to_possibly_replace_my_chatgpt/n4dxurw/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753122838,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m5pmox","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4dsc4y","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"GPTrack_ai","can_mod_post":false,"created_utc":1753121288,"send_replies":true,"parent_id":"t3_1m5pmox","score":2,"author_fullname":"t2_1tpuoj72sa","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Qwen/Qwen3-235B-A22B-Instruct-2507","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4dsc4y","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Qwen/Qwen3-235B-A22B-Instruct-2507&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5pmox/looking_to_possibly_replace_my_chatgpt/n4dsc4y/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753121288,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m5pmox","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4ifsqq","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Talpositiveia","can_mod_post":false,"created_utc":1753186324,"send_replies":true,"parent_id":"t3_1m5pmox","score":1,"author_fullname":"t2_rp9qubui","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"If you’re only dealing with text input and output, then Qwen 3 32B with ‘thinking’ enabled might be the most suitable choice (as 4o’s “text intelligence” is actually quite poor), or perhaps it’s your only option.\\n\\nHowever, if you’re not some kind of privacy fanatic, using online models is usually stronger and more cost-effective.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4ifsqq","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;If you’re only dealing with text input and output, then Qwen 3 32B with ‘thinking’ enabled might be the most suitable choice (as 4o’s “text intelligence” is actually quite poor), or perhaps it’s your only option.&lt;/p&gt;\\n\\n&lt;p&gt;However, if you’re not some kind of privacy fanatic, using online models is usually stronger and more cost-effective.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5pmox/looking_to_possibly_replace_my_chatgpt/n4ifsqq/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753186324,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m5pmox","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),n=()=>e.jsx(l,{data:a});export{n as default};
