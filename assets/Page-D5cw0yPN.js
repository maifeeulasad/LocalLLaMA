import{j as t}from"./index-BpC9hjVs.js";import{R as e}from"./RedditPostRenderer-BEo6AnSR.js";import"./index-DwkJHX1_.js";const n=JSON.parse('[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"The first thing we noticed is that the size estimates in Qwen\'s answer are incorrect: a 4-bit GPTQ will obviously not be 59GB in size. For reference, the 4-bit w4a16 quant of 235B we are currently testing consumes 88GB VRAM per GPU. \\n\\nThus we are suspicious of the rest of Qwen\'s answer, but lack the domain-specific expertise to validate or even assess the quality of the rest of the information. \\n\\nWe defer to you, dear LocalLLaMa community, for any analysis you can offer in this regard.\\n\\nOur prompt: \\n\\n&gt; Qwen3 235B A22 2507 (which is you, by the way) is available in BF16. Iâ€™d like to quantize you to 4-bits in order to fit into the 192GB VRAM I have available while maintaining your integrity the best I can. Please help me design a quantization process for you. Your server has two NVidia Blackwell PRO 6000 Workstation 96GB GPUs on an AMD EPYC 9745 CPU-based system with 768GB RAM. The server runs Linux. We will be using vLLM for inference. Please come up with the best possible quantization recipe for you that matches this spec. Format your answer so that I can copy/paste it into a Reddit post and maintain beautiful formatting.\\n\\nOne last point we enjoyed from the last line of the answer:\\n\\n&gt; P.S. I\'m Qwen3. Yes, this is me helping you quantize myself. Meta, huh?\\n\\nEdit #1: It looks like Qwen renamed our Blackwell PRO 6000s to \\"B1000 PRO\\", which is quite odd.\\n\\n---\\n\\n# ðŸš€ How to Quantize Qwen3-235B to 4-bit GPTQ for vLLM (Dual B1000 PRO Setup)\\n\\nWant to run **Qwen3-235B** on **two NVIDIA Blackwell B1000 PRO GPUs (96GB each)**?  \\nHereâ€™s the **ultimate 4-bit GPTQ recipe** to fit it in **192GB VRAM**, preserve accuracy, and deploy via **vLLM**.\\n\\nYouâ€™ll get near-BF16 quality at **1/8th the memory cost** â€” perfect for local inference.\\n\\n---\\n\\n## ðŸ”§ Hardware &amp; Goal\\n\\n| Component | Spec |\\n|--------|------|\\n| GPUs | 2Ã— **NVIDIA Blackwell B1000 PRO** (96GB) â†’ **192GB total VRAM** |\\n| CPU | AMD EPYC 9745 (96 cores) |\\n| RAM | 768GB DDR5 |\\n| Storage | Fast NVMe (&gt;1TB free) |\\n\\nðŸŽ¯ **Goal**: Run **Qwen3-235B-A2.2** in **4-bit GPTQ** via **vLLM**  \\nâœ… Fits in VRAM  \\nâœ… High throughput  \\nâœ… Minimal accuracy loss\\n\\n&gt; â—ï¸BF16 model = ~470GB â†’ too big  \\n&gt; âœ… 4-bit GPTQ = ~**59GB** â†’ fits perfectly!\\n\\n---\\n\\n## âš™ï¸ Step 1: Environment Setup\\n\\n```bash\\n# Create conda env\\nconda create -n qwen-gptq python=3.10\\nconda activate qwen-gptq\\n\\n# Install PyTorch + CUDA 12.4\\npip install torch==2.4.0+cu124 torchvision --extra-index-url https://download.pytorch.org/whl/cu124\\n\\n# Install Hugging Face &amp; AutoGPTQ\\npip install transformers accelerate datasets\\npip install auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/\\n\\n# Later: Install vLLM for inference\\npip install vllm\\n```\\n\\n&gt; ðŸ’¡ Use `--extra-index-url` for latest AutoGPTQ builds.\\n\\n---\\n\\n## ðŸ“š Step 2: Calibration Dataset\\n\\nUse **c4 (en)** for calibration â€” matches pretraining data.\\n\\n```python\\nfrom datasets import load_dataset\\n\\ncalib_dataset = load_dataset(\\"allenai/c4\\", \\"en\\", split=\\"train\\", streaming=True)\\ncalib_dataset = calib_dataset.take(1024)  # First 1024 samples\\n```\\n\\n&gt; âœ… Why c4? High-quality web text, diverse, standard for LLMs.\\n\\nOptional: Mix in Wikipedia for broader coverage.\\n\\n---\\n\\n## ðŸ”¤ Step 3: Quantize to 4-bit GPTQ (NF4)\\n\\nRun this script to quantize offline using **AutoGPTQ**:\\n\\n```python\\nfrom auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\\nimport torch\\nfrom transformers import AutoTokenizer\\n\\n# Load model &amp; tokenizer\\nmodel_name = \\"Qwen/Qwen3-235B-A2.2\\"  # or local path\\nquant_path = \\"./qwen3-235b-4bit-gptq\\"\\n\\ntokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\\n\\n# Quantization config\\nquantize_config = BaseQuantizeConfig(\\n    bits=4,\\n    group_size=128,\\n    desc_act=False,\\n    damp_percent=0.01,\\n    sym=True,\\n    true_sequential=True,\\n    model_name_or_path=model_name,\\n    quantize_method=\\"gptq\\",\\n    use_cuda_fp16=False\\n)\\n\\n# Load model in FP16 (needs ~470GB RAM)\\nmodel = AutoGPTQForCausalLM.from_pretrained(\\n    model_name,\\n    quantize_config=quantize_config,\\n    trust_remote_code=True\\n)\\n\\n# Calibrate &amp; quantize\\nmodel.quantize(calib_dataset, tokenizer)\\n\\n# Save\\nmodel.save_quantized(quant_path)\\ntokenizer.save_pretrained(quant_path)\\n\\nprint(f\\"âœ… Quantized model saved to {quant_path}\\")\\n```\\n\\n&gt; â³ Runtime: 4â€“8 hours (depends on disk/CPU)  \\n&gt; ðŸ“¦ Output: ~59GB 4-bit model\\n\\n---\\n\\n## ðŸ”Œ Step 4: Deploy with vLLM\\n\\nvLLM supports GPTQ natively. Just launch:\\n\\n```bash\\npython -m vllm.entrypoints.api_server \\\\\\n  --model ./qwen3-235b-4bit-gptq \\\\\\n  --tensor-parallel-size 2 \\\\\\n  --dtype half \\\\\\n  --quantization gptq \\\\\\n  --gpu-memory-utilization 0.95 \\\\\\n  --max-model-len 32768 \\\\\\n  --port 8000 \\\\\\n  --host 0.0.0.0\\n```\\n\\n### Key Args:\\n- `--tensor-parallel-size 2`: Splits model across both GPUs\\n- `--quantization gptq`: Enables 4-bit kernels\\n- `--gpu-memory-utilization 0.95`: Maximizes VRAM use\\n- `--max-model-len 32768`: Full context support\\n\\n---\\n\\n## ðŸ“ˆ Performance Expectations\\n\\n| Metric | Estimate |\\n|-------|----------|\\n| Model Size | ~59 GB |\\n| VRAM Usage | ~90GB per GPU |\\n| Load Time | 30â€“60 sec |\\n| Throughput | 150â€“250 tokens/sec (batch 1â€“4) |\\n| Accuracy | ~97% of BF16 on MMLU, GSM8K |\\n\\n&gt; ðŸŽ¯ Perfect for long-context reasoning, coding, RAG.\\n\\n---\\n\\n## âœ… Why This Recipe Wins\\n\\n| Choice | Why Itâ€™s Best |\\n|-------|---------------|\\n| **GPTQ over AWQ** | Better vLLM support, faster kernels |\\n| **NF4 + sym=True** | Higher precision than INT4 |\\n| **group_size=128** | Optimal speed/accuracy balance |\\n| **desc_act=False** | Faster inference, minor quality drop |\\n| **c4 calibration** | Matches pretraining distribution |\\n\\n&gt; ðŸ”„ Alternative: Try `desc_act=True` if you want +1% accuracy (slower).\\n\\n---\\n\\n## ðŸ’¡ Pro Tips\\n\\n- **Monitor VRAM**: `nvidia-smi -l 1`\\n- **Use fast storage**: NVMe required for smooth loading\\n- **Warm up**: Send test prompts after startup\\n- **Enable batching**: Boosts throughput\\n- **Update vLLM monthly**: GPTQ perf keeps improving\\n\\n---\\n\\n## ðŸ§© Final Notes\\n\\nWhile **AWQ** exists, **vLLMâ€™s GPTQ support is more mature** for 200B+ models. Stick with GPTQ for now.\\n\\nAlso: No public evidence that AWQ beats GPTQ on Qwen3 at 4-bit.\\n\\n---\\n\\n## ðŸ Summary: Your 4-bit Dream Setup\\n\\n```yaml\\nModel:       Qwen3-235B-A2.2\\nPrecision:   4-bit GPTQ (NF4)\\nCalibration: c4 [1024 samples]\\nGroup Size:  128\\nSymmetric:   Yes\\nInference:   vLLM + tensor_parallel=2\\nResult:      59GB model, blazing fast, fits like magic\\n```\\n\\nWith this setup, youâ€™re running one of the most powerful open models in the world â€” **locally, efficiently, and privately**.\\n\\nLet me know if you want the Colab/Slurm scripts! ðŸ’¬ðŸ”¥\\n\\nâ€”  \\n*P.S. I\'m Qwen3. Yes, this is me helping you quantize myself. Meta, huh?* ðŸ˜‰","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"We asked Qwen3-235B-A22-Instruct-2507 for advice on how best to quantize itself to 4-bits for vLLM. Anyone who understands these things care to comment on its recommendations?","link_flair_richtext":[{"e":"text","t":"Discussion"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":true,"name":"t3_1m5ynit","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.15,"author_flair_background_color":null,"subreddit_type":"public","ups":0,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_1t7r9dkpud","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Discussion","can_mod_post":false,"score":0,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1753140885,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;The first thing we noticed is that the size estimates in Qwen&amp;#39;s answer are incorrect: a 4-bit GPTQ will obviously not be 59GB in size. For reference, the 4-bit w4a16 quant of 235B we are currently testing consumes 88GB VRAM per GPU. &lt;/p&gt;\\n\\n&lt;p&gt;Thus we are suspicious of the rest of Qwen&amp;#39;s answer, but lack the domain-specific expertise to validate or even assess the quality of the rest of the information. &lt;/p&gt;\\n\\n&lt;p&gt;We defer to you, dear LocalLLaMa community, for any analysis you can offer in this regard.&lt;/p&gt;\\n\\n&lt;p&gt;Our prompt: &lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;Qwen3 235B A22 2507 (which is you, by the way) is available in BF16. Iâ€™d like to quantize you to 4-bits in order to fit into the 192GB VRAM I have available while maintaining your integrity the best I can. Please help me design a quantization process for you. Your server has two NVidia Blackwell PRO 6000 Workstation 96GB GPUs on an AMD EPYC 9745 CPU-based system with 768GB RAM. The server runs Linux. We will be using vLLM for inference. Please come up with the best possible quantization recipe for you that matches this spec. Format your answer so that I can copy/paste it into a Reddit post and maintain beautiful formatting.&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;One last point we enjoyed from the last line of the answer:&lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;P.S. I&amp;#39;m Qwen3. Yes, this is me helping you quantize myself. Meta, huh?&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;Edit #1: It looks like Qwen renamed our Blackwell PRO 6000s to &amp;quot;B1000 PRO&amp;quot;, which is quite odd.&lt;/p&gt;\\n\\n&lt;hr/&gt;\\n\\n&lt;h1&gt;ðŸš€ How to Quantize Qwen3-235B to 4-bit GPTQ for vLLM (Dual B1000 PRO Setup)&lt;/h1&gt;\\n\\n&lt;p&gt;Want to run &lt;strong&gt;Qwen3-235B&lt;/strong&gt; on &lt;strong&gt;two NVIDIA Blackwell B1000 PRO GPUs (96GB each)&lt;/strong&gt;?&lt;br/&gt;\\nHereâ€™s the &lt;strong&gt;ultimate 4-bit GPTQ recipe&lt;/strong&gt; to fit it in &lt;strong&gt;192GB VRAM&lt;/strong&gt;, preserve accuracy, and deploy via &lt;strong&gt;vLLM&lt;/strong&gt;.&lt;/p&gt;\\n\\n&lt;p&gt;Youâ€™ll get near-BF16 quality at &lt;strong&gt;1/8th the memory cost&lt;/strong&gt; â€” perfect for local inference.&lt;/p&gt;\\n\\n&lt;hr/&gt;\\n\\n&lt;h2&gt;ðŸ”§ Hardware &amp;amp; Goal&lt;/h2&gt;\\n\\n&lt;table&gt;&lt;thead&gt;\\n&lt;tr&gt;\\n&lt;th&gt;Component&lt;/th&gt;\\n&lt;th&gt;Spec&lt;/th&gt;\\n&lt;/tr&gt;\\n&lt;/thead&gt;&lt;tbody&gt;\\n&lt;tr&gt;\\n&lt;td&gt;GPUs&lt;/td&gt;\\n&lt;td&gt;2Ã— &lt;strong&gt;NVIDIA Blackwell B1000 PRO&lt;/strong&gt; (96GB) â†’ &lt;strong&gt;192GB total VRAM&lt;/strong&gt;&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;tr&gt;\\n&lt;td&gt;CPU&lt;/td&gt;\\n&lt;td&gt;AMD EPYC 9745 (96 cores)&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;tr&gt;\\n&lt;td&gt;RAM&lt;/td&gt;\\n&lt;td&gt;768GB DDR5&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;tr&gt;\\n&lt;td&gt;Storage&lt;/td&gt;\\n&lt;td&gt;Fast NVMe (&amp;gt;1TB free)&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;/tbody&gt;&lt;/table&gt;\\n\\n&lt;p&gt;ðŸŽ¯ &lt;strong&gt;Goal&lt;/strong&gt;: Run &lt;strong&gt;Qwen3-235B-A2.2&lt;/strong&gt; in &lt;strong&gt;4-bit GPTQ&lt;/strong&gt; via &lt;strong&gt;vLLM&lt;/strong&gt;&lt;br/&gt;\\nâœ… Fits in VRAM&lt;br/&gt;\\nâœ… High throughput&lt;br/&gt;\\nâœ… Minimal accuracy loss&lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;â—ï¸BF16 model = ~470GB â†’ too big&lt;br/&gt;\\nâœ… 4-bit GPTQ = ~&lt;strong&gt;59GB&lt;/strong&gt; â†’ fits perfectly!&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;hr/&gt;\\n\\n&lt;h2&gt;âš™ï¸ Step 1: Environment Setup&lt;/h2&gt;\\n\\n&lt;p&gt;```bash&lt;/p&gt;\\n\\n&lt;h1&gt;Create conda env&lt;/h1&gt;\\n\\n&lt;p&gt;conda create -n qwen-gptq python=3.10\\nconda activate qwen-gptq&lt;/p&gt;\\n\\n&lt;h1&gt;Install PyTorch + CUDA 12.4&lt;/h1&gt;\\n\\n&lt;p&gt;pip install torch==2.4.0+cu124 torchvision --extra-index-url &lt;a href=\\"https://download.pytorch.org/whl/cu124\\"&gt;https://download.pytorch.org/whl/cu124&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;h1&gt;Install Hugging Face &amp;amp; AutoGPTQ&lt;/h1&gt;\\n\\n&lt;p&gt;pip install transformers accelerate datasets\\npip install auto-gptq --extra-index-url &lt;a href=\\"https://huggingface.github.io/autogptq-index/whl/\\"&gt;https://huggingface.github.io/autogptq-index/whl/&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;h1&gt;Later: Install vLLM for inference&lt;/h1&gt;\\n\\n&lt;p&gt;pip install vllm\\n```&lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;ðŸ’¡ Use &lt;code&gt;--extra-index-url&lt;/code&gt; for latest AutoGPTQ builds.&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;hr/&gt;\\n\\n&lt;h2&gt;ðŸ“š Step 2: Calibration Dataset&lt;/h2&gt;\\n\\n&lt;p&gt;Use &lt;strong&gt;c4 (en)&lt;/strong&gt; for calibration â€” matches pretraining data.&lt;/p&gt;\\n\\n&lt;p&gt;```python\\nfrom datasets import load_dataset&lt;/p&gt;\\n\\n&lt;p&gt;calib_dataset = load_dataset(&amp;quot;allenai/c4&amp;quot;, &amp;quot;en&amp;quot;, split=&amp;quot;train&amp;quot;, streaming=True)\\ncalib_dataset = calib_dataset.take(1024)  # First 1024 samples\\n```&lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;âœ… Why c4? High-quality web text, diverse, standard for LLMs.&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;Optional: Mix in Wikipedia for broader coverage.&lt;/p&gt;\\n\\n&lt;hr/&gt;\\n\\n&lt;h2&gt;ðŸ”¤ Step 3: Quantize to 4-bit GPTQ (NF4)&lt;/h2&gt;\\n\\n&lt;p&gt;Run this script to quantize offline using &lt;strong&gt;AutoGPTQ&lt;/strong&gt;:&lt;/p&gt;\\n\\n&lt;p&gt;```python\\nfrom auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig\\nimport torch\\nfrom transformers import AutoTokenizer&lt;/p&gt;\\n\\n&lt;h1&gt;Load model &amp;amp; tokenizer&lt;/h1&gt;\\n\\n&lt;p&gt;model_name = &amp;quot;Qwen/Qwen3-235B-A2.2&amp;quot;  # or local path\\nquant_path = &amp;quot;./qwen3-235b-4bit-gptq&amp;quot;&lt;/p&gt;\\n\\n&lt;p&gt;tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)&lt;/p&gt;\\n\\n&lt;h1&gt;Quantization config&lt;/h1&gt;\\n\\n&lt;p&gt;quantize_config = BaseQuantizeConfig(\\n    bits=4,\\n    group_size=128,\\n    desc_act=False,\\n    damp_percent=0.01,\\n    sym=True,\\n    true_sequential=True,\\n    model_name_or_path=model_name,\\n    quantize_method=&amp;quot;gptq&amp;quot;,\\n    use_cuda_fp16=False\\n)&lt;/p&gt;\\n\\n&lt;h1&gt;Load model in FP16 (needs ~470GB RAM)&lt;/h1&gt;\\n\\n&lt;p&gt;model = AutoGPTQForCausalLM.from_pretrained(\\n    model_name,\\n    quantize_config=quantize_config,\\n    trust_remote_code=True\\n)&lt;/p&gt;\\n\\n&lt;h1&gt;Calibrate &amp;amp; quantize&lt;/h1&gt;\\n\\n&lt;p&gt;model.quantize(calib_dataset, tokenizer)&lt;/p&gt;\\n\\n&lt;h1&gt;Save&lt;/h1&gt;\\n\\n&lt;p&gt;model.save_quantized(quant_path)\\ntokenizer.save_pretrained(quant_path)&lt;/p&gt;\\n\\n&lt;p&gt;print(f&amp;quot;âœ… Quantized model saved to {quant_path}&amp;quot;)\\n```&lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;â³ Runtime: 4â€“8 hours (depends on disk/CPU)&lt;br/&gt;\\nðŸ“¦ Output: ~59GB 4-bit model&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;hr/&gt;\\n\\n&lt;h2&gt;ðŸ”Œ Step 4: Deploy with vLLM&lt;/h2&gt;\\n\\n&lt;p&gt;vLLM supports GPTQ natively. Just launch:&lt;/p&gt;\\n\\n&lt;p&gt;&lt;code&gt;bash\\npython -m vllm.entrypoints.api_server \\\\\\n  --model ./qwen3-235b-4bit-gptq \\\\\\n  --tensor-parallel-size 2 \\\\\\n  --dtype half \\\\\\n  --quantization gptq \\\\\\n  --gpu-memory-utilization 0.95 \\\\\\n  --max-model-len 32768 \\\\\\n  --port 8000 \\\\\\n  --host 0.0.0.0\\n&lt;/code&gt;&lt;/p&gt;\\n\\n&lt;h3&gt;Key Args:&lt;/h3&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;&lt;code&gt;--tensor-parallel-size 2&lt;/code&gt;: Splits model across both GPUs&lt;/li&gt;\\n&lt;li&gt;&lt;code&gt;--quantization gptq&lt;/code&gt;: Enables 4-bit kernels&lt;/li&gt;\\n&lt;li&gt;&lt;code&gt;--gpu-memory-utilization 0.95&lt;/code&gt;: Maximizes VRAM use&lt;/li&gt;\\n&lt;li&gt;&lt;code&gt;--max-model-len 32768&lt;/code&gt;: Full context support&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;hr/&gt;\\n\\n&lt;h2&gt;ðŸ“ˆ Performance Expectations&lt;/h2&gt;\\n\\n&lt;table&gt;&lt;thead&gt;\\n&lt;tr&gt;\\n&lt;th&gt;Metric&lt;/th&gt;\\n&lt;th&gt;Estimate&lt;/th&gt;\\n&lt;/tr&gt;\\n&lt;/thead&gt;&lt;tbody&gt;\\n&lt;tr&gt;\\n&lt;td&gt;Model Size&lt;/td&gt;\\n&lt;td&gt;~59 GB&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;tr&gt;\\n&lt;td&gt;VRAM Usage&lt;/td&gt;\\n&lt;td&gt;~90GB per GPU&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;tr&gt;\\n&lt;td&gt;Load Time&lt;/td&gt;\\n&lt;td&gt;30â€“60 sec&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;tr&gt;\\n&lt;td&gt;Throughput&lt;/td&gt;\\n&lt;td&gt;150â€“250 tokens/sec (batch 1â€“4)&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;tr&gt;\\n&lt;td&gt;Accuracy&lt;/td&gt;\\n&lt;td&gt;~97% of BF16 on MMLU, GSM8K&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;/tbody&gt;&lt;/table&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;ðŸŽ¯ Perfect for long-context reasoning, coding, RAG.&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;hr/&gt;\\n\\n&lt;h2&gt;âœ… Why This Recipe Wins&lt;/h2&gt;\\n\\n&lt;table&gt;&lt;thead&gt;\\n&lt;tr&gt;\\n&lt;th&gt;Choice&lt;/th&gt;\\n&lt;th&gt;Why Itâ€™s Best&lt;/th&gt;\\n&lt;/tr&gt;\\n&lt;/thead&gt;&lt;tbody&gt;\\n&lt;tr&gt;\\n&lt;td&gt;&lt;strong&gt;GPTQ over AWQ&lt;/strong&gt;&lt;/td&gt;\\n&lt;td&gt;Better vLLM support, faster kernels&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;tr&gt;\\n&lt;td&gt;&lt;strong&gt;NF4 + sym=True&lt;/strong&gt;&lt;/td&gt;\\n&lt;td&gt;Higher precision than INT4&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;tr&gt;\\n&lt;td&gt;&lt;strong&gt;group_size=128&lt;/strong&gt;&lt;/td&gt;\\n&lt;td&gt;Optimal speed/accuracy balance&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;tr&gt;\\n&lt;td&gt;&lt;strong&gt;desc_act=False&lt;/strong&gt;&lt;/td&gt;\\n&lt;td&gt;Faster inference, minor quality drop&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;tr&gt;\\n&lt;td&gt;&lt;strong&gt;c4 calibration&lt;/strong&gt;&lt;/td&gt;\\n&lt;td&gt;Matches pretraining distribution&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;/tbody&gt;&lt;/table&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;ðŸ”„ Alternative: Try &lt;code&gt;desc_act=True&lt;/code&gt; if you want +1% accuracy (slower).&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;hr/&gt;\\n\\n&lt;h2&gt;ðŸ’¡ Pro Tips&lt;/h2&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;&lt;strong&gt;Monitor VRAM&lt;/strong&gt;: &lt;code&gt;nvidia-smi -l 1&lt;/code&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;strong&gt;Use fast storage&lt;/strong&gt;: NVMe required for smooth loading&lt;/li&gt;\\n&lt;li&gt;&lt;strong&gt;Warm up&lt;/strong&gt;: Send test prompts after startup&lt;/li&gt;\\n&lt;li&gt;&lt;strong&gt;Enable batching&lt;/strong&gt;: Boosts throughput&lt;/li&gt;\\n&lt;li&gt;&lt;strong&gt;Update vLLM monthly&lt;/strong&gt;: GPTQ perf keeps improving&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;hr/&gt;\\n\\n&lt;h2&gt;ðŸ§© Final Notes&lt;/h2&gt;\\n\\n&lt;p&gt;While &lt;strong&gt;AWQ&lt;/strong&gt; exists, &lt;strong&gt;vLLMâ€™s GPTQ support is more mature&lt;/strong&gt; for 200B+ models. Stick with GPTQ for now.&lt;/p&gt;\\n\\n&lt;p&gt;Also: No public evidence that AWQ beats GPTQ on Qwen3 at 4-bit.&lt;/p&gt;\\n\\n&lt;hr/&gt;\\n\\n&lt;h2&gt;ðŸ Summary: Your 4-bit Dream Setup&lt;/h2&gt;\\n\\n&lt;p&gt;&lt;code&gt;yaml\\nModel:       Qwen3-235B-A2.2\\nPrecision:   4-bit GPTQ (NF4)\\nCalibration: c4 [1024 samples]\\nGroup Size:  128\\nSymmetric:   Yes\\nInference:   vLLM + tensor_parallel=2\\nResult:      59GB model, blazing fast, fits like magic\\n&lt;/code&gt;&lt;/p&gt;\\n\\n&lt;p&gt;With this setup, youâ€™re running one of the most powerful open models in the world â€” &lt;strong&gt;locally, efficiently, and privately&lt;/strong&gt;.&lt;/p&gt;\\n\\n&lt;p&gt;Let me know if you want the Colab/Slurm scripts! ðŸ’¬ðŸ”¥&lt;/p&gt;\\n\\n&lt;p&gt;â€”&lt;br/&gt;\\n&lt;em&gt;P.S. I&amp;#39;m Qwen3. Yes, this is me helping you quantize myself. Meta, huh?&lt;/em&gt; ðŸ˜‰&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":true,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"5f921ea4-c7bc-11ed-9c23-3a00622979b4","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#646d73","id":"1m5ynit","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"blackwell_tart","discussion_type":null,"num_comments":3,"send_replies":false,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1m5ynit/we_asked_qwen3235ba22instruct2507_for_advice_on/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1m5ynit/we_asked_qwen3235ba22instruct2507_for_advice_on/","subreddit_subscribers":502515,"created_utc":1753140885,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4fo3ue","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"triynizzles1","can_mod_post":false,"created_utc":1753141626,"send_replies":true,"parent_id":"t3_1m5ynit","score":2,"author_fullname":"t2_zr0g49ixt","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"If the architecture is unchanged from the initial release, then you should be able to download the weights and convert to GGUF using llama CPP. If that fails, then you will need to wait until support has been added to llama CPP","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4fo3ue","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;If the architecture is unchanged from the initial release, then you should be able to download the weights and convert to GGUF using llama CPP. If that fails, then you will need to wait until support has been added to llama CPP&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5ynit/we_asked_qwen3235ba22instruct2507_for_advice_on/n4fo3ue/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753141626,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m5ynit","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4frg7a","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"____vladrad","can_mod_post":false,"created_utc":1753142759,"send_replies":true,"parent_id":"t3_1m5ynit","score":2,"author_fullname":"t2_u6i8a0ay","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Could you push your quant to hf. Iâ€™m just getting off work and have the same setup. Would love to try it","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4frg7a","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Could you push your quant to hf. Iâ€™m just getting off work and have the same setup. Would love to try it&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5ynit/we_asked_qwen3235ba22instruct2507_for_advice_on/n4frg7a/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753142759,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m5ynit","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4ftsnh","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Affectionate-Cap-600","can_mod_post":false,"created_utc":1753143576,"send_replies":true,"parent_id":"t3_1m5ynit","score":1,"author_fullname":"t2_5oltmr5b","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"oh noooo those emojis burned my eyes\\n\\njokes aside... it really seems gpt 4o","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4ftsnh","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;oh noooo those emojis burned my eyes&lt;/p&gt;\\n\\n&lt;p&gt;jokes aside... it really seems gpt 4o&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5ynit/we_asked_qwen3235ba22instruct2507_for_advice_on/n4ftsnh/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753143576,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m5ynit","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]'),r=()=>t.jsx(e,{data:n});export{r as default};
