import{j as e}from"./index-DQXiEb7D.js";import{R as l}from"./RedditPostRenderer-BjndLgq8.js";import"./index-B-ILyjT1.js";const t=JSON.parse('[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"32GB VRAM suppose to fit 24-27B models at 8b quant right?\\n\\nHere is what i am trying via `vllm serve`\\n\\nThis works fine\\n\\n```\\n--model unsloth/Devstral-Small-2505-unsloth-bnb-4bit  --port 80  --quantization=\\"bitsandbytes\\" --load-format bitsandbytes --pipeline-parallel-size 2  --max-num-seqs 1 --max-model-len 40960\\n```\\n\\nEven qwen3-32B AWQ works fine:\\n\\n```\\n--model Qwen/Qwen3-32B-AWQ --port 80  --tensor-parallel-size 2  --chat-template /qwen3_nonthinking.jinja\\n```\\n\\n\\nBut this errors out with OOM\\n\\n\\n\\n```bash\\n--model unsloth/gemma-3-27b-it-qat-unsloth-bnb-4bit  --port 80  --quantization=\\"bitsandbytes\\" --load-format bitsandbytes --pipeline-parallel-size 2 --max-num-seqs 1        \\n```\\n\\nerror :\\n\\n```\\ninference-1    | (VllmWorker rank=1 pid=165) ERROR 07-10 12:57:03 [multiproc_executor.py:487] ValueError: Free memory on device (15.34/15.57 GiB) on startup is less than desired GPU memory utilization (1.0, 15.57 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.\\n```\\n\\n","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Whats wrong with my vLLM Config? I have 2x4070TiSupers and I couldn\'t run many models at bnb-4bit Quants.","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lwmxbx","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":1,"author_flair_background_color":null,"subreddit_type":"public","ups":2,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_86dk0gye","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":2,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":1752230407,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1752178448,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;32GB VRAM suppose to fit 24-27B models at 8b quant right?&lt;/p&gt;\\n\\n&lt;p&gt;Here is what i am trying via &lt;code&gt;vllm serve&lt;/code&gt;&lt;/p&gt;\\n\\n&lt;p&gt;This works fine&lt;/p&gt;\\n\\n&lt;p&gt;&lt;code&gt;\\n--model unsloth/Devstral-Small-2505-unsloth-bnb-4bit  --port 80  --quantization=&amp;quot;bitsandbytes&amp;quot; --load-format bitsandbytes --pipeline-parallel-size 2  --max-num-seqs 1 --max-model-len 40960\\n&lt;/code&gt;&lt;/p&gt;\\n\\n&lt;p&gt;Even qwen3-32B AWQ works fine:&lt;/p&gt;\\n\\n&lt;p&gt;&lt;code&gt;\\n--model Qwen/Qwen3-32B-AWQ --port 80  --tensor-parallel-size 2  --chat-template /qwen3_nonthinking.jinja\\n&lt;/code&gt;&lt;/p&gt;\\n\\n&lt;p&gt;But this errors out with OOM&lt;/p&gt;\\n\\n&lt;p&gt;&lt;code&gt;bash\\n--model unsloth/gemma-3-27b-it-qat-unsloth-bnb-4bit  --port 80  --quantization=&amp;quot;bitsandbytes&amp;quot; --load-format bitsandbytes --pipeline-parallel-size 2 --max-num-seqs 1        \\n&lt;/code&gt;&lt;/p&gt;\\n\\n&lt;p&gt;error :&lt;/p&gt;\\n\\n&lt;p&gt;&lt;code&gt;\\ninference-1    | (VllmWorker rank=1 pid=165) ERROR 07-10 12:57:03 [multiproc_executor.py:487] ValueError: Free memory on device (15.34/15.57 GiB) on startup is less than desired GPU memory utilization (1.0, 15.57 GiB). Decrease GPU memory utilization or reduce GPU memory used by other processes.\\n&lt;/code&gt;&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1lwmxbx","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"Voxandr","discussion_type":null,"num_comments":24,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lwmxbx/whats_wrong_with_my_vllm_config_i_have/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lwmxbx/whats_wrong_with_my_vllm_config_i_have/","subreddit_subscribers":497505,"created_utc":1752178448,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2g441d","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Voxandr","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2g03ua","score":1,"author_fullname":"t2_86dk0gye","approved_by":null,"mod_note":null,"all_awardings":[],"body":"i am not sure , with CPU offloading it is loading now..","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n2g441d","is_submitter":true,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;i am not sure , with CPU offloading it is loading now..&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lwmxbx","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwmxbx/whats_wrong_with_my_vllm_config_i_have/n2g441d/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752186512,"author_flair_text":null,"treatment_tags":[],"created_utc":1752186512,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2g03ua","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DinoAmino","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2fqj34","score":2,"author_fullname":"t2_j1v7f","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Ah, maybe because unsloth can\'t be used to locally fine-tune on multi GPU (but can in the cloud using their fine-tuning service)  ... so can these bnb quants even run multi GPU?","edited":false,"author_flair_css_class":null,"name":"t1_n2g03ua","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Ah, maybe because unsloth can&amp;#39;t be used to locally fine-tune on multi GPU (but can in the cloud using their fine-tuning service)  ... so can these bnb quants even run multi GPU?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lwmxbx","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwmxbx/whats_wrong_with_my_vllm_config_i_have/n2g03ua/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752185227,"author_flair_text":null,"collapsed":false,"created_utc":1752185227,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n2fqj34","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Voxandr","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2fo5vi","score":2,"author_fullname":"t2_86dk0gye","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"bnb don\'t like tensor-parallel it won\'t run at all","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2fqj34","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;bnb don&amp;#39;t like tensor-parallel it won&amp;#39;t run at all&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lwmxbx","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwmxbx/whats_wrong_with_my_vllm_config_i_have/n2fqj34/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752182305,"author_flair_text":null,"treatment_tags":[],"created_utc":1752182305,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2fpnf3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Voxandr","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2fonja","score":1,"author_fullname":"t2_86dk0gye","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"even with --enforce-egar and fp8 kv-cache , it still out of memory\\n\\n```\\n--model unsloth/gemma-3-27b-it-qat-unsloth-bnb-4bit  --port 80  --quantization=\\"bitsandbytes\\" --load-format bitsandbytes --pipeline-parallel-size 2 --max-num-seqs 1 --enforce-eager --kv-cache-dtype fp8_e4m3\\n\\n```\\n\\nI guess i just need 2GB more. but what i am confused is 4Bit quant of 32B model should be just around 14GB max VRAM right? \\nAlso i cannot set --max-model-len on gemma","edited":false,"author_flair_css_class":null,"name":"t1_n2fpnf3","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;even with --enforce-egar and fp8 kv-cache , it still out of memory&lt;/p&gt;\\n\\n&lt;p&gt;```\\n--model unsloth/gemma-3-27b-it-qat-unsloth-bnb-4bit  --port 80  --quantization=&amp;quot;bitsandbytes&amp;quot; --load-format bitsandbytes --pipeline-parallel-size 2 --max-num-seqs 1 --enforce-eager --kv-cache-dtype fp8_e4m3&lt;/p&gt;\\n\\n&lt;p&gt;```&lt;/p&gt;\\n\\n&lt;p&gt;I guess i just need 2GB more. but what i am confused is 4Bit quant of 32B model should be just around 14GB max VRAM right? \\nAlso i cannot set --max-model-len on gemma&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lwmxbx","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwmxbx/whats_wrong_with_my_vllm_config_i_have/n2fpnf3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752182038,"author_flair_text":null,"collapsed":false,"created_utc":1752182038,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2fonja","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Voxandr","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2fo5vi","score":1,"author_fullname":"t2_86dk0gye","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Thanks , gonna try --enforce-egar instead of offloading 2GB to memory , which will slow things down significantly. also how about v1 vs v0 engine (if i do fp8-kvcahce i could save a lot more too but it will engine v0 only)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2fonja","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thanks , gonna try --enforce-egar instead of offloading 2GB to memory , which will slow things down significantly. also how about v1 vs v0 engine (if i do fp8-kvcahce i could save a lot more too but it will engine v0 only)&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lwmxbx","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwmxbx/whats_wrong_with_my_vllm_config_i_have/n2fonja/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752181741,"author_flair_text":null,"treatment_tags":[],"created_utc":1752181741,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2fwm5b","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DinoAmino","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2frtgu","score":2,"author_fullname":"t2_j1v7f","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Sorry! I thought we were talking about vLLM. I haven\'t used unsloth.\\n\\nEdit: I\'ma derp. We are. But I haven\'t used bnb either. I can tell you that some chosen options will enforce V0 because they aren\'t supported by V1.","edited":1752184550,"author_flair_css_class":null,"name":"t1_n2fwm5b","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Sorry! I thought we were talking about vLLM. I haven&amp;#39;t used unsloth.&lt;/p&gt;\\n\\n&lt;p&gt;Edit: I&amp;#39;ma derp. We are. But I haven&amp;#39;t used bnb either. I can tell you that some chosen options will enforce V0 because they aren&amp;#39;t supported by V1.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lwmxbx","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwmxbx/whats_wrong_with_my_vllm_config_i_have/n2fwm5b/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752184140,"author_flair_text":null,"collapsed":false,"created_utc":1752184140,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n2frtgu","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Voxandr","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2fo5vi","score":1,"author_fullname":"t2_86dk0gye","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Error is :\\n```\\ninference-1    | [rank0]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 200.00 MiB. GPU 0 has a total capacity of 15.57 GiB of which 191.50 MiB is free. Process 227494 has 14.85 GiB memory in use. Of the allocated memory 14.25 GiB is allocated by PyTorch, and 277.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variable\\n```\\n\\n\\nwith\\n\\n```\\n--model unsloth/gemma-3-27b-it-qat-unsloth-bnb-4bit  --port 80  --quantization=\\"bitsandbytes\\" --load-format bitsandbytes --pipeline-parallel-size 2 --max-num-seqs 1 --enforce-eager --kv-cache-dtype fp8_e4m3 --max-model-len 10240\\n\\n```\\nEdit:\\nLooks like 2nd GPU is not fulling being used. At the time of error NVTOP shows 100% using on first GPU but 2nd GPU have 4GB remaining.","edited":1752230450,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2frtgu","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Error is :\\n&lt;code&gt;\\ninference-1    | [rank0]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 200.00 MiB. GPU 0 has a total capacity of 15.57 GiB of which 191.50 MiB is free. Process 227494 has 14.85 GiB memory in use. Of the allocated memory 14.25 GiB is allocated by PyTorch, and 277.20 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variable\\n&lt;/code&gt;&lt;/p&gt;\\n\\n&lt;p&gt;with&lt;/p&gt;\\n\\n&lt;p&gt;```\\n--model unsloth/gemma-3-27b-it-qat-unsloth-bnb-4bit  --port 80  --quantization=&amp;quot;bitsandbytes&amp;quot; --load-format bitsandbytes --pipeline-parallel-size 2 --max-num-seqs 1 --enforce-eager --kv-cache-dtype fp8_e4m3 --max-model-len 10240&lt;/p&gt;\\n\\n&lt;p&gt;```\\nEdit:\\nLooks like 2nd GPU is not fulling being used. At the time of error NVTOP shows 100% using on first GPU but 2nd GPU have 4GB remaining.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lwmxbx","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwmxbx/whats_wrong_with_my_vllm_config_i_have/n2frtgu/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752182692,"author_flair_text":null,"treatment_tags":[],"created_utc":1752182692,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2fo5vi","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DinoAmino","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2fmwzv","score":2,"author_fullname":"t2_j1v7f","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"It shouldn\'t affect speed. If it does it shouldn\'t be significant. I always use it to save VRAM and the  INT8 and FP8 quants are still twice as fast as q8 GGUFs.\\n\\nEdit. Oh, swap out pipeline parallel for tensor parallel and see how that works out for you.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2fo5vi","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It shouldn&amp;#39;t affect speed. If it does it shouldn&amp;#39;t be significant. I always use it to save VRAM and the  INT8 and FP8 quants are still twice as fast as q8 GGUFs.&lt;/p&gt;\\n\\n&lt;p&gt;Edit. Oh, swap out pipeline parallel for tensor parallel and see how that works out for you.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lwmxbx","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwmxbx/whats_wrong_with_my_vllm_config_i_have/n2fo5vi/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752181594,"author_flair_text":null,"treatment_tags":[],"created_utc":1752181594,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n2fmwzv","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Voxandr","can_mod_post":false,"created_utc":1752181233,"send_replies":true,"parent_id":"t1_n2fesoa","score":1,"author_fullname":"t2_86dk0gye","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Thanks , trying right now but `--enforce-eager`  would slow everything down right?\\ni got  gemma3-27b-qat wokring by offloading 2GB to CPU\\n```\\n --model unsloth/gemma-3-27b-it-qat-unsloth-bnb-4bit  --port 80  --quantization=\\"bitsandbytes\\" --load-format bitsandbytes --pipeline-parallel-size 2 --max-num-seqs 1 --cpu-offload-gb 2\\n```","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2fmwzv","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thanks , trying right now but &lt;code&gt;--enforce-eager&lt;/code&gt;  would slow everything down right?\\ni got  gemma3-27b-qat wokring by offloading 2GB to CPU\\n&lt;code&gt;\\n --model unsloth/gemma-3-27b-it-qat-unsloth-bnb-4bit  --port 80  --quantization=&amp;quot;bitsandbytes&amp;quot; --load-format bitsandbytes --pipeline-parallel-size 2 --max-num-seqs 1 --cpu-offload-gb 2\\n&lt;/code&gt;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lwmxbx","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwmxbx/whats_wrong_with_my_vllm_config_i_have/n2fmwzv/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752181233,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2fesoa","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DinoAmino","can_mod_post":false,"created_utc":1752178924,"send_replies":true,"parent_id":"t3_1lwmxbx","score":3,"author_fullname":"t2_j1v7f","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"The last config looks good but I think it\'s still too much context for your VRAM. Try stepping down to 12k. Adding --enforce-eager should help save some VRAM too.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2fesoa","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The last config looks good but I think it&amp;#39;s still too much context for your VRAM. Try stepping down to 12k. Adding --enforce-eager should help save some VRAM too.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwmxbx/whats_wrong_with_my_vllm_config_i_have/n2fesoa/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752178924,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lwmxbx","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"d2642412-d9ce-11ed-ae30-32b11309f5bd","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2gd13a","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Voxandr","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2gbz8k","score":1,"author_fullname":"t2_86dk0gye","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"bnb-4bit can\'t work with Tensor Parallel . I think bnb-4bit is the problem.  \\nFor Mistral one , i think it woon\'t just fit as Fp8 - in 32GB VRam. Whats your setup?","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n2gd13a","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;bnb-4bit can&amp;#39;t work with Tensor Parallel . I think bnb-4bit is the problem.&lt;br/&gt;\\nFor Mistral one , i think it woon&amp;#39;t just fit as Fp8 - in 32GB VRam. Whats your setup?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lwmxbx","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwmxbx/whats_wrong_with_my_vllm_config_i_have/n2gd13a/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752189391,"author_flair_text":null,"treatment_tags":[],"created_utc":1752189391,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":6,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2gbz8k","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Capable-Ad-7494","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2gb28g","score":2,"author_fullname":"t2_9so78ol2","approved_by":null,"mod_note":null,"all_awardings":[],"body":"Ah, i read the mistral one, fair enough.\\n\\nThe only difference with this command and that qwen command is that you’re using pipeline parallel. instead of tensor parallel, try tensor parallel.","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n2gbz8k","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Ah, i read the mistral one, fair enough.&lt;/p&gt;\\n\\n&lt;p&gt;The only difference with this command and that qwen command is that you’re using pipeline parallel. instead of tensor parallel, try tensor parallel.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lwmxbx","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwmxbx/whats_wrong_with_my_vllm_config_i_have/n2gbz8k/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752189049,"author_flair_text":null,"treatment_tags":[],"created_utc":1752189049,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n2gb28g","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Voxandr","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2gamff","score":1,"author_fullname":"t2_86dk0gye","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I am using unsloth bnb4bit qaunt of gemma-3-27b-qat","edited":false,"author_flair_css_class":null,"name":"t1_n2gb28g","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I am using unsloth bnb4bit qaunt of gemma-3-27b-qat&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lwmxbx","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwmxbx/whats_wrong_with_my_vllm_config_i_have/n2gb28g/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752188753,"author_flair_text":null,"collapsed":false,"created_utc":1752188753,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2gamff","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Capable-Ad-7494","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2g9w61","score":2,"author_fullname":"t2_9so78ol2","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Oh, i just read your command to start it.\\n\\nVLLM probably has some overhead when quantizing from bf16 to fp8 at runtime. That doesn’t look like an fp8 model as much as it looks like a bf16 model.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2gamff","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Oh, i just read your command to start it.&lt;/p&gt;\\n\\n&lt;p&gt;VLLM probably has some overhead when quantizing from bf16 to fp8 at runtime. That doesn’t look like an fp8 model as much as it looks like a bf16 model.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lwmxbx","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwmxbx/whats_wrong_with_my_vllm_config_i_have/n2gamff/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752188615,"author_flair_text":null,"treatment_tags":[],"created_utc":1752188615,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n2g9w61","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Capable-Ad-7494","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2g8rh5","score":2,"author_fullname":"t2_9so78ol2","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Keep in mind there’s per gpu overhead with TP to some degree.\\n\\nTurn off cudagraphs, so —enforce-eager\\n\\nWhat does the OOM say, and what’s your idle vram usage?","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2g9w61","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Keep in mind there’s per gpu overhead with TP to some degree.&lt;/p&gt;\\n\\n&lt;p&gt;Turn off cudagraphs, so —enforce-eager&lt;/p&gt;\\n\\n&lt;p&gt;What does the OOM say, and what’s your idle vram usage?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lwmxbx","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwmxbx/whats_wrong_with_my_vllm_config_i_have/n2g9w61/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752188380,"author_flair_text":null,"treatment_tags":[],"created_utc":1752188380,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n2g8rh5","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Voxandr","can_mod_post":false,"created_utc":1752188016,"send_replies":true,"parent_id":"t1_n2g6bwq","score":1,"author_fullname":"t2_86dk0gye","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Tried till 0.8 , still OOM. ended up offloading 2gb to CPU and i only get 1k context window.\\n\\n    anythingllm-1  | [backend] error: Error: 400 litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: OpenAIException - This model\'s maximum context length is 1024 tokens. However, you requested 1087 tokens (63 in the messages, 1024 in the completion). Please reduce the length of the messages or completion. None","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2g8rh5","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Tried till 0.8 , still OOM. ended up offloading 2gb to CPU and i only get 1k context window.&lt;/p&gt;\\n\\n&lt;pre&gt;&lt;code&gt;anythingllm-1  | [backend] error: Error: 400 litellm.ContextWindowExceededError: litellm.BadRequestError: ContextWindowExceededError: OpenAIException - This model&amp;#39;s maximum context length is 1024 tokens. However, you requested 1087 tokens (63 in the messages, 1024 in the completion). Please reduce the length of the messages or completion. None\\n&lt;/code&gt;&lt;/pre&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lwmxbx","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwmxbx/whats_wrong_with_my_vllm_config_i_have/n2g8rh5/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752188016,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2g6bwq","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ortegaalfredo","can_mod_post":false,"created_utc":1752187230,"send_replies":true,"parent_id":"t3_1lwmxbx","score":3,"author_fullname":"t2_g177e","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Memory utilization of 1.0 always cause problems because the system always use a little bit of GPU for stuff like the browser, etc. Try it at 0.95 or 0.90.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2g6bwq","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"Alpaca"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Memory utilization of 1.0 always cause problems because the system always use a little bit of GPU for stuff like the browser, etc. Try it at 0.95 or 0.90.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwmxbx/whats_wrong_with_my_vllm_config_i_have/n2g6bwq/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752187230,"author_flair_text":"Alpaca","treatment_tags":[],"link_id":"t3_1lwmxbx","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bd9e9e","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2ixfwb","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Voxandr","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2i5g1v","score":1,"author_fullname":"t2_86dk0gye","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"WOW thanks alot , gonna try that model. I gave up on unsloth quants. I don\'t know what infernece engine to run unsloth , vLLM fails hards it seems .","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2ixfwb","is_submitter":true,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;WOW thanks alot , gonna try that model. I gave up on unsloth quants. I don&amp;#39;t know what infernece engine to run unsloth , vLLM fails hards it seems .&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lwmxbx","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwmxbx/whats_wrong_with_my_vllm_config_i_have/n2ixfwb/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752229988,"author_flair_text":null,"treatment_tags":[],"created_utc":1752229988,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2i5g1v","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Unhappy_Power702","can_mod_post":false,"created_utc":1752214471,"send_replies":true,"parent_id":"t1_n2i3mvo","score":3,"author_fullname":"t2_azcig5ir","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I tested it for you. Using the Gemma-3-27B-QAT-AWQ model, it can support approximately 85K\\\\~90K context length.  \\n\\nHardware environment: RTX A6000 \\\\*2, with 33% GPU memory utilization (simulating your 16G GPU \\\\*2).  \\n\\nRun command:  \\n\\n\\\\`\\\\`\\\\`\\n\\nCUDA\\\\_VISIBLE\\\\_DEVICES=2,3 VLLM\\\\_USE\\\\_V1=1 vllm serve /root/models/gemma-3-27b-it-qat-autoawq \\\\\\\\  \\n\\n\\\\--served-model-name Gemma3-27B \\\\\\\\  \\n\\n\\\\--max-model-len 85000 \\\\\\\\  \\n\\n\\\\--tensor-parallel-size 2 \\\\\\\\  \\n\\n\\\\--host [0.0.0.0](http://0.0.0.0) \\\\\\\\  \\n\\n\\\\--gpu-memory-utilization 0.33 \\\\\\\\  \\n\\n\\\\--trust-remote-code \\\\\\\\  \\n\\n\\\\--limit\\\\_mm\\\\_per\\\\_prompt \'image=4\' \\\\\\\\  \\n\\n\\\\--port 57851  \\n\\n\\\\`\\\\`\\\\`  \\n\\nFor your setup, the recommended run command is:  \\n\\n\\\\`\\\\`\\\\`\\n\\nvllm serve /root/models/gemma-3-27b-it-qat-autoawq \\\\\\\\  \\n\\n\\\\--served-model-name Gemma3-27B \\\\\\\\  \\n\\n\\\\--max-model-len 85000 \\\\\\\\  \\n\\n\\\\--tensor-parallel-size 2 \\\\\\\\  \\n\\n\\\\--host [0.0.0.0](http://0.0.0.0) \\\\\\\\  \\n\\n\\\\--gpu-memory-utilization 0.95 \\\\\\\\  \\n\\n\\\\--trust-remote-code \\\\\\\\  \\n\\n\\\\--limit\\\\_mm\\\\_per\\\\_prompt \'image=4\' \\\\\\\\  \\n\\n\\\\--port 57851  \\n\\n\\\\`\\\\`\\\\`  \\n\\nModel ID: gaunernst/gemma-3-27b-it-qat-autoawq","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2i5g1v","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I tested it for you. Using the Gemma-3-27B-QAT-AWQ model, it can support approximately 85K~90K context length.  &lt;/p&gt;\\n\\n&lt;p&gt;Hardware environment: RTX A6000 *2, with 33% GPU memory utilization (simulating your 16G GPU *2).  &lt;/p&gt;\\n\\n&lt;p&gt;Run command:  &lt;/p&gt;\\n\\n&lt;p&gt;```&lt;/p&gt;\\n\\n&lt;p&gt;CUDA_VISIBLE_DEVICES=2,3 VLLM_USE_V1=1 vllm serve /root/models/gemma-3-27b-it-qat-autoawq \\\\  &lt;/p&gt;\\n\\n&lt;p&gt;--served-model-name Gemma3-27B \\\\  &lt;/p&gt;\\n\\n&lt;p&gt;--max-model-len 85000 \\\\  &lt;/p&gt;\\n\\n&lt;p&gt;--tensor-parallel-size 2 \\\\  &lt;/p&gt;\\n\\n&lt;p&gt;--host &lt;a href=\\"http://0.0.0.0\\"&gt;0.0.0.0&lt;/a&gt; \\\\  &lt;/p&gt;\\n\\n&lt;p&gt;--gpu-memory-utilization 0.33 \\\\  &lt;/p&gt;\\n\\n&lt;p&gt;--trust-remote-code \\\\  &lt;/p&gt;\\n\\n&lt;p&gt;--limit_mm_per_prompt &amp;#39;image=4&amp;#39; \\\\  &lt;/p&gt;\\n\\n&lt;p&gt;--port 57851  &lt;/p&gt;\\n\\n&lt;p&gt;```  &lt;/p&gt;\\n\\n&lt;p&gt;For your setup, the recommended run command is:  &lt;/p&gt;\\n\\n&lt;p&gt;```&lt;/p&gt;\\n\\n&lt;p&gt;vllm serve /root/models/gemma-3-27b-it-qat-autoawq \\\\  &lt;/p&gt;\\n\\n&lt;p&gt;--served-model-name Gemma3-27B \\\\  &lt;/p&gt;\\n\\n&lt;p&gt;--max-model-len 85000 \\\\  &lt;/p&gt;\\n\\n&lt;p&gt;--tensor-parallel-size 2 \\\\  &lt;/p&gt;\\n\\n&lt;p&gt;--host &lt;a href=\\"http://0.0.0.0\\"&gt;0.0.0.0&lt;/a&gt; \\\\  &lt;/p&gt;\\n\\n&lt;p&gt;--gpu-memory-utilization 0.95 \\\\  &lt;/p&gt;\\n\\n&lt;p&gt;--trust-remote-code \\\\  &lt;/p&gt;\\n\\n&lt;p&gt;--limit_mm_per_prompt &amp;#39;image=4&amp;#39; \\\\  &lt;/p&gt;\\n\\n&lt;p&gt;--port 57851  &lt;/p&gt;\\n\\n&lt;p&gt;```  &lt;/p&gt;\\n\\n&lt;p&gt;Model ID: gaunernst/gemma-3-27b-it-qat-autoawq&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lwmxbx","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwmxbx/whats_wrong_with_my_vllm_config_i_have/n2i5g1v/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752214471,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2ixce1","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Voxandr","can_mod_post":false,"created_utc":1752229939,"send_replies":true,"parent_id":"t1_n2i3mvo","score":1,"author_fullname":"t2_86dk0gye","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Thanks alot for the pointers!  I will give you a Hi!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2ixce1","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thanks alot for the pointers!  I will give you a Hi!&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lwmxbx","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwmxbx/whats_wrong_with_my_vllm_config_i_have/n2ixce1/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752229939,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2i3mvo","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Unhappy_Power702","can_mod_post":false,"created_utc":1752213535,"send_replies":true,"parent_id":"t3_1lwmxbx","score":3,"author_fullname":"t2_azcig5ir","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"1. Use TP instead of PP \\n\\n2. Pay attention to adjusting the context window size, as vLLM will create KV cache \\n\\n3. In most cases, we use AWQ for 4-bit quantization and GPTQ for 8-bit quantization (for Ada Lovelace GPUs, FP8 can also be used directly) \\n\\n4. If you have more questions, you can send an email to flymyd@foxmail.com.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2i3mvo","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;ol&gt;\\n&lt;li&gt;&lt;p&gt;Use TP instead of PP &lt;/p&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;p&gt;Pay attention to adjusting the context window size, as vLLM will create KV cache &lt;/p&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;p&gt;In most cases, we use AWQ for 4-bit quantization and GPTQ for 8-bit quantization (for Ada Lovelace GPUs, FP8 can also be used directly) &lt;/p&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;p&gt;If you have more questions, you can send an email to &lt;a href=\\"mailto:flymyd@foxmail.com\\"&gt;flymyd@foxmail.com&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;\\n&lt;/ol&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwmxbx/whats_wrong_with_my_vllm_config_i_have/n2i3mvo/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752213535,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lwmxbx","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2g3ul8","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Voxandr","can_mod_post":false,"created_utc":1752186428,"send_replies":true,"parent_id":"t1_n2g11e1","score":1,"author_fullname":"t2_86dk0gye","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Thanks alot , gonan try. Oh it isn\'t QAT one? i think i should quantize myself and release QAT ones.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2g3ul8","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thanks alot , gonan try. Oh it isn&amp;#39;t QAT one? i think i should quantize myself and release QAT ones.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lwmxbx","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwmxbx/whats_wrong_with_my_vllm_config_i_have/n2g3ul8/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752186428,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2g11e1","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DinoAmino","can_mod_post":false,"created_utc":1752185528,"send_replies":true,"parent_id":"t3_1lwmxbx","score":1,"author_fullname":"t2_j1v7f","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Hey, try this model out. It\'s tested on vLLM so practically guaranteed to work. You just need to find out the max xyz length that will work for you.\\n\\nhttps://huggingface.co/RedHatAI/gemma-3-27b-it-quantized.w4a16","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2g11e1","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Hey, try this model out. It&amp;#39;s tested on vLLM so practically guaranteed to work. You just need to find out the max xyz length that will work for you.&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://huggingface.co/RedHatAI/gemma-3-27b-it-quantized.w4a16\\"&gt;https://huggingface.co/RedHatAI/gemma-3-27b-it-quantized.w4a16&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwmxbx/whats_wrong_with_my_vllm_config_i_have/n2g11e1/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752185528,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lwmxbx","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2irsoi","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"EmilPi","can_mod_post":false,"created_utc":1752227025,"send_replies":true,"parent_id":"t3_1lwmxbx","score":1,"author_fullname":"t2_jti45lwl","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I used devstral with GGUF with vllm. To do this, you may need to use download model and put original non-weight files there, or use huggingface id where all those files exist.\\n\\nYou don\'t need unsloth quants to run gemma. Here is how I run gemma with official QAT GGUF with llama.cpp: [https://www.reddit.com/r/LocalLLaMA/comments/1lvun89/how\\\\_to\\\\_run\\\\_gemma\\\\_3\\\\_27b\\\\_qat\\\\_with\\\\_128k\\\\_context/](https://www.reddit.com/r/LocalLLaMA/comments/1lvun89/how_to_run_gemma_3_27b_qat_with_128k_context/) \\\\- of course, you need to reduce context length, to run on 2x16 GB VRAM instead of 2x24 GB VRAM.\\n\\nAnd an idea how to run on here is relevant part of my working for Devstral config  - should be working for gemma GGUF when stripping all mistral specific options, using --kv-cache-dtype fp8\\\\_e4m3 (or whatever you architecture supports), reducing --tensor-parallel-size to 2 and --max-model-len to some value:\\n\\n    vllm serve /mnt/models/gguf/unsloth/Devstral-Small-2505/Devstral-Small-2505-Q8_0.gguf --max-num-seqs 1 --max-model-len 8192 --gpu-memory-utilization 0.92 --enable-auto-tool-choice --tool-call-parser mistral --quantization gguf --tool-call-parser mistral --enable-sleep-mode --enable-chunked-prefill --enable-prefix-caching --tensor-parallel-size 2 --kv-cache-dtype fp8_e4m3 --port 80 --served-model-name Devstral-Small-2505","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2irsoi","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I used devstral with GGUF with vllm. To do this, you may need to use download model and put original non-weight files there, or use huggingface id where all those files exist.&lt;/p&gt;\\n\\n&lt;p&gt;You don&amp;#39;t need unsloth quants to run gemma. Here is how I run gemma with official QAT GGUF with llama.cpp: &lt;a href=\\"https://www.reddit.com/r/LocalLLaMA/comments/1lvun89/how_to_run_gemma_3_27b_qat_with_128k_context/\\"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1lvun89/how_to_run_gemma_3_27b_qat_with_128k_context/&lt;/a&gt; - of course, you need to reduce context length, to run on 2x16 GB VRAM instead of 2x24 GB VRAM.&lt;/p&gt;\\n\\n&lt;p&gt;And an idea how to run on here is relevant part of my working for Devstral config  - should be working for gemma GGUF when stripping all mistral specific options, using --kv-cache-dtype fp8_e4m3 (or whatever you architecture supports), reducing --tensor-parallel-size to 2 and --max-model-len to some value:&lt;/p&gt;\\n\\n&lt;pre&gt;&lt;code&gt;vllm serve /mnt/models/gguf/unsloth/Devstral-Small-2505/Devstral-Small-2505-Q8_0.gguf --max-num-seqs 1 --max-model-len 8192 --gpu-memory-utilization 0.92 --enable-auto-tool-choice --tool-call-parser mistral --quantization gguf --tool-call-parser mistral --enable-sleep-mode --enable-chunked-prefill --enable-prefix-caching --tensor-parallel-size 2 --kv-cache-dtype fp8_e4m3 --port 80 --served-model-name Devstral-Small-2505\\n&lt;/code&gt;&lt;/pre&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwmxbx/whats_wrong_with_my_vllm_config_i_have/n2irsoi/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752227025,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lwmxbx","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]'),r=()=>e.jsx(l,{data:t});export{r as default};
