import{j as e}from"./index-F0NXdzZX.js";import{R as l}from"./RedditPostRenderer-CoEZB1dt.js";import"./index-DrtravXm.js";const a=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"In terms of minutes/hours or number of query/response?\\n\\nI'm averaging around 90 minutes on good days and 30 minutes on bad days.","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"How much do you use your local model on average on a day?","link_flair_richtext":[{"e":"text","t":"Discussion"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lxbynb","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.88,"author_flair_background_color":"#bbbdbf","subreddit_type":"public","ups":18,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","is_original_content":false,"author_fullname":"t2_ah13x","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Discussion","can_mod_post":false,"score":18,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1752252611,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"richtext","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;In terms of minutes/hours or number of query/response?&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;m averaging around 90 minutes on good days and 30 minutes on bad days.&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"5f921ea4-c7bc-11ed-9c23-3a00622979b4","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":"llama.cpp","treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#646d73","id":"1lxbynb","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"segmond","discussion_type":null,"num_comments":26,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":"light","permalink":"/r/LocalLLaMA/comments/1lxbynb/how_much_do_you_use_your_local_model_on_average/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lxbynb/how_much_do_you_use_your_local_model_on_average/","subreddit_subscribers":498115,"created_utc":1752252611,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2on5mu","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Corporate_Drone31","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2n2leb","score":1,"author_fullname":"t2_32o8hu91","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"You need a pre-filtering stage to keep only the relevant entities. There's a lot of ways to do it: embedding, keyword-based, even an LLM-based picker for more complex but powerful choosing. ","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2on5mu","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You need a pre-filtering stage to keep only the relevant entities. There&amp;#39;s a lot of ways to do it: embedding, keyword-based, even an LLM-based picker for more complex but powerful choosing. &lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lxbynb","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxbynb/how_much_do_you_use_your_local_model_on_average/n2on5mu/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752299591,"author_flair_text":null,"treatment_tags":[],"created_utc":1752299591,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2n2leb","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ubrtnk","can_mod_post":false,"created_utc":1752276892,"send_replies":true,"parent_id":"t1_n2l1fnw","score":3,"author_fullname":"t2_7b5i2","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I would be interested in knowing more about your home assistant integration. I have mine integrated and it's overwhelmed with the number of entities I have exposed","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2n2leb","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I would be interested in knowing more about your home assistant integration. I have mine integrated and it&amp;#39;s overwhelmed with the number of entities I have exposed&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lxbynb","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxbynb/how_much_do_you_use_your_local_model_on_average/n2n2leb/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752276892,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2ovsyd","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"doubledaylogistics","can_mod_post":false,"created_utc":1752304371,"send_replies":true,"parent_id":"t1_n2l1fnw","score":1,"author_fullname":"t2_h7jafhqn4","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Would love to hear more about your setup/how you did this, I've got pretty much all those same use cases and would love to get something like this!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2ovsyd","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Would love to hear more about your setup/how you did this, I&amp;#39;ve got pretty much all those same use cases and would love to get something like this!&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lxbynb","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxbynb/how_much_do_you_use_your_local_model_on_average/n2ovsyd/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752304371,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2l1fnw","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Boricua-vet","can_mod_post":false,"created_utc":1752254556,"send_replies":true,"parent_id":"t3_1lxbynb","score":17,"author_fullname":"t2_vnvnb9oa","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I use 4 LLM's primarily every single day, one fine tuned to control music assistant which I can ask it to play any artist, song or playlist on any speaker across the entire home or multiple speakers depending on how I form the request. The second one is my conversational LLM which is integrated into home assistant and it handles conversations and anything related to home assistant that assist would not be able to do. The third is a fine tuned vision fine tuned LLM that works with frigate that process all video feeds and provides context to snapshots and provides voice alerts on any room I am located using presence sensors and the fourth one is used for general code production, Yaml verification and correction.  I have a fifth one for Immich for processing images but that is all automated and I really have no interaction with it so it does not count.\\n\\nI would say 2 to 3 hours daily at a minimum between all models and on a very productive day 4 to 5 hours a day.\\n\\nMy conversational LLM, Music LLM and code production LLM are what I certainly use the most.\\n\\nIf you need to know the order of which I use the most,\\n\\n1- Conversational LLM as it handles my reminders, appointments and house automation's.\\n\\n2- Code LLM. no explanation needed here.\\n\\n3- LLM for music assistant, I use this a lot.\\n\\n4- Security Vision model.\\n\\nOrdered from most used to least.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2l1fnw","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I use 4 LLM&amp;#39;s primarily every single day, one fine tuned to control music assistant which I can ask it to play any artist, song or playlist on any speaker across the entire home or multiple speakers depending on how I form the request. The second one is my conversational LLM which is integrated into home assistant and it handles conversations and anything related to home assistant that assist would not be able to do. The third is a fine tuned vision fine tuned LLM that works with frigate that process all video feeds and provides context to snapshots and provides voice alerts on any room I am located using presence sensors and the fourth one is used for general code production, Yaml verification and correction.  I have a fifth one for Immich for processing images but that is all automated and I really have no interaction with it so it does not count.&lt;/p&gt;\\n\\n&lt;p&gt;I would say 2 to 3 hours daily at a minimum between all models and on a very productive day 4 to 5 hours a day.&lt;/p&gt;\\n\\n&lt;p&gt;My conversational LLM, Music LLM and code production LLM are what I certainly use the most.&lt;/p&gt;\\n\\n&lt;p&gt;If you need to know the order of which I use the most,&lt;/p&gt;\\n\\n&lt;p&gt;1- Conversational LLM as it handles my reminders, appointments and house automation&amp;#39;s.&lt;/p&gt;\\n\\n&lt;p&gt;2- Code LLM. no explanation needed here.&lt;/p&gt;\\n\\n&lt;p&gt;3- LLM for music assistant, I use this a lot.&lt;/p&gt;\\n\\n&lt;p&gt;4- Security Vision model.&lt;/p&gt;\\n\\n&lt;p&gt;Ordered from most used to least.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxbynb/how_much_do_you_use_your_local_model_on_average/n2l1fnw/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752254556,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lxbynb","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":17}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"d2642412-d9ce-11ed-ae30-32b11309f5bd","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"d2642412-d9ce-11ed-ae30-32b11309f5bd","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"d2642412-d9ce-11ed-ae30-32b11309f5bd","distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2m7u6q","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"segmond","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2m2b8h","score":3,"author_fullname":"t2_ah13x","approved_by":null,"mod_note":null,"all_awardings":[],"body":"ok, I feel better. :-p  I'm getting 35tk/s on Q4 on my 3090s with llama.cpp","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n2m7u6q","is_submitter":true,"collapsed":false,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;ok, I feel better. :-p  I&amp;#39;m getting 35tk/s on Q4 on my 3090s with llama.cpp&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lxbynb","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxbynb/how_much_do_you_use_your_local_model_on_average/n2m7u6q/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752266893,"author_flair_text":"llama.cpp","treatment_tags":[],"created_utc":1752266893,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n2m2b8h","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ortegaalfredo","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2m1sll","score":1,"author_fullname":"t2_g177e","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Single prompt is about 24 tok/s, 150 is using batching of 20.","edited":false,"author_flair_css_class":null,"name":"t1_n2m2b8h","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"Alpaca"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Single prompt is about 24 tok/s, 150 is using batching of 20.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lxbynb","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":"light","treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxbynb/how_much_do_you_use_your_local_model_on_average/n2m2b8h/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752265269,"author_flair_text":"Alpaca","collapsed":false,"created_utc":1752265269,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":"#bd9e9e","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2m1sll","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"segmond","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2m10lo","score":2,"author_fullname":"t2_ah13x","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"wow, that is fast!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2m1sll","is_submitter":true,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;wow, that is fast!&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lxbynb","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxbynb/how_much_do_you_use_your_local_model_on_average/n2m1sll/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752265119,"author_flair_text":"llama.cpp","treatment_tags":[],"created_utc":1752265119,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n2m10lo","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"ortegaalfredo","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2lnw9o","score":4,"author_fullname":"t2_g177e","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"No, 8x3090, AWQ quant.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2m10lo","is_submitter":false,"collapsed":false,"author_flair_richtext":[{"e":"text","t":"Alpaca"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;No, 8x3090, AWQ quant.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lxbynb","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxbynb/how_much_do_you_use_your_local_model_on_average/n2m10lo/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752264890,"author_flair_text":"Alpaca","treatment_tags":[],"created_utc":1752264890,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":"#bd9e9e","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}}],"before":null}},"user_reports":[],"saved":false,"id":"n2lnw9o","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"segmond","can_mod_post":false,"created_utc":1752260957,"send_replies":true,"parent_id":"t1_n2l917z","score":2,"author_fullname":"t2_ah13x","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"blackwell pro 6000?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2lnw9o","is_submitter":true,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;blackwell pro 6000?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lxbynb","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxbynb/how_much_do_you_use_your_local_model_on_average/n2lnw9o/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752260957,"author_flair_text":"llama.cpp","treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n2l917z","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"ortegaalfredo","can_mod_post":false,"created_utc":1752256647,"send_replies":true,"parent_id":"t3_1lxbynb","score":9,"author_fullname":"t2_g177e","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Just heated a whole room for 96 hours with qwen-235B at 150 tok/s","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2l917z","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"Alpaca"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Just heated a whole room for 96 hours with qwen-235B at 150 tok/s&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxbynb/how_much_do_you_use_your_local_model_on_average/n2l917z/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752256647,"author_flair_text":"Alpaca","treatment_tags":[],"link_id":"t3_1lxbynb","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bd9e9e","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":9}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2u17cu","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Red_Redditor_Reddit","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2mxyqi","score":2,"author_fullname":"t2_8eelmfjg","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"It's super easy.  I guess to be more specific, I'm training the prompt.  Basically I have it process my notes and then I'll fix all the problems with the output.  I'll put the origional and the revised bback in the system prompt as an example.  Each day, I'll do this, with the model getting better each time.  After about ten days, it's basically perfect and I'll start removing the first ones that aren't as good.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2u17cu","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It&amp;#39;s super easy.  I guess to be more specific, I&amp;#39;m training the prompt.  Basically I have it process my notes and then I&amp;#39;ll fix all the problems with the output.  I&amp;#39;ll put the origional and the revised bback in the system prompt as an example.  Each day, I&amp;#39;ll do this, with the model getting better each time.  After about ten days, it&amp;#39;s basically perfect and I&amp;#39;ll start removing the first ones that aren&amp;#39;t as good.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lxbynb","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxbynb/how_much_do_you_use_your_local_model_on_average/n2u17cu/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752374258,"author_flair_text":null,"treatment_tags":[],"created_utc":1752374258,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n2mxyqi","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"fgoricha","can_mod_post":false,"created_utc":1752275308,"send_replies":true,"parent_id":"t1_n2l3lxk","score":3,"author_fullname":"t2_40xsg56g","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Can you talk more about your training process? I would be interested to learn more!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2mxyqi","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Can you talk more about your training process? I would be interested to learn more!&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lxbynb","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxbynb/how_much_do_you_use_your_local_model_on_average/n2mxyqi/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752275308,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n2l3lxk","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Red_Redditor_Reddit","can_mod_post":false,"created_utc":1752255153,"send_replies":true,"parent_id":"t3_1lxbynb","score":6,"author_fullname":"t2_8eelmfjg","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I use it to make engineering notes more presentable and clear.  I trained my LLM over a week or two to take chicken scratch or some recording and turn it into a clear and understandable report.  My computer is CPU only, so usually I will give it the crap report and come back after fifteen minutes when it's done.\\n\\nEdit: that's also just for work.  Sometimes when I'm at home, I'll have the LLM give me a summery and details of long texts like bills in congress.  The most recent example was the \\"big beautiful bill\\".  I was able to get a baseline idea of what was in the bill without having to spend hours or days reading it.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2l3lxk","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I use it to make engineering notes more presentable and clear.  I trained my LLM over a week or two to take chicken scratch or some recording and turn it into a clear and understandable report.  My computer is CPU only, so usually I will give it the crap report and come back after fifteen minutes when it&amp;#39;s done.&lt;/p&gt;\\n\\n&lt;p&gt;Edit: that&amp;#39;s also just for work.  Sometimes when I&amp;#39;m at home, I&amp;#39;ll have the LLM give me a summery and details of long texts like bills in congress.  The most recent example was the &amp;quot;big beautiful bill&amp;quot;.  I was able to get a baseline idea of what was in the bill without having to spend hours or days reading it.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxbynb/how_much_do_you_use_your_local_model_on_average/n2l3lxk/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752255153,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lxbynb","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2l9ad3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DinoAmino","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2l3sk5","score":1,"author_fullname":"t2_j1v7f","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Ah, when you put it that way then ... I almost never use LLMs locally for anything but coding for work. Sometimes I'll use it for websearx as a stepping stone. I just don't trust their internal knowledge.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2l9ad3","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Ah, when you put it that way then ... I almost never use LLMs locally for anything but coding for work. Sometimes I&amp;#39;ll use it for websearx as a stepping stone. I just don&amp;#39;t trust their internal knowledge.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lxbynb","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxbynb/how_much_do_you_use_your_local_model_on_average/n2l9ad3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752256718,"author_flair_text":null,"treatment_tags":[],"created_utc":1752256718,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2l3sk5","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"segmond","can_mod_post":false,"created_utc":1752255204,"send_replies":true,"parent_id":"t1_n2l246b","score":3,"author_fullname":"t2_ah13x","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"wowzers, so I guess you are using it for work?  I'm more curious on the personal side of things outside of work, those using it at home or before/after work.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2l3sk5","is_submitter":true,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;wowzers, so I guess you are using it for work?  I&amp;#39;m more curious on the personal side of things outside of work, those using it at home or before/after work.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lxbynb","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxbynb/how_much_do_you_use_your_local_model_on_average/n2l3sk5/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752255204,"author_flair_text":"llama.cpp","treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n2l246b","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Lissanro","can_mod_post":false,"created_utc":1752254743,"send_replies":true,"parent_id":"t3_1lxbynb","score":3,"author_fullname":"t2_fpfao9g","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I don't have long-term stats, but over the last few days, I am using R1 0528 (I am running IQ4\\\\_K\\\\_M quant using ik\\\\_llama.cpp) around 12-15 hours per day. When I need vision, I use Qwen2.5-VL 72B. On goods days that include overnight agentic tasks it may be over 20 hours/day. Not sure how many queries, today I am using Cline and it did many dozens of queries, but if counting only my prompts, it still more than a dozen today, and today is not even close to be over. I also use normal chat about just as much, it is often more efficient than Cline because I can precisely control context, but Cline is helpful when there are a bunch of small files to edit or create, or to bootstrap a project.","edited":1752255866,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2l246b","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I don&amp;#39;t have long-term stats, but over the last few days, I am using R1 0528 (I am running IQ4_K_M quant using ik_llama.cpp) around 12-15 hours per day. When I need vision, I use Qwen2.5-VL 72B. On goods days that include overnight agentic tasks it may be over 20 hours/day. Not sure how many queries, today I am using Cline and it did many dozens of queries, but if counting only my prompts, it still more than a dozen today, and today is not even close to be over. I also use normal chat about just as much, it is often more efficient than Cline because I can precisely control context, but Cline is helpful when there are a bunch of small files to edit or create, or to bootstrap a project.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxbynb/how_much_do_you_use_your_local_model_on_average/n2l246b/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752254743,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lxbynb","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2ntg9g","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"LazyChampionship5819","can_mod_post":false,"created_utc":1752286630,"send_replies":true,"parent_id":"t1_n2lpvl7","score":1,"author_fullname":"t2_q0r16vgq7","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Same here 😭","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2ntg9g","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Same here 😭&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lxbynb","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxbynb/how_much_do_you_use_your_local_model_on_average/n2ntg9g/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752286630,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2rmcgg","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"segmond","can_mod_post":false,"created_utc":1752344224,"send_replies":true,"parent_id":"t1_n2lpvl7","score":1,"author_fullname":"t2_ah13x","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"mistral, devstral, codestral, qwen2.5coder?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2rmcgg","is_submitter":true,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;mistral, devstral, codestral, qwen2.5coder?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lxbynb","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxbynb/how_much_do_you_use_your_local_model_on_average/n2rmcgg/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752344224,"author_flair_text":"llama.cpp","treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2lpvl7","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"kacoef","can_mod_post":false,"created_utc":1752261542,"send_replies":true,"parent_id":"t3_1lxbynb","score":2,"author_fullname":"t2_3ox3um3h","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"16 gb vr and 32 gb ram don't allow me using any good agent coding llm. so dont using them anymore ;(","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2lpvl7","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;16 gb vr and 32 gb ram don&amp;#39;t allow me using any good agent coding llm. so dont using them anymore ;(&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxbynb/how_much_do_you_use_your_local_model_on_average/n2lpvl7/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752261542,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lxbynb","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2nzvf8","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Rich_Artist_8327","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2nw9l2","score":1,"author_fullname":"t2_1jk2ep8a52","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"For my use case Ollama works fine","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2nzvf8","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;For my use case Ollama works fine&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lxbynb","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxbynb/how_much_do_you_use_your_local_model_on_average/n2nzvf8/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752289093,"author_flair_text":null,"treatment_tags":[],"created_utc":1752289093,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2nw9l2","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"random-tomato","can_mod_post":false,"created_utc":1752287695,"send_replies":true,"parent_id":"t1_n2ngkir","score":1,"author_fullname":"t2_fmd6oq5v6","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Just FYI Ollama isn't really for production environments, you're probably better off with something like vLLM which gives much faster speeds and is much, much more efficient for multi-user inference.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2nw9l2","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Just FYI Ollama isn&amp;#39;t really for production environments, you&amp;#39;re probably better off with something like vLLM which gives much faster speeds and is much, much more efficient for multi-user inference.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lxbynb","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxbynb/how_much_do_you_use_your_local_model_on_average/n2nw9l2/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752287695,"author_flair_text":"llama.cpp","treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2ngkir","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Rich_Artist_8327","can_mod_post":false,"created_utc":1752281915,"send_replies":true,"parent_id":"t3_1lxbynb","score":2,"author_fullname":"t2_1jk2ep8a52","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"My web site is using my GPU servers running Ollama constantly, during peak hours all my gpus are allmost fully utilized.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2ngkir","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;My web site is using my GPU servers running Ollama constantly, during peak hours all my gpus are allmost fully utilized.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxbynb/how_much_do_you_use_your_local_model_on_average/n2ngkir/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752281915,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lxbynb","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2pj01b","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ontologicalmemes","can_mod_post":false,"created_utc":1752318003,"send_replies":true,"parent_id":"t3_1lxbynb","score":0,"author_fullname":"t2_21pp8tew","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Why only 90 minutes?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2pj01b","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Why only 90 minutes?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxbynb/how_much_do_you_use_your_local_model_on_average/n2pj01b/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752318003,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lxbynb","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}}]`),n=()=>e.jsx(l,{data:a});export{n as default};
