import{j as e}from"./index-F0NXdzZX.js";import{R as t}from"./RedditPostRenderer-CoEZB1dt.js";import"./index-DrtravXm.js";const a=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"I need some recommendations on what to do to implement prompt/persona memory across my local setup. I've read up on vector databases and levels to set, but am looking for a step by step on which compoments to implement. I would love to have the solution self-hosted and local, and I am a full time AI user with 40% of my day job leveraging this day-to-day.\\n\\nCurrently running an NVIDIA P40 with 24GB of vRAM in an Ubuntu 24.04 server with Docker (64GB memory, AMD 5800X). I currently use Big-AGI as my front end with Ollama (willing to change this up).  I have a GGUF for Gemma 32B to allow for large token sets, but again, willing to change that.\\n\\nAny suggestions to implement prompt/persona memory across this? Thanks!\\n\\nEdit 1: I am looking at [https://github.com/n8n-io](https://github.com/n8n-io) which seems to provide a lot of this, but would love some suggestions here.\\n\\nEdit 2: Further context on my desired state: I currently prompt-based RAG per prompt 'chain', where I add my private documents to a thread for context. This becomes cumbersome *across* prompts, and I need more of a persona that can learn across common threads.","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Help on prompt memory and personas - what to do?","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lp4cht","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":1,"author_flair_background_color":null,"subreddit_type":"public","ups":3,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_1kei2yfn60","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":3,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":1751386697,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"post_hint":"self","content_categories":null,"is_self":true,"mod_note":null,"created":1751383057,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;I need some recommendations on what to do to implement prompt/persona memory across my local setup. I&amp;#39;ve read up on vector databases and levels to set, but am looking for a step by step on which compoments to implement. I would love to have the solution self-hosted and local, and I am a full time AI user with 40% of my day job leveraging this day-to-day.&lt;/p&gt;\\n\\n&lt;p&gt;Currently running an NVIDIA P40 with 24GB of vRAM in an Ubuntu 24.04 server with Docker (64GB memory, AMD 5800X). I currently use Big-AGI as my front end with Ollama (willing to change this up).  I have a GGUF for Gemma 32B to allow for large token sets, but again, willing to change that.&lt;/p&gt;\\n\\n&lt;p&gt;Any suggestions to implement prompt/persona memory across this? Thanks!&lt;/p&gt;\\n\\n&lt;p&gt;Edit 1: I am looking at &lt;a href=\\"https://github.com/n8n-io\\"&gt;https://github.com/n8n-io&lt;/a&gt; which seems to provide a lot of this, but would love some suggestions here.&lt;/p&gt;\\n\\n&lt;p&gt;Edit 2: Further context on my desired state: I currently prompt-based RAG per prompt &amp;#39;chain&amp;#39;, where I add my private documents to a thread for context. This becomes cumbersome &lt;em&gt;across&lt;/em&gt; prompts, and I need more of a persona that can learn across common threads.&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":true,"is_crosspostable":false,"pinned":false,"over_18":false,"preview":{"images":[{"source":{"url":"https://external-preview.redd.it/bRQcNXstcv2BiIay-8r1LtsiuiWNo7QpTk2ap1nCfB8.png?auto=webp&amp;s=efbc31599628540bf6de0664b721f2ffdf487f15","width":280,"height":280},"resolutions":[{"url":"https://external-preview.redd.it/bRQcNXstcv2BiIay-8r1LtsiuiWNo7QpTk2ap1nCfB8.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=caffcb0ba5849a49df0852148cab50d60ff168c5","width":108,"height":108},{"url":"https://external-preview.redd.it/bRQcNXstcv2BiIay-8r1LtsiuiWNo7QpTk2ap1nCfB8.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=2e5f2595f9dcf63abb9b3673df803a19766e608d","width":216,"height":216}],"variants":{},"id":"bRQcNXstcv2BiIay-8r1LtsiuiWNo7QpTk2ap1nCfB8"}],"enabled":false},"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1lp4cht","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"TheRealKevinChrist","discussion_type":null,"num_comments":2,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lp4cht/help_on_prompt_memory_and_personas_what_to_do/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lp4cht/help_on_prompt_memory_and_personas_what_to_do/","subreddit_subscribers":493458,"created_utc":1751383057,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0uxdem","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"TheRealKevinChrist","can_mod_post":false,"created_utc":1751416239,"send_replies":true,"parent_id":"t1_n0uvv0r","score":2,"author_fullname":"t2_1kei2yfn60","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Thank you! And yes, I was misconstruing an LLMs capability and you put it best on the persona angle. \\n\\nI suppose this is why people pay for things like ChatGPT because they’ve done all the lifting on things like this, but will check out your suggestions!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0uxdem","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thank you! And yes, I was misconstruing an LLMs capability and you put it best on the persona angle. &lt;/p&gt;\\n\\n&lt;p&gt;I suppose this is why people pay for things like ChatGPT because they’ve done all the lifting on things like this, but will check out your suggestions!&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lp4cht","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp4cht/help_on_prompt_memory_and_personas_what_to_do/n0uxdem/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751416239,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n0uvv0r","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ShengrenR","can_mod_post":false,"created_utc":1751415690,"send_replies":true,"parent_id":"t3_1lp4cht","score":1,"author_fullname":"t2_ji4n4","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"\\"I need more of a persona that can learn across common threads\\" - LLMs are not learning, they're static artifacts - any over-time changes to behavior are purely due to modifications of the context. If you would like the thing to take down important information over time and have a system to reference that, that's an application that's built \\\\*around\\\\* the core LLM that's dynamically identifying, storing, retrieving relevant information, but will have no fundamental 'learning' in any way unless it's constantly in the context window. To that end - you could have a system that dynamically modifies your system-prompt such that you retain key things it 'must' absolutely know and retain, but you have a limited amount of space to keep those in before you start impacting the model performance in both speed and behavior.\\n\\nI don't know of any off-the-shelf setup that will do all this for you, so you'll need to wear some dev shoes at some point, but you can go a decent ways vibe-coding if you're not a dev already.  You likely want to look into graph-rag and how to incorporate that into your workflow. Somebody built [https://www.reddit.com/r/LocalLLaMA/comments/1hgc64u/tangent\\\\_the\\\\_ai\\\\_chat\\\\_canvas\\\\_that\\\\_grows\\\\_with\\\\_you/](https://www.reddit.com/r/LocalLLaMA/comments/1hgc64u/tangent_the_ai_chat_canvas_that_grows_with_you/) a while ago and that looked like a fun project, but appears to have run out of steam 5mo ago, so you'd need to fork/revive the thing to get it where you want it.\\n\\n  \\nIf you like n8n you might also like dify, ymmv; haystack and langgraph and crewai and griptape, etc etc are also options that will do the framework pieces, depending on your tech knowledge.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0uvv0r","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;&amp;quot;I need more of a persona that can learn across common threads&amp;quot; - LLMs are not learning, they&amp;#39;re static artifacts - any over-time changes to behavior are purely due to modifications of the context. If you would like the thing to take down important information over time and have a system to reference that, that&amp;#39;s an application that&amp;#39;s built *around* the core LLM that&amp;#39;s dynamically identifying, storing, retrieving relevant information, but will have no fundamental &amp;#39;learning&amp;#39; in any way unless it&amp;#39;s constantly in the context window. To that end - you could have a system that dynamically modifies your system-prompt such that you retain key things it &amp;#39;must&amp;#39; absolutely know and retain, but you have a limited amount of space to keep those in before you start impacting the model performance in both speed and behavior.&lt;/p&gt;\\n\\n&lt;p&gt;I don&amp;#39;t know of any off-the-shelf setup that will do all this for you, so you&amp;#39;ll need to wear some dev shoes at some point, but you can go a decent ways vibe-coding if you&amp;#39;re not a dev already.  You likely want to look into graph-rag and how to incorporate that into your workflow. Somebody built &lt;a href=\\"https://www.reddit.com/r/LocalLLaMA/comments/1hgc64u/tangent_the_ai_chat_canvas_that_grows_with_you/\\"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1hgc64u/tangent_the_ai_chat_canvas_that_grows_with_you/&lt;/a&gt; a while ago and that looked like a fun project, but appears to have run out of steam 5mo ago, so you&amp;#39;d need to fork/revive the thing to get it where you want it.&lt;/p&gt;\\n\\n&lt;p&gt;If you like n8n you might also like dify, ymmv; haystack and langgraph and crewai and griptape, etc etc are also options that will do the framework pieces, depending on your tech knowledge.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp4cht/help_on_prompt_memory_and_personas_what_to_do/n0uvv0r/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751415690,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lp4cht","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),r=()=>e.jsx(t,{data:a});export{r as default};
