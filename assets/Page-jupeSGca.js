import{j as e}from"./index-CWmJdUH_.js";import{R as l}from"./RedditPostRenderer-D2iunoQ9.js";import"./index-BCg9RP6g.js";const t=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Hi everyone,\\nI'm trying to run MythoMax-L2-13B-GPTQ on RunPod using the text-generation-webui (Oobabooga).\\n\\nThe model loads, the WebUI starts fine, and I can open the interface. However, when I try to generate text, the model just replies with empty lines or no output at all.\\n\\nHere's what I've tried:\\n\\nLaunched the pod with \\"One Click Installer\\"\\n\\nUsed the --model MythoMax-L2-13B-GPTQ flag\\n\\nActivated the virtual environment properly (.venv)\\n\\nTried server.py with --listen-port 8888\\n\\n\\nI also noticed that the HTTP service still shows as \\"Not Ready\\", even though I can access the UI.\\n\\nQuestions:\\n\\n1. Is this a model compatibility issue or a memory issue (even though the pod has 24GB+ VRAM)?\\n\\n\\n2. Do I need to adjust settings.json or model loader parameters manually?\\n\\n\\n3. How do I verify that the model is correctly quantized and loaded?\\n\\n\\n\\nWould appreciate any advice from folks who've made MythoMax or similar NSFW models work on RunPod!\\n\\nThanks in advance.","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Trouble running MythoMax-L2-13B-GPTQ on RunPod â€“ Model loads but returns empty responses","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1m3smiz","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.75,"author_flair_background_color":null,"subreddit_type":"public","ups":2,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_b46y73fz","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":2,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1752919648,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Hi everyone,\\nI&amp;#39;m trying to run MythoMax-L2-13B-GPTQ on RunPod using the text-generation-webui (Oobabooga).&lt;/p&gt;\\n\\n&lt;p&gt;The model loads, the WebUI starts fine, and I can open the interface. However, when I try to generate text, the model just replies with empty lines or no output at all.&lt;/p&gt;\\n\\n&lt;p&gt;Here&amp;#39;s what I&amp;#39;ve tried:&lt;/p&gt;\\n\\n&lt;p&gt;Launched the pod with &amp;quot;One Click Installer&amp;quot;&lt;/p&gt;\\n\\n&lt;p&gt;Used the --model MythoMax-L2-13B-GPTQ flag&lt;/p&gt;\\n\\n&lt;p&gt;Activated the virtual environment properly (.venv)&lt;/p&gt;\\n\\n&lt;p&gt;Tried server.py with --listen-port 8888&lt;/p&gt;\\n\\n&lt;p&gt;I also noticed that the HTTP service still shows as &amp;quot;Not Ready&amp;quot;, even though I can access the UI.&lt;/p&gt;\\n\\n&lt;p&gt;Questions:&lt;/p&gt;\\n\\n&lt;ol&gt;\\n&lt;li&gt;&lt;p&gt;Is this a model compatibility issue or a memory issue (even though the pod has 24GB+ VRAM)?&lt;/p&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;p&gt;Do I need to adjust settings.json or model loader parameters manually?&lt;/p&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;p&gt;How do I verify that the model is correctly quantized and loaded?&lt;/p&gt;&lt;/li&gt;\\n&lt;/ol&gt;\\n\\n&lt;p&gt;Would appreciate any advice from folks who&amp;#39;ve made MythoMax or similar NSFW models work on RunPod!&lt;/p&gt;\\n\\n&lt;p&gt;Thanks in advance.&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1m3smiz","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"Icy_Blacksmith8549","discussion_type":null,"num_comments":4,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1m3smiz/trouble_running_mythomaxl213bgptq_on_runpod_model/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1m3smiz/trouble_running_mythomaxl213bgptq_on_runpod_model/","subreddit_subscribers":501753,"created_utc":1752919648,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n43z6d5","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ArsNeph","can_mod_post":false,"send_replies":true,"parent_id":"t1_n42bbar","score":1,"author_fullname":"t2_vt0xkv60d","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"NP :)","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n43z6d5","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;NP :)&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m3smiz","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3smiz/trouble_running_mythomaxl213bgptq_on_runpod_model/n43z6d5/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752983665,"author_flair_text":null,"treatment_tags":[],"created_utc":1752983665,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n42bbar","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Icy_Blacksmith8549","can_mod_post":false,"created_utc":1752961203,"send_replies":true,"parent_id":"t1_n40r3lc","score":1,"author_fullname":"t2_b46y73fz","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Thanks man really new to this but I will definetly check it out","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n42bbar","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thanks man really new to this but I will definetly check it out&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m3smiz","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3smiz/trouble_running_mythomaxl213bgptq_on_runpod_model/n42bbar/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752961203,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n40r3lc","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ArsNeph","can_mod_post":false,"created_utc":1752943271,"send_replies":true,"parent_id":"t3_1m3smiz","score":2,"author_fullname":"t2_vt0xkv60d","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I believe that the GPTQ and AWQ inference engines have been deprecated from Oobabooga webUI. Try an EXL2 or .GGUF instead.\\n\\nI highly recommend against using mythomax, it is an ancient model, and far inferior to newer ones in every single way. If you want a model of the same size, try Mag Mell 12B in 8 bit with 16384 context, it is considered a legendary model. With 24GB, it's also worth experimenting with larger models, like Cydonia V4 24B, and Synthia 27B at Q5KM. You can also try QwQ Snowdrop 32B, although I wouldn't really recommend it. Valkyrie 49B at 3 bit is also an option.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n40r3lc","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I believe that the GPTQ and AWQ inference engines have been deprecated from Oobabooga webUI. Try an EXL2 or .GGUF instead.&lt;/p&gt;\\n\\n&lt;p&gt;I highly recommend against using mythomax, it is an ancient model, and far inferior to newer ones in every single way. If you want a model of the same size, try Mag Mell 12B in 8 bit with 16384 context, it is considered a legendary model. With 24GB, it&amp;#39;s also worth experimenting with larger models, like Cydonia V4 24B, and Synthia 27B at Q5KM. You can also try QwQ Snowdrop 32B, although I wouldn&amp;#39;t really recommend it. Valkyrie 49B at 3 bit is also an option.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3smiz/trouble_running_mythomaxl213bgptq_on_runpod_model/n40r3lc/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752943271,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m3smiz","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n41oar2","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Specialist-String598","can_mod_post":false,"created_utc":1752953692,"send_replies":true,"parent_id":"t3_1m3smiz","score":1,"author_fullname":"t2_1t5o3lghq5","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"People still use mythomax?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n41oar2","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;People still use mythomax?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3smiz/trouble_running_mythomaxl213bgptq_on_runpod_model/n41oar2/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752953692,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m3smiz","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),r=()=>e.jsx(l,{data:t});export{r as default};
