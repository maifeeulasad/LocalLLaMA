import{j as e}from"./index-BUtHYhT3.js";import{R as l}from"./RedditPostRenderer-BaN1Fn7z.js";import"./index-Cli9kp5v.js";const t=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Title. Anyone here experienced when it comes to using this model for text classification? Any tips?\\n\\n(Using Q6\\\\_K\\\\_L by the way).","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Fine-tuning Qwen3-32B for sentiment analysis.","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lss6b9","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.56,"author_flair_background_color":null,"subreddit_type":"public","ups":1,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_thdbilga","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":1,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1751774648,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Title. Anyone here experienced when it comes to using this model for text classification? Any tips?&lt;/p&gt;\\n\\n&lt;p&gt;(Using Q6_K_L by the way).&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":true,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1lss6b9","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"Known_Bed_8000","discussion_type":null,"num_comments":5,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lss6b9/finetuning_qwen332b_for_sentiment_analysis/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lss6b9/finetuning_qwen332b_for_sentiment_analysis/","subreddit_subscribers":495396,"created_utc":1751774648,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1lamec","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"random-tomato","can_mod_post":false,"created_utc":1751780785,"send_replies":true,"parent_id":"t3_1lss6b9","score":9,"author_fullname":"t2_fmd6oq5v6","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Qwen3 32B is way, way, WAY overkill for sentiment analysis. Unless you're analyzing texts of 4-8k+ tokens or something that needs complex reasoning to figure out the text's intent, it's better to stick to more basic stuff like BERT, which can even be fine-tuned on CPU.\\n\\n(By the way, you can't fine tune GGUFs. I'm too lazy to link it, but if you look into Unsloth, they have some good docs for fine tuning. If you really want to go LLM route, a 1B parameter model will be enough for most of your tasks)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1lamec","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Qwen3 32B is way, way, WAY overkill for sentiment analysis. Unless you&amp;#39;re analyzing texts of 4-8k+ tokens or something that needs complex reasoning to figure out the text&amp;#39;s intent, it&amp;#39;s better to stick to more basic stuff like BERT, which can even be fine-tuned on CPU.&lt;/p&gt;\\n\\n&lt;p&gt;(By the way, you can&amp;#39;t fine tune GGUFs. I&amp;#39;m too lazy to link it, but if you look into Unsloth, they have some good docs for fine tuning. If you really want to go LLM route, a 1B parameter model will be enough for most of your tasks)&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lss6b9/finetuning_qwen332b_for_sentiment_analysis/n1lamec/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751780785,"author_flair_text":"llama.cpp","treatment_tags":[],"link_id":"t3_1lss6b9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":9}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1l9bxa","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"rnosov","can_mod_post":false,"created_utc":1751780089,"send_replies":true,"parent_id":"t3_1lss6b9","score":4,"author_fullname":"t2_18x6fa","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Qwen3 is a reasoner although you can switch it off. If you want reasoning you'd need to provide at least a few reasoning samples to cold start it using SFT. You can either finish training with GRPO where it will come up with it's own reasoning (as long as you have ground truth labels) or continue with SFT (if you reasoning samples aren't a problem). Without reasoning it's pretty dumb so it's better to keep it. For numerical stability people normally train with weights in 16 bit precision, so for LoRA you'd need at least A100 or even H200. You can then plug the LoRA adapter into pretty much any inference engine to work with any quant. Other than giant MoE models that are super hard to fine-tune its as good as it gets.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1l9bxa","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Qwen3 is a reasoner although you can switch it off. If you want reasoning you&amp;#39;d need to provide at least a few reasoning samples to cold start it using SFT. You can either finish training with GRPO where it will come up with it&amp;#39;s own reasoning (as long as you have ground truth labels) or continue with SFT (if you reasoning samples aren&amp;#39;t a problem). Without reasoning it&amp;#39;s pretty dumb so it&amp;#39;s better to keep it. For numerical stability people normally train with weights in 16 bit precision, so for LoRA you&amp;#39;d need at least A100 or even H200. You can then plug the LoRA adapter into pretty much any inference engine to work with any quant. Other than giant MoE models that are super hard to fine-tune its as good as it gets.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lss6b9/finetuning_qwen332b_for_sentiment_analysis/n1l9bxa/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751780089,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lss6b9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1ma5vz","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Raz4r","can_mod_post":false,"created_utc":1751801406,"send_replies":true,"parent_id":"t1_n1lpli3","score":1,"author_fullname":"t2_108sov","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"There is a huge literature on how to use llms for classification. Op, Search for paper discussing \\"in context learning\\".","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1ma5vz","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;There is a huge literature on how to use llms for classification. Op, Search for paper discussing &amp;quot;in context learning&amp;quot;.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lss6b9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lss6b9/finetuning_qwen332b_for_sentiment_analysis/n1ma5vz/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751801406,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1lpli3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"un_passant","can_mod_post":false,"created_utc":1751789393,"send_replies":true,"parent_id":"t3_1lss6b9","score":2,"author_fullname":"t2_7rqtc","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Don't use a sledgehammer to crack a nut !\\n\\nUse the right tool for the job : a BERT like encoder decoder.\\n\\nLLM are not meant for classification.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1lpli3","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Don&amp;#39;t use a sledgehammer to crack a nut !&lt;/p&gt;\\n\\n&lt;p&gt;Use the right tool for the job : a BERT like encoder decoder.&lt;/p&gt;\\n\\n&lt;p&gt;LLM are not meant for classification.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lss6b9/finetuning_qwen332b_for_sentiment_analysis/n1lpli3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751789393,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lss6b9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1ng0dl","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Wonderful_Second5322","can_mod_post":false,"created_utc":1751816698,"send_replies":true,"parent_id":"t3_1lss6b9","score":1,"author_fullname":"t2_z0tqo4itn","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"FOMO? You can just use the lstm layer dude","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1ng0dl","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;FOMO? You can just use the lstm layer dude&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lss6b9/finetuning_qwen332b_for_sentiment_analysis/n1ng0dl/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751816698,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lss6b9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),s=()=>e.jsx(l,{data:t});export{s as default};
