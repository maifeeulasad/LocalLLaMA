import{j as t}from"./index-M4edQi1P.js";import{R as e}from"./RedditPostRenderer-CESBGIIy.js";import"./index-DFpL1mt4.js";const l=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"I ran into problems when I replaced the [GTX-1070](https://www.techpowerup.com/gpu-specs/geforce-gtx-1070.c2840) with[ GTX 1080Ti](https://www.techpowerup.com/gpu-specs/geforce-gtx-1080-ti.c2877). NVTOP would show about 7GB of VRAM usage. So I had to adjust the num\\\\_gpu value to 63. Nice improvement.\\n\\nThese were my steps:\\n\\n\`time ollama run --verbose gemma3:12b-it-qat\`  \\n\`&gt;&gt;&gt;/set parameter num_gpu 63\`  \\n\`Set parameter 'num_gpu' to '63'\`  \\n\`&gt;&gt;&gt;/save mygemma3\`  \\nCreated new model 'mygemma3'\\n\\n|NAME|eval rate|prompt eval rate|total duration|\\n|:-|:-|:-|:-|\\n|gemma3:12b-it-qat|6.69|118.6|3m2.831s|\\n|mygemma3:latest|24.74|349.2|0m38.677s|\\n\\nHere are a few other models:\\n\\n|NAME|eval rate|prompt eval rate|total duration|\\n|:-|:-|:-|:-|\\n|deepseek-r1:14b|22.72|51.83|34.07208103|\\n|mygemma3:latest|23.97|321.68|47.22412009|\\n|gemma3:12b|16.84|96.54|1m20.845913225|\\n|gemma3:12b-it-qat|13.33|159.54|1m36.518625216|\\n|gemma3:27b|3.65|9.49|7m30.344502487|\\n|gemma3n:e2b-it-q8\\\\_0|45.95|183.27|30.09576316|\\n|granite3.1-moe:3b-instruct-q8\\\\_0|88.46|546.45|8.24215104|\\n|llama3.1:8b|38.29|174.13|16.73243012|\\n|minicpm-v:8b|37.67|188.41|4.663153513|\\n|mistral:7b-instruct-v0.2-q5\\\\_K\\\\_M|40.33|176.14|5.90872581|\\n|olmo2:13b|12.18|107.56|26.67653928|\\n|phi4:14b|23.56|116.84|16.40753603|\\n|qwen3:14b|22.66|156.32|36.78135622|\\n\\nI had each model create a CSV format from the ollama --verbose output and the following models failed.\\n\\n&gt;FAILED:\\n\\n&gt;minicpm-v:8b\\n\\n&gt;olmo2:13b\\n\\n&gt;granite3.1-moe:3b-instruct-q8\\\\_0\\n\\n&gt;mistral:7b-instruct-v0.2-q5\\\\_K\\\\_M\\n\\n&gt;gemma3n:e2b-it-q8\\\\_0\\n\\nI cut GPU total power from 250 to 188 using:\\n\\n\`sudo nvidia-smi -i 0 -pl 188\`\\n\\nResulted in 'eval rate'\\n\\n250 watts=24.7\\n\\n188 watts=23.6\\n\\nNot much of a hit to drop 25% power usage. I also tested the bare minimum of 125 watts but that resulted in a 25% reduction in eval rate. Still that makes running several cards viable.\\n\\nI have a more in depth review on my [blog](https://tabletuser.blogspot.com/2025/07/gtx-1080ti-optimized-for-ollama.html)","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Nvidia GTX-1080Ti Ollama review","link_flair_richtext":[{"e":"text","t":"Other"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1m3i9p3","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.56,"author_flair_background_color":null,"subreddit_type":"public","ups":2,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_h52tr","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Other","can_mod_post":false,"score":2,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"post_hint":"self","content_categories":null,"is_self":true,"mod_note":null,"created":1752883983,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;I ran into problems when I replaced the &lt;a href=\\"https://www.techpowerup.com/gpu-specs/geforce-gtx-1070.c2840\\"&gt;GTX-1070&lt;/a&gt; with&lt;a href=\\"https://www.techpowerup.com/gpu-specs/geforce-gtx-1080-ti.c2877\\"&gt; GTX 1080Ti&lt;/a&gt;. NVTOP would show about 7GB of VRAM usage. So I had to adjust the num_gpu value to 63. Nice improvement.&lt;/p&gt;\\n\\n&lt;p&gt;These were my steps:&lt;/p&gt;\\n\\n&lt;p&gt;&lt;code&gt;time ollama run --verbose gemma3:12b-it-qat&lt;/code&gt;&lt;br/&gt;\\n&lt;code&gt;&amp;gt;&amp;gt;&amp;gt;/set parameter num_gpu 63&lt;/code&gt;&lt;br/&gt;\\n&lt;code&gt;Set parameter &amp;#39;num_gpu&amp;#39; to &amp;#39;63&amp;#39;&lt;/code&gt;&lt;br/&gt;\\n&lt;code&gt;&amp;gt;&amp;gt;&amp;gt;/save mygemma3&lt;/code&gt;&lt;br/&gt;\\nCreated new model &amp;#39;mygemma3&amp;#39;&lt;/p&gt;\\n\\n&lt;table&gt;&lt;thead&gt;\\n&lt;tr&gt;\\n&lt;th align=\\"left\\"&gt;NAME&lt;/th&gt;\\n&lt;th align=\\"left\\"&gt;eval rate&lt;/th&gt;\\n&lt;th align=\\"left\\"&gt;prompt eval rate&lt;/th&gt;\\n&lt;th align=\\"left\\"&gt;total duration&lt;/th&gt;\\n&lt;/tr&gt;\\n&lt;/thead&gt;&lt;tbody&gt;\\n&lt;tr&gt;\\n&lt;td align=\\"left\\"&gt;gemma3:12b-it-qat&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;6.69&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;118.6&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;3m2.831s&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;tr&gt;\\n&lt;td align=\\"left\\"&gt;mygemma3:latest&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;24.74&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;349.2&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;0m38.677s&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;/tbody&gt;&lt;/table&gt;\\n\\n&lt;p&gt;Here are a few other models:&lt;/p&gt;\\n\\n&lt;table&gt;&lt;thead&gt;\\n&lt;tr&gt;\\n&lt;th align=\\"left\\"&gt;NAME&lt;/th&gt;\\n&lt;th align=\\"left\\"&gt;eval rate&lt;/th&gt;\\n&lt;th align=\\"left\\"&gt;prompt eval rate&lt;/th&gt;\\n&lt;th align=\\"left\\"&gt;total duration&lt;/th&gt;\\n&lt;/tr&gt;\\n&lt;/thead&gt;&lt;tbody&gt;\\n&lt;tr&gt;\\n&lt;td align=\\"left\\"&gt;deepseek-r1:14b&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;22.72&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;51.83&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;34.07208103&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;tr&gt;\\n&lt;td align=\\"left\\"&gt;mygemma3:latest&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;23.97&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;321.68&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;47.22412009&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;tr&gt;\\n&lt;td align=\\"left\\"&gt;gemma3:12b&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;16.84&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;96.54&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;1m20.845913225&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;tr&gt;\\n&lt;td align=\\"left\\"&gt;gemma3:12b-it-qat&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;13.33&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;159.54&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;1m36.518625216&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;tr&gt;\\n&lt;td align=\\"left\\"&gt;gemma3:27b&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;3.65&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;9.49&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;7m30.344502487&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;tr&gt;\\n&lt;td align=\\"left\\"&gt;gemma3n:e2b-it-q8_0&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;45.95&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;183.27&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;30.09576316&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;tr&gt;\\n&lt;td align=\\"left\\"&gt;granite3.1-moe:3b-instruct-q8_0&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;88.46&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;546.45&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;8.24215104&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;tr&gt;\\n&lt;td align=\\"left\\"&gt;llama3.1:8b&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;38.29&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;174.13&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;16.73243012&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;tr&gt;\\n&lt;td align=\\"left\\"&gt;minicpm-v:8b&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;37.67&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;188.41&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;4.663153513&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;tr&gt;\\n&lt;td align=\\"left\\"&gt;mistral:7b-instruct-v0.2-q5_K_M&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;40.33&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;176.14&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;5.90872581&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;tr&gt;\\n&lt;td align=\\"left\\"&gt;olmo2:13b&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;12.18&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;107.56&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;26.67653928&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;tr&gt;\\n&lt;td align=\\"left\\"&gt;phi4:14b&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;23.56&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;116.84&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;16.40753603&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;tr&gt;\\n&lt;td align=\\"left\\"&gt;qwen3:14b&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;22.66&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;156.32&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;36.78135622&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;/tbody&gt;&lt;/table&gt;\\n\\n&lt;p&gt;I had each model create a CSV format from the ollama --verbose output and the following models failed.&lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;FAILED:&lt;/p&gt;\\n\\n&lt;p&gt;minicpm-v:8b&lt;/p&gt;\\n\\n&lt;p&gt;olmo2:13b&lt;/p&gt;\\n\\n&lt;p&gt;granite3.1-moe:3b-instruct-q8_0&lt;/p&gt;\\n\\n&lt;p&gt;mistral:7b-instruct-v0.2-q5_K_M&lt;/p&gt;\\n\\n&lt;p&gt;gemma3n:e2b-it-q8_0&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;I cut GPU total power from 250 to 188 using:&lt;/p&gt;\\n\\n&lt;p&gt;&lt;code&gt;sudo nvidia-smi -i 0 -pl 188&lt;/code&gt;&lt;/p&gt;\\n\\n&lt;p&gt;Resulted in &amp;#39;eval rate&amp;#39;&lt;/p&gt;\\n\\n&lt;p&gt;250 watts=24.7&lt;/p&gt;\\n\\n&lt;p&gt;188 watts=23.6&lt;/p&gt;\\n\\n&lt;p&gt;Not much of a hit to drop 25% power usage. I also tested the bare minimum of 125 watts but that resulted in a 25% reduction in eval rate. Still that makes running several cards viable.&lt;/p&gt;\\n\\n&lt;p&gt;I have a more in depth review on my &lt;a href=\\"https://tabletuser.blogspot.com/2025/07/gtx-1080ti-optimized-for-ollama.html\\"&gt;blog&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"preview":{"images":[{"source":{"url":"https://external-preview.redd.it/yvwiuTaYv0uQ6GQt62kPStPIoLv939HObYZp5JrJUJM.jpeg?auto=webp&amp;s=b63f81c8609b5d0228f4b6778ec2a6db10661d58","width":601,"height":270},"resolutions":[{"url":"https://external-preview.redd.it/yvwiuTaYv0uQ6GQt62kPStPIoLv939HObYZp5JrJUJM.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=4fbed1483bfc1b48741ebdc49e2f0d1f9ee45d74","width":108,"height":48},{"url":"https://external-preview.redd.it/yvwiuTaYv0uQ6GQt62kPStPIoLv939HObYZp5JrJUJM.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=dcec3fff1f0b0a7b9260bb4f1c3d4113ccf38876","width":216,"height":97},{"url":"https://external-preview.redd.it/yvwiuTaYv0uQ6GQt62kPStPIoLv939HObYZp5JrJUJM.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=a53890f9383a606d65481240bc3cad874528ef2f","width":320,"height":143}],"variants":{},"id":"yvwiuTaYv0uQ6GQt62kPStPIoLv939HObYZp5JrJUJM"}],"enabled":false},"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"7a7848d2-bf8e-11ed-8c2f-765d15199f78","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#94e044","id":"1m3i9p3","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"tabletuser_blogspot","discussion_type":null,"num_comments":3,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1m3i9p3/nvidia_gtx1080ti_ollama_review/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1m3i9p3/nvidia_gtx1080ti_ollama_review/","subreddit_subscribers":501753,"created_utc":1752883983,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n44hvhg","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"tabletuser_blogspot","can_mod_post":false,"created_utc":1752992729,"send_replies":true,"parent_id":"t1_n3xdi6l","score":1,"author_fullname":"t2_h52tr","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Thanks for your post. I've had to do some research and more testing to validate my numbers. \\n\\nYes, most models listed are default Q4\\\\_K\\\\_M from Ollama models. \\n\\nGemma3 is an odd beast. gemma3:12b-it-q4\\\\_K\\\\_M shows over 11GB Vram on my RX 7900 GRE 16GB system. Seems like context size and default caching are contributing to offloading on the GTX 1080Ti 11GB. GTX 1xxx systems lack tensor cores so that account for theoretical vs actual numbers. I have two GTX-1080Ti and ran on different systems to validate the slower than expected numbers. Thanks to your input I'm researching how to squeeze extra juice out of old GTX 1080Ti. \\n\\nMy goal to to have a budget 30B cable systems off four GTX 1080Ti. Actually it would be great if network capable inference was easier to setup. I have a few 1070(4), 1080(1) and 1080Ti(2) and that could get me into 70B territory.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n44hvhg","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thanks for your post. I&amp;#39;ve had to do some research and more testing to validate my numbers. &lt;/p&gt;\\n\\n&lt;p&gt;Yes, most models listed are default Q4_K_M from Ollama models. &lt;/p&gt;\\n\\n&lt;p&gt;Gemma3 is an odd beast. gemma3:12b-it-q4_K_M shows over 11GB Vram on my RX 7900 GRE 16GB system. Seems like context size and default caching are contributing to offloading on the GTX 1080Ti 11GB. GTX 1xxx systems lack tensor cores so that account for theoretical vs actual numbers. I have two GTX-1080Ti and ran on different systems to validate the slower than expected numbers. Thanks to your input I&amp;#39;m researching how to squeeze extra juice out of old GTX 1080Ti. &lt;/p&gt;\\n\\n&lt;p&gt;My goal to to have a budget 30B cable systems off four GTX 1080Ti. Actually it would be great if network capable inference was easier to setup. I have a few 1070(4), 1080(1) and 1080Ti(2) and that could get me into 70B territory.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m3i9p3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3i9p3/nvidia_gtx1080ti_ollama_review/n44hvhg/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752992729,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3xdi6l","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ForsookComparison","can_mod_post":false,"created_utc":1752890928,"send_replies":true,"parent_id":"t3_1m3i9p3","score":3,"author_fullname":"t2_on5es7pe3","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"always love getting datapoints, thanks friend!\\n\\nCan I ask you what level of quantization you were using for these? Or was it the Ollama default (q4 I want to say?) ?\\n\\nAsking because some of these look a bit low. Gemma3-12b for example is 8GB. At ~440GB/s your 1080ti should be able to read that 55 tokens/second theoretical max. While you won't be getting that, you should be way closer to that than the 13-16 tokens/second you were actually getting.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3xdi6l","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;always love getting datapoints, thanks friend!&lt;/p&gt;\\n\\n&lt;p&gt;Can I ask you what level of quantization you were using for these? Or was it the Ollama default (q4 I want to say?) ?&lt;/p&gt;\\n\\n&lt;p&gt;Asking because some of these look a bit low. Gemma3-12b for example is 8GB. At ~440GB/s your 1080ti should be able to read that 55 tokens/second theoretical max. While you won&amp;#39;t be getting that, you should be way closer to that than the 13-16 tokens/second you were actually getting.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3i9p3/nvidia_gtx1080ti_ollama_review/n3xdi6l/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752890928,"author_flair_text":"llama.cpp","treatment_tags":[],"link_id":"t3_1m3i9p3","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3xh4ga","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Marksta","can_mod_post":false,"created_utc":1752892343,"send_replies":true,"parent_id":"t3_1m3i9p3","score":3,"author_fullname":"t2_559a1","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Messing around with num_gpu parameters kinda defeats the one purpose of using Ollama over llama.cpp directly. Instead of playing games with saving custom models, should probably just use llama.cpp directly. Llama-swap is the same concept of what you're doing but comes with a web GUI to let you save different configs and launch them, swap around. \\n\\nAlso honorable mention to GPUStack for bringing the same feature set, along with multi-system management, replication rules for migrating LLMs, and cross system resource clustering with the experimental RPC features of llama.cpp.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3xh4ga","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Messing around with num_gpu parameters kinda defeats the one purpose of using Ollama over llama.cpp directly. Instead of playing games with saving custom models, should probably just use llama.cpp directly. Llama-swap is the same concept of what you&amp;#39;re doing but comes with a web GUI to let you save different configs and launch them, swap around. &lt;/p&gt;\\n\\n&lt;p&gt;Also honorable mention to GPUStack for bringing the same feature set, along with multi-system management, replication rules for migrating LLMs, and cross system resource clustering with the experimental RPC features of llama.cpp.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3i9p3/nvidia_gtx1080ti_ollama_review/n3xh4ga/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752892343,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m3i9p3","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}}]`),r=()=>t.jsx(e,{data:l});export{r as default};
