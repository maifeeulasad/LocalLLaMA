import{j as e}from"./index-B8Dr9DfQ.js";import{R as t}from"./RedditPostRenderer-BvcEF1oP.js";import"./index-BohwUeOq.js";const a=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Hi all, in my company, we have 8 repos of ui code. We mainly use React and our own internally developed component library which is a seperate repo. Now, the problem statement is to develop a chat app similar to open ai that can generate code using our components library or code that follows our rules/style of code. The local model needs to have the context of our entire 8 repos. How do i go about achieving this? What are the different approaches to it? Which local llms are currently great for such coding tasks? \\n\\nFYI, we should be able to train the model on our company's macbook air m1.","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Training 8 repos of UI code","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1ltm49x","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.72,"author_flair_background_color":null,"subreddit_type":"public","ups":9,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_1sfhn6c33u","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":9,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1751866370,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Hi all, in my company, we have 8 repos of ui code. We mainly use React and our own internally developed component library which is a seperate repo. Now, the problem statement is to develop a chat app similar to open ai that can generate code using our components library or code that follows our rules/style of code. The local model needs to have the context of our entire 8 repos. How do i go about achieving this? What are the different approaches to it? Which local llms are currently great for such coding tasks? &lt;/p&gt;\\n\\n&lt;p&gt;FYI, we should be able to train the model on our company&amp;#39;s macbook air m1.&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1ltm49x","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"pomatotappu","discussion_type":null,"num_comments":25,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1ltm49x/training_8_repos_of_ui_code/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1ltm49x/training_8_repos_of_ui_code/","subreddit_subscribers":496034,"created_utc":1751866370,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1rx86l","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"dbuildofficial","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1rifb4","score":3,"author_fullname":"t2_5k8ox0g9","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"guess it depends on your objectives, but i'd put the doc up to snuff, you need that for training anyway. And AI can handle most of it (even a smallish one). \\n\\nRAG is \\"not that complicated\\" compared to training and you do not have to start all other again on new model releases :). and the results are definitely good !\\n\\nand it would still work with a model trained on your stuff, even more so probably.\\n\\nThen again, i'd understand if you'd want to spend your CEO money to have fun trying new stuff ;)\\n\\nBut your use case you presented just now is even more on the RAG side.\\n\\nticket &gt; AI analysis &gt; human improvement &gt; RAG search for existing solution/UI components that fits &gt; coding AI\\n\\nIn [litechat.dev](http://litechat.dev), you should try activating rules for form/mermaid/chart/flow/beat/runn blocks so the models know what to do (small shield under the prompt area) and ask the ai for something related, it is very similar (hence with less components) and can give you a good feel of what rag can do, just skipping the retreival part :P\\n\\nfor a flow/mermaid \\"Can you explain how HTTP work using a Flow block ?\\" (say mermaid instead of flow for...mermaid)\\n\\nfor a form \\"I have to configure my apache vhost, I have a bunch of information i dont understand, could you use a Form block to gather what you need ? you'll create the file with the data i give you !\\"\\n\\nEach of the page listed here are an example of for runnable block exports (from a race between model using litechat \\\\^\\\\^) that i bundled together","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n1rx86l","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;guess it depends on your objectives, but i&amp;#39;d put the doc up to snuff, you need that for training anyway. And AI can handle most of it (even a smallish one). &lt;/p&gt;\\n\\n&lt;p&gt;RAG is &amp;quot;not that complicated&amp;quot; compared to training and you do not have to start all other again on new model releases :). and the results are definitely good !&lt;/p&gt;\\n\\n&lt;p&gt;and it would still work with a model trained on your stuff, even more so probably.&lt;/p&gt;\\n\\n&lt;p&gt;Then again, i&amp;#39;d understand if you&amp;#39;d want to spend your CEO money to have fun trying new stuff ;)&lt;/p&gt;\\n\\n&lt;p&gt;But your use case you presented just now is even more on the RAG side.&lt;/p&gt;\\n\\n&lt;p&gt;ticket &amp;gt; AI analysis &amp;gt; human improvement &amp;gt; RAG search for existing solution/UI components that fits &amp;gt; coding AI&lt;/p&gt;\\n\\n&lt;p&gt;In &lt;a href=\\"http://litechat.dev\\"&gt;litechat.dev&lt;/a&gt;, you should try activating rules for form/mermaid/chart/flow/beat/runn blocks so the models know what to do (small shield under the prompt area) and ask the ai for something related, it is very similar (hence with less components) and can give you a good feel of what rag can do, just skipping the retreival part :P&lt;/p&gt;\\n\\n&lt;p&gt;for a flow/mermaid &amp;quot;Can you explain how HTTP work using a Flow block ?&amp;quot; (say mermaid instead of flow for...mermaid)&lt;/p&gt;\\n\\n&lt;p&gt;for a form &amp;quot;I have to configure my apache vhost, I have a bunch of information i dont understand, could you use a Form block to gather what you need ? you&amp;#39;ll create the file with the data i give you !&amp;quot;&lt;/p&gt;\\n\\n&lt;p&gt;Each of the page listed here are an example of for runnable block exports (from a race between model using litechat ^^) that i bundled together&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ltm49x","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltm49x/training_8_repos_of_ui_code/n1rx86l/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751877066,"author_flair_text":null,"treatment_tags":[],"created_utc":1751877066,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n1rifb4","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"pomatotappu","can_mod_post":false,"created_utc":1751868454,"send_replies":true,"parent_id":"t1_n1rhgt3","score":2,"author_fullname":"t2_1sfhn6c33u","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"The docs for our libraries are bad tbh. Also, it's not only about the component library but i should be able to support this functionality too - let's say an issue gets created on jira, i should be able to paste it into the chat app, give some more context about the bug and which repo it belongs too and then the model should be able to generate the response in this context.","edited":1751868779,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1rifb4","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The docs for our libraries are bad tbh. Also, it&amp;#39;s not only about the component library but i should be able to support this functionality too - let&amp;#39;s say an issue gets created on jira, i should be able to paste it into the chat app, give some more context about the bug and which repo it belongs too and then the model should be able to generate the response in this context.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ltm49x","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltm49x/training_8_repos_of_ui_code/n1rifb4/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751868454,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n1rhgt3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"dbuildofficial","can_mod_post":false,"created_utc":1751867932,"send_replies":true,"parent_id":"t3_1ltm49x","score":3,"author_fullname":"t2_5k8ox0g9","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"not certain training would be your best, the cheapest or even the fastest option (take that with a grain of salt, my knowledge in training is not fat !)\\n\\nI think you would be better off using a RAG in a workflow if your components docs are good !\\n\\nit would go like this :\\n\\nuser request -&gt; RAG to collect best matching component -&gt; feed the list to a selection model -&gt; get the selected components doc -&gt; feed that your coding AI with the user prompt\\n\\nyou could try that using [litechat.dev](http://litechat.dev) (I am the dev, no account, no tracking, BYOK and ollama/OpenAI API compatibility for local models, plugged with no shame :D) that you host yourself with only an http server (local first, no coms with the server after initial load !).\\n\\nI created a workflow functionality exactly for this kind of things ! :D \\n\\n\\\\* you create your prompts in the prompt library with all their input ,\\n\\n\\\\* connect your RAG via MCP (I support both https and stdio but you need to run the provided mcp-bridge for the later),\\n\\n\\\\* create your workflow using the prompts in the library (it should sort the chaining of outputs by itself by prompting the AI to do so ;) )\\n\\n\\\\* boom, Bob is you Uncle !\\n\\nIt is a bit quircky as a work in progress but would give you a good prototyping area :)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1rhgt3","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;not certain training would be your best, the cheapest or even the fastest option (take that with a grain of salt, my knowledge in training is not fat !)&lt;/p&gt;\\n\\n&lt;p&gt;I think you would be better off using a RAG in a workflow if your components docs are good !&lt;/p&gt;\\n\\n&lt;p&gt;it would go like this :&lt;/p&gt;\\n\\n&lt;p&gt;user request -&amp;gt; RAG to collect best matching component -&amp;gt; feed the list to a selection model -&amp;gt; get the selected components doc -&amp;gt; feed that your coding AI with the user prompt&lt;/p&gt;\\n\\n&lt;p&gt;you could try that using &lt;a href=\\"http://litechat.dev\\"&gt;litechat.dev&lt;/a&gt; (I am the dev, no account, no tracking, BYOK and ollama/OpenAI API compatibility for local models, plugged with no shame :D) that you host yourself with only an http server (local first, no coms with the server after initial load !).&lt;/p&gt;\\n\\n&lt;p&gt;I created a workflow functionality exactly for this kind of things ! :D &lt;/p&gt;\\n\\n&lt;p&gt;* you create your prompts in the prompt library with all their input ,&lt;/p&gt;\\n\\n&lt;p&gt;* connect your RAG via MCP (I support both https and stdio but you need to run the provided mcp-bridge for the later),&lt;/p&gt;\\n\\n&lt;p&gt;* create your workflow using the prompts in the library (it should sort the chaining of outputs by itself by prompting the AI to do so ;) )&lt;/p&gt;\\n\\n&lt;p&gt;* boom, Bob is you Uncle !&lt;/p&gt;\\n\\n&lt;p&gt;It is a bit quircky as a work in progress but would give you a good prototyping area :)&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltm49x/training_8_repos_of_ui_code/n1rhgt3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751867932,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1ltm49x","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1vpk2e","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"dbuildofficial","can_mod_post":false,"created_utc":1751925703,"send_replies":true,"parent_id":"t1_n1vhpnw","score":2,"author_fullname":"t2_5k8ox0g9","approved_by":null,"mod_note":null,"all_awardings":[],"body":"Sorry i didn't do reddit speleology \\\\^\\\\^\\n\\nlike I said, \\"I haven't any professional XP in either but I experimented a bit with RAG\\" so no, not directly but my use case was fairly similar issue with my 37 \\"ongoing\\" projects (god's damn it... I think I have a problem -\\\\_-'... ) many of which are in TS. \\n\\nI wanted to \\"index\\" my codebases and be able to find similar stuff I already 've done before to an AI when asking it something. I still was making the final selecting and feeding of the retrieval to LLMs (in local cause my R7 4800U \\\\[ram 16GB\\\\] was not a fan of my obsession and online because there was no way i would splurge coins to goof around like that ATM so free chat it was !)\\n\\nI am aware (as I copied and pasted and even sometimes went and retrieve the bits and bobs of code myself) that is is not perfect but a couple days worth of work will have tangible result on a scaled test you could demonstrate (even on a smaller repo),\\n\\nI am absolutely certain training would not be in the case of OP (yup, i'm a cocky bastard sometimes).\\n\\nReading between the lines, it feel like the boss in question is some sort of \\"if it ain't broken or bringing in cash, don't fix it, we always worked that way and it's fine\\" kind of guy (I've had a few of those...) so investing personal time and resources on something like RAG might have better ROI (and now i speak like an investor... bravo !) to OP in terms of tangible real life skills.\\n\\nOnce again, I am very concious tuning (in combination with RAG if you really want the best) would be better in an ideal situation, which it doesn't appear to be (but as I said, just 2 cents fro a random dev on the web)","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n1vpk2e","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Sorry i didn&amp;#39;t do reddit speleology ^^&lt;/p&gt;\\n\\n&lt;p&gt;like I said, &amp;quot;I haven&amp;#39;t any professional XP in either but I experimented a bit with RAG&amp;quot; so no, not directly but my use case was fairly similar issue with my 37 &amp;quot;ongoing&amp;quot; projects (god&amp;#39;s damn it... I think I have a problem -_-&amp;#39;... ) many of which are in TS. &lt;/p&gt;\\n\\n&lt;p&gt;I wanted to &amp;quot;index&amp;quot; my codebases and be able to find similar stuff I already &amp;#39;ve done before to an AI when asking it something. I still was making the final selecting and feeding of the retrieval to LLMs (in local cause my R7 4800U [ram 16GB] was not a fan of my obsession and online because there was no way i would splurge coins to goof around like that ATM so free chat it was !)&lt;/p&gt;\\n\\n&lt;p&gt;I am aware (as I copied and pasted and even sometimes went and retrieve the bits and bobs of code myself) that is is not perfect but a couple days worth of work will have tangible result on a scaled test you could demonstrate (even on a smaller repo),&lt;/p&gt;\\n\\n&lt;p&gt;I am absolutely certain training would not be in the case of OP (yup, i&amp;#39;m a cocky bastard sometimes).&lt;/p&gt;\\n\\n&lt;p&gt;Reading between the lines, it feel like the boss in question is some sort of &amp;quot;if it ain&amp;#39;t broken or bringing in cash, don&amp;#39;t fix it, we always worked that way and it&amp;#39;s fine&amp;quot; kind of guy (I&amp;#39;ve had a few of those...) so investing personal time and resources on something like RAG might have better ROI (and now i speak like an investor... bravo !) to OP in terms of tangible real life skills.&lt;/p&gt;\\n\\n&lt;p&gt;Once again, I am very concious tuning (in combination with RAG if you really want the best) would be better in an ideal situation, which it doesn&amp;#39;t appear to be (but as I said, just 2 cents fro a random dev on the web)&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1ltm49x","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltm49x/training_8_repos_of_ui_code/n1vpk2e/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751925703,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":7,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n1vhpnw","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"FullstackSensei","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1va1c0","score":2,"author_fullname":"t2_17n3nqtj56","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Have you ever worked in an environment similar to what OP describes? Genuine question. I've seen what OP describes numerous times over the years.\\n\\nIf you look at my comment history, this is probably the first time I'm recommending tuning over RAG. When you have 8 repos of components, more often than not there are a ton of unwritten rules about how to use them, how to compose them, and what goes with what. RAG will be harder than tuning because documenting what components are available in each repo alone will not be very useful. Writing documentation about what goes with what won't help much either because it'll only enable the LLM to cover trivial use cases.\\n\\nThe components being updated isn't as big of an issue as you'd expect. Those updates rarely affect the interfaces those components expose, otherwise such updates would break all existing code/applications using those components. When companies build their own tooling or components, backwards compatibility is always at the forefront for those building and maintaining them.\\nA\\nThe reason why tuning is better for this specific case is embedding all possible combinations of use cases into the LLM so that the model builds an \\"intuition\\" of which and how componentes are used and composed with each other. No vector store will be able to give you that.","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n1vhpnw","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Have you ever worked in an environment similar to what OP describes? Genuine question. I&amp;#39;ve seen what OP describes numerous times over the years.&lt;/p&gt;\\n\\n&lt;p&gt;If you look at my comment history, this is probably the first time I&amp;#39;m recommending tuning over RAG. When you have 8 repos of components, more often than not there are a ton of unwritten rules about how to use them, how to compose them, and what goes with what. RAG will be harder than tuning because documenting what components are available in each repo alone will not be very useful. Writing documentation about what goes with what won&amp;#39;t help much either because it&amp;#39;ll only enable the LLM to cover trivial use cases.&lt;/p&gt;\\n\\n&lt;p&gt;The components being updated isn&amp;#39;t as big of an issue as you&amp;#39;d expect. Those updates rarely affect the interfaces those components expose, otherwise such updates would break all existing code/applications using those components. When companies build their own tooling or components, backwards compatibility is always at the forefront for those building and maintaining them.\\nA\\nThe reason why tuning is better for this specific case is embedding all possible combinations of use cases into the LLM so that the model builds an &amp;quot;intuition&amp;quot; of which and how componentes are used and composed with each other. No vector store will be able to give you that.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1ltm49x","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltm49x/training_8_repos_of_ui_code/n1vhpnw/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751923146,"author_flair_text":null,"treatment_tags":[],"created_utc":1751923146,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":6,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n1va1c0","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"dbuildofficial","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1rp8ex","score":2,"author_fullname":"t2_5k8ox0g9","approved_by":null,"mod_note":null,"all_awardings":[],"body":"I also considered the also a notion of cost here. fine tuning will probably get you marginally better results but what do you do on repos update (if new components or projects are to be created/deleted, you are going to have to do something about it, which is probably retrain...) RAG is inherently more flexible. \\n\\nAlso, with RAG, you can know what is wrong, if your retrieval is giving you bad matches, too long ones or what not, it does not take a phd and you can experiment on small scale with more or less immediate effect and (no offense) the use cas does not seem that complicated (coding rules enforcement + component doc retrieval is nothing a good agent wouldn't do even without RAG or training)\\n\\nI haven't any professional XP in either but I experimented a bit with RAG and the barrier to \\"better than stock\\" (or even \\"good enough\\" it was to get in which project I had used some specific stuff so I concede the simpleness of this task) seems way cheaper and easier to get to than with training (only read about it and maybe people are making a fuss for nothing, but it definitely did not look that way...)\\n\\nI really think in this case, both would be ideal, but OP most certainly does not have the technical know how (no offense ! I would not get hired for that skill !) and RAG seems more appropriate to start with (though \\"vector\\", it is still some form of database, paradigm that devs have had to handle daily for decades, it might be easier to grasp, 'tis for me !)","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n1va1c0","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I also considered the also a notion of cost here. fine tuning will probably get you marginally better results but what do you do on repos update (if new components or projects are to be created/deleted, you are going to have to do something about it, which is probably retrain...) RAG is inherently more flexible. &lt;/p&gt;\\n\\n&lt;p&gt;Also, with RAG, you can know what is wrong, if your retrieval is giving you bad matches, too long ones or what not, it does not take a phd and you can experiment on small scale with more or less immediate effect and (no offense) the use cas does not seem that complicated (coding rules enforcement + component doc retrieval is nothing a good agent wouldn&amp;#39;t do even without RAG or training)&lt;/p&gt;\\n\\n&lt;p&gt;I haven&amp;#39;t any professional XP in either but I experimented a bit with RAG and the barrier to &amp;quot;better than stock&amp;quot; (or even &amp;quot;good enough&amp;quot; it was to get in which project I had used some specific stuff so I concede the simpleness of this task) seems way cheaper and easier to get to than with training (only read about it and maybe people are making a fuss for nothing, but it definitely did not look that way...)&lt;/p&gt;\\n\\n&lt;p&gt;I really think in this case, both would be ideal, but OP most certainly does not have the technical know how (no offense ! I would not get hired for that skill !) and RAG seems more appropriate to start with (though &amp;quot;vector&amp;quot;, it is still some form of database, paradigm that devs have had to handle daily for decades, it might be easier to grasp, &amp;#39;tis for me !)&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1ltm49x","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltm49x/training_8_repos_of_ui_code/n1va1c0/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751920536,"author_flair_text":null,"treatment_tags":[],"created_utc":1751920536,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1rsftj","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"FullstackSensei","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1rqbty","score":1,"author_fullname":"t2_17n3nqtj56","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Here's a couple of Blog posts about tuning to give you a better idea of the process (I'm not associated with either):\\nhttps://www.oxen.ai/blog/training-a-rust-1-5b-coder-lm-with-reinforcement-learning-grpo\\nhttps://www.together.ai/blog/deepcoder","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n1rsftj","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Here&amp;#39;s a couple of Blog posts about tuning to give you a better idea of the process (I&amp;#39;m not associated with either):\\n&lt;a href=\\"https://www.oxen.ai/blog/training-a-rust-1-5b-coder-lm-with-reinforcement-learning-grpo\\"&gt;https://www.oxen.ai/blog/training-a-rust-1-5b-coder-lm-with-reinforcement-learning-grpo&lt;/a&gt;\\n&lt;a href=\\"https://www.together.ai/blog/deepcoder\\"&gt;https://www.together.ai/blog/deepcoder&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1ltm49x","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltm49x/training_8_repos_of_ui_code/n1rsftj/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751874190,"author_flair_text":null,"treatment_tags":[],"created_utc":1751874190,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":6,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1rqbty","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"pomatotappu","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1rp8ex","score":1,"author_fullname":"t2_1sfhn6c33u","approved_by":null,"mod_note":null,"all_awardings":[],"body":"No such experts as such in our team/company but I'm willing to learn and do it as it's assigned to me. Also, thanks for the runpod suggestion, will most probably rent out a gpu there. Will read more about both RAG and fine-tuning and come back.","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n1rqbty","is_submitter":true,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;No such experts as such in our team/company but I&amp;#39;m willing to learn and do it as it&amp;#39;s assigned to me. Also, thanks for the runpod suggestion, will most probably rent out a gpu there. Will read more about both RAG and fine-tuning and come back.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1ltm49x","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltm49x/training_8_repos_of_ui_code/n1rqbty/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751872919,"author_flair_text":null,"treatment_tags":[],"created_utc":1751872919,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1rp8ex","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"FullstackSensei","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1ro6mz","score":4,"author_fullname":"t2_17n3nqtj56","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Getting useful results with RAG will be equally challenging if you expect anything beyond trivial solutions to simple requests, even if you have amazing documentation, which you say you don't have.\\n\\nYour initial intuition about fine tuning is correct, but you need someone who's both good at dataset generation and understands your libraries and how you use them to generate a lot of synthetic examples. You don't need a lot of money to rent a chonky GPU server on runpod or vast to generate said dataset, but you need said expert in both.","edited":false,"author_flair_css_class":null,"name":"t1_n1rp8ex","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Getting useful results with RAG will be equally challenging if you expect anything beyond trivial solutions to simple requests, even if you have amazing documentation, which you say you don&amp;#39;t have.&lt;/p&gt;\\n\\n&lt;p&gt;Your initial intuition about fine tuning is correct, but you need someone who&amp;#39;s both good at dataset generation and understands your libraries and how you use them to generate a lot of synthetic examples. You don&amp;#39;t need a lot of money to rent a chonky GPU server on runpod or vast to generate said dataset, but you need said expert in both.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1ltm49x","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltm49x/training_8_repos_of_ui_code/n1rp8ex/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751872275,"author_flair_text":null,"collapsed":false,"created_utc":1751872275,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1tfe38","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"LA_rent_Aficionado","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1tdk4u","score":2,"author_fullname":"t2_t8zbiflk","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"For immediate returns absolutely but I think as a long term play, having a model trained on a custom corpus of data would make the model more effective at finding relevant data within a RAG DB","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n1tfe38","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;For immediate returns absolutely but I think as a long term play, having a model trained on a custom corpus of data would make the model more effective at finding relevant data within a RAG DB&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1ltm49x","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltm49x/training_8_repos_of_ui_code/n1tfe38/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751900126,"author_flair_text":null,"treatment_tags":[],"created_utc":1751900126,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":6,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n1tdk4u","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DinoAmino","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1t0ois","score":2,"author_fullname":"t2_j1v7f","approved_by":null,"mod_note":null,"all_awardings":[],"body":"Should be the other way around. RAG First. It's easier to do and it's immediately useful. Then you can use your RAG to generate custom datasets for training.","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n1tdk4u","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Should be the other way around. RAG First. It&amp;#39;s easier to do and it&amp;#39;s immediately useful. Then you can use your RAG to generate custom datasets for training.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1ltm49x","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltm49x/training_8_repos_of_ui_code/n1tdk4u/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751899581,"author_flair_text":null,"treatment_tags":[],"created_utc":1751899581,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n1t0ois","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"LA_rent_Aficionado","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1ro6mz","score":3,"author_fullname":"t2_t8zbiflk","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"You could train and do RAG for more accurate RAG I suspect","edited":false,"author_flair_css_class":null,"name":"t1_n1t0ois","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You could train and do RAG for more accurate RAG I suspect&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1ltm49x","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltm49x/training_8_repos_of_ui_code/n1t0ois/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751895579,"author_flair_text":null,"collapsed":false,"created_utc":1751895579,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n1ro6mz","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"pomatotappu","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1rnvia","score":1,"author_fullname":"t2_1sfhn6c33u","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Yeah, got to know this in one of the other comments. Thanks for the insight. As per my understanding from the other comments, RAG would be a better option for my use case.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1ro6mz","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yeah, got to know this in one of the other comments. Thanks for the insight. As per my understanding from the other comments, RAG would be a better option for my use case.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ltm49x","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltm49x/training_8_repos_of_ui_code/n1ro6mz/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751871677,"author_flair_text":null,"treatment_tags":[],"created_utc":1751871677,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1rnvia","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"FullstackSensei","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1rhx57","score":4,"author_fullname":"t2_17n3nqtj56","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"If you need to ask how your dataset needs to look like, you'll almost certainly not get what you expect from fine tuning.\\n\\nFine tuning is a very advanced topic and doing it properly requires a lot of understanding of the process and data preparation.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n1rnvia","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;If you need to ask how your dataset needs to look like, you&amp;#39;ll almost certainly not get what you expect from fine tuning.&lt;/p&gt;\\n\\n&lt;p&gt;Fine tuning is a very advanced topic and doing it properly requires a lot of understanding of the process and data preparation.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ltm49x","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltm49x/training_8_repos_of_ui_code/n1rnvia/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751871502,"author_flair_text":null,"treatment_tags":[],"created_utc":1751871502,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1ruxqa","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Ok-Pipe-5151","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1rhx57","score":1,"author_fullname":"t2_uxbdufm8b","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Consider looking at huggingface datasets as reference. There are hundreds of frontend/component development datasets, most are for react itself. You can model your datasets similar to those","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n1ruxqa","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Consider looking at huggingface datasets as reference. There are hundreds of frontend/component development datasets, most are for react itself. You can model your datasets similar to those&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ltm49x","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltm49x/training_8_repos_of_ui_code/n1ruxqa/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751875668,"author_flair_text":null,"treatment_tags":[],"created_utc":1751875668,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1t1e1d","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"LA_rent_Aficionado","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1rhx57","score":1,"author_fullname":"t2_t8zbiflk","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"You can do unsupervised fine tuning with unsloth or even break up your codebase into chunks, run a dataset generator script through an llm that could, for instance, instruct the llm to make prompts for each block of code \\n\\nOr you can do both steps.  The unsloth documentation only gets you started and is really only helpful for instruct prompts if I recall correct - that said, I just asked ChatGPT with web search on and it really steered me in the right direction with setting up and unsloth training script","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n1t1e1d","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You can do unsupervised fine tuning with unsloth or even break up your codebase into chunks, run a dataset generator script through an llm that could, for instance, instruct the llm to make prompts for each block of code &lt;/p&gt;\\n\\n&lt;p&gt;Or you can do both steps.  The unsloth documentation only gets you started and is really only helpful for instruct prompts if I recall correct - that said, I just asked ChatGPT with web search on and it really steered me in the right direction with setting up and unsloth training script&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ltm49x","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltm49x/training_8_repos_of_ui_code/n1t1e1d/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751895819,"author_flair_text":null,"treatment_tags":[],"created_utc":1751895819,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1rhx57","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"pomatotappu","can_mod_post":false,"created_utc":1751868179,"send_replies":true,"parent_id":"t1_n1rgqbh","score":1,"author_fullname":"t2_1sfhn6c33u","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Suppose i go through the gpu server route, how would my dataset look like for tuning with unsloth. I've looked at a few tutorials related to unsloth and they majorly focus on sinple fine tuning. Is there any resource where i can  learn on fine-tuning an llm with my own codebases.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1rhx57","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Suppose i go through the gpu server route, how would my dataset look like for tuning with unsloth. I&amp;#39;ve looked at a few tutorials related to unsloth and they majorly focus on sinple fine tuning. Is there any resource where i can  learn on fine-tuning an llm with my own codebases.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ltm49x","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltm49x/training_8_repos_of_ui_code/n1rhx57/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751868179,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1rgqbh","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Ok-Pipe-5151","can_mod_post":false,"created_utc":1751867536,"send_replies":true,"parent_id":"t3_1ltm49x","score":5,"author_fullname":"t2_uxbdufm8b","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Fine tuning in m1 macbook is not feasible. You should rent a gpu server, it costs only a few bucks per hour depending on the size of the gpu \\n\\n\\nFor model, you can use devstral or qwen coder. Both are great models. Use a library like unsloth or PEFT for fine tuning efficiently. Unsloth has detailed guides on how to do that in their docs","edited":1751875542,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1rgqbh","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Fine tuning in m1 macbook is not feasible. You should rent a gpu server, it costs only a few bucks per hour depending on the size of the gpu &lt;/p&gt;\\n\\n&lt;p&gt;For model, you can use devstral or qwen coder. Both are great models. Use a library like unsloth or PEFT for fine tuning efficiently. Unsloth has detailed guides on how to do that in their docs&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltm49x/training_8_repos_of_ui_code/n1rgqbh/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751867536,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1ltm49x","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1rqfw1","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"pomatotappu","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1rnie9","score":3,"author_fullname":"t2_1sfhn6c33u","approved_by":null,"mod_note":null,"all_awardings":[],"body":"Thanks, will read the resources that you've shared, rent out a gpu on runpod and come back.","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n1rqfw1","is_submitter":true,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thanks, will read the resources that you&amp;#39;ve shared, rent out a gpu on runpod and come back.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1ltm49x","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltm49x/training_8_repos_of_ui_code/n1rqfw1/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751872986,"author_flair_text":null,"treatment_tags":[],"created_utc":1751872986,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1vb4hl","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"dbuildofficial","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1rnie9","score":2,"author_fullname":"t2_5k8ox0g9","approved_by":null,"mod_note":null,"all_awardings":[],"body":"thank you for increasing my confirmation bias and inflating my ego ! that is kindda what I wanted to say but clearly you have more knowledge \\\\^\\\\^","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n1vb4hl","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;thank you for increasing my confirmation bias and inflating my ego ! that is kindda what I wanted to say but clearly you have more knowledge ^^&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1ltm49x","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltm49x/training_8_repos_of_ui_code/n1vb4hl/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751920896,"author_flair_text":null,"treatment_tags":[],"created_utc":1751920896,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n1rnie9","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"linkillion","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1rlme9","score":2,"author_fullname":"t2_koqhxep","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"It does depend on how much your company cares about data handling. If everything must be local, then I would first caution that it won't be easy. I'm guessing that your team is fairly small, with only 8 codebases? How many LOC are we talking about here?  \\n\\nI don't think fine tuning is out of the question, but fine tuning results directly correlate with the quality of your dataset. When you fine tune you need an instruct dataset, which means having basically a set of questions and answers. The questions should ideally be as close as possible to what you are going to ask it in the future, for example, \\"write a react component for a statistics dashboard that uses XYZ feature\\" and the response would be some example in your codebase of said component. Creating a large, high quality dataset is really hard and is where a lot of SOTA models excel, for example anthropic has really really good datasets which is why their models excel at programming out of the box. If you do this yourself, expect to spend many many hours on it, because garbage in equals garbage out. You can assist this process with ai creating the dataset, but this relies on a) a SOTA non local model and b) your codebase being full of good examples, which, frankly is probably not accurate. For more info on this, unsloth has a good guide about what you need to start fine tuning: https://docs.unsloth.ai/get-started/fine-tuning-guide.\\n\\nSo really that leaves you in a hard spot where you either spend a hell of a lot of time making your own fine tuning dataset, or you use a non local model. That's why in my opinion I think you're more likely to get better results, faster, with an RAG system. It won't be quite the same, but you could setup a system where you inject a prompt instructing the model to use RAG to search relevant components and closely mirror results in their answers. As for setting up an RAG system, it's constantly changing, but OpenwebUI makes it fairly easy (you can lookup guides for that). You could also use a more custom solution, something like this, https://blog.lancedb.com/rag-codebase-1/. Whatever you do, if you're coming in blind expect to spend a lot of time setting it up and customizing it, and expect to be essentially behind the times as soon as you set it up. That said, the capabilities of a local system will never get worse, so if you do get a productivity boost it won't just go away, you just might be able to achieve better with constant refinement and updating models/etc. \\n\\nIf you get the money for a mvp then I would see what the best system you can build for your money (which I can't help you with, I'm very far behind in the hardware space right now). After you purchase components, come back here and ask what the best model for your use case is. It will probably some variant of qwen, devstral, or deepseek, but it moves so fast it's not really prudent to recommend a specific model right now. They won't get worse. \\n\\nEither way you are clearly open to learn and figure out what's best for your company so you're already doing better than most people insisting on shoving AI into their workflow right now.","edited":false,"author_flair_css_class":null,"name":"t1_n1rnie9","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It does depend on how much your company cares about data handling. If everything must be local, then I would first caution that it won&amp;#39;t be easy. I&amp;#39;m guessing that your team is fairly small, with only 8 codebases? How many LOC are we talking about here?  &lt;/p&gt;\\n\\n&lt;p&gt;I don&amp;#39;t think fine tuning is out of the question, but fine tuning results directly correlate with the quality of your dataset. When you fine tune you need an instruct dataset, which means having basically a set of questions and answers. The questions should ideally be as close as possible to what you are going to ask it in the future, for example, &amp;quot;write a react component for a statistics dashboard that uses XYZ feature&amp;quot; and the response would be some example in your codebase of said component. Creating a large, high quality dataset is really hard and is where a lot of SOTA models excel, for example anthropic has really really good datasets which is why their models excel at programming out of the box. If you do this yourself, expect to spend many many hours on it, because garbage in equals garbage out. You can assist this process with ai creating the dataset, but this relies on a) a SOTA non local model and b) your codebase being full of good examples, which, frankly is probably not accurate. For more info on this, unsloth has a good guide about what you need to start fine tuning: &lt;a href=\\"https://docs.unsloth.ai/get-started/fine-tuning-guide\\"&gt;https://docs.unsloth.ai/get-started/fine-tuning-guide&lt;/a&gt;.&lt;/p&gt;\\n\\n&lt;p&gt;So really that leaves you in a hard spot where you either spend a hell of a lot of time making your own fine tuning dataset, or you use a non local model. That&amp;#39;s why in my opinion I think you&amp;#39;re more likely to get better results, faster, with an RAG system. It won&amp;#39;t be quite the same, but you could setup a system where you inject a prompt instructing the model to use RAG to search relevant components and closely mirror results in their answers. As for setting up an RAG system, it&amp;#39;s constantly changing, but OpenwebUI makes it fairly easy (you can lookup guides for that). You could also use a more custom solution, something like this, &lt;a href=\\"https://blog.lancedb.com/rag-codebase-1/\\"&gt;https://blog.lancedb.com/rag-codebase-1/&lt;/a&gt;. Whatever you do, if you&amp;#39;re coming in blind expect to spend a lot of time setting it up and customizing it, and expect to be essentially behind the times as soon as you set it up. That said, the capabilities of a local system will never get worse, so if you do get a productivity boost it won&amp;#39;t just go away, you just might be able to achieve better with constant refinement and updating models/etc. &lt;/p&gt;\\n\\n&lt;p&gt;If you get the money for a mvp then I would see what the best system you can build for your money (which I can&amp;#39;t help you with, I&amp;#39;m very far behind in the hardware space right now). After you purchase components, come back here and ask what the best model for your use case is. It will probably some variant of qwen, devstral, or deepseek, but it moves so fast it&amp;#39;s not really prudent to recommend a specific model right now. They won&amp;#39;t get worse. &lt;/p&gt;\\n\\n&lt;p&gt;Either way you are clearly open to learn and figure out what&amp;#39;s best for your company so you&amp;#39;re already doing better than most people insisting on shoving AI into their workflow right now.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1ltm49x","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltm49x/training_8_repos_of_ui_code/n1rnie9/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751871297,"author_flair_text":null,"collapsed":false,"created_utc":1751871297,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n1rlme9","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"pomatotappu","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1rjo23","score":1,"author_fullname":"t2_1sfhn6c33u","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Thanks for this insight. Really helpful. If this is the case, no point in doing a poc. Suppose let's say i get the budget for poc, how can i go about this? This is the first time I'm working on something related to llms. So, please excuse me if these questions are bothering you. Which models should i look into and how to prepare the dataset? Are there any resources/blogs explaining such tasks? Rag or fine-tuning or any other approach is fine too.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1rlme9","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thanks for this insight. Really helpful. If this is the case, no point in doing a poc. Suppose let&amp;#39;s say i get the budget for poc, how can i go about this? This is the first time I&amp;#39;m working on something related to llms. So, please excuse me if these questions are bothering you. Which models should i look into and how to prepare the dataset? Are there any resources/blogs explaining such tasks? Rag or fine-tuning or any other approach is fine too.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ltm49x","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltm49x/training_8_repos_of_ui_code/n1rlme9/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751870237,"author_flair_text":null,"treatment_tags":[],"created_utc":1751870237,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1rjo23","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"linkillion","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1riwo4","score":2,"author_fullname":"t2_koqhxep","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"That's tough because a proof of concept on a 16gb machine won't really be convincing imo. It's too slow and won't allow any contextual understanding or cohesivity. At best you can write some boilerplate UI elements about as fast as copying and pasting between codebases, but for what I think you want, you really need the context window and reliability of a large model. \\n\\n\\nI'd experiment with RAG on your mac to see if it does what you need, if not, depending on your privacy concerns, use openrouter and test with models you could use, should the company give you a budget (qwen/devstral). I wouldn't bother fine tuning a small model with limited data, personally. ","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n1rjo23","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;That&amp;#39;s tough because a proof of concept on a 16gb machine won&amp;#39;t really be convincing imo. It&amp;#39;s too slow and won&amp;#39;t allow any contextual understanding or cohesivity. At best you can write some boilerplate UI elements about as fast as copying and pasting between codebases, but for what I think you want, you really need the context window and reliability of a large model. &lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;d experiment with RAG on your mac to see if it does what you need, if not, depending on your privacy concerns, use openrouter and test with models you could use, should the company give you a budget (qwen/devstral). I wouldn&amp;#39;t bother fine tuning a small model with limited data, personally. &lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ltm49x","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltm49x/training_8_repos_of_ui_code/n1rjo23/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751869139,"author_flair_text":null,"treatment_tags":[],"created_utc":1751869139,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n1riwo4","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"pomatotappu","can_mod_post":false,"created_utc":1751868718,"send_replies":true,"parent_id":"t1_n1ria93","score":1,"author_fullname":"t2_1sfhn6c33u","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Currently for a proof of concept, we are doing it on mac. If it turns out good or useful, the company will surely get us a good machine for this purpose.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1riwo4","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Currently for a proof of concept, we are doing it on mac. If it turns out good or useful, the company will surely get us a good machine for this purpose.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ltm49x","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltm49x/training_8_repos_of_ui_code/n1riwo4/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751868718,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"2b12e2b8-fdc0-11ee-9a03-6e2f48afd456","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1uqzp7","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Former-Ad-5757","can_mod_post":false,"created_utc":1751914241,"send_replies":true,"parent_id":"t1_n1ria93","score":1,"author_fullname":"t2_ihsdiwk6k","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I would like to add that I myself atleast have very good results with just very shallow finetunes on very little data. Not to “really” finetune the model, but just to give your stylistic style a .01% over another style. Imho it also helps a lot with rag etc.\\n\\nThe goal is not to really finetune the model, just whenever the next token can be general style or your style then your style should get a little boost, but only a little boost because you don’t have the data for a real finetune","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1uqzp7","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"Llama 3"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I would like to add that I myself atleast have very good results with just very shallow finetunes on very little data. Not to “really” finetune the model, but just to give your stylistic style a .01% over another style. Imho it also helps a lot with rag etc.&lt;/p&gt;\\n\\n&lt;p&gt;The goal is not to really finetune the model, just whenever the next token can be general style or your style then your style should get a little boost, but only a little boost because you don’t have the data for a real finetune&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ltm49x","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltm49x/training_8_repos_of_ui_code/n1uqzp7/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751914241,"author_flair_text":"Llama 3","treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":"#c7b594","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1ria93","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"linkillion","can_mod_post":false,"created_utc":1751868378,"send_replies":true,"parent_id":"t3_1ltm49x","score":2,"author_fullname":"t2_koqhxep","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Assuming you want stylistic coherence across your apps, I would use a non-local model (2.5 pro with 1 mil context window can probably fit your whole codebase) to ingest and create a design document, which has various components and styles that are shared across your system which you can then feed a local model like qwen when you build your UI. Fine tuning almost certainly won't achieve what you're looking for here, especially with that little data to feed it. \\n\\n\\nYou could also add comments (with ai or yourself) around various components and then feed your codebase into a RAG system, then setup a tool call to search for relevant components. \\n\\n\\nThe simplest approach would be feeding your code into RAG into a project in openwebui and then have people chat with that. \\n\\n\\nAs for models, if you're looking to infer on the M1 mac with 16gb you're really really limited in terms of speed and quality. Qwen3-8b-r1-0528 would probably be your best bet if I had to guess, but others may know better. \\n\\n\\nI'll give you a hint though, if you actually want production usefulness you're wasting your time unless you drop cash on a much much better machine and run a quantized version of qwen3-235b or devstral (which I've heard good things about but haven't personally tried)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1ria93","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Assuming you want stylistic coherence across your apps, I would use a non-local model (2.5 pro with 1 mil context window can probably fit your whole codebase) to ingest and create a design document, which has various components and styles that are shared across your system which you can then feed a local model like qwen when you build your UI. Fine tuning almost certainly won&amp;#39;t achieve what you&amp;#39;re looking for here, especially with that little data to feed it. &lt;/p&gt;\\n\\n&lt;p&gt;You could also add comments (with ai or yourself) around various components and then feed your codebase into a RAG system, then setup a tool call to search for relevant components. &lt;/p&gt;\\n\\n&lt;p&gt;The simplest approach would be feeding your code into RAG into a project in openwebui and then have people chat with that. &lt;/p&gt;\\n\\n&lt;p&gt;As for models, if you&amp;#39;re looking to infer on the M1 mac with 16gb you&amp;#39;re really really limited in terms of speed and quality. Qwen3-8b-r1-0528 would probably be your best bet if I had to guess, but others may know better. &lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;ll give you a hint though, if you actually want production usefulness you&amp;#39;re wasting your time unless you drop cash on a much much better machine and run a quantized version of qwen3-235b or devstral (which I&amp;#39;ve heard good things about but haven&amp;#39;t personally tried)&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltm49x/training_8_repos_of_ui_code/n1ria93/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751868378,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1ltm49x","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}}]`),r=()=>e.jsx(t,{data:a});export{r as default};
