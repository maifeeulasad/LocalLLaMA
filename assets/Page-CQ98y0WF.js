import{j as e}from"./index-BlGsFJYy.js";import{R as t}from"./RedditPostRenderer-B6uvq_Zl.js";import"./index-DDvVtNwD.js";const l=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Hi everyone,\\n\\nI'm new to LLM and thinking about the following build to use for local LLM (on a VM in Proxmox, will be using PCI passthrough for GPUs) and homelab (some VMs and/or containers for software development, tinkering, etc.) combo. Could you please share your thoughts? Thanks in advance...\\n\\n**Mainboard:** JGINYUE X99-8D4/2.5G [https://jginyue.com/index/Article/show/cat\\\\_id/48/id/219](https://jginyue.com/index/Article/show/cat_id/48/id/219)\\n\\nhttps://preview.redd.it/zhe921uxjpbf1.png?width=3300&amp;format=png&amp;auto=webp&amp;s=05ce2b011c36c3c10bb99153c0303e5129fe8f4e\\n\\nor MACHINIST X99 D8 MAX (4x PCI 3.0 x16)\\n\\nhttps://preview.redd.it/ieuygd1xjpbf1.png?width=960&amp;format=png&amp;auto=webp&amp;s=86c983db63b1f8c86864bc80d0cc6be436510b1c\\n\\n**CPUs:** 2 x E5-2699A V4 or 2 x E5-2699 V4 (40 PCI lanes per CPU) Total 44 cores, 88 threads\\n\\n**Memory:** 256 GB (up to 512 GB)\\n\\n**GPUs:** 2x Asus ROG Strix 3090 OC\\n\\n**Power Supply:** min 1200w\\n\\n**Case:** mining rig + PCI 3.0/4.0 risers","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Thoughts on local LLM &amp; Proxmox homelab using Chinese x99 dual Xeon board + 2x3090","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":123,"top_awarded_type":null,"hide_score":false,"media_metadata":{"ieuygd1xjpbf1":{"status":"valid","e":"Image","m":"image/png","p":[{"y":108,"x":108,"u":"https://preview.redd.it/ieuygd1xjpbf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=d627589fa350d44eea5b5785fc7e9c1b6b83f36e"},{"y":216,"x":216,"u":"https://preview.redd.it/ieuygd1xjpbf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=6d18dc807a0ed65a97985c54dd09e529ba9b1f69"},{"y":320,"x":320,"u":"https://preview.redd.it/ieuygd1xjpbf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=aec1aafd5d9152710eeec9cc69feb80087d9469a"},{"y":640,"x":640,"u":"https://preview.redd.it/ieuygd1xjpbf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=66424894ac15d339bb7fee6f3bb3c8996d8f5ac7"},{"y":960,"x":960,"u":"https://preview.redd.it/ieuygd1xjpbf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=bb97958f83ae8124abecabc26bcc7223b52ec2be"}],"s":{"y":960,"x":960,"u":"https://preview.redd.it/ieuygd1xjpbf1.png?width=960&amp;format=png&amp;auto=webp&amp;s=86c983db63b1f8c86864bc80d0cc6be436510b1c"},"id":"ieuygd1xjpbf1"},"zhe921uxjpbf1":{"status":"valid","e":"Image","m":"image/png","p":[{"y":94,"x":108,"u":"https://preview.redd.it/zhe921uxjpbf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=c2e6f5d5b8bbe595f32db43d2b89bdf0a3f14d55"},{"y":189,"x":216,"u":"https://preview.redd.it/zhe921uxjpbf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=374eb0f78a445b65e2bcd4efd39d0f92f904db4d"},{"y":281,"x":320,"u":"https://preview.redd.it/zhe921uxjpbf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=1f66e2752c9a3c5b6d750ff16f7991a4113eedc7"},{"y":562,"x":640,"u":"https://preview.redd.it/zhe921uxjpbf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=ddb19a39e3576e875c44887795c02db49f57393c"},{"y":843,"x":960,"u":"https://preview.redd.it/zhe921uxjpbf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=8e8775a907fb448f499794ab68a51c78e4ccb2a3"},{"y":949,"x":1080,"u":"https://preview.redd.it/zhe921uxjpbf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=40223c55294735c7fabb795ac409aeca188cbb5a"}],"s":{"y":2900,"x":3300,"u":"https://preview.redd.it/zhe921uxjpbf1.png?width=3300&amp;format=png&amp;auto=webp&amp;s=05ce2b011c36c3c10bb99153c0303e5129fe8f4e"},"id":"zhe921uxjpbf1"}},"name":"t3_1luz92k","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.6,"author_flair_background_color":null,"subreddit_type":"public","ups":1,"total_awards_received":0,"media_embed":{},"thumbnail_width":140,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_6gt8ct53","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":1,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"https://a.thumbs.redditmedia.com/6aWZ0xKOG8quekcHyrX4K2kLmgfZdLO_Gm-PfzPFH28.jpg","edited":1752021778,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1752006185,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;m new to LLM and thinking about the following build to use for local LLM (on a VM in Proxmox, will be using PCI passthrough for GPUs) and homelab (some VMs and/or containers for software development, tinkering, etc.) combo. Could you please share your thoughts? Thanks in advance...&lt;/p&gt;\\n\\n&lt;p&gt;&lt;strong&gt;Mainboard:&lt;/strong&gt; JGINYUE X99-8D4/2.5G &lt;a href=\\"https://jginyue.com/index/Article/show/cat_id/48/id/219\\"&gt;https://jginyue.com/index/Article/show/cat_id/48/id/219&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://preview.redd.it/zhe921uxjpbf1.png?width=3300&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=05ce2b011c36c3c10bb99153c0303e5129fe8f4e\\"&gt;https://preview.redd.it/zhe921uxjpbf1.png?width=3300&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=05ce2b011c36c3c10bb99153c0303e5129fe8f4e&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;or MACHINIST X99 D8 MAX (4x PCI 3.0 x16)&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://preview.redd.it/ieuygd1xjpbf1.png?width=960&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=86c983db63b1f8c86864bc80d0cc6be436510b1c\\"&gt;https://preview.redd.it/ieuygd1xjpbf1.png?width=960&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=86c983db63b1f8c86864bc80d0cc6be436510b1c&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;&lt;strong&gt;CPUs:&lt;/strong&gt; 2 x E5-2699A V4 or 2 x E5-2699 V4 (40 PCI lanes per CPU) Total 44 cores, 88 threads&lt;/p&gt;\\n\\n&lt;p&gt;&lt;strong&gt;Memory:&lt;/strong&gt; 256 GB (up to 512 GB)&lt;/p&gt;\\n\\n&lt;p&gt;&lt;strong&gt;GPUs:&lt;/strong&gt; 2x Asus ROG Strix 3090 OC&lt;/p&gt;\\n\\n&lt;p&gt;&lt;strong&gt;Power Supply:&lt;/strong&gt; min 1200w&lt;/p&gt;\\n\\n&lt;p&gt;&lt;strong&gt;Case:&lt;/strong&gt; mining rig + PCI 3.0/4.0 risers&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":true,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1luz92k","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"Long-Caterpillar675","discussion_type":null,"num_comments":20,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1luz92k/thoughts_on_local_llm_proxmox_homelab_using/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1luz92k/thoughts_on_local_llm_proxmox_homelab_using/","subreddit_subscribers":496591,"created_utc":1752006185,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"d2642412-d9ce-11ed-ae30-32b11309f5bd","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n225m8h","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"HugoCortell","can_mod_post":false,"created_utc":1752009401,"send_replies":true,"parent_id":"t1_n2201o1","score":1,"author_fullname":"t2_61s8b5gv","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"With or without GPUs?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n225m8h","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;With or without GPUs?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1luz92k","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1luz92k/thoughts_on_local_llm_proxmox_homelab_using/n225m8h/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752009401,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2201o1","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ortegaalfredo","can_mod_post":false,"created_utc":1752007867,"send_replies":true,"parent_id":"t3_1luz92k","score":2,"author_fullname":"t2_g177e","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I'm using one of those, 128GB + 4x3090 plus a E5-2680 v4.\\n\\nAt the moment I'm running deepeek-V3-IQ1, it works at 6 tok/s, Qwen3-235B-Q4 works at \\\\~12 tok/s\\n\\nEven if they are old, they use ddr4 memory and the PCIE bus is 16x so they are not slow, about 1-2 tok/s slower than modern threadripper systems that cost 10X.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2201o1","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"Alpaca"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;m using one of those, 128GB + 4x3090 plus a E5-2680 v4.&lt;/p&gt;\\n\\n&lt;p&gt;At the moment I&amp;#39;m running deepeek-V3-IQ1, it works at 6 tok/s, Qwen3-235B-Q4 works at ~12 tok/s&lt;/p&gt;\\n\\n&lt;p&gt;Even if they are old, they use ddr4 memory and the PCIE bus is 16x so they are not slow, about 1-2 tok/s slower than modern threadripper systems that cost 10X.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1luz92k/thoughts_on_local_llm_proxmox_homelab_using/n2201o1/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752007867,"author_flair_text":"Alpaca","treatment_tags":[],"link_id":"t3_1luz92k","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bd9e9e","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n21xuef","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"NewtMurky","can_mod_post":false,"created_utc":1752007284,"send_replies":true,"parent_id":"t3_1luz92k","score":1,"author_fullname":"t2_73yvfgv3","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"If you already have a PC with 2+ pcie slots, then just buy GPUs and PSU. Test this setup first and only after that think if there is a real reason to spend thousands of dollars on 3-5 times slower inference with a model that gives you 5-10% of additional accuracy.\\n\\nLLM will make mistakes even on super expensive hardware.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n21xuef","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;If you already have a PC with 2+ pcie slots, then just buy GPUs and PSU. Test this setup first and only after that think if there is a real reason to spend thousands of dollars on 3-5 times slower inference with a model that gives you 5-10% of additional accuracy.&lt;/p&gt;\\n\\n&lt;p&gt;LLM will make mistakes even on super expensive hardware.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1luz92k/thoughts_on_local_llm_proxmox_homelab_using/n21xuef/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752007284,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1luz92k","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n21zyip","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"a_beautiful_rhind","can_mod_post":false,"created_utc":1752007844,"send_replies":true,"parent_id":"t1_n21y9w8","score":2,"author_fullname":"t2_h5utwre7","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Seconded, don't buy V3's. V4s are dirt cheap.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n21zyip","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Seconded, don&amp;#39;t buy V3&amp;#39;s. V4s are dirt cheap.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1luz92k","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1luz92k/thoughts_on_local_llm_proxmox_homelab_using/n21zyip/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752007844,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n238ywg","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"FullstackSensei","can_mod_post":false,"created_utc":1752021827,"send_replies":true,"parent_id":"t1_n233i6k","score":1,"author_fullname":"t2_17n3nqtj56","approved_by":null,"mod_note":null,"all_awardings":[],"body":"You're right that FMA and AVX2 were added together, but Intel distinguishes between them in all their documentation, and even has separate flags to designate support for each. AVX/AVX2 are basically 256-bit extensions of MMX (unlike SSE which does bona fide FP operations). FMA, OTOH, adds a whole new type of operation: a fused multiplication and addition, with the intermediate result kept at a higher precision. And while FMA can use the full 256-bit YMM registers, it also works on XMM only, meaning Intel could decide to implement FMA on a CPU without AVX without breaking anything in the instruction set rules. \\n\\nOn the implementation front, you could get away using AVX1 instructions only in a matrix multiplication kernel (but still use FMA) with minimal performance impact (the heavy lifting here is done by the prefetcher), but removing FMA from the kernel will have a huge performance impact even if you used AVX2, because each 256-bit wide FMA instruction will have to be serialized into four SSE multiplications and additions. That's why using a CPU without FMA (or AVX2 if you prefer) shows such a cliff drop in matrix multiplication performance. This is true even if the matrices being multiplied fit in cache, so it's not a memory bandwidth issue.","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n238ywg","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You&amp;#39;re right that FMA and AVX2 were added together, but Intel distinguishes between them in all their documentation, and even has separate flags to designate support for each. AVX/AVX2 are basically 256-bit extensions of MMX (unlike SSE which does bona fide FP operations). FMA, OTOH, adds a whole new type of operation: a fused multiplication and addition, with the intermediate result kept at a higher precision. And while FMA can use the full 256-bit YMM registers, it also works on XMM only, meaning Intel could decide to implement FMA on a CPU without AVX without breaking anything in the instruction set rules. &lt;/p&gt;\\n\\n&lt;p&gt;On the implementation front, you could get away using AVX1 instructions only in a matrix multiplication kernel (but still use FMA) with minimal performance impact (the heavy lifting here is done by the prefetcher), but removing FMA from the kernel will have a huge performance impact even if you used AVX2, because each 256-bit wide FMA instruction will have to be serialized into four SSE multiplications and additions. That&amp;#39;s why using a CPU without FMA (or AVX2 if you prefer) shows such a cliff drop in matrix multiplication performance. This is true even if the matrices being multiplied fit in cache, so it&amp;#39;s not a memory bandwidth issue.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1luz92k","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1luz92k/thoughts_on_local_llm_proxmox_homelab_using/n238ywg/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752021827,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":7,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n233i6k","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"thomthehound","can_mod_post":false,"send_replies":true,"parent_id":"t1_n22qgo1","score":1,"author_fullname":"t2_vxbs7cf4","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I don't know what to say about this. Intel CPUs that don't have AVX2 also don't have FMA, and you need the vector registers from AVX to make good use of it anyway. If you are arguing the very specific technical point that FMA isn't necessarily part of AVX2... OK, I guess I could have been more precise. Although AVX2 instructions are still used in LLM kernels.","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n233i6k","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I don&amp;#39;t know what to say about this. Intel CPUs that don&amp;#39;t have AVX2 also don&amp;#39;t have FMA, and you need the vector registers from AVX to make good use of it anyway. If you are arguing the very specific technical point that FMA isn&amp;#39;t necessarily part of AVX2... OK, I guess I could have been more precise. Although AVX2 instructions are still used in LLM kernels.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1luz92k","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1luz92k/thoughts_on_local_llm_proxmox_homelab_using/n233i6k/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752019968,"author_flair_text":null,"treatment_tags":[],"created_utc":1752019968,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":6,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n22qgo1","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"FullstackSensei","can_mod_post":false,"send_replies":true,"parent_id":"t1_n22mx9r","score":1,"author_fullname":"t2_17n3nqtj56","approved_by":null,"mod_note":null,"all_awardings":[],"body":"No I'm not. Google the instructions and see for yourself! I thought the same for a looong time, and only found about this less than two years ago. AVX and AVX2 are mainly about data shuffling. The only floating point instructions in AVX2 are for gather. The Actual vector floating point multiplication instructions are in FMA (FMA3, because AMD did it first with FMA4). AVX-512 only adds to the confusion because there are three sets of FMA instructions, each introduced in a separate family of cores, but not necessarily continued in subsequent cores.","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n22qgo1","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;No I&amp;#39;m not. Google the instructions and see for yourself! I thought the same for a looong time, and only found about this less than two years ago. AVX and AVX2 are mainly about data shuffling. The only floating point instructions in AVX2 are for gather. The Actual vector floating point multiplication instructions are in FMA (FMA3, because AMD did it first with FMA4). AVX-512 only adds to the confusion because there are three sets of FMA instructions, each introduced in a separate family of cores, but not necessarily continued in subsequent cores.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1luz92k","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1luz92k/thoughts_on_local_llm_proxmox_homelab_using/n22qgo1/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752015743,"author_flair_text":null,"treatment_tags":[],"created_utc":1752015743,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n22mx9r","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"thomthehound","can_mod_post":false,"send_replies":true,"parent_id":"t1_n22md1o","score":1,"author_fullname":"t2_vxbs7cf4","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I think you might be confusing AVX with MMX. AVX certainly is used for floats, and AVX2 introduced FMA instructions to the original AVX, as well as adding integer support.","edited":false,"author_flair_css_class":null,"name":"t1_n22mx9r","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I think you might be confusing AVX with MMX. AVX certainly is used for floats, and AVX2 introduced FMA instructions to the original AVX, as well as adding integer support.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1luz92k","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1luz92k/thoughts_on_local_llm_proxmox_homelab_using/n22mx9r/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752014618,"author_flair_text":null,"collapsed":false,"created_utc":1752014618,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n22md1o","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"FullstackSensei","can_mod_post":false,"send_replies":true,"parent_id":"t1_n22hi1u","score":1,"author_fullname":"t2_17n3nqtj56","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"AVX and AVX2 are about integer operations. AVX2 is used in loading and storing results, but the bulk of matrix multiplication is done by the FMA instructions. That's my only point. Both were introduced with Haswell, which is E5v3. The reason v4 is much faster has more to do with prefetch, increased cache size and decrease in cache latency. Broadwell was the OG 14nm CPU, before the ++++ shitshow.\\n\\nWhy would quad channel cause any latency? Ditto for registered memory. I'd understand that in generic workloads, but matrix multiplication is a textbook case for prefetch prediction. There are zero surprises and zero branches for prefetch to worry about.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n22md1o","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;AVX and AVX2 are about integer operations. AVX2 is used in loading and storing results, but the bulk of matrix multiplication is done by the FMA instructions. That&amp;#39;s my only point. Both were introduced with Haswell, which is E5v3. The reason v4 is much faster has more to do with prefetch, increased cache size and decrease in cache latency. Broadwell was the OG 14nm CPU, before the ++++ shitshow.&lt;/p&gt;\\n\\n&lt;p&gt;Why would quad channel cause any latency? Ditto for registered memory. I&amp;#39;d understand that in generic workloads, but matrix multiplication is a textbook case for prefetch prediction. There are zero surprises and zero branches for prefetch to worry about.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1luz92k","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1luz92k/thoughts_on_local_llm_proxmox_homelab_using/n22md1o/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752014437,"author_flair_text":null,"treatment_tags":[],"created_utc":1752014437,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n22hi1u","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"thomthehound","can_mod_post":false,"send_replies":true,"parent_id":"t1_n22avir","score":1,"author_fullname":"t2_vxbs7cf4","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yeah, I have one myself that I bought during the pandemic. It served me well. But it certainly is old. I just use it as a home media server now.\\n\\nI certainly could be wrong, but I'm 90% sure that you need a special compile of llama.cpp (and likely other backends as well) if you don't have AVX2 support, and that BLAS is slower on those builds. To be the other 10% sure, I would need to test it for myself, but I don't currently have any PCs without AVX2. Regardless, having special compiles for everything would get annoying very quickly.\\n\\nAs for the memory... I think the \\\\~16% speed difference would be noticeable for LLMs, but I agree it isn't the end of the world. There is also a latency hit that comes with quad channel, on top of the need for registered DIMMS, but that's probably not of much concern in this case.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n22hi1u","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yeah, I have one myself that I bought during the pandemic. It served me well. But it certainly is old. I just use it as a home media server now.&lt;/p&gt;\\n\\n&lt;p&gt;I certainly could be wrong, but I&amp;#39;m 90% sure that you need a special compile of llama.cpp (and likely other backends as well) if you don&amp;#39;t have AVX2 support, and that BLAS is slower on those builds. To be the other 10% sure, I would need to test it for myself, but I don&amp;#39;t currently have any PCs without AVX2. Regardless, having special compiles for everything would get annoying very quickly.&lt;/p&gt;\\n\\n&lt;p&gt;As for the memory... I think the ~16% speed difference would be noticeable for LLMs, but I agree it isn&amp;#39;t the end of the world. There is also a latency hit that comes with quad channel, on top of the need for registered DIMMS, but that&amp;#39;s probably not of much concern in this case.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1luz92k","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1luz92k/thoughts_on_local_llm_proxmox_homelab_using/n22hi1u/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752012912,"author_flair_text":null,"treatment_tags":[],"created_utc":1752012912,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n22avir","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"FullstackSensei","can_mod_post":false,"created_utc":1752010931,"send_replies":true,"parent_id":"t1_n21y9w8","score":1,"author_fullname":"t2_17n3nqtj56","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Small correction: it's not Avx2 that you care about for LLMs, but FMA and possibly F16C (for up-casting f16 to f32. Otherwise, you're right about V4.\\n\\nI wouldn't say quad channel 2400 is too slow. It's not much slower than current desktop platforms with DDR5-5600.\\n\\nLGA2011-3 is starting to get long on the tooth, but it's still a great low-cost platform for LLMs.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n22avir","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Small correction: it&amp;#39;s not Avx2 that you care about for LLMs, but FMA and possibly F16C (for up-casting f16 to f32. Otherwise, you&amp;#39;re right about V4.&lt;/p&gt;\\n\\n&lt;p&gt;I wouldn&amp;#39;t say quad channel 2400 is too slow. It&amp;#39;s not much slower than current desktop platforms with DDR5-5600.&lt;/p&gt;\\n\\n&lt;p&gt;LGA2011-3 is starting to get long on the tooth, but it&amp;#39;s still a great low-cost platform for LLMs.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1luz92k","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1luz92k/thoughts_on_local_llm_proxmox_homelab_using/n22avir/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752010931,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n21y9w8","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"thomthehound","can_mod_post":false,"created_utc":1752007398,"send_replies":true,"parent_id":"t3_1luz92k","score":1,"author_fullname":"t2_vxbs7cf4","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"If you plan on using the CPUs in any capacity for number crunching, you want the V4's and not the V3's that everybody recommends for gaming. The V3's don't have AVX2. But, even then, I wouldn't expect a whole lot out of them. Even with quad channel, DDR4-2400 is just too slow, so anything that spills over from the 3090's is going to hurt performance in a huge way.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n21y9w8","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;If you plan on using the CPUs in any capacity for number crunching, you want the V4&amp;#39;s and not the V3&amp;#39;s that everybody recommends for gaming. The V3&amp;#39;s don&amp;#39;t have AVX2. But, even then, I wouldn&amp;#39;t expect a whole lot out of them. Even with quad channel, DDR4-2400 is just too slow, so anything that spills over from the 3090&amp;#39;s is going to hurt performance in a huge way.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1luz92k/thoughts_on_local_llm_proxmox_homelab_using/n21y9w8/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752007398,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1luz92k","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n22qg2e","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Willing_Landscape_61","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2259w7","score":1,"author_fullname":"t2_8lvrytgw","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Indeed. For big MoE, I'd get just one GPU and put the extra money on the Epyc Gen 2 with 8 memory channels bought second hand obviously.\\nThat's what I did actually (except 4090 instead of 3090).","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n22qg2e","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Indeed. For big MoE, I&amp;#39;d get just one GPU and put the extra money on the Epyc Gen 2 with 8 memory channels bought second hand obviously.\\nThat&amp;#39;s what I did actually (except 4090 instead of 3090).&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1luz92k","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1luz92k/thoughts_on_local_llm_proxmox_homelab_using/n22qg2e/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752015737,"author_flair_text":null,"treatment_tags":[],"created_utc":1752015737,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2259w7","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"HugoCortell","can_mod_post":false,"created_utc":1752009303,"send_replies":true,"parent_id":"t1_n21zegd","score":5,"author_fullname":"t2_61s8b5gv","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"&gt;Why not an Epyc Gen 2 mobo with 8 memory channels per socket ?\\n\\nI suspect it may be because Xeons cost $25 each (+$50 for the mobo) compared to the entire yearly GDP of Southern Djibouti for a single Epyc.\\n\\nThen again, OP wants to use two 3090s, so he's clearly not *that* broke.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2259w7","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;Why not an Epyc Gen 2 mobo with 8 memory channels per socket ?&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;I suspect it may be because Xeons cost $25 each (+$50 for the mobo) compared to the entire yearly GDP of Southern Djibouti for a single Epyc.&lt;/p&gt;\\n\\n&lt;p&gt;Then again, OP wants to use two 3090s, so he&amp;#39;s clearly not &lt;em&gt;that&lt;/em&gt; broke.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1luz92k","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1luz92k/thoughts_on_local_llm_proxmox_homelab_using/n2259w7/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752009303,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}}],"before":null}},"user_reports":[],"saved":false,"id":"n21zegd","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"un_passant","can_mod_post":false,"created_utc":1752007694,"send_replies":true,"parent_id":"t3_1luz92k","score":1,"author_fullname":"t2_7rqtc","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"IMHO, it makes no sense to pick mobos with so few memory channels.\\n\\nNot sure what you will use your server for besides LLM, but I presume proxmox means you will do something else with it too ?\\n\\nDepending on the other uses constraints, it might make sense to go dual socket, but I would only use a VM on one of the socket to do the LLM (because NUMA).\\n\\nWhy not an Epyc Gen 2 mobo with 8 memory channels per socket ?\\n\\nIt's both faster (more memory channels used) and cheaper (less dense memory sticks are cheaper per GB).","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n21zegd","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;IMHO, it makes no sense to pick mobos with so few memory channels.&lt;/p&gt;\\n\\n&lt;p&gt;Not sure what you will use your server for besides LLM, but I presume proxmox means you will do something else with it too ?&lt;/p&gt;\\n\\n&lt;p&gt;Depending on the other uses constraints, it might make sense to go dual socket, but I would only use a VM on one of the socket to do the LLM (because NUMA).&lt;/p&gt;\\n\\n&lt;p&gt;Why not an Epyc Gen 2 mobo with 8 memory channels per socket ?&lt;/p&gt;\\n\\n&lt;p&gt;It&amp;#39;s both faster (more memory channels used) and cheaper (less dense memory sticks are cheaper per GB).&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1luz92k/thoughts_on_local_llm_proxmox_homelab_using/n21zegd/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752007694,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1luz92k","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n21zqrn","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"a_beautiful_rhind","can_mod_post":false,"created_utc":1752007786,"send_replies":true,"parent_id":"t3_1luz92k","score":1,"author_fullname":"t2_h5utwre7","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Those older xenons are getting a bit long in the tooth and only support 2400 memory.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n21zqrn","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Those older xenons are getting a bit long in the tooth and only support 2400 memory.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1luz92k/thoughts_on_local_llm_proxmox_homelab_using/n21zqrn/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752007786,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1luz92k","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n21zwr2","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Super-Strategy893","can_mod_post":false,"created_utc":1752007831,"send_replies":true,"parent_id":"t3_1luz92k","score":3,"author_fullname":"t2_cfzcngu7","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"My home lab is made up of an MR9A x99 with Xeon E5 2697, 128GB of RAM and 2x RTX 3090.\\n\\n\\n\\nI do not recommend using dual Xeon boards. I have worked with these boards in scientific applications and they often lose performance when compared to a single CPU, due to the way data is transferred from one CPU to another. Another problem with them is the unstable BIOS. Many things that should be simple become very complex on these dual CPU boards.\\n\\n\\n\\nThe setup I use works without any problems. I already had a previous x79 setup that also worked very well with Proxmox and GPU passthrough.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n21zwr2","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;My home lab is made up of an MR9A x99 with Xeon E5 2697, 128GB of RAM and 2x RTX 3090.&lt;/p&gt;\\n\\n&lt;p&gt;I do not recommend using dual Xeon boards. I have worked with these boards in scientific applications and they often lose performance when compared to a single CPU, due to the way data is transferred from one CPU to another. Another problem with them is the unstable BIOS. Many things that should be simple become very complex on these dual CPU boards.&lt;/p&gt;\\n\\n&lt;p&gt;The setup I use works without any problems. I already had a previous x79 setup that also worked very well with Proxmox and GPU passthrough.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1luz92k/thoughts_on_local_llm_proxmox_homelab_using/n21zwr2/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752007831,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1luz92k","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n227bfi","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"HugoCortell","can_mod_post":false,"created_utc":1752009889,"send_replies":true,"parent_id":"t3_1luz92k","score":2,"author_fullname":"t2_61s8b5gv","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Personally, I'd recommend the **HUANANZHI X99 F8D PLUS**, it has DDR4 and 5 PCIe slots so that instead of buying 2 expensive 3090s, you can run 5 inexpensive \\\\~10GB mining cards that will perform just as well (since you'll be bottlenecked by the RAM to GPU transfer and not the GPU speed).\\n\\nIf I had $1000 to burn, that would be my choice, personally speaking.","edited":1752010765,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n227bfi","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Personally, I&amp;#39;d recommend the &lt;strong&gt;HUANANZHI X99 F8D PLUS&lt;/strong&gt;, it has DDR4 and 5 PCIe slots so that instead of buying 2 expensive 3090s, you can run 5 inexpensive ~10GB mining cards that will perform just as well (since you&amp;#39;ll be bottlenecked by the RAM to GPU transfer and not the GPU speed).&lt;/p&gt;\\n\\n&lt;p&gt;If I had $1000 to burn, that would be my choice, personally speaking.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1luz92k/thoughts_on_local_llm_proxmox_homelab_using/n227bfi/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752009889,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1luz92k","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n22c840","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"FullstackSensei","can_mod_post":false,"created_utc":1752011328,"send_replies":true,"parent_id":"t3_1luz92k","score":1,"author_fullname":"t2_17n3nqtj56","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I don't recommend any of these Chinese boards, X99 or otherwise. They're not cheap compared to used server boards from known brands like supermicro, Asrock (Rack), Gigabyte, or Asus. Those boards basically cannibalise broken boards for the chipsets and refurbish them onto new PCBs, but if you look at the reviews, they often lack the quality and features of server boards from reputable brands, and can suffer from BIOS instability issues.\\n\\nI suggest going for any of the myriad of dual 2011-3 boards from the well known brands. Even if they're 10 years old, they're made to a much higher standard, using much higher quality components (VRMs, capacitors, etc), and have much better support.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n22c840","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I don&amp;#39;t recommend any of these Chinese boards, X99 or otherwise. They&amp;#39;re not cheap compared to used server boards from known brands like supermicro, Asrock (Rack), Gigabyte, or Asus. Those boards basically cannibalise broken boards for the chipsets and refurbish them onto new PCBs, but if you look at the reviews, they often lack the quality and features of server boards from reputable brands, and can suffer from BIOS instability issues.&lt;/p&gt;\\n\\n&lt;p&gt;I suggest going for any of the myriad of dual 2011-3 boards from the well known brands. Even if they&amp;#39;re 10 years old, they&amp;#39;re made to a much higher standard, using much higher quality components (VRMs, capacitors, etc), and have much better support.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1luz92k/thoughts_on_local_llm_proxmox_homelab_using/n22c840/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752011328,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1luz92k","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n22hgun","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Longjumpingfish0403","can_mod_post":false,"created_utc":1752012902,"send_replies":true,"parent_id":"t3_1luz92k","score":1,"author_fullname":"t2_jarttha4","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"If you're new to LLM and considering Proxmox, it might be worth looking into server-grade boards from brands like Supermicro or Gigabyte instead of Chinese X99 boards. They usually offer better stability and support. Also, dual Xeon setups can sometimes cause performance hiccups due to data transfer between CPUs. Epyc Gen 2 with 8 memory channels could provide better performance for similar pricing, especially with Proxmox's virtualization demands. Just something to think about if future-proofing is a priority.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n22hgun","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;If you&amp;#39;re new to LLM and considering Proxmox, it might be worth looking into server-grade boards from brands like Supermicro or Gigabyte instead of Chinese X99 boards. They usually offer better stability and support. Also, dual Xeon setups can sometimes cause performance hiccups due to data transfer between CPUs. Epyc Gen 2 with 8 memory channels could provide better performance for similar pricing, especially with Proxmox&amp;#39;s virtualization demands. Just something to think about if future-proofing is a priority.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1luz92k/thoughts_on_local_llm_proxmox_homelab_using/n22hgun/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752012902,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1luz92k","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),r=()=>e.jsx(t,{data:l});export{r as default};
