import{j as e}from"./index-CeRg6Q3f.js";import{R as a}from"./RedditPostRenderer-D7n1g-D8.js";import"./index-DPToWe3n.js";const t=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Hi everyone,\\n\\nJust dropped our paper on a simple but effective approach that got us an 8.7% accuracy boost over baseline (58.4% vs 49.7%) and absolutely crushed GPT-4.1's zero-shot performance (32%) on emotion classification.\\n\\nThis tutorial comes in 3 different formats:\\n1. This LocalLLaMA post - summary and discussion\\n2. Our blog post - [Beating ChatGPT with a dollar and a dream](https://syv.ai/viden/beating-chatgpt-dollar-dream)\\n3. Our research paper - [Two-Stage Reasoning-Infused Learning: Improving Classification with LLM-Generated Reasoning](https://arxiv.org/abs/2507.00214)\\n\\nThe TL;DR: Instead of training models to just spit out labels, we taught a seperate model to output ONLY reasoning given a instruction and answer. We then use that reasoning to augment other datasets. Think chain-of-thought but generated by a model optimized to generate the reasoning.\\n\\nWhat we did:\\n\\nStage 1: Fine-tuned Llama-3.2-1B on a general reasoning dataset (350k examples) to create \\"Llama-R-Gen\\" - basically a reasoning generator that can take any (Question, Answer) pair and explain why that answer makes sense.\\n\\nStage 2: Used Llama-R-Gen to augment our emotion classification dataset by generating reasoning for each text-emotion pair. Then trained a downstream classifier to output reasoning + prediction in one go.\\n\\nKey results:\\n- 58.4% accuracy vs 49.7% baseline (statistically significant, p &lt; .001)\\n- Massive gains on sadness (+19.6%), fear (+18.2%), anger (+4.0%)\\n- Built-in interpretability - model explains its reasoning for every prediction\\n- Domain transfer works - reasoning learned from math/code/science transferred beautifully to emotion classification\\n\\nThe interesting bits:\\n\\nWhat worked:\\n- The reasoning generator trained on logical problems (math, code, science) transferred surprisingly well to the fuzzy world of emotion classification\\n- Models that \\"think out loud\\" during training seem to learn more robust representations\\n- Single model outputs both explanation and prediction - no separate explainability module needed\\n\\nWhat didn't:\\n- Completely collapsed on the \\"surprise\\" class (66 samples, 3.3% of data) - likely due to poor reasoning generation for severely underrepresented classes\\n- More computationally expensive than standard fine-tuning\\n- Quality heavily depends on the initial reasoning generator\\n\\nTechnical details:\\n- Base model: Llama-3.2-1B-Instruct (both stages)\\n- Reasoning dataset: [syvai/reasoning-gen](https://huggingface.co/datasets/syvai/reasoning-gen) (derived from Mixture-of-Thoughts)\\n- Target task: dair-ai/emotion (6 basic emotions)\\n- Training: Axolotl framework on A40 GPU\\n- Reasoning generator model: [syvai/reasoning-gen-1b](https://huggingface.co/syvai/reasoning-gen-1b)\\n- Datasets: [syvai/emotion-reasoning](https://huggingface.co/datasets/syvai/emotion-reasoning) and [syvai/no-emotion-reasoning](https://huggingface.co/datasets/syvai/no-emotion-reasoning)\\n\\nThe approach is pretty generalizable - we're thinking about applying it to other classification tasks where intermediate reasoning steps could help (NLI, QA, multi-label classification, etc.).","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Here is how we beat ChatGPT at classification with 1 dollar in cloud compute","link_flair_richtext":[{"e":"text","t":"Tutorial | Guide"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lvcb72","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.85,"author_flair_background_color":null,"subreddit_type":"public","ups":99,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_62puf","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Tutorial | Guide","can_mod_post":false,"score":99,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":1752045180,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"post_hint":"self","content_categories":null,"is_self":true,"mod_note":null,"created":1752044873,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;\\n\\n&lt;p&gt;Just dropped our paper on a simple but effective approach that got us an 8.7% accuracy boost over baseline (58.4% vs 49.7%) and absolutely crushed GPT-4.1&amp;#39;s zero-shot performance (32%) on emotion classification.&lt;/p&gt;\\n\\n&lt;p&gt;This tutorial comes in 3 different formats:\\n1. This LocalLLaMA post - summary and discussion\\n2. Our blog post - &lt;a href=\\"https://syv.ai/viden/beating-chatgpt-dollar-dream\\"&gt;Beating ChatGPT with a dollar and a dream&lt;/a&gt;\\n3. Our research paper - &lt;a href=\\"https://arxiv.org/abs/2507.00214\\"&gt;Two-Stage Reasoning-Infused Learning: Improving Classification with LLM-Generated Reasoning&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;The TL;DR: Instead of training models to just spit out labels, we taught a seperate model to output ONLY reasoning given a instruction and answer. We then use that reasoning to augment other datasets. Think chain-of-thought but generated by a model optimized to generate the reasoning.&lt;/p&gt;\\n\\n&lt;p&gt;What we did:&lt;/p&gt;\\n\\n&lt;p&gt;Stage 1: Fine-tuned Llama-3.2-1B on a general reasoning dataset (350k examples) to create &amp;quot;Llama-R-Gen&amp;quot; - basically a reasoning generator that can take any (Question, Answer) pair and explain why that answer makes sense.&lt;/p&gt;\\n\\n&lt;p&gt;Stage 2: Used Llama-R-Gen to augment our emotion classification dataset by generating reasoning for each text-emotion pair. Then trained a downstream classifier to output reasoning + prediction in one go.&lt;/p&gt;\\n\\n&lt;p&gt;Key results:\\n- 58.4% accuracy vs 49.7% baseline (statistically significant, p &amp;lt; .001)\\n- Massive gains on sadness (+19.6%), fear (+18.2%), anger (+4.0%)\\n- Built-in interpretability - model explains its reasoning for every prediction\\n- Domain transfer works - reasoning learned from math/code/science transferred beautifully to emotion classification&lt;/p&gt;\\n\\n&lt;p&gt;The interesting bits:&lt;/p&gt;\\n\\n&lt;p&gt;What worked:\\n- The reasoning generator trained on logical problems (math, code, science) transferred surprisingly well to the fuzzy world of emotion classification\\n- Models that &amp;quot;think out loud&amp;quot; during training seem to learn more robust representations\\n- Single model outputs both explanation and prediction - no separate explainability module needed&lt;/p&gt;\\n\\n&lt;p&gt;What didn&amp;#39;t:\\n- Completely collapsed on the &amp;quot;surprise&amp;quot; class (66 samples, 3.3% of data) - likely due to poor reasoning generation for severely underrepresented classes\\n- More computationally expensive than standard fine-tuning\\n- Quality heavily depends on the initial reasoning generator&lt;/p&gt;\\n\\n&lt;p&gt;Technical details:\\n- Base model: Llama-3.2-1B-Instruct (both stages)\\n- Reasoning dataset: &lt;a href=\\"https://huggingface.co/datasets/syvai/reasoning-gen\\"&gt;syvai/reasoning-gen&lt;/a&gt; (derived from Mixture-of-Thoughts)\\n- Target task: dair-ai/emotion (6 basic emotions)\\n- Training: Axolotl framework on A40 GPU\\n- Reasoning generator model: &lt;a href=\\"https://huggingface.co/syvai/reasoning-gen-1b\\"&gt;syvai/reasoning-gen-1b&lt;/a&gt;\\n- Datasets: &lt;a href=\\"https://huggingface.co/datasets/syvai/emotion-reasoning\\"&gt;syvai/emotion-reasoning&lt;/a&gt; and &lt;a href=\\"https://huggingface.co/datasets/syvai/no-emotion-reasoning\\"&gt;syvai/no-emotion-reasoning&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;The approach is pretty generalizable - we&amp;#39;re thinking about applying it to other classification tasks where intermediate reasoning steps could help (NLI, QA, multi-label classification, etc.).&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"preview":{"images":[{"source":{"url":"https://external-preview.redd.it/RxSSnT1e2v-RTp4_naQVkWAAFjrxq70GgL7g4G9qABA.png?auto=webp&amp;s=a9d24f583d7b2574603ae8d72c49b280f34bbbd4","width":965,"height":1386},"resolutions":[{"url":"https://external-preview.redd.it/RxSSnT1e2v-RTp4_naQVkWAAFjrxq70GgL7g4G9qABA.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=d3bc762d2895449456d8ae731ab05d4a9ff08669","width":108,"height":155},{"url":"https://external-preview.redd.it/RxSSnT1e2v-RTp4_naQVkWAAFjrxq70GgL7g4G9qABA.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=93346608db5cd935a59fa4d5a5eda50804747968","width":216,"height":310},{"url":"https://external-preview.redd.it/RxSSnT1e2v-RTp4_naQVkWAAFjrxq70GgL7g4G9qABA.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=d19468e5d7c0abac69fa5e7da790d84234eecbe8","width":320,"height":459},{"url":"https://external-preview.redd.it/RxSSnT1e2v-RTp4_naQVkWAAFjrxq70GgL7g4G9qABA.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=693b7aba21c7c17a6c3a895ec1f03306cc7e5a41","width":640,"height":919},{"url":"https://external-preview.redd.it/RxSSnT1e2v-RTp4_naQVkWAAFjrxq70GgL7g4G9qABA.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=375f73f70720f070c5e1baae29ec6e1e97a1bd85","width":960,"height":1378}],"variants":{},"id":"RxSSnT1e2v-RTp4_naQVkWAAFjrxq70GgL7g4G9qABA"}],"enabled":false},"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"449b05a6-bf8e-11ed-b4bd-66961e47bd50","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#0079d3","id":"1lvcb72","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"iamMess","discussion_type":null,"num_comments":42,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lvcb72/here_is_how_we_beat_chatgpt_at_classification/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lvcb72/here_is_how_we_beat_chatgpt_at_classification/","subreddit_subscribers":497355,"created_utc":1752044873,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2d607o","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Accomplished_Mode170","can_mod_post":false,"send_replies":true,"parent_id":"t1_n25yj7p","score":2,"author_fullname":"t2_4hfmiefj","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"This ^^^ tokens are state vectors that represent spline fitting by agents with their own logprobs","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2d607o","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;This &lt;sup&gt;&lt;sup&gt;^&lt;/sup&gt;&lt;/sup&gt; tokens are state vectors that represent spline fitting by agents with their own logprobs&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lvcb72","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lvcb72/here_is_how_we_beat_chatgpt_at_classification/n2d607o/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752156142,"author_flair_text":null,"treatment_tags":[],"created_utc":1752156142,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n25yj7p","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Mbando","can_mod_post":false,"created_utc":1752064733,"send_replies":true,"parent_id":"t1_n24wwd7","score":15,"author_fullname":"t2_kmzba","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"1. Vanilla BERT is 80-90% accurate on emotional classification. [Finetunes](https://huggingface.co/bhadresh-savani/distilbert-base-uncased-emotion) are in the 94% range.\\n\\n2. Important to remember that \\"reasoning\\" outputs (both intermediate and final) don't provide reliable explainability and lack fidelity.  They are discrete tokens that partially represent the internal representations of learned pathways inside the model.  Useful, but they are not [faithful](https://assets.anthropic.com/m/71876fabef0f0ed4/original/reasoning_models_paper.pdf) and not [explanation](https://arxiv.org/pdf/2505.23945).","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n25yj7p","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;ol&gt;\\n&lt;li&gt;&lt;p&gt;Vanilla BERT is 80-90% accurate on emotional classification. &lt;a href=\\"https://huggingface.co/bhadresh-savani/distilbert-base-uncased-emotion\\"&gt;Finetunes&lt;/a&gt; are in the 94% range.&lt;/p&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;p&gt;Important to remember that &amp;quot;reasoning&amp;quot; outputs (both intermediate and final) don&amp;#39;t provide reliable explainability and lack fidelity.  They are discrete tokens that partially represent the internal representations of learned pathways inside the model.  Useful, but they are not &lt;a href=\\"https://assets.anthropic.com/m/71876fabef0f0ed4/original/reasoning_models_paper.pdf\\"&gt;faithful&lt;/a&gt; and not &lt;a href=\\"https://arxiv.org/pdf/2505.23945\\"&gt;explanation&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;\\n&lt;/ol&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lvcb72","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lvcb72/here_is_how_we_beat_chatgpt_at_classification/n25yj7p/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752064733,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":15}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2d76j6","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Accomplished_Mode170","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2d6pdv","score":1,"author_fullname":"t2_4hfmiefj","approved_by":null,"mod_note":null,"all_awardings":[],"body":"[Source](https://arxiv.org/html/2402.09099v3)","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n2d76j6","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;&lt;a href=\\"https://arxiv.org/html/2402.09099v3\\"&gt;Source&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lvcb72","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lvcb72/here_is_how_we_beat_chatgpt_at_classification/n2d76j6/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752156493,"author_flair_text":null,"treatment_tags":[],"created_utc":1752156493,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2d6pdv","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Accomplished_Mode170","can_mod_post":false,"send_replies":true,"parent_id":"t1_n28nzqi","score":1,"author_fullname":"t2_4hfmiefj","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yes, because reasoning is spline fitting; some people just get the wrong corpus, pre-training, or goof the ICL üìä","edited":false,"author_flair_css_class":null,"name":"t1_n2d6pdv","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yes, because reasoning is spline fitting; some people just get the wrong corpus, pre-training, or goof the ICL üìä&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lvcb72","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lvcb72/here_is_how_we_beat_chatgpt_at_classification/n2d6pdv/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752156352,"author_flair_text":null,"collapsed":false,"created_utc":1752156352,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n28nzqi","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Pyros-SD-Models","can_mod_post":false,"send_replies":true,"parent_id":"t1_n279r3e","score":3,"author_fullname":"t2_ttxtnqqb","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Yes, there are other papers like those, but reading them would help. Just reading headlines is already questionable with news, but with papers it's on another level.\\n\\nNo paper outright says \\"You can never use the reasoning output of an LLM because they are trash\\"\\n\\nAryasomayajula R. Bharadwaj (2024), ‚ÄúUnderstanding Hidden Computations in Chain-of-Thought Reasoning‚Äù, has as its core claim:\\n\\n&gt;Can a Transformer trained to output filler tokens (\\"...\\") instead of an explicit chain-of-thought (CoT) still internally perform the multi-step reasoning required by a synthetic 3SUM-style task?\\n\\nAnd the answer:\\n\\n&gt;Yes; hidden reasoning tokens remain latent and can be recovered by a modified decoder.\\n\\nIt literally shows the exact opposite of what you're arguing.\\n\\nThe second one, Advait Sarkar (2024), ‚ÄúLarge Language Models Cannot Explain Themselves‚Äù, argues:\\n\\n&gt;Statements that LLMs generate when asked to ‚Äúexplain‚Äù their own answers are not faithful mechanism-level explanations but post-hoc rationalizations (\\"explanations\\"). They can mislead users and should be treated with guardrails.\\n\\nWhich does not mean that these post-hoc rationalizations are wrong.\\n\\nAll the papers are showing is that an LLM usually knows the answer instantly, and then generates the reasoning after the fact. That reasoning is still correct most of the time.\\n\\nNobody cares about if it's an illusion, as long as the illusion is correct","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n28nzqi","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yes, there are other papers like those, but reading them would help. Just reading headlines is already questionable with news, but with papers it&amp;#39;s on another level.&lt;/p&gt;\\n\\n&lt;p&gt;No paper outright says &amp;quot;You can never use the reasoning output of an LLM because they are trash&amp;quot;&lt;/p&gt;\\n\\n&lt;p&gt;Aryasomayajula R. Bharadwaj (2024), ‚ÄúUnderstanding Hidden Computations in Chain-of-Thought Reasoning‚Äù, has as its core claim:&lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;Can a Transformer trained to output filler tokens (&amp;quot;...&amp;quot;) instead of an explicit chain-of-thought (CoT) still internally perform the multi-step reasoning required by a synthetic 3SUM-style task?&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;And the answer:&lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;Yes; hidden reasoning tokens remain latent and can be recovered by a modified decoder.&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;It literally shows the exact opposite of what you&amp;#39;re arguing.&lt;/p&gt;\\n\\n&lt;p&gt;The second one, Advait Sarkar (2024), ‚ÄúLarge Language Models Cannot Explain Themselves‚Äù, argues:&lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;Statements that LLMs generate when asked to ‚Äúexplain‚Äù their own answers are not faithful mechanism-level explanations but post-hoc rationalizations (&amp;quot;explanations&amp;quot;). They can mislead users and should be treated with guardrails.&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;Which does not mean that these post-hoc rationalizations are wrong.&lt;/p&gt;\\n\\n&lt;p&gt;All the papers are showing is that an LLM usually knows the answer instantly, and then generates the reasoning after the fact. That reasoning is still correct most of the time.&lt;/p&gt;\\n\\n&lt;p&gt;Nobody cares about if it&amp;#39;s an illusion, as long as the illusion is correct&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lvcb72","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lvcb72/here_is_how_we_beat_chatgpt_at_classification/n28nzqi/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752092298,"author_flair_text":null,"treatment_tags":[],"created_utc":1752092298,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2efhl0","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"empirical-sadboy","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2blt9o","score":1,"author_fullname":"t2_rzuzjxcd","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"False equivalence","edited":false,"author_flair_css_class":null,"name":"t1_n2efhl0","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;False equivalence&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lvcb72","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lvcb72/here_is_how_we_beat_chatgpt_at_classification/n2efhl0/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752168918,"author_flair_text":null,"collapsed":false,"created_utc":1752168918,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2blt9o","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"davikrehalt","can_mod_post":false,"send_replies":true,"parent_id":"t1_n279r3e","score":1,"author_fullname":"t2_6okc6","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Eh but we ask humans to explain their actions anyway for similar reasons","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2blt9o","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Eh but we ask humans to explain their actions anyway for similar reasons&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lvcb72","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lvcb72/here_is_how_we_beat_chatgpt_at_classification/n2blt9o/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752130830,"author_flair_text":null,"treatment_tags":[],"created_utc":1752130830,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n279r3e","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"empirical-sadboy","can_mod_post":false,"send_replies":true,"parent_id":"t1_n24yk9x","score":8,"author_fullname":"t2_rzuzjxcd","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"But there is no explanation with an LLM? The natural language reasoning LLMs generate does not necessarily reflect the true \\"reasoning\\"  that happens in the hidden layers via matrix multiplication. It's an illusion of explanation. LLMs cannot introspect into their hidden layers just like people cannot accurately introspect about their brain's processes.\\n\\nhttps://arxiv.org/abs/2405.04382\\n\\nhttps://arxiv.org/abs/2412.04537\\n\\nhttps://assets.anthropic.com/m/71876fabef0f0ed4/original/reasoning_models_paper.pdf\\n\\nThere are other papers like this","edited":1752078779,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n279r3e","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;But there is no explanation with an LLM? The natural language reasoning LLMs generate does not necessarily reflect the true &amp;quot;reasoning&amp;quot;  that happens in the hidden layers via matrix multiplication. It&amp;#39;s an illusion of explanation. LLMs cannot introspect into their hidden layers just like people cannot accurately introspect about their brain&amp;#39;s processes.&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://arxiv.org/abs/2405.04382\\"&gt;https://arxiv.org/abs/2405.04382&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://arxiv.org/abs/2412.04537\\"&gt;https://arxiv.org/abs/2412.04537&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://assets.anthropic.com/m/71876fabef0f0ed4/original/reasoning_models_paper.pdf\\"&gt;https://assets.anthropic.com/m/71876fabef0f0ed4/original/reasoning_models_paper.pdf&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;There are other papers like this&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lvcb72","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lvcb72/here_is_how_we_beat_chatgpt_at_classification/n279r3e/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752078544,"author_flair_text":null,"treatment_tags":[],"created_utc":1752078544,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":8}}],"before":null}},"user_reports":[],"saved":false,"id":"n24yk9x","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"iamMess","can_mod_post":false,"created_utc":1752047810,"send_replies":true,"parent_id":"t1_n24wwd7","score":12,"author_fullname":"t2_62puf","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Also a possibility, and possibly better performance. It doesn‚Äôt provide the explainability though. \\n\\nOur reasoning gen model can also be used to augment other dataset with reasoning. For example, there is a big need for multi turn reasoning dataset, which currently (to my knowledge) does not exist.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n24yk9x","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Also a possibility, and possibly better performance. It doesn‚Äôt provide the explainability though. &lt;/p&gt;\\n\\n&lt;p&gt;Our reasoning gen model can also be used to augment other dataset with reasoning. For example, there is a big need for multi turn reasoning dataset, which currently (to my knowledge) does not exist.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lvcb72","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lvcb72/here_is_how_we_beat_chatgpt_at_classification/n24yk9x/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752047810,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":12}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2cafp1","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Mundane_Ad8936","can_mod_post":false,"send_replies":true,"parent_id":"t1_n29ib9l","score":1,"author_fullname":"t2_j26ktvd7","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Yeah that makes sense.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2cafp1","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yeah that makes sense.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lvcb72","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lvcb72/here_is_how_we_beat_chatgpt_at_classification/n2cafp1/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752144528,"author_flair_text":null,"treatment_tags":[],"created_utc":1752144528,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n29ib9l","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"SkyFeistyLlama8","can_mod_post":false,"send_replies":true,"parent_id":"t1_n25jb7i","score":2,"author_fullname":"t2_1hgbaqgbnq","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"BERT also doesn't do well with multilingual queries, either those in a non-English language entirely or those that mix multiple languages.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n29ib9l","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;BERT also doesn&amp;#39;t do well with multilingual queries, either those in a non-English language entirely or those that mix multiple languages.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lvcb72","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lvcb72/here_is_how_we_beat_chatgpt_at_classification/n29ib9l/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752101170,"author_flair_text":null,"treatment_tags":[],"created_utc":1752101170,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2dnj8s","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"richapeee","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2ca6lq","score":1,"author_fullname":"t2_2wzb0ndi","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"This sounds pretty interesting. Do you have any sources I can use to learn more about these deep insights?","edited":false,"author_flair_css_class":null,"name":"t1_n2dnj8s","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;This sounds pretty interesting. Do you have any sources I can use to learn more about these deep insights?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lvcb72","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lvcb72/here_is_how_we_beat_chatgpt_at_classification/n2dnj8s/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752161149,"author_flair_text":null,"collapsed":false,"created_utc":1752161149,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2ca6lq","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Mundane_Ad8936","can_mod_post":false,"send_replies":true,"parent_id":"t1_n29pt8o","score":2,"author_fullname":"t2_j26ktvd7","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"BERT sees \\"Logistics costs going up\\" and flags it - keywords match. But \\"Retail demand falling\\" needs you to think: less retail = less shipping = logistics companies screwed. BERT can't make that jump.\\n\\nEncoders like BERT are pattern matchers. They see text, match patterns, done. Decoders like Gemma actually reason through the chain - retail drops, so supply chains shrink, so logistics loses business.\\n\\nEncoder-decoders are stuck in the middle - better than BERT but still can't chain reasoning like pure decoders.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2ca6lq","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;BERT sees &amp;quot;Logistics costs going up&amp;quot; and flags it - keywords match. But &amp;quot;Retail demand falling&amp;quot; needs you to think: less retail = less shipping = logistics companies screwed. BERT can&amp;#39;t make that jump.&lt;/p&gt;\\n\\n&lt;p&gt;Encoders like BERT are pattern matchers. They see text, match patterns, done. Decoders like Gemma actually reason through the chain - retail drops, so supply chains shrink, so logistics loses business.&lt;/p&gt;\\n\\n&lt;p&gt;Encoder-decoders are stuck in the middle - better than BERT but still can&amp;#39;t chain reasoning like pure decoders.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lvcb72","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lvcb72/here_is_how_we_beat_chatgpt_at_classification/n2ca6lq/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752144408,"author_flair_text":null,"treatment_tags":[],"created_utc":1752144408,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n29pt8o","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Willing_Landscape_61","can_mod_post":false,"send_replies":true,"parent_id":"t1_n25jb7i","score":2,"author_fullname":"t2_8lvrytgw","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"\\". It's good at classifications based on what the text says not what it means.\\"\\nSorry, I don't understand what you mean.\\nWould mind expending on the distinction and how/ encoders, encoder-decoders and decoders only models differ on this?\\nThx.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n29pt8o","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;&amp;quot;. It&amp;#39;s good at classifications based on what the text says not what it means.&amp;quot;\\nSorry, I don&amp;#39;t understand what you mean.\\nWould mind expending on the distinction and how/ encoders, encoder-decoders and decoders only models differ on this?\\nThx.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lvcb72","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lvcb72/here_is_how_we_beat_chatgpt_at_classification/n29pt8o/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752103597,"author_flair_text":null,"treatment_tags":[],"created_utc":1752103597,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n25jb7i","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Mundane_Ad8936","can_mod_post":false,"created_utc":1752058817,"send_replies":false,"parent_id":"t1_n24wwd7","score":9,"author_fullname":"t2_j26ktvd7","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"BERT doesnt work if the classification reasoning requirement is to complex.  It's good at classifications based on what the text says not what it means. \\n\\nFor a (simplified) example both of these are risks for the logistics industry but one directly states it and the other it's implied if you use logic and reasoning. \\n\\nLogistics costs are going up. \\n\\nVS\\n\\nRetail demand is falling.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n25jb7i","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;BERT doesnt work if the classification reasoning requirement is to complex.  It&amp;#39;s good at classifications based on what the text says not what it means. &lt;/p&gt;\\n\\n&lt;p&gt;For a (simplified) example both of these are risks for the logistics industry but one directly states it and the other it&amp;#39;s implied if you use logic and reasoning. &lt;/p&gt;\\n\\n&lt;p&gt;Logistics costs are going up. &lt;/p&gt;\\n\\n&lt;p&gt;VS&lt;/p&gt;\\n\\n&lt;p&gt;Retail demand is falling.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lvcb72","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lvcb72/here_is_how_we_beat_chatgpt_at_classification/n25jb7i/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752058817,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":9}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"7d1f04e6-4920-11ef-b2e1-2e580594e1a1","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n25vzxo","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"asankhs","can_mod_post":false,"created_utc":1752063832,"send_replies":true,"parent_id":"t1_n24wwd7","score":2,"author_fullname":"t2_e0bph","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yeah, I was wondering the same, a Bert-style model will be better for classification. In fact you can even use an adaptive classifier [https://github.com/codelion/adaptive-classifier](https://github.com/codelion/adaptive-classifier) without fine-tuning for it.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n25vzxo","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"Llama 3.1"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yeah, I was wondering the same, a Bert-style model will be better for classification. In fact you can even use an adaptive classifier &lt;a href=\\"https://github.com/codelion/adaptive-classifier\\"&gt;https://github.com/codelion/adaptive-classifier&lt;/a&gt; without fine-tuning for it.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lvcb72","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lvcb72/here_is_how_we_beat_chatgpt_at_classification/n25vzxo/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752063832,"author_flair_text":"Llama 3.1","treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":"#93b1ba","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n27en8m","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"smahs9","can_mod_post":false,"created_utc":1752079879,"send_replies":true,"parent_id":"t1_n24wwd7","score":2,"author_fullname":"t2_neyagc1uz","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Bert is encoder only. T5 is an encoder decoder family. Decoder models may have their problems with these problem types, but they are not too far behind, especially with fine tuning, and have the tooling advantage on their side. Compare serving a llama with vllm versus serving a T5 torch model wrapped by transformers (though these models can be a great way to learn how to serve efficiently).","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n27en8m","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Bert is encoder only. T5 is an encoder decoder family. Decoder models may have their problems with these problem types, but they are not too far behind, especially with fine tuning, and have the tooling advantage on their side. Compare serving a llama with vllm versus serving a T5 torch model wrapped by transformers (though these models can be a great way to learn how to serve efficiently).&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lvcb72","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lvcb72/here_is_how_we_beat_chatgpt_at_classification/n27en8m/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752079879,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n24wwd7","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Willing_Landscape_61","can_mod_post":false,"created_utc":1752046866,"send_replies":true,"parent_id":"t3_1lvcb72","score":56,"author_fullname":"t2_8lvrytgw","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"For classification, why not use an encoder-decoder (e.g. BERT like) model ?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n24wwd7","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;For classification, why not use an encoder-decoder (e.g. BERT like) model ?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lvcb72/here_is_how_we_beat_chatgpt_at_classification/n24wwd7/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752046866,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lvcb72","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":56}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n25jf6z","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"iamMess","can_mod_post":false,"send_replies":true,"parent_id":"t1_n25j6zr","score":5,"author_fullname":"t2_62puf","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Qwen3 is also a great model. As mentioned previously, this is less about the performance and more about the method. If we went for full performance we would have chosen other models and probably also spent a lot more time improving the dataset.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n25jf6z","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Qwen3 is also a great model. As mentioned previously, this is less about the performance and more about the method. If we went for full performance we would have chosen other models and probably also spent a lot more time improving the dataset.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lvcb72","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lvcb72/here_is_how_we_beat_chatgpt_at_classification/n25jf6z/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752058866,"author_flair_text":null,"treatment_tags":[],"created_utc":1752058866,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n28p0ah","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Pro-editor-1105","can_mod_post":false,"send_replies":true,"parent_id":"t1_n25j6zr","score":1,"author_fullname":"t2_uptissiz","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I have trained qwen3 and llama3.2, 3.2, even though it was 3 vs 7b actually performed better at the task at hand because of how good llama models are to train.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n28p0ah","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I have trained qwen3 and llama3.2, 3.2, even though it was 3 vs 7b actually performed better at the task at hand because of how good llama models are to train.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lvcb72","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lvcb72/here_is_how_we_beat_chatgpt_at_classification/n28p0ah/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752092578,"author_flair_text":null,"treatment_tags":[],"created_utc":1752092578,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n25j6zr","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ExtremeAcceptable289","can_mod_post":false,"send_replies":true,"parent_id":"t1_n24un8s","score":3,"author_fullname":"t2_8hpbax1b","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Why not something like Qwen3 then which is newer and outperforms Llama?","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n25j6zr","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Why not something like Qwen3 then which is newer and outperforms Llama?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lvcb72","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lvcb72/here_is_how_we_beat_chatgpt_at_classification/n25j6zr/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752058767,"author_flair_text":null,"treatment_tags":[],"created_utc":1752058767,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n260dpr","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"iamMess","can_mod_post":false,"created_utc":1752065372,"send_replies":true,"parent_id":"t1_n25zy9h","score":2,"author_fullname":"t2_62puf","approved_by":null,"mod_note":null,"all_awardings":[],"body":"Then we would go over a dollar for computer üòÄ","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n260dpr","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Then we would go over a dollar for computer üòÄ&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lvcb72","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lvcb72/here_is_how_we_beat_chatgpt_at_classification/n260dpr/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752065372,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":7,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n25zy9h","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"segmond","can_mod_post":false,"send_replies":true,"parent_id":"t1_n25bs90","score":2,"author_fullname":"t2_ah13x","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"outside of performance, with it being so cheap, why not a 4b?","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n25zy9h","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;outside of performance, with it being so cheap, why not a 4b?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lvcb72","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lvcb72/here_is_how_we_beat_chatgpt_at_classification/n25zy9h/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752065226,"author_flair_text":"llama.cpp","treatment_tags":[],"created_utc":1752065226,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":6,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n25bs90","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"iamMess","can_mod_post":false,"send_replies":true,"parent_id":"t1_n250gxx","score":1,"author_fullname":"t2_62puf","approved_by":null,"mod_note":null,"all_awardings":[],"body":"Will do :)","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n25bs90","is_submitter":true,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Will do :)&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lvcb72","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lvcb72/here_is_how_we_beat_chatgpt_at_classification/n25bs90/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752055255,"author_flair_text":null,"treatment_tags":[],"created_utc":1752055255,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n250gxx","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"dreamai87","can_mod_post":false,"send_replies":true,"parent_id":"t1_n24ve00","score":2,"author_fullname":"t2_q1q7nlxa","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Try Gemma 1b as well","edited":false,"author_flair_css_class":null,"name":"t1_n250gxx","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Try Gemma 1b as well&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lvcb72","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lvcb72/here_is_how_we_beat_chatgpt_at_classification/n250gxx/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752048893,"author_flair_text":null,"collapsed":false,"created_utc":1752048893,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n24ve00","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"iamMess","can_mod_post":false,"send_replies":true,"parent_id":"t1_n24v405","score":4,"author_fullname":"t2_62puf","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Yeah. We‚Äôre also working on a better TTS and STT model using llama3 as a base model. We‚Äôve considered using Qwen, but they are not as multilingual as the llama models.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n24ve00","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yeah. We‚Äôre also working on a better TTS and STT model using llama3 as a base model. We‚Äôve considered using Qwen, but they are not as multilingual as the llama models.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lvcb72","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lvcb72/here_is_how_we_beat_chatgpt_at_classification/n24ve00/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752046000,"author_flair_text":null,"treatment_tags":[],"created_utc":1752046000,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}}],"before":null}},"user_reports":[],"saved":false,"id":"n24v405","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Apart_Boat9666","can_mod_post":false,"send_replies":true,"parent_id":"t1_n24un8s","score":1,"author_fullname":"t2_13wer47gox","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Got it, I was seeing a lot of TTS and other models were using Llama 3 in 2025.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n24v405","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Got it, I was seeing a lot of TTS and other models were using Llama 3 in 2025.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lvcb72","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lvcb72/here_is_how_we_beat_chatgpt_at_classification/n24v405/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752045846,"author_flair_text":null,"treatment_tags":[],"created_utc":1752045846,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n24un8s","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"iamMess","can_mod_post":false,"created_utc":1752045582,"send_replies":true,"parent_id":"t1_n24u6lf","score":24,"author_fullname":"t2_62puf","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"We used LLaMA because they are well supported and easy to train. I'm certain that using SOTA models would improve performance, but it would cost us a lot more if we need to train a 600b model than 1b model.   \\n  \\nAlso this is more about the method than the actual performance. It can easily be scaled by changing the model to a better one :)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n24un8s","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;We used LLaMA because they are well supported and easy to train. I&amp;#39;m certain that using SOTA models would improve performance, but it would cost us a lot more if we need to train a 600b model than 1b model.   &lt;/p&gt;\\n\\n&lt;p&gt;Also this is more about the method than the actual performance. It can easily be scaled by changing the model to a better one :)&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lvcb72","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lvcb72/here_is_how_we_beat_chatgpt_at_classification/n24un8s/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752045582,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":24}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n25ncg5","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"RMCPhoto","can_mod_post":false,"created_utc":1752060542,"send_replies":true,"parent_id":"t1_n24u6lf","score":2,"author_fullname":"t2_ehhvb","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"They are easy and cheap to train with predictable results.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n25ncg5","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;They are easy and cheap to train with predictable results.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lvcb72","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lvcb72/here_is_how_we_beat_chatgpt_at_classification/n25ncg5/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752060542,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n24u6lf","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Apart_Boat9666","can_mod_post":false,"created_utc":1752045327,"send_replies":true,"parent_id":"t3_1lvcb72","score":20,"author_fullname":"t2_13wer47gox","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I have a question: Why do most people use Llama models as a base model? If state-of-the-art (SOTA) models were used instead, would that not increase performance?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n24u6lf","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I have a question: Why do most people use Llama models as a base model? If state-of-the-art (SOTA) models were used instead, would that not increase performance?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lvcb72/here_is_how_we_beat_chatgpt_at_classification/n24u6lf/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752045327,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lvcb72","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":20}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n26gjtl","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"iamMess","can_mod_post":false,"created_utc":1752070428,"send_replies":true,"parent_id":"t1_n26gh7n","score":2,"author_fullname":"t2_62puf","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"üòÇ","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n26gjtl","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;üòÇ&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lvcb72","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lvcb72/here_is_how_we_beat_chatgpt_at_classification/n26gjtl/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752070428,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n26gh7n","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"RunningMidget","can_mod_post":false,"created_utc":1752070407,"send_replies":true,"parent_id":"t3_1lvcb72","score":9,"author_fullname":"t2_8py80","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"&gt;Massive gains on sadness \\n\\nMe_irl","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n26gh7n","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;Massive gains on sadness &lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;Me_irl&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lvcb72/here_is_how_we_beat_chatgpt_at_classification/n26gh7n/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752070407,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lvcb72","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":9}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n25i1xb","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"iamMess","can_mod_post":false,"created_utc":1752058272,"send_replies":true,"parent_id":"t1_n25dizt","score":3,"author_fullname":"t2_62puf","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"That is true. A more nuanced baseline might have been asking it to CoT then provide answer.\\n\\nTo be honest I don't think it will improve much. The original emotion dataset is very hard even for humans.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n25i1xb","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;That is true. A more nuanced baseline might have been asking it to CoT then provide answer.&lt;/p&gt;\\n\\n&lt;p&gt;To be honest I don&amp;#39;t think it will improve much. The original emotion dataset is very hard even for humans.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lvcb72","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lvcb72/here_is_how_we_beat_chatgpt_at_classification/n25i1xb/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752058272,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n25dizt","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"xmBQWugdxjaA","can_mod_post":false,"created_utc":1752056143,"send_replies":true,"parent_id":"t3_1lvcb72","score":3,"author_fullname":"t2_nyyscwdgr","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Isn't there an issue that the baseline downstream classifier without reasoning literally can't do as much processing as the reasoning case since its token output is so constrained in comparison?\\n\\nI wonder how they would compare (providing the reasoning and not) if the downstream classifier itself were already a reasoning model like DeepSeek R1 (so both cases could output intermediate thinking tokens for more processing) ?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n25dizt","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Isn&amp;#39;t there an issue that the baseline downstream classifier without reasoning literally can&amp;#39;t do as much processing as the reasoning case since its token output is so constrained in comparison?&lt;/p&gt;\\n\\n&lt;p&gt;I wonder how they would compare (providing the reasoning and not) if the downstream classifier itself were already a reasoning model like DeepSeek R1 (so both cases could output intermediate thinking tokens for more processing) ?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lvcb72/here_is_how_we_beat_chatgpt_at_classification/n25dizt/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752056143,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lvcb72","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n25inwp","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Qual_","can_mod_post":false,"created_utc":1752058538,"send_replies":true,"parent_id":"t3_1lvcb72","score":2,"author_fullname":"t2_c3ca7","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I once tried to do this with gemma, and from the results gemma got a lot of incorrect classification (way less than a BERT model trained on the dataset, then I looked at the dataset, it was shit. Like it felt like the dataset was generated with GPT 2. And the \\"errors\\" of gemma were actually correct. \\n\\nhttps://preview.redd.it/b5k4eu1gxtbf1.png?width=1276&amp;format=png&amp;auto=webp&amp;s=367b533da3a2437c009d176df770d47bdadc441d","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n25inwp","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I once tried to do this with gemma, and from the results gemma got a lot of incorrect classification (way less than a BERT model trained on the dataset, then I looked at the dataset, it was shit. Like it felt like the dataset was generated with GPT 2. And the &amp;quot;errors&amp;quot; of gemma were actually correct. &lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://preview.redd.it/b5k4eu1gxtbf1.png?width=1276&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=367b533da3a2437c009d176df770d47bdadc441d\\"&gt;https://preview.redd.it/b5k4eu1gxtbf1.png?width=1276&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=367b533da3a2437c009d176df770d47bdadc441d&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lvcb72/here_is_how_we_beat_chatgpt_at_classification/n25inwp/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752058538,"media_metadata":{"b5k4eu1gxtbf1":{"status":"valid","e":"Image","m":"image/png","p":[{"y":11,"x":108,"u":"https://preview.redd.it/b5k4eu1gxtbf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=6bc4e28516851db3e2d21cef7e6cdb788e936c79"},{"y":22,"x":216,"u":"https://preview.redd.it/b5k4eu1gxtbf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=e934c2bd2bdae30c1f736b7353f766b4fb279dfd"},{"y":33,"x":320,"u":"https://preview.redd.it/b5k4eu1gxtbf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=b5f53e4933babca75a307610b326de89e8284bb1"},{"y":67,"x":640,"u":"https://preview.redd.it/b5k4eu1gxtbf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=841e2e1ecdc1e146069d9548a977f1e3a286fea7"},{"y":101,"x":960,"u":"https://preview.redd.it/b5k4eu1gxtbf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=1ec99ccb7dc4fcef5b2932751c12373844d99cd3"},{"y":114,"x":1080,"u":"https://preview.redd.it/b5k4eu1gxtbf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=2132b02b3e222ff825af381ee7a92417699e3d4b"}],"s":{"y":135,"x":1276,"u":"https://preview.redd.it/b5k4eu1gxtbf1.png?width=1276&amp;format=png&amp;auto=webp&amp;s=367b533da3a2437c009d176df770d47bdadc441d"},"id":"b5k4eu1gxtbf1"}},"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lvcb72","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n25shu4","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Mbando","can_mod_post":false,"created_utc":1752062553,"send_replies":true,"parent_id":"t3_1lvcb72","score":2,"author_fullname":"t2_kmzba","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Thanks for sharing this‚Äîit's genuinely interesting. Two points I‚Äôd like to clarify: \\n\\nFirst, while it seems surprising or intriguing that a reasoning dataset from culturally \\"hard\\" logical domains transfers so well to something culturally seen as \\"soft\\" like emotional data, from an ML perspective it makes perfect sense. All these tasks‚Äîwhether math, coding, or emotion labeling‚Äîprovide reward-dense, verifiable signals, making them suitable for supervised learning via gradient descent. Ultimately, the neural network is  minimizing loss as it maps input tokens to output tokens.\\n\\nSecond, it‚Äôs important to highlight that this isn't ‚Äúreasoning‚Äù in the sense of reproducible processes from first principles. A broad body of literature shows that while intermediate reasoning trace output from large language models improve performance, they lack [fidelity](https://arxiv.org/pdf/2506.06971)‚Äîthey are not reliable explanations of the underlying decision-making. Rather, these  reasoning outputs are best understood as discrete tokens partially reflecting complex, continuous, high-dimensional vectors near the model‚Äôs output layer. Instead of interpreting these outputs like human logical arguments or proofs, we should view them as sequences in token space, capturing patterns of internal loss optimization within the model.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n25shu4","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thanks for sharing this‚Äîit&amp;#39;s genuinely interesting. Two points I‚Äôd like to clarify: &lt;/p&gt;\\n\\n&lt;p&gt;First, while it seems surprising or intriguing that a reasoning dataset from culturally &amp;quot;hard&amp;quot; logical domains transfers so well to something culturally seen as &amp;quot;soft&amp;quot; like emotional data, from an ML perspective it makes perfect sense. All these tasks‚Äîwhether math, coding, or emotion labeling‚Äîprovide reward-dense, verifiable signals, making them suitable for supervised learning via gradient descent. Ultimately, the neural network is  minimizing loss as it maps input tokens to output tokens.&lt;/p&gt;\\n\\n&lt;p&gt;Second, it‚Äôs important to highlight that this isn&amp;#39;t ‚Äúreasoning‚Äù in the sense of reproducible processes from first principles. A broad body of literature shows that while intermediate reasoning trace output from large language models improve performance, they lack &lt;a href=\\"https://arxiv.org/pdf/2506.06971\\"&gt;fidelity&lt;/a&gt;‚Äîthey are not reliable explanations of the underlying decision-making. Rather, these  reasoning outputs are best understood as discrete tokens partially reflecting complex, continuous, high-dimensional vectors near the model‚Äôs output layer. Instead of interpreting these outputs like human logical arguments or proofs, we should view them as sequences in token space, capturing patterns of internal loss optimization within the model.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lvcb72/here_is_how_we_beat_chatgpt_at_classification/n25shu4/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752062553,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lvcb72","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n27b0ly","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"empirical-sadboy","can_mod_post":false,"created_utc":1752078892,"send_replies":true,"parent_id":"t3_1lvcb72","score":1,"author_fullname":"t2_rzuzjxcd","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Would love to see a comparison to a fine-tune encoder-only model like a BERT.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n27b0ly","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Would love to see a comparison to a fine-tune encoder-only model like a BERT.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lvcb72/here_is_how_we_beat_chatgpt_at_classification/n27b0ly/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752078892,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lvcb72","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ff54b802-c910-11ed-be9d-ea867d8afa86","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n28yha7","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"s_arme","can_mod_post":false,"created_utc":1752095162,"send_replies":true,"parent_id":"t3_1lvcb72","score":1,"author_fullname":"t2_69i5g4ss","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"How do you manage to host the inference economically?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n28yha7","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"Llama 33B"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;How do you manage to host the inference economically?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lvcb72/here_is_how_we_beat_chatgpt_at_classification/n28yha7/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752095162,"author_flair_text":"Llama 33B","treatment_tags":[],"link_id":"t3_1lvcb72","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2b0dj4","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"jazir5","can_mod_post":false,"send_replies":true,"parent_id":"t1_n255vqg","score":1,"author_fullname":"t2_8u27g","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Have you tried a pair of having one model do only reasoning and then use a zero shot model like DeepSeek v3 to play the other role? Or even further chains, 2 reasoning loops then the 3rd is a zero shot.\\n\\nThat way it would be able to see whether reasoning performance using a different model for reasoning independently improves a non-tuned zero shot model's performance.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2b0dj4","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Have you tried a pair of having one model do only reasoning and then use a zero shot model like DeepSeek v3 to play the other role? Or even further chains, 2 reasoning loops then the 3rd is a zero shot.&lt;/p&gt;\\n\\n&lt;p&gt;That way it would be able to see whether reasoning performance using a different model for reasoning independently improves a non-tuned zero shot model&amp;#39;s performance.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lvcb72","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lvcb72/here_is_how_we_beat_chatgpt_at_classification/n2b0dj4/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752120358,"author_flair_text":null,"treatment_tags":[],"created_utc":1752120358,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n255vqg","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"iamMess","can_mod_post":false,"created_utc":1752052021,"send_replies":true,"parent_id":"t1_n2514yq","score":3,"author_fullname":"t2_62puf","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"We tried your method, but it doesn‚Äôt really work. Rather it thinks about the instruction you gave it, which we do not want. \\n\\nYes, the model is small and the reasoning is complex, but we still see a decent improvement. We also mention in the paper that using a larger model would probably yield better results.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n255vqg","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;We tried your method, but it doesn‚Äôt really work. Rather it thinks about the instruction you gave it, which we do not want. &lt;/p&gt;\\n\\n&lt;p&gt;Yes, the model is small and the reasoning is complex, but we still see a decent improvement. We also mention in the paper that using a larger model would probably yield better results.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lvcb72","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lvcb72/here_is_how_we_beat_chatgpt_at_classification/n255vqg/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752052021,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n2514yq","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Chromix_","can_mod_post":false,"created_utc":1752049276,"send_replies":true,"parent_id":"t3_1lvcb72","score":1,"author_fullname":"t2_k7w2h","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"&gt;we taught a seperate model to output ONLY reasoning given a instruction and answer\\n\\nWhat was that step needed for? Fine-tuning costs (a dollar). Couldn't you have simply taken Qwen3, asked something like \\"Evaluate in detail whether the answer is correct\\" and used \\"&lt;/think&gt;\\" as stop token to get exactly what you needed?\\n\\nTraining reasoning format on a code, math and science dataset and then using that to reason over emotions puts a lot of faith in the generalization ability of the LLM. Also, wasn't a 1B model rather small for such lengthy, complex reasoning?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2514yq","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;we taught a seperate model to output ONLY reasoning given a instruction and answer&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;What was that step needed for? Fine-tuning costs (a dollar). Couldn&amp;#39;t you have simply taken Qwen3, asked something like &amp;quot;Evaluate in detail whether the answer is correct&amp;quot; and used &amp;quot;&amp;lt;/think&amp;gt;&amp;quot; as stop token to get exactly what you needed?&lt;/p&gt;\\n\\n&lt;p&gt;Training reasoning format on a code, math and science dataset and then using that to reason over emotions puts a lot of faith in the generalization ability of the LLM. Also, wasn&amp;#39;t a 1B model rather small for such lengthy, complex reasoning?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lvcb72/here_is_how_we_beat_chatgpt_at_classification/n2514yq/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752049276,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lvcb72","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),o=()=>e.jsx(a,{data:t});export{o as default};
