import{j as e}from"./index-BgwOAK4-.js";import{R as l}from"./RedditPostRenderer-BOBjDTFu.js";import"./index-BL22wVg5.js";const t=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Don't rush out and buy that 5090TI just yet (if you can even find one lol)!\\n\\nI just inferenced \\\\~2.13 tok/sec with 2k context using a dynamic quant of the full R1 671B model (not a distill) after *disabling* my 3090TI GPU on a 96GB RAM gaming rig. The secret trick is to *not* load anything but kv cache into RAM and let \`llama.cpp\` use its default behavior to \`mmap()\` the model files off of a fast NVMe SSD. The rest of your system RAM acts as disk cache for the active weights.\\n\\nYesterday a bunch of folks got the dynamic quant flavors of \`unsloth/DeepSeek-R1-GGUF\` running on gaming rigs in another thread here. I myself got the \`DeepSeek-R1-UD-Q2_K_XL\` flavor going between 1\\\\~2 toks/sec and 2k\\\\~16k context on 96GB RAM + 24GB VRAM experimenting with context length and up to 8 concurrent slots inferencing for increased aggregate throuput.\\n\\nAfter experimenting with various setups, the bottle neck is clearly my Gen 5 x4 NVMe SSD card as the CPU doesn't go over \\\\~30%, the GPU was basically idle, and the power supply fan doesn't even come on. So while slow, it isn't heating up the room.\\n\\nSo instead of a $2k GPU what about $1.5k for 4x NVMe SSDs on an expansion card for 2TB \\"VRAM\\" giving theoretical max sequential read \\"memory\\" bandwidth of \\\\~48GB/s? This less expensive setup would likely give better price/performance for big MoEs on home rigs. If you forgo a GPU, you could have 16 lanes of PCIe 5.0 all for NVMe drives on gamer class motherboards.\\n\\nIf anyone has a fast read IOPs drive array, I'd love to hear what kind of speeds you can get. I gotta bug Wendell over at Level1Techs lol...\\n\\nP.S. In my opinion this quantized R1 671B beats the pants off any of the distill model toys. While slow and limited in context, it is still likely the best thing available for home users for many applications.\\n\\nJust need to figure out how to short circuit the \`&lt;think&gt;Blah blah&lt;/think&gt;\` stuff by injecting a \`&lt;/think&gt;\` into the assistant prompt to see if it gives decent results without all the yapping haha...","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"DeepSeek R1 671B over 2 tok/sec *without* GPU on local gaming rig!","link_flair_richtext":[{"e":"text","t":"Discussion"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1idseqb","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.99,"author_flair_background_color":"#bbbdbf","subreddit_type":"public","ups":1323,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","is_original_content":false,"author_fullname":"t2_n321yfw5","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Discussion","can_mod_post":false,"score":1323,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1738258384,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"richtext","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Don&amp;#39;t rush out and buy that 5090TI just yet (if you can even find one lol)!&lt;/p&gt;\\n\\n&lt;p&gt;I just inferenced ~2.13 tok/sec with 2k context using a dynamic quant of the full R1 671B model (not a distill) after &lt;em&gt;disabling&lt;/em&gt; my 3090TI GPU on a 96GB RAM gaming rig. The secret trick is to &lt;em&gt;not&lt;/em&gt; load anything but kv cache into RAM and let &lt;code&gt;llama.cpp&lt;/code&gt; use its default behavior to &lt;code&gt;mmap()&lt;/code&gt; the model files off of a fast NVMe SSD. The rest of your system RAM acts as disk cache for the active weights.&lt;/p&gt;\\n\\n&lt;p&gt;Yesterday a bunch of folks got the dynamic quant flavors of &lt;code&gt;unsloth/DeepSeek-R1-GGUF&lt;/code&gt; running on gaming rigs in another thread here. I myself got the &lt;code&gt;DeepSeek-R1-UD-Q2_K_XL&lt;/code&gt; flavor going between 1~2 toks/sec and 2k~16k context on 96GB RAM + 24GB VRAM experimenting with context length and up to 8 concurrent slots inferencing for increased aggregate throuput.&lt;/p&gt;\\n\\n&lt;p&gt;After experimenting with various setups, the bottle neck is clearly my Gen 5 x4 NVMe SSD card as the CPU doesn&amp;#39;t go over ~30%, the GPU was basically idle, and the power supply fan doesn&amp;#39;t even come on. So while slow, it isn&amp;#39;t heating up the room.&lt;/p&gt;\\n\\n&lt;p&gt;So instead of a $2k GPU what about $1.5k for 4x NVMe SSDs on an expansion card for 2TB &amp;quot;VRAM&amp;quot; giving theoretical max sequential read &amp;quot;memory&amp;quot; bandwidth of ~48GB/s? This less expensive setup would likely give better price/performance for big MoEs on home rigs. If you forgo a GPU, you could have 16 lanes of PCIe 5.0 all for NVMe drives on gamer class motherboards.&lt;/p&gt;\\n\\n&lt;p&gt;If anyone has a fast read IOPs drive array, I&amp;#39;d love to hear what kind of speeds you can get. I gotta bug Wendell over at Level1Techs lol...&lt;/p&gt;\\n\\n&lt;p&gt;P.S. In my opinion this quantized R1 671B beats the pants off any of the distill model toys. While slow and limited in context, it is still likely the best thing available for home users for many applications.&lt;/p&gt;\\n\\n&lt;p&gt;Just need to figure out how to short circuit the &lt;code&gt;&amp;lt;think&amp;gt;Blah blah&amp;lt;/think&amp;gt;&lt;/code&gt; stuff by injecting a &lt;code&gt;&amp;lt;/think&amp;gt;&lt;/code&gt; into the assistant prompt to see if it gives decent results without all the yapping haha...&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"5f921ea4-c7bc-11ed-9c23-3a00622979b4","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":"llama.cpp","treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#646d73","id":"1idseqb","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"VoidAlchemy","discussion_type":null,"num_comments":319,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":"light","permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/","subreddit_subscribers":492315,"created_utc":1738258384,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"more","data":{"count":3,"name":"t1_mbm2gpt","id":"mbm2gpt","parent_id":"t1_ma8o478","depth":4,"children":["mbm2gpt","me1lv8r"]}}],"before":null}},"user_reports":[],"saved":false,"id":"ma8o478","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"fallingdowndizzyvr","can_mod_post":false,"send_replies":true,"parent_id":"t1_ma6zw0v","score":17,"author_fullname":"t2_o65i6kx","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"And people will still miss what the important thing is. It's not the SSD. So all the talk about setting up RAID SSD arrays in this thread misses the point. It's the 96GB of RAM. Which is used as a big cache for the SSD. If you don't have that, say only 32GB of RAM. The performance tanks precipitously. So what people should do instead of spending hundreds on SSD arrays, is to spend that money on getting more RAM.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_ma8o478","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;And people will still miss what the important thing is. It&amp;#39;s not the SSD. So all the talk about setting up RAID SSD arrays in this thread misses the point. It&amp;#39;s the 96GB of RAM. Which is used as a big cache for the SSD. If you don&amp;#39;t have that, say only 32GB of RAM. The performance tanks precipitously. So what people should do instead of spending hundreds on SSD arrays, is to spend that money on getting more RAM.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1idseqb","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma8o478/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738349074,"author_flair_text":null,"treatment_tags":[],"created_utc":1738349074,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":17}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"maaa490","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"VoidAlchemy","can_mod_post":false,"send_replies":true,"parent_id":"t1_ma6zw0v","score":2,"author_fullname":"t2_n321yfw5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Keep'm coming! [https://www.reddit.com/r/LocalLLaMA/comments/1ieosbx/relatively\\\\_budget\\\\_671b\\\\_r1\\\\_cpu\\\\_inference/](https://www.reddit.com/r/LocalLLaMA/comments/1ieosbx/relatively_budget_671b_r1_cpu_inference/)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_maaa490","is_submitter":true,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Keep&amp;#39;m coming! &lt;a href=\\"https://www.reddit.com/r/LocalLLaMA/comments/1ieosbx/relatively_budget_671b_r1_cpu_inference/\\"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1ieosbx/relatively_budget_671b_r1_cpu_inference/&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1idseqb","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/maaa490/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738365889,"author_flair_text":"llama.cpp","treatment_tags":[],"created_utc":1738365889,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"ma6zw0v","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"MoffKalast","can_mod_post":false,"send_replies":true,"parent_id":"t1_ma1veai","score":18,"author_fullname":"t2_d2nyh","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"And it will be posted again.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_ma6zw0v","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;And it will be posted again.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1idseqb","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma6zw0v/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738331389,"author_flair_text":null,"treatment_tags":[],"created_utc":1738331389,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":18}}],"before":null}},"user_reports":[],"saved":false,"id":"ma1veai","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"fallingdowndizzyvr","can_mod_post":false,"created_utc":1738261316,"send_replies":true,"parent_id":"t1_ma1tpgm","score":62,"author_fullname":"t2_o65i6kx","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"The same has been posted before.\\n\\nhttps://www.reddit.com/r/LocalLLaMA/comments/1iczucy/running_deepseek_r1_iq2xxs_200gb_from_ssd/","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_ma1veai","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The same has been posted before.&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://www.reddit.com/r/LocalLLaMA/comments/1iczucy/running_deepseek_r1_iq2xxs_200gb_from_ssd/\\"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1iczucy/running_deepseek_r1_iq2xxs_200gb_from_ssd/&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1idseqb","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma1veai/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738261316,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":62}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"more","data":{"count":3,"name":"t1_mbukuer","id":"mbukuer","parent_id":"t1_mbrrr3a","depth":3,"children":["mbukuer"]}}],"before":null}},"user_reports":[],"saved":false,"id":"mbrrr3a","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Flan-Sudden","can_mod_post":false,"send_replies":true,"parent_id":"t1_ma4sy4t","score":2,"author_fullname":"t2_796vhh5a","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Thanks for the details. That's crazy, I barley get 2.7t/s on IQ1\\\\_M on ddr5 5000 with 4 sticks on my 13900k with no-mmap (all in ram).","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_mbrrr3a","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thanks for the details. That&amp;#39;s crazy, I barley get 2.7t/s on IQ1_M on ddr5 5000 with 4 sticks on my 13900k with no-mmap (all in ram).&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1idseqb","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/mbrrr3a/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1739068708,"author_flair_text":null,"treatment_tags":[],"created_utc":1739068708,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"ma4sy4t","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"VoidAlchemy","can_mod_post":false,"created_utc":1738292939,"send_replies":true,"parent_id":"t1_ma1tpgm","score":52,"author_fullname":"t2_n321yfw5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"haha thanks, i was surprised by the response and glad there are so many scrappy intrepid hackers around here.\\n\\nI took some better notes and just updated the gist with some benchmarks.\\n\\n| \`ctx-size\` | \`n-gpu-layers\` | \`expert_used_count\` | \`Cached high water mark (GiB)\`| \`generation (tok/sec)\` |\\n| --- | --- | --- | --- | --- |\\n| 2048 | 5 | 8 | ~82 | 1.45 |\\n| 2048 | 5 | 4 | ~82 | 2.28 |\\n| 2048 | 0 | 8 | ~82 | 1.28 |\\n| 2048 | 0 | 4 | ~82 | 2.20 |\\n| 8192 | 5 | 8 | ~67 | 1.25 |\\n| 8192 | 5 | 4 | ~67 | 2.12 |\\n| 8192 | 0 | 8 | ~66 | 1.10 |\\n| 8192 | 0 | 4 | ~66 | 1.81 |","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_ma4sy4t","is_submitter":true,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;haha thanks, i was surprised by the response and glad there are so many scrappy intrepid hackers around here.&lt;/p&gt;\\n\\n&lt;p&gt;I took some better notes and just updated the gist with some benchmarks.&lt;/p&gt;\\n\\n&lt;table&gt;&lt;thead&gt;\\n&lt;tr&gt;\\n&lt;th&gt;&lt;code&gt;ctx-size&lt;/code&gt;&lt;/th&gt;\\n&lt;th&gt;&lt;code&gt;n-gpu-layers&lt;/code&gt;&lt;/th&gt;\\n&lt;th&gt;&lt;code&gt;expert_used_count&lt;/code&gt;&lt;/th&gt;\\n&lt;th&gt;&lt;code&gt;Cached high water mark (GiB)&lt;/code&gt;&lt;/th&gt;\\n&lt;th&gt;&lt;code&gt;generation (tok/sec)&lt;/code&gt;&lt;/th&gt;\\n&lt;/tr&gt;\\n&lt;/thead&gt;&lt;tbody&gt;\\n&lt;tr&gt;\\n&lt;td&gt;2048&lt;/td&gt;\\n&lt;td&gt;5&lt;/td&gt;\\n&lt;td&gt;8&lt;/td&gt;\\n&lt;td&gt;~82&lt;/td&gt;\\n&lt;td&gt;1.45&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;tr&gt;\\n&lt;td&gt;2048&lt;/td&gt;\\n&lt;td&gt;5&lt;/td&gt;\\n&lt;td&gt;4&lt;/td&gt;\\n&lt;td&gt;~82&lt;/td&gt;\\n&lt;td&gt;2.28&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;tr&gt;\\n&lt;td&gt;2048&lt;/td&gt;\\n&lt;td&gt;0&lt;/td&gt;\\n&lt;td&gt;8&lt;/td&gt;\\n&lt;td&gt;~82&lt;/td&gt;\\n&lt;td&gt;1.28&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;tr&gt;\\n&lt;td&gt;2048&lt;/td&gt;\\n&lt;td&gt;0&lt;/td&gt;\\n&lt;td&gt;4&lt;/td&gt;\\n&lt;td&gt;~82&lt;/td&gt;\\n&lt;td&gt;2.20&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;tr&gt;\\n&lt;td&gt;8192&lt;/td&gt;\\n&lt;td&gt;5&lt;/td&gt;\\n&lt;td&gt;8&lt;/td&gt;\\n&lt;td&gt;~67&lt;/td&gt;\\n&lt;td&gt;1.25&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;tr&gt;\\n&lt;td&gt;8192&lt;/td&gt;\\n&lt;td&gt;5&lt;/td&gt;\\n&lt;td&gt;4&lt;/td&gt;\\n&lt;td&gt;~67&lt;/td&gt;\\n&lt;td&gt;2.12&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;tr&gt;\\n&lt;td&gt;8192&lt;/td&gt;\\n&lt;td&gt;0&lt;/td&gt;\\n&lt;td&gt;8&lt;/td&gt;\\n&lt;td&gt;~66&lt;/td&gt;\\n&lt;td&gt;1.10&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;tr&gt;\\n&lt;td&gt;8192&lt;/td&gt;\\n&lt;td&gt;0&lt;/td&gt;\\n&lt;td&gt;4&lt;/td&gt;\\n&lt;td&gt;~66&lt;/td&gt;\\n&lt;td&gt;1.81&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;/tbody&gt;&lt;/table&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1idseqb","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma4sy4t/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738292939,"author_flair_text":"llama.cpp","treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":52}}],"before":null}},"user_reports":[],"saved":false,"id":"ma1tpgm","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"medialoungeguy","can_mod_post":false,"created_utc":1738260849,"send_replies":true,"parent_id":"t3_1idseqb","score":424,"author_fullname":"t2_riqcc","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"This is peak localLlama posting. Thank you.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_ma1tpgm","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;This is peak localLlama posting. Thank you.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma1tpgm/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738260849,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1idseqb","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":424}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"c00d2408-d9ce-11ed-89c3-96d9f437a7a1","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"mafocs3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"carnachion","can_mod_post":false,"send_replies":true,"parent_id":"t1_ma4sa0k","score":3,"author_fullname":"t2_12wk9e","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Well, I ran it. The Q2 model to be more specific.  \\nI had to use only 50 threads, as the server was being partially used, to other processes were eating up the memory bandwidth for sure.  \\nThe results for the same short prompt were.\\n\\nCPU only 50 threads:  \\nllama\\\\_perf\\\\_sampler\\\\_print:    sampling time =     105.63 ms /  1177 runs   (    0.09 ms per token, 11142.56 tokens per second)  \\nllama\\\\_perf\\\\_context\\\\_print:        load time =   27105.59 ms  \\nllama\\\\_perf\\\\_context\\\\_print: prompt eval time =    2398.08 ms /    18 tokens (  133.23 ms per token,     7.51 tokens per second)  \\nllama\\\\_perf\\\\_context\\\\_print:        eval time =  662605.51 ms /  1158 runs   (  572.20 ms per token,     1.75 tokens per second)  \\nllama\\\\_perf\\\\_context\\\\_print:       total time =  665458.10 ms /  1176 tokens\\n\\nCPU only mmap off  \\nllama\\\\_perf\\\\_sampler\\\\_print:    sampling time =     135.33 ms /  1177 runs   (    0.11 ms per token,  8697.32 tokens per second)  \\nllama\\\\_perf\\\\_context\\\\_print:        load time = 2134109.09 ms  \\nllama\\\\_perf\\\\_context\\\\_print: prompt eval time =    3232.92 ms /    18 tokens (  179.61 ms per token,     5.57 tokens per second)  \\nllama\\\\_perf\\\\_context\\\\_print:        eval time =  869933.07 ms /  1158 runs   (  751.24 ms per token,     1.33 tokens per second)  \\nllama\\\\_perf\\\\_context\\\\_print:       total time =  875186.19 ms /  1176 tokens\\n\\nOffload 6 layers to 3 Tesla T4  \\nllama\\\\_perf\\\\_sampler\\\\_print:    sampling time =     145.20 ms /  1425 runs   (    0.10 ms per token,  9814.12 tokens per second)  \\nllama\\\\_perf\\\\_context\\\\_print:        load time = 1952521.14 ms  \\nllama\\\\_perf\\\\_context\\\\_print: prompt eval time =    2756.44 ms /    18 tokens (  153.14 ms per token,     6.53 tokens per second)  \\nllama\\\\_perf\\\\_context\\\\_print:        eval time =  919676.51 ms /  1406 runs   (  654.11 ms per token,     1.53 tokens per second)  \\nllama\\\\_perf\\\\_context\\\\_print:       total time =  923507.70 ms /  1424 tokens\\n\\nOffload 6 layers to 3 Tesla T4 mmap off  \\nllama\\\\_perf\\\\_sampler\\\\_print:    sampling time =     152.04 ms /  1425 runs   (    0.11 ms per token,  9372.47 tokens per second)  \\nllama\\\\_perf\\\\_context\\\\_print:        load time = 1019494.32 ms  \\nllama\\\\_perf\\\\_context\\\\_print: prompt eval time =    3025.99 ms /    18 tokens (  168.11 ms per token,     5.95 tokens per second)  \\nllama\\\\_perf\\\\_context\\\\_print:        eval time =  908589.03 ms /  1406 runs   (  646.22 ms per token,     1.55 tokens per second)  \\nllama\\\\_perf\\\\_context\\\\_print:       total time =  912223.43 ms /  1424 tokens\\n\\nNot good, but as soon as the server is totally free, I will try again and report back.","edited":false,"author_flair_css_class":null,"name":"t1_mafocs3","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Well, I ran it. The Q2 model to be more specific.&lt;br/&gt;\\nI had to use only 50 threads, as the server was being partially used, to other processes were eating up the memory bandwidth for sure.&lt;br/&gt;\\nThe results for the same short prompt were.&lt;/p&gt;\\n\\n&lt;p&gt;CPU only 50 threads:&lt;br/&gt;\\nllama_perf_sampler_print:    sampling time =     105.63 ms /  1177 runs   (    0.09 ms per token, 11142.56 tokens per second)&lt;br/&gt;\\nllama_perf_context_print:        load time =   27105.59 ms&lt;br/&gt;\\nllama_perf_context_print: prompt eval time =    2398.08 ms /    18 tokens (  133.23 ms per token,     7.51 tokens per second)&lt;br/&gt;\\nllama_perf_context_print:        eval time =  662605.51 ms /  1158 runs   (  572.20 ms per token,     1.75 tokens per second)&lt;br/&gt;\\nllama_perf_context_print:       total time =  665458.10 ms /  1176 tokens&lt;/p&gt;\\n\\n&lt;p&gt;CPU only mmap off&lt;br/&gt;\\nllama_perf_sampler_print:    sampling time =     135.33 ms /  1177 runs   (    0.11 ms per token,  8697.32 tokens per second)&lt;br/&gt;\\nllama_perf_context_print:        load time = 2134109.09 ms&lt;br/&gt;\\nllama_perf_context_print: prompt eval time =    3232.92 ms /    18 tokens (  179.61 ms per token,     5.57 tokens per second)&lt;br/&gt;\\nllama_perf_context_print:        eval time =  869933.07 ms /  1158 runs   (  751.24 ms per token,     1.33 tokens per second)&lt;br/&gt;\\nllama_perf_context_print:       total time =  875186.19 ms /  1176 tokens&lt;/p&gt;\\n\\n&lt;p&gt;Offload 6 layers to 3 Tesla T4&lt;br/&gt;\\nllama_perf_sampler_print:    sampling time =     145.20 ms /  1425 runs   (    0.10 ms per token,  9814.12 tokens per second)&lt;br/&gt;\\nllama_perf_context_print:        load time = 1952521.14 ms&lt;br/&gt;\\nllama_perf_context_print: prompt eval time =    2756.44 ms /    18 tokens (  153.14 ms per token,     6.53 tokens per second)&lt;br/&gt;\\nllama_perf_context_print:        eval time =  919676.51 ms /  1406 runs   (  654.11 ms per token,     1.53 tokens per second)&lt;br/&gt;\\nllama_perf_context_print:       total time =  923507.70 ms /  1424 tokens&lt;/p&gt;\\n\\n&lt;p&gt;Offload 6 layers to 3 Tesla T4 mmap off&lt;br/&gt;\\nllama_perf_sampler_print:    sampling time =     152.04 ms /  1425 runs   (    0.11 ms per token,  9372.47 tokens per second)&lt;br/&gt;\\nllama_perf_context_print:        load time = 1019494.32 ms&lt;br/&gt;\\nllama_perf_context_print: prompt eval time =    3025.99 ms /    18 tokens (  168.11 ms per token,     5.95 tokens per second)&lt;br/&gt;\\nllama_perf_context_print:        eval time =  908589.03 ms /  1406 runs   (  646.22 ms per token,     1.55 tokens per second)&lt;br/&gt;\\nllama_perf_context_print:       total time =  912223.43 ms /  1424 tokens&lt;/p&gt;\\n\\n&lt;p&gt;Not good, but as soon as the server is totally free, I will try again and report back.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1idseqb","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/mafocs3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738442159,"author_flair_text":null,"collapsed":false,"created_utc":1738442159,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"ma9r2kh","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"tronathan","can_mod_post":false,"send_replies":true,"parent_id":"t1_ma4sa0k","score":2,"author_fullname":"t2_3aqn7","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Is llama.cpp currently smart enough to split this model across multiple cards, as with other models? If so, someone banana-brained enough could run 10x3090 (epyc? oculink)?) w/reduced pl (power level) and actually fit the thing into VRAM... \\n\\nI'm only thinking about it cause I've got 3x 3090's on the shelf, waiting for a build, and 2x more in my inference sytstem","edited":false,"author_flair_css_class":null,"name":"t1_ma9r2kh","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Is llama.cpp currently smart enough to split this model across multiple cards, as with other models? If so, someone banana-brained enough could run 10x3090 (epyc? oculink)?) w/reduced pl (power level) and actually fit the thing into VRAM... &lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;m only thinking about it cause I&amp;#39;ve got 3x 3090&amp;#39;s on the shelf, waiting for a build, and 2x more in my inference sytstem&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1idseqb","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma9r2kh/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738360131,"author_flair_text":null,"collapsed":false,"created_utc":1738360131,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"ma4sa0k","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"VoidAlchemy","can_mod_post":false,"send_replies":true,"parent_id":"t1_ma3zbbz","score":14,"author_fullname":"t2_n321yfw5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Yes you could fit the entire \\\\~212GB quantized model weights in RAM and the bottle neck would be your aggregate RAM i/o bandwidth depending on how many sticks/memory controllers your server rig has. \\"Decently\\" is very relative depending on what your application is though haha...\\n\\nI ran some benchmarks and 24GB VRAM did slightly increase speed as those weights were super fast compared to running off my disk/cache. But it isn't a lot faster, and dedicating PCIe lanes to fast storage might work for smaller RAM setups like gamer rigs.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_ma4sa0k","is_submitter":true,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yes you could fit the entire ~212GB quantized model weights in RAM and the bottle neck would be your aggregate RAM i/o bandwidth depending on how many sticks/memory controllers your server rig has. &amp;quot;Decently&amp;quot; is very relative depending on what your application is though haha...&lt;/p&gt;\\n\\n&lt;p&gt;I ran some benchmarks and 24GB VRAM did slightly increase speed as those weights were super fast compared to running off my disk/cache. But it isn&amp;#39;t a lot faster, and dedicating PCIe lanes to fast storage might work for smaller RAM setups like gamer rigs.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1idseqb","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma4sa0k/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738292704,"author_flair_text":"llama.cpp","treatment_tags":[],"created_utc":1738292704,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":14}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"ma4a3ux","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"pallavnawani","can_mod_post":false,"send_replies":true,"parent_id":"t1_ma3zbbz","score":6,"author_fullname":"t2_xshwy7","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"it will obviously run faster with GPU offloading. Since you have a 512GB RAM Server, you could try running a 4Bit (Or 3Bit) quant in either of those machines.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_ma4a3ux","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;it will obviously run faster with GPU offloading. Since you have a 512GB RAM Server, you could try running a 4Bit (Or 3Bit) quant in either of those machines.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1idseqb","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma4a3ux/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738286563,"author_flair_text":null,"treatment_tags":[],"created_utc":1738286563,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"ma4sfxx","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"fzzzy","can_mod_post":false,"send_replies":true,"parent_id":"t1_ma3zbbz","score":2,"author_fullname":"t2_533l","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Yes. Get a quant that fits in ram and turn off mmap for even better performance.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_ma4sfxx","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yes. Get a quant that fits in ram and turn off mmap for even better performance.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1idseqb","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma4sfxx/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738292762,"author_flair_text":null,"treatment_tags":[],"created_utc":1738292762,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"ma3zbbz","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"carnachion","can_mod_post":false,"send_replies":true,"parent_id":"t1_ma1zzz2","score":8,"author_fullname":"t2_12wk9e","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Do you think it would run decently in a 512 Gb RAM server with two SATA SSDs in raid0?\\n I have a server with this config in my work maybe it is worth trying. There also one similar with three teslas T4 (16 Gb), but the op said it was faster without the GPU, thus, maybe I should just try to run CPU only.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_ma3zbbz","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Do you think it would run decently in a 512 Gb RAM server with two SATA SSDs in raid0?\\n I have a server with this config in my work maybe it is worth trying. There also one similar with three teslas T4 (16 Gb), but the op said it was faster without the GPU, thus, maybe I should just try to run CPU only.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1idseqb","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma3zbbz/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738283051,"author_flair_text":null,"treatment_tags":[],"created_utc":1738283051,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":8}}],"before":null}},"user_reports":[],"saved":false,"id":"ma1zzz2","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"spookperson","can_mod_post":false,"created_utc":1738262570,"send_replies":true,"parent_id":"t1_ma1m2qv","score":75,"author_fullname":"t2_is8jf","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"You can read the blog post with links to the dynamic quants and instructions to run here: [https://unsloth.ai/blog/deepseekr1-dynamic](https://unsloth.ai/blog/deepseekr1-dynamic)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_ma1zzz2","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"Vicuna"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You can read the blog post with links to the dynamic quants and instructions to run here: &lt;a href=\\"https://unsloth.ai/blog/deepseekr1-dynamic\\"&gt;https://unsloth.ai/blog/deepseekr1-dynamic&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1idseqb","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma1zzz2/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738262570,"author_flair_text":"Vicuna","treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":"#9f9286","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":75}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"ma4f6gc","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"DrVonSinistro","can_mod_post":false,"send_replies":true,"parent_id":"t1_ma41fun","score":10,"author_fullname":"t2_cy3wb","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"RAM is cheap in the grand scheme of things. But other than RAM, all my 56 cores are working at a solid steady 100% for the whole duration of the inference. So this indicate me that I have more memory bandwidth (I'm around 90Gb/s - Quad DDR4) than computing power. So RAM isn't everything.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_ma4f6gc","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;RAM is cheap in the grand scheme of things. But other than RAM, all my 56 cores are working at a solid steady 100% for the whole duration of the inference. So this indicate me that I have more memory bandwidth (I&amp;#39;m around 90Gb/s - Quad DDR4) than computing power. So RAM isn&amp;#39;t everything.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1idseqb","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma4f6gc/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738288247,"author_flair_text":null,"treatment_tags":[],"created_utc":1738288247,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":10}},{"kind":"more","data":{"count":2,"name":"t1_ma8wyog","id":"ma8wyog","parent_id":"t1_ma41fun","depth":3,"children":["ma8wyog"]}}],"before":null}},"user_reports":[],"saved":false,"id":"ma41fun","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"LetterRip","can_mod_post":false,"send_replies":true,"parent_id":"t1_ma3k9pm","score":8,"author_fullname":"t2_3zb81","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"The biggest thing is getting MLA working, which would reduce RAM a lot .","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_ma41fun","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The biggest thing is getting MLA working, which would reduce RAM a lot .&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1idseqb","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma41fun/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738283738,"author_flair_text":null,"treatment_tags":[],"created_utc":1738283738,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":8}}],"before":null}},"user_reports":[],"saved":false,"id":"ma3k9pm","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"DrVonSinistro","can_mod_post":false,"created_utc":1738278319,"send_replies":true,"parent_id":"t1_ma1m2qv","score":25,"author_fullname":"t2_cy3wb","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I tried DeepSeek v2.5 236B Q4KM on cpu only all in ram, I also have 256gb. With 16k context I got 2.7 token seconds. Its amazing. With Llama.cpp being optimized everyday, I think we might see further speed gains soon.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_ma3k9pm","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I tried DeepSeek v2.5 236B Q4KM on cpu only all in ram, I also have 256gb. With 16k context I got 2.7 token seconds. Its amazing. With Llama.cpp being optimized everyday, I think we might see further speed gains soon.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1idseqb","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma3k9pm/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738278319,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":25}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"ma79xo5","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DFinsterwalder","can_mod_post":false,"send_replies":true,"parent_id":"t1_ma3cg9a","score":6,"author_fullname":"t2_g8oqo","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Hmm from what I see the 1.58 Bit version gets to around 16 Token/s on an M2 Ultra with 192 GB RAM. That should fit in the RAM.  [https://x.com/ggerganov/status/1884358147403571466](https://x.com/ggerganov/status/1884358147403571466)","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_ma79xo5","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Hmm from what I see the 1.58 Bit version gets to around 16 Token/s on an M2 Ultra with 192 GB RAM. That should fit in the RAM.  &lt;a href=\\"https://x.com/ggerganov/status/1884358147403571466\\"&gt;https://x.com/ggerganov/status/1884358147403571466&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1idseqb","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma79xo5/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738334757,"author_flair_text":null,"treatment_tags":[],"created_utc":1738334757,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"more","data":{"count":3,"name":"t1_majbtcx","id":"majbtcx","parent_id":"t1_ma6bzpz","depth":5,"children":["majbtcx"]}}],"before":null}},"user_reports":[],"saved":false,"id":"ma6bzpz","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"No_Afternoon_4260","can_mod_post":false,"send_replies":true,"parent_id":"t1_ma3fgy3","score":2,"author_fullname":"t2_cj9kap4bx","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"If you offload the kv cache to gpu I think the ssd is only used in read","edited":false,"author_flair_css_class":null,"name":"t1_ma6bzpz","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;If you offload the kv cache to gpu I think the ssd is only used in read&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1idseqb","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":"light","treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma6bzpz/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738320736,"author_flair_text":"llama.cpp","collapsed":false,"created_utc":1738320736,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"ma3fgy3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Lht9791","can_mod_post":false,"send_replies":true,"parent_id":"t1_ma3e5ot","score":4,"author_fullname":"t2_cdofslb","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Here’s R1 on your rig:\\n[edit: it had a lot more to say so I send it by dm]\\n\\n**64GB RAM Configuration:**\\n   - **Marginally feasible but unstable**. While mmap allows partial loading, the smaller RAM size may struggle with the KV cache (5–10GB for 2K context) and system overhead, leading to frequent SSD swaps and degraded performance. Expect slower speeds (1–3 tokens/sec) and potential instability under load.","edited":1738277171,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_ma3fgy3","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Here’s R1 on your rig:\\n[edit: it had a lot more to say so I send it by dm]&lt;/p&gt;\\n\\n&lt;p&gt;&lt;strong&gt;64GB RAM Configuration:&lt;/strong&gt;\\n   - &lt;strong&gt;Marginally feasible but unstable&lt;/strong&gt;. While mmap allows partial loading, the smaller RAM size may struggle with the KV cache (5–10GB for 2K context) and system overhead, leading to frequent SSD swaps and degraded performance. Expect slower speeds (1–3 tokens/sec) and potential instability under load.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1idseqb","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma3fgy3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738276891,"author_flair_text":null,"treatment_tags":[],"created_utc":1738276891,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}}],"before":null}},"user_reports":[],"saved":false,"id":"ma3e5ot","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Snorty-Pig","can_mod_post":false,"send_replies":true,"parent_id":"t1_ma3cg9a","score":8,"author_fullname":"t2_4r94zllp","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I have an M4 Max 64gb and this is probably still out of my league. Any smaller dynamic quantized versions that might run?","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_ma3e5ot","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I have an M4 Max 64gb and this is probably still out of my league. Any smaller dynamic quantized versions that might run?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1idseqb","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma3e5ot/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738276503,"author_flair_text":null,"treatment_tags":[],"created_utc":1738276503,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":8}},{"kind":"t1","data":{"total_awards_received":0,"approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"ups":2,"removal_reason":null,"link_id":"t3_1idseqb","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"more","data":{"count":1,"name":"t1_ma5d9hv","id":"ma5d9hv","parent_id":"t1_ma4oznr","depth":3,"children":["ma5d9hv"]}}],"before":null}},"user_reports":[],"saved":false,"id":"ma4oznr","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":"DELETED","no_follow":true,"author":"[deleted]","can_mod_post":false,"created_utc":1738291561,"send_replies":true,"parent_id":"t1_ma3cg9a","score":2,"approved_by":null,"report_reasons":null,"all_awardings":[],"subreddit_id":"t5_81eyvm","body":"[removed]","edited":false,"author_flair_css_class":null,"downs":0,"is_submitter":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;[removed]&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"associated_award":null,"stickied":false,"subreddit_type":"public","can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":"dark","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma4oznr/","num_reports":null,"locked":false,"name":"t1_ma4oznr","created":1738291561,"subreddit":"LocalLLaMA","author_flair_text":null,"treatment_tags":[],"collapsed":true,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":"","collapsed_because_crowd_control":null,"mod_reports":[],"mod_note":null,"distinguished":null}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"majfabl","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DFinsterwalder","can_mod_post":false,"send_replies":true,"parent_id":"t1_majek5u","score":2,"author_fullname":"t2_g8oqo","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"hmm it looks like only the K cache is in 4 bits and the V cache is in 16 bit. I thought both should be 4bit.   \\n  \\nllama\\\\_kv\\\\_cache\\\\_init: kv\\\\_size = 8192, offload = 1, type\\\\_k = 'q4\\\\_0', type\\\\_v = 'f16', n\\\\_layer = 61, can\\\\_shift = 0\\n\\nllama\\\\_kv\\\\_cache\\\\_init:      Metal KV buffer size =  3640.00 MiB\\n\\nllama\\\\_kv\\\\_cache\\\\_init:        CPU KV buffer size = 18564.00 MiB\\n\\nllama\\\\_init\\\\_from\\\\_model: KV self size  = 22204.00 MiB, K (q4\\\\_0): 6588.00 MiB, V (f16): 15616.00 MiB\\n\\nllama\\\\_init\\\\_from\\\\_model:        CPU  output buffer size =     0.49 MiB\\n\\nllama\\\\_init\\\\_from\\\\_model:      Metal compute buffer size =  2218.00 MiB\\n\\nllama\\\\_init\\\\_from\\\\_model:        CPU compute buffer size =  2218.01 MiB\\n\\nI probably need to check if I setup everything correctly and if llama.cpp is compiled with flash attention. Ill report back if I get it to higher speeds.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_majfabl","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;hmm it looks like only the K cache is in 4 bits and the V cache is in 16 bit. I thought both should be 4bit.   &lt;/p&gt;\\n\\n&lt;p&gt;llama_kv_cache_init: kv_size = 8192, offload = 1, type_k = &amp;#39;q4_0&amp;#39;, type_v = &amp;#39;f16&amp;#39;, n_layer = 61, can_shift = 0&lt;/p&gt;\\n\\n&lt;p&gt;llama_kv_cache_init:      Metal KV buffer size =  3640.00 MiB&lt;/p&gt;\\n\\n&lt;p&gt;llama_kv_cache_init:        CPU KV buffer size = 18564.00 MiB&lt;/p&gt;\\n\\n&lt;p&gt;llama_init_from_model: KV self size  = 22204.00 MiB, K (q4_0): 6588.00 MiB, V (f16): 15616.00 MiB&lt;/p&gt;\\n\\n&lt;p&gt;llama_init_from_model:        CPU  output buffer size =     0.49 MiB&lt;/p&gt;\\n\\n&lt;p&gt;llama_init_from_model:      Metal compute buffer size =  2218.00 MiB&lt;/p&gt;\\n\\n&lt;p&gt;llama_init_from_model:        CPU compute buffer size =  2218.01 MiB&lt;/p&gt;\\n\\n&lt;p&gt;I probably need to check if I setup everything correctly and if llama.cpp is compiled with flash attention. Ill report back if I get it to higher speeds.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1idseqb","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/majfabl/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738496309,"author_flair_text":null,"treatment_tags":[],"created_utc":1738496309,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"majqaje","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Lht9791","can_mod_post":false,"send_replies":true,"parent_id":"t1_majek5u","score":2,"author_fullname":"t2_cdofslb","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Still … very cool. :)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_majqaje","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Still … very cool. :)&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1idseqb","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/majqaje/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738501792,"author_flair_text":null,"treatment_tags":[],"created_utc":1738501792,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"majek5u","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DFinsterwalder","can_mod_post":false,"send_replies":true,"parent_id":"t1_ma3cg9a","score":2,"author_fullname":"t2_g8oqo","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I tried it on my M3 Max 128GB following the unsloth blog post here (including the command for mac there). [https://unsloth.ai/blog/deepseekr1-dynamic](https://unsloth.ai/blog/deepseekr1-dynamic)\\n\\nHowever I had OOM problems when offloading so many layers. It does work when I lower the n-gpu-layers quite a bit (30 didnt work but 10 works now). \\n\\nIt's great that it runs at all, but it's quite slow with roughly around 1 tok/s (flappy bird eval is still running so cant provide exact numbers yet). But   \\n  \\nHere is a video running it: [https://x.com/DFinsterwalder/status/1886013170826789008](https://x.com/DFinsterwalder/status/1886013170826789008)","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_majek5u","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I tried it on my M3 Max 128GB following the unsloth blog post here (including the command for mac there). &lt;a href=\\"https://unsloth.ai/blog/deepseekr1-dynamic\\"&gt;https://unsloth.ai/blog/deepseekr1-dynamic&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;However I had OOM problems when offloading so many layers. It does work when I lower the n-gpu-layers quite a bit (30 didnt work but 10 works now). &lt;/p&gt;\\n\\n&lt;p&gt;It&amp;#39;s great that it runs at all, but it&amp;#39;s quite slow with roughly around 1 tok/s (flappy bird eval is still running so cant provide exact numbers yet). But   &lt;/p&gt;\\n\\n&lt;p&gt;Here is a video running it: &lt;a href=\\"https://x.com/DFinsterwalder/status/1886013170826789008\\"&gt;https://x.com/DFinsterwalder/status/1886013170826789008&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1idseqb","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/majek5u/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738495896,"author_flair_text":null,"treatment_tags":[],"created_utc":1738495896,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"ma68dal","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MarinatedPickachu","can_mod_post":false,"send_replies":true,"parent_id":"t1_ma3cg9a","score":2,"author_fullname":"t2_bwdb8qqfj","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Seriously, what makes you think it could give you reasonable token/s estimates? These numbers are just hallucinated","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_ma68dal","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Seriously, what makes you think it could give you reasonable token/s estimates? These numbers are just hallucinated&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1idseqb","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma68dal/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738318559,"author_flair_text":null,"treatment_tags":[],"created_utc":1738318559,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"more","data":{"count":5,"name":"t1_ma77hjy","id":"ma77hjy","parent_id":"t1_ma3cg9a","depth":2,"children":["ma77hjy"]}}],"before":null}},"user_reports":[],"saved":false,"id":"ma3cg9a","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Lht9791","can_mod_post":false,"created_utc":1738276006,"send_replies":true,"parent_id":"t1_ma1m2qv","score":27,"author_fullname":"t2_cdofslb","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Hey guys! Can DeepSeek “distill” a MacBook yet?  \\n\\nDS R1 just used u/VoidAlchemy’s analysis to confirm that I can get up 20 tokens/second on a MacBook Pro Max 4 with 128 GB (if I had one…). \\n\\n### **Feasibility for MacBook Pro 4 Max (M4 Max, 128GB RAM):**  \\n**Yes**, the dynamically quantized DeepSeek R1 model (~212GB) runs **even more efficiently** on a 128GB configuration. Here’s the breakdown:  \\n\\n—\\n\\n### **Key Advantages Over 96GB:**  \\n1. **Larger RAM Cache:**  \\n   - With **128GB RAM**, more of the model’s active layers can stay cached, reducing SSD access frequency.  \\n   - This minimizes latency from disk reads and leverages the M4 Max’s **410–546 GB/s memory bandwidth** more effectively.  \\n\\n2. **KV Cache Flexibility:**  \\n   - Supports larger context windows (e.g., **4K–8K**) without sacrificing speed, as the extra RAM accommodates expanded KV cache requirements (~10–20GB for 8K context).  \\n\\n3. **Reduced SSD Bottleneck:**  \\n   - Fewer SSD reads are needed for layer swaps, allowing the NVMe SSD (likely **6–8 GB/s sequential reads**) to focus on sustained throughput rather than random access.  \\n\\n—\\n\\n### **Speed Estimates:**  \\n| Scenario                | 96GB Estimate | 128GB Estimate | Improvement Factor |  \\n|-————————|—————|-—————|———————|  \\n| **Conservative (2K context)** | 6–12 tok/s    | **8–16 tok/s** | +25–30%            |  \\n| **Optimistic (2K context)**   | 15 tok/s      | **18–22 tok/s**| +20–40%            |  \\n| **8K Context**           | 3–6 tok/s     | **5–10 tok/s** | +60–70%            |  \\n\\n#### **Why the Boost?**  \\n- **Reduced SSD Dependency:** More RAM allows ~60% of the quantized model (128GB vs. 212GB total) to stay cached during inference, cutting SSD read demands by ~40%.  \\n- **Unified Memory Efficiency:** Apple’s architecture avoids PCIe bottlenecks, ensuring near-instant layer swaps between RAM and compute units.  \\n\\n—\\n\\n### **Practical Implications:**  \\n- **Real-Time Use:** At **8–16 tok/s**, the model becomes viable for interactive tasks (e.g., chatbots, coding assistants) with minimal lag.  \\n- **Larger Contexts:** 8K context at **5–10 tok/s** enables complex document analysis or codebase navigation without catastrophic speed drops.  \\n- **Stability:** 128GB eliminates memory pressure, reducing the risk of SSD thrashing or system instability during prolonged sessions.  \\n\\n—\\n\\n### **Limitations:**  \\n- **SSD Wear:** Frequent writes (e.g., swapping layers) could degrade SSD lifespan over years of heavy use, but this is negligible for most users.  \\n- **Thermals:** Sustained **18+ tok/s** may push the M4 Max’s thermal envelope, but Apple’s cooling can likely handle bursts.  \\n\\n—\\n\\n### **Conclusion:**  \\nA **128GB M4 Max MacBook Pro 4** is **ideal** for running the quantized DeepSeek R1 model, achieving **8–22 tokens/second** depending on context length. This configuration outperforms the 96GB version by **20–40%**, making it one of the most capable consumer-grade setups for local LLM inference. For users prioritizing speed and flexibility, the 128GB upgrade is justified.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_ma3cg9a","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Hey guys! Can DeepSeek “distill” a MacBook yet?  &lt;/p&gt;\\n\\n&lt;p&gt;DS R1 just used &lt;a href=\\"/u/VoidAlchemy\\"&gt;u/VoidAlchemy&lt;/a&gt;’s analysis to confirm that I can get up 20 tokens/second on a MacBook Pro Max 4 with 128 GB (if I had one…). &lt;/p&gt;\\n\\n&lt;h3&gt;&lt;strong&gt;Feasibility for MacBook Pro 4 Max (M4 Max, 128GB RAM):&lt;/strong&gt;&lt;/h3&gt;\\n\\n&lt;p&gt;&lt;strong&gt;Yes&lt;/strong&gt;, the dynamically quantized DeepSeek R1 model (~212GB) runs &lt;strong&gt;even more efficiently&lt;/strong&gt; on a 128GB configuration. Here’s the breakdown:  &lt;/p&gt;\\n\\n&lt;p&gt;—&lt;/p&gt;\\n\\n&lt;h3&gt;&lt;strong&gt;Key Advantages Over 96GB:&lt;/strong&gt;&lt;/h3&gt;\\n\\n&lt;ol&gt;\\n&lt;li&gt;&lt;p&gt;&lt;strong&gt;Larger RAM Cache:&lt;/strong&gt;  &lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;With &lt;strong&gt;128GB RAM&lt;/strong&gt;, more of the model’s active layers can stay cached, reducing SSD access frequency.&lt;br/&gt;&lt;/li&gt;\\n&lt;li&gt;This minimizes latency from disk reads and leverages the M4 Max’s &lt;strong&gt;410–546 GB/s memory bandwidth&lt;/strong&gt; more effectively.&lt;br/&gt;&lt;/li&gt;\\n&lt;/ul&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;p&gt;&lt;strong&gt;KV Cache Flexibility:&lt;/strong&gt;  &lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;Supports larger context windows (e.g., &lt;strong&gt;4K–8K&lt;/strong&gt;) without sacrificing speed, as the extra RAM accommodates expanded KV cache requirements (~10–20GB for 8K context).&lt;br/&gt;&lt;/li&gt;\\n&lt;/ul&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;p&gt;&lt;strong&gt;Reduced SSD Bottleneck:&lt;/strong&gt;  &lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;Fewer SSD reads are needed for layer swaps, allowing the NVMe SSD (likely &lt;strong&gt;6–8 GB/s sequential reads&lt;/strong&gt;) to focus on sustained throughput rather than random access.&lt;br/&gt;&lt;/li&gt;\\n&lt;/ul&gt;&lt;/li&gt;\\n&lt;/ol&gt;\\n\\n&lt;p&gt;—&lt;/p&gt;\\n\\n&lt;h3&gt;&lt;strong&gt;Speed Estimates:&lt;/strong&gt;&lt;/h3&gt;\\n\\n&lt;p&gt;| Scenario                | 96GB Estimate | 128GB Estimate | Improvement Factor |&lt;br/&gt;\\n|-————————|—————|-—————|———————|&lt;br/&gt;\\n| &lt;strong&gt;Conservative (2K context)&lt;/strong&gt; | 6–12 tok/s    | &lt;strong&gt;8–16 tok/s&lt;/strong&gt; | +25–30%            |&lt;br/&gt;\\n| &lt;strong&gt;Optimistic (2K context)&lt;/strong&gt;   | 15 tok/s      | &lt;strong&gt;18–22 tok/s&lt;/strong&gt;| +20–40%            |&lt;br/&gt;\\n| &lt;strong&gt;8K Context&lt;/strong&gt;           | 3–6 tok/s     | &lt;strong&gt;5–10 tok/s&lt;/strong&gt; | +60–70%            |  &lt;/p&gt;\\n\\n&lt;h4&gt;&lt;strong&gt;Why the Boost?&lt;/strong&gt;&lt;/h4&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;&lt;strong&gt;Reduced SSD Dependency:&lt;/strong&gt; More RAM allows ~60% of the quantized model (128GB vs. 212GB total) to stay cached during inference, cutting SSD read demands by ~40%.&lt;br/&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;strong&gt;Unified Memory Efficiency:&lt;/strong&gt; Apple’s architecture avoids PCIe bottlenecks, ensuring near-instant layer swaps between RAM and compute units.&lt;br/&gt;&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;p&gt;—&lt;/p&gt;\\n\\n&lt;h3&gt;&lt;strong&gt;Practical Implications:&lt;/strong&gt;&lt;/h3&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;&lt;strong&gt;Real-Time Use:&lt;/strong&gt; At &lt;strong&gt;8–16 tok/s&lt;/strong&gt;, the model becomes viable for interactive tasks (e.g., chatbots, coding assistants) with minimal lag.&lt;br/&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;strong&gt;Larger Contexts:&lt;/strong&gt; 8K context at &lt;strong&gt;5–10 tok/s&lt;/strong&gt; enables complex document analysis or codebase navigation without catastrophic speed drops.&lt;br/&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;strong&gt;Stability:&lt;/strong&gt; 128GB eliminates memory pressure, reducing the risk of SSD thrashing or system instability during prolonged sessions.&lt;br/&gt;&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;p&gt;—&lt;/p&gt;\\n\\n&lt;h3&gt;&lt;strong&gt;Limitations:&lt;/strong&gt;&lt;/h3&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;&lt;strong&gt;SSD Wear:&lt;/strong&gt; Frequent writes (e.g., swapping layers) could degrade SSD lifespan over years of heavy use, but this is negligible for most users.&lt;br/&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;strong&gt;Thermals:&lt;/strong&gt; Sustained &lt;strong&gt;18+ tok/s&lt;/strong&gt; may push the M4 Max’s thermal envelope, but Apple’s cooling can likely handle bursts.&lt;br/&gt;&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;p&gt;—&lt;/p&gt;\\n\\n&lt;h3&gt;&lt;strong&gt;Conclusion:&lt;/strong&gt;&lt;/h3&gt;\\n\\n&lt;p&gt;A &lt;strong&gt;128GB M4 Max MacBook Pro 4&lt;/strong&gt; is &lt;strong&gt;ideal&lt;/strong&gt; for running the quantized DeepSeek R1 model, achieving &lt;strong&gt;8–22 tokens/second&lt;/strong&gt; depending on context length. This configuration outperforms the 96GB version by &lt;strong&gt;20–40%&lt;/strong&gt;, making it one of the most capable consumer-grade setups for local LLM inference. For users prioritizing speed and flexibility, the 128GB upgrade is justified.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1idseqb","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma3cg9a/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738276006,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":27}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"ma1y76d","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"fatihmtlm","can_mod_post":false,"created_utc":1738262078,"send_replies":true,"parent_id":"t1_ma1m2qv","score":10,"author_fullname":"t2_2l6wzu3h","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Check OP's comment","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_ma1y76d","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Check OP&amp;#39;s comment&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1idseqb","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma1y76d/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738262078,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":10}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"ma1oqo5","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Paulonemillionand3","can_mod_post":false,"created_utc":1738259482,"send_replies":true,"parent_id":"t1_ma1m2qv","score":11,"author_fullname":"t2_mcapx","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"yes. workflow please! pointless without.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_ma1oqo5","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;yes. workflow please! pointless without.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1idseqb","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma1oqo5/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738259482,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":11}}],"before":null}},"user_reports":[],"saved":false,"id":"ma1m2qv","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"ResearchCrafty1804","can_mod_post":false,"created_utc":1738258757,"send_replies":true,"parent_id":"t3_1idseqb","score":140,"author_fullname":"t2_c705ri9b","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Can you give as instructions to replicate your setup with commands? I have a server with 256GB RAM and fast NVMe ssd with Raid and I would like to test it as well when the server becomes available","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_ma1m2qv","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Can you give as instructions to replicate your setup with commands? I have a server with 256GB RAM and fast NVMe ssd with Raid and I would like to test it as well when the server becomes available&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma1m2qv/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738258757,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1idseqb","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":140}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"more","data":{"count":1,"name":"t1_ma3jtt1","id":"ma3jtt1","parent_id":"t1_ma3hxp9","depth":6,"children":["ma3jtt1"]}}],"before":null}},"user_reports":[],"saved":false,"id":"ma3hxp9","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"perk11","can_mod_post":false,"send_replies":true,"parent_id":"t1_ma32jzv","score":3,"author_fullname":"t2_ar5cq","approved_by":null,"mod_note":null,"all_awardings":[],"body":"By the way you can also use swap to avoid it, I had 32 GB of swap and the kernel just swapped everything else out, but didn't kill llama.cpp.","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_ma3hxp9","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;By the way you can also use swap to avoid it, I had 32 GB of swap and the kernel just swapped everything else out, but didn&amp;#39;t kill llama.cpp.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1idseqb","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma3hxp9/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738277618,"author_flair_text":null,"treatment_tags":[],"created_utc":1738277618,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"ma32jzv","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"VoidAlchemy","can_mod_post":false,"send_replies":true,"parent_id":"t1_ma30n3n","score":5,"author_fullname":"t2_n321yfw5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Right, the only reason I used the sytemd-run on llama-serve was to get the linux kernel to stop OOMkilling. The screenshot is with llama-serve limited to 88/85 G, but notice it does not actually allocate the RAM.\\n\\nSome folks had success without using the systemd-run work-around and their systems did not OOMkill llama-serve.\\n\\nHope that is more clear than mud haha...","edited":false,"author_flair_css_class":null,"name":"t1_ma32jzv","is_submitter":true,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Right, the only reason I used the sytemd-run on llama-serve was to get the linux kernel to stop OOMkilling. The screenshot is with llama-serve limited to 88/85 G, but notice it does not actually allocate the RAM.&lt;/p&gt;\\n\\n&lt;p&gt;Some folks had success without using the systemd-run work-around and their systems did not OOMkill llama-serve.&lt;/p&gt;\\n\\n&lt;p&gt;Hope that is more clear than mud haha...&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1idseqb","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":"light","treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma32jzv/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738273209,"author_flair_text":"llama.cpp","collapsed":false,"created_utc":1738273209,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}}],"before":null}},"user_reports":[],"saved":false,"id":"ma30n3n","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"henryclw","can_mod_post":false,"send_replies":true,"parent_id":"t1_ma2keq6","score":4,"author_fullname":"t2_ibl2d6y5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Sorry I didn’t quite catch you. Is this screenshot correspond to the command “sudo systemd-run —scope -p MemoryMax=88G -p MemoryHigh=85G”? Or this screenshot is the situation where you limit the memory usage of llama.cpp to a lower amount like 8GB?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_ma30n3n","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Sorry I didn’t quite catch you. Is this screenshot correspond to the command “sudo systemd-run —scope -p MemoryMax=88G -p MemoryHigh=85G”? Or this screenshot is the situation where you limit the memory usage of llama.cpp to a lower amount like 8GB?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1idseqb","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma30n3n/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738272675,"author_flair_text":null,"treatment_tags":[],"created_utc":1738272675,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}}],"before":null}},"user_reports":[],"saved":false,"id":"ma2keq6","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"VoidAlchemy","can_mod_post":false,"send_replies":true,"parent_id":"t1_ma1v086","score":24,"author_fullname":"t2_n321yfw5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"https://preview.redd.it/ptvibfnbv6ge1.png?width=2099&amp;format=png&amp;auto=webp&amp;s=1d67787575adc888a869dc35890650e32530269e\\n\\nNo not exactly. If you see on the left of this \\\\\`btop\\\\\` output, almost all my RAM is available. The weights are not \\"loaded\\" or malloc'd so to speak. They are mmap'd on disk into memory address. Notice how all the \\"available\\" RAM is marked as \\"Cached\\". So whatever weights are being used regularly won't have to actually hit the disk.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_ma2keq6","is_submitter":true,"collapsed":false,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;&lt;a href=\\"https://preview.redd.it/ptvibfnbv6ge1.png?width=2099&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1d67787575adc888a869dc35890650e32530269e\\"&gt;https://preview.redd.it/ptvibfnbv6ge1.png?width=2099&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1d67787575adc888a869dc35890650e32530269e&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;No not exactly. If you see on the left of this \`btop\` output, almost all my RAM is available. The weights are not &amp;quot;loaded&amp;quot; or malloc&amp;#39;d so to speak. They are mmap&amp;#39;d on disk into memory address. Notice how all the &amp;quot;available&amp;quot; RAM is marked as &amp;quot;Cached&amp;quot;. So whatever weights are being used regularly won&amp;#39;t have to actually hit the disk.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1idseqb","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma2keq6/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738268193,"media_metadata":{"ptvibfnbv6ge1":{"status":"valid","e":"Image","m":"image/png","p":[{"y":39,"x":108,"u":"https://preview.redd.it/ptvibfnbv6ge1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=9fbca34ee71fa35e3409b0af118f96589936b37e"},{"y":78,"x":216,"u":"https://preview.redd.it/ptvibfnbv6ge1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=e58bca72722aaabe8e8d19fdf701b45c49d90844"},{"y":116,"x":320,"u":"https://preview.redd.it/ptvibfnbv6ge1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=913e4e355db95b6b6d6f77c31dc2d55fe58b6928"},{"y":233,"x":640,"u":"https://preview.redd.it/ptvibfnbv6ge1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=8d6c557f124857342b2fce009bab8976c8035064"},{"y":350,"x":960,"u":"https://preview.redd.it/ptvibfnbv6ge1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=5ce71ac2230f7c2e186f7b20d7b2f059f286885d"},{"y":394,"x":1080,"u":"https://preview.redd.it/ptvibfnbv6ge1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=6f6731ee1a35c786e42a961baa41a7d18d82b8d5"}],"s":{"y":766,"x":2099,"u":"https://preview.redd.it/ptvibfnbv6ge1.png?width=2099&amp;format=png&amp;auto=webp&amp;s=1d67787575adc888a869dc35890650e32530269e"},"id":"ptvibfnbv6ge1"}},"author_flair_text":"llama.cpp","treatment_tags":[],"created_utc":1738268193,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":24}},{"kind":"more","data":{"count":9,"name":"t1_ma2bk9k","id":"ma2bk9k","parent_id":"t1_ma1v086","depth":2,"children":["ma2bk9k"]}}],"before":null}},"user_reports":[],"saved":false,"id":"ma1v086","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"henryclw","can_mod_post":false,"created_utc":1738261209,"send_replies":true,"parent_id":"t1_ma1mkcq","score":9,"author_fullname":"t2_ibl2d6y5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"So you still loaded around 80GB of the model weights","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_ma1v086","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;So you still loaded around 80GB of the model weights&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1idseqb","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma1v086/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738261209,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":9}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"more","data":{"count":1,"name":"t1_ma3kqi1","id":"ma3kqi1","parent_id":"t1_ma3f6aq","depth":2,"children":["ma3kqi1"]}}],"before":null}},"user_reports":[],"saved":false,"id":"ma3f6aq","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"perk11","can_mod_post":false,"created_utc":1738276804,"send_replies":true,"parent_id":"t1_ma1mkcq","score":2,"author_fullname":"t2_ar5cq","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I tried your gist and it's working but I keep getting \\"\\"code\\":500,\\"message\\":\\"context shift is disabled\\",\\"type\\":\\"server_error\\"\\" on longer responses. Have you ran into that? I'm going to try to disable flash attention...","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_ma3f6aq","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I tried your gist and it&amp;#39;s working but I keep getting &amp;quot;&amp;quot;code&amp;quot;:500,&amp;quot;message&amp;quot;:&amp;quot;context shift is disabled&amp;quot;,&amp;quot;type&amp;quot;:&amp;quot;server_error&amp;quot;&amp;quot; on longer responses. Have you ran into that? I&amp;#39;m going to try to disable flash attention...&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1idseqb","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma3f6aq/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738276804,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"ma1mkcq","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"VoidAlchemy","can_mod_post":false,"created_utc":1738258892,"send_replies":true,"parent_id":"t3_1idseqb","score":101,"author_fullname":"t2_n321yfw5","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I have some [llama.cpp commands, logs, and generation examples in a gist](https://gist.github.com/ubergarm/0681a59c3304ae06ae930ca468d9fba6) for anyone interested in the details. Also a thread the model's HF page.\\n\\nThe model's opinions on r/LocalLLaMA and Closed AI are pretty humerous:\\n\\n&gt; Closed AI’s the tidy apartment. We’re the anarchist commune with a llama in the lobby. *And honestly?* I’d rather explain to my landlord why my server’s mining DOGE than let some Silicon Valley suit decide my prompts are “too spicy.”","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_ma1mkcq","is_submitter":true,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I have some &lt;a href=\\"https://gist.github.com/ubergarm/0681a59c3304ae06ae930ca468d9fba6\\"&gt;llama.cpp commands, logs, and generation examples in a gist&lt;/a&gt; for anyone interested in the details. Also a thread the model&amp;#39;s HF page.&lt;/p&gt;\\n\\n&lt;p&gt;The model&amp;#39;s opinions on &lt;a href=\\"/r/LocalLLaMA\\"&gt;r/LocalLLaMA&lt;/a&gt; and Closed AI are pretty humerous:&lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;Closed AI’s the tidy apartment. We’re the anarchist commune with a llama in the lobby. &lt;em&gt;And honestly?&lt;/em&gt; I’d rather explain to my landlord why my server’s mining DOGE than let some Silicon Valley suit decide my prompts are “too spicy.”&lt;/p&gt;\\n&lt;/blockquote&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma1mkcq/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738258892,"author_flair_text":"llama.cpp","treatment_tags":[],"link_id":"t3_1idseqb","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":101}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"ma31975","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Turbo_mafia","can_mod_post":false,"send_replies":true,"parent_id":"t1_ma2tak9","score":5,"author_fullname":"t2_yv2xk","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Paid 3.5k for both not cheap but workstation is for dev work.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_ma31975","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Paid 3.5k for both not cheap but workstation is for dev work.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1idseqb","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma31975/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738272846,"author_flair_text":null,"treatment_tags":[],"created_utc":1738272846,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"more","data":{"count":1,"name":"t1_ma36s9v","id":"ma36s9v","parent_id":"t1_ma2tak9","depth":2,"children":["ma36s9v"]}}],"before":null}},"user_reports":[],"saved":false,"id":"ma2tak9","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"kwiksi1ver","can_mod_post":false,"created_utc":1738270647,"send_replies":true,"parent_id":"t1_ma249w9","score":7,"author_fullname":"t2_56zg8","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Aren't those $2500+ processors each? \\n\\nI think op was showing that they used their gaming PC. I'd venture to guess their whole machine was cheaper than one of those 9654's.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_ma2tak9","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Aren&amp;#39;t those $2500+ processors each? &lt;/p&gt;\\n\\n&lt;p&gt;I think op was showing that they used their gaming PC. I&amp;#39;d venture to guess their whole machine was cheaper than one of those 9654&amp;#39;s.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1idseqb","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma2tak9/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738270647,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":7}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"ma30kkc","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Turbo_mafia","can_mod_post":false,"send_replies":true,"parent_id":"t1_ma2ytyi","score":5,"author_fullname":"t2_yv2xk","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"This is the full model unqunatized model straight from oollama 671b, made a mistake, it is q4.164k context length.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_ma30kkc","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;This is the full model unqunatized model straight from oollama 671b, made a mistake, it is q4.164k context length.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1idseqb","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma30kkc/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738272655,"author_flair_text":null,"treatment_tags":[],"created_utc":1738272655,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}}],"before":null}},"user_reports":[],"saved":false,"id":"ma2ytyi","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"VoidAlchemy","can_mod_post":false,"created_utc":1738272178,"send_replies":true,"parent_id":"t1_ma249w9","score":5,"author_fullname":"t2_n321yfw5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Oh very cool to see some numbers. Wat only 1.75 tok/sec generation speed? This must be the full unquantized model? tbh, if so, still very impressive you got it going! \\n\\nHave you tried the unsloth dynamic quants? Here is what I got with your prompt:\\n\\n\`\`\`\\n&lt;think&gt;\\nOkay, the user wants a short poem. Let me start by considering the structure. Maybe a haiku or a quatrain? Since it's short, perhaps a four-line stanza with rhyme.\\n\\nFirst, I need a theme. Nature is a common topic. Let's think of seasons. Spring is vibrant. Maybe something about a garden or a sunset.\\n\\nNext, think of imagery. Words like \\"whispers,\\" \\"petals,\\" \\"dance.\\" Rhymes: \\"light\\" and \\"night,\\" or \\"sky\\" and \\"fly.\\"\\n\\nLet me draft the first line. \\"Beneath the moon's soft light,\\" sets a calm scene. Second line: \\"Whispers of petals take flight,\\" using alliteration with \\"whispers\\" and \\"petals.\\"\\n\\nThird line: \\"In the garden’s quiet dance,\\" introduces movement. Then end with a emotional note: \\"Love blooms at first glance.\\" Rhyme scheme AABB.\\n\\nCheck syllable count. Each line roughly 8-9 syllables. Flows well. Make sure the imagery is coherent and the poem feels cohesive. Maybe adjust words for better flow. Change \\"take flight\\" to \\"drift in flight\\" for smoother transition. Finalize the lines. Done.\\n&lt;/think&gt;\\n\\n**Moonlit Serenade**\\n\\nBeneath the moon’s soft light,\\nWhispers of petals take flight—\\nA garden’s quiet dance,\\nLove blooms at first glance.\\n\\nprompt eval time =    2444.45 ms /     6 tokens (  407.41 ms per token,     2.45 tokens per second)\\n       eval time =  215842.05 ms /   299 tokens (  721.88 ms per token,     1.39 tokens per second)\\n      total time =  218286.50 ms /   305 tokens\\n\`\`\`","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_ma2ytyi","is_submitter":true,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Oh very cool to see some numbers. Wat only 1.75 tok/sec generation speed? This must be the full unquantized model? tbh, if so, still very impressive you got it going! &lt;/p&gt;\\n\\n&lt;p&gt;Have you tried the unsloth dynamic quants? Here is what I got with your prompt:&lt;/p&gt;\\n\\n&lt;p&gt;\`\`\`\\n&amp;lt;think&amp;gt;\\nOkay, the user wants a short poem. Let me start by considering the structure. Maybe a haiku or a quatrain? Since it&amp;#39;s short, perhaps a four-line stanza with rhyme.&lt;/p&gt;\\n\\n&lt;p&gt;First, I need a theme. Nature is a common topic. Let&amp;#39;s think of seasons. Spring is vibrant. Maybe something about a garden or a sunset.&lt;/p&gt;\\n\\n&lt;p&gt;Next, think of imagery. Words like &amp;quot;whispers,&amp;quot; &amp;quot;petals,&amp;quot; &amp;quot;dance.&amp;quot; Rhymes: &amp;quot;light&amp;quot; and &amp;quot;night,&amp;quot; or &amp;quot;sky&amp;quot; and &amp;quot;fly.&amp;quot;&lt;/p&gt;\\n\\n&lt;p&gt;Let me draft the first line. &amp;quot;Beneath the moon&amp;#39;s soft light,&amp;quot; sets a calm scene. Second line: &amp;quot;Whispers of petals take flight,&amp;quot; using alliteration with &amp;quot;whispers&amp;quot; and &amp;quot;petals.&amp;quot;&lt;/p&gt;\\n\\n&lt;p&gt;Third line: &amp;quot;In the garden’s quiet dance,&amp;quot; introduces movement. Then end with a emotional note: &amp;quot;Love blooms at first glance.&amp;quot; Rhyme scheme AABB.&lt;/p&gt;\\n\\n&lt;p&gt;Check syllable count. Each line roughly 8-9 syllables. Flows well. Make sure the imagery is coherent and the poem feels cohesive. Maybe adjust words for better flow. Change &amp;quot;take flight&amp;quot; to &amp;quot;drift in flight&amp;quot; for smoother transition. Finalize the lines. Done.\\n&amp;lt;/think&amp;gt;&lt;/p&gt;\\n\\n&lt;p&gt;&lt;strong&gt;Moonlit Serenade&lt;/strong&gt;&lt;/p&gt;\\n\\n&lt;p&gt;Beneath the moon’s soft light,\\nWhispers of petals take flight—\\nA garden’s quiet dance,\\nLove blooms at first glance.&lt;/p&gt;\\n\\n&lt;p&gt;prompt eval time =    2444.45 ms /     6 tokens (  407.41 ms per token,     2.45 tokens per second)\\n       eval time =  215842.05 ms /   299 tokens (  721.88 ms per token,     1.39 tokens per second)\\n      total time =  218286.50 ms /   305 tokens\\n\`\`\`&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1idseqb","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma2ytyi/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738272178,"author_flair_text":"llama.cpp","treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"ma4d7f5","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"poli-cya","can_mod_post":false,"send_replies":true,"parent_id":"t1_ma373hl","score":2,"author_fullname":"t2_q8g93lhv4","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Thanks, super interesting.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_ma4d7f5","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thanks, super interesting.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1idseqb","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma4d7f5/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738287591,"author_flair_text":null,"treatment_tags":[],"created_utc":1738287591,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"ma373hl","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Turbo_mafia","can_mod_post":false,"send_replies":true,"parent_id":"t1_ma2x6vz","score":4,"author_fullname":"t2_yv2xk","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"PS C:\\\\\\\\Windows\\\\\\\\System32&gt; ollama show deepseek-r1:671b\\n\\n  Model\\n\\narchitecture        deepseek2\\n\\nparameters          671.0B\\n\\ncontext length      163840\\n\\nembedding length    7168\\n\\nquantization        Q4\\\\_K\\\\_M\\n\\n\\n\\n  Parameters\\n\\nstop    \\"&lt;｜begin▁of▁sentence｜&gt;\\"\\n\\nstop    \\"&lt;｜end▁of▁sentence｜&gt;\\"\\n\\nstop    \\"&lt;｜User｜&gt;\\"\\n\\nstop    \\"&lt;｜Assistant｜&gt;\\"\\n\\n\\n\\n  License\\n\\nMIT License\\n\\nCopyright (c) 2023 DeepSeek","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_ma373hl","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;PS C:\\\\Windows\\\\System32&amp;gt; ollama show deepseek-r1:671b&lt;/p&gt;\\n\\n&lt;p&gt;Model&lt;/p&gt;\\n\\n&lt;p&gt;architecture        deepseek2&lt;/p&gt;\\n\\n&lt;p&gt;parameters          671.0B&lt;/p&gt;\\n\\n&lt;p&gt;context length      163840&lt;/p&gt;\\n\\n&lt;p&gt;embedding length    7168&lt;/p&gt;\\n\\n&lt;p&gt;quantization        Q4_K_M&lt;/p&gt;\\n\\n&lt;p&gt;Parameters&lt;/p&gt;\\n\\n&lt;p&gt;stop    &amp;quot;&amp;lt;｜begin▁of▁sentence｜&amp;gt;&amp;quot;&lt;/p&gt;\\n\\n&lt;p&gt;stop    &amp;quot;&amp;lt;｜end▁of▁sentence｜&amp;gt;&amp;quot;&lt;/p&gt;\\n\\n&lt;p&gt;stop    &amp;quot;&amp;lt;｜User｜&amp;gt;&amp;quot;&lt;/p&gt;\\n\\n&lt;p&gt;stop    &amp;quot;&amp;lt;｜Assistant｜&amp;gt;&amp;quot;&lt;/p&gt;\\n\\n&lt;p&gt;License&lt;/p&gt;\\n\\n&lt;p&gt;MIT License&lt;/p&gt;\\n\\n&lt;p&gt;Copyright (c) 2023 DeepSeek&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1idseqb","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma373hl/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738274466,"author_flair_text":null,"treatment_tags":[],"created_utc":1738274466,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}}],"before":null}},"user_reports":[],"saved":false,"id":"ma2x6vz","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"poli-cya","can_mod_post":false,"created_utc":1738271726,"send_replies":true,"parent_id":"t1_ma249w9","score":3,"author_fullname":"t2_q8g93lhv4","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Which quant is this?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_ma2x6vz","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Which quant is this?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1idseqb","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma2x6vz/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738271726,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"more","data":{"count":2,"name":"t1_mcadpk3","id":"mcadpk3","parent_id":"t1_ma249w9","depth":1,"children":["mcadpk3"]}}],"before":null}},"user_reports":[],"saved":false,"id":"ma249w9","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Turbo_mafia","can_mod_post":false,"created_utc":1738263738,"send_replies":true,"parent_id":"t3_1idseqb","score":20,"author_fullname":"t2_yv2xk","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Dual EPYC 9654\\n\\n768 GB - 24 channel \\n\\nWindows server 2025 - Linux would be faster\\n\\n\\n\\n    ollama run deepseek-r1:671b --verbose\\n\\n    &gt;&gt;&gt; write a short poem &lt;think&gt; Okay, the user wants me to write a short poem. Let's start by thinking about themes they might like. Maybe something universal and uplifting? Nature often works well for poetry because it's relatable and vivid.  I should consider the structure—probably a simple rhyme scheme so it flows nicely. AABB or ABAB maybe. Need to keep it concise, just four stanzas perhaps. Let me brainstorm some imagery: dawn, forests, rivers... these elements can symbolize beginnings and persistence.  Wait, adding contrast might add depth—like shadows and light, storms and calm. That creates a dynamic feel. Also, including elements like whispers, roots, tides can give it a sense of movement and growth.  Now check the rhythm. Each line should have a similar meter. Let me read through to see if it's smooth. Avoid forced rhymes; make sure each couplet makes sense together. The ending should tie everything up with a hopeful message about time and journeys. Yeah, that works. &lt;/think&gt;  **Echoes of Dawn**  Beneath the veil of waking skies, a blush of light begins to rise— the forest hums a whispered hymn, as shadows shrink from morning’s rim.  A river carves its ancient creed through stones that hold forgotten seed. The wind, both architect and thief, unfurls the map of every leaf.  We trace the paths where storms have been, where roots embrace what lies unseen— and in the pulse of tides, we find the fragile threads that bind mankind.  For time is but a breath undone, a fleeting dance with moon and sun. Yet here we stand, both lost and found, our echoes woven into ground.  total duration:       3m24.6020313s load duration:        15.0067ms prompt eval count:    7 token(s) prompt eval duration: 999ms prompt eval rate:     7.01 tokens/s eval count:           356 token(s) eval duration:        3m23.587s eval rate:            1.75 tokens/s &gt;&gt;&gt; Send a message (/? for help)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_ma249w9","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Dual EPYC 9654&lt;/p&gt;\\n\\n&lt;p&gt;768 GB - 24 channel &lt;/p&gt;\\n\\n&lt;p&gt;Windows server 2025 - Linux would be faster&lt;/p&gt;\\n\\n&lt;pre&gt;&lt;code&gt;ollama run deepseek-r1:671b --verbose\\n\\n&amp;gt;&amp;gt;&amp;gt; write a short poem &amp;lt;think&amp;gt; Okay, the user wants me to write a short poem. Let&amp;#39;s start by thinking about themes they might like. Maybe something universal and uplifting? Nature often works well for poetry because it&amp;#39;s relatable and vivid.  I should consider the structure—probably a simple rhyme scheme so it flows nicely. AABB or ABAB maybe. Need to keep it concise, just four stanzas perhaps. Let me brainstorm some imagery: dawn, forests, rivers... these elements can symbolize beginnings and persistence.  Wait, adding contrast might add depth—like shadows and light, storms and calm. That creates a dynamic feel. Also, including elements like whispers, roots, tides can give it a sense of movement and growth.  Now check the rhythm. Each line should have a similar meter. Let me read through to see if it&amp;#39;s smooth. Avoid forced rhymes; make sure each couplet makes sense together. The ending should tie everything up with a hopeful message about time and journeys. Yeah, that works. &amp;lt;/think&amp;gt;  **Echoes of Dawn**  Beneath the veil of waking skies, a blush of light begins to rise— the forest hums a whispered hymn, as shadows shrink from morning’s rim.  A river carves its ancient creed through stones that hold forgotten seed. The wind, both architect and thief, unfurls the map of every leaf.  We trace the paths where storms have been, where roots embrace what lies unseen— and in the pulse of tides, we find the fragile threads that bind mankind.  For time is but a breath undone, a fleeting dance with moon and sun. Yet here we stand, both lost and found, our echoes woven into ground.  total duration:       3m24.6020313s load duration:        15.0067ms prompt eval count:    7 token(s) prompt eval duration: 999ms prompt eval rate:     7.01 tokens/s eval count:           356 token(s) eval duration:        3m23.587s eval rate:            1.75 tokens/s &amp;gt;&amp;gt;&amp;gt; Send a message (/? for help)\\n&lt;/code&gt;&lt;/pre&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma249w9/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738263738,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1idseqb","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":20}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"ma375v9","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"VoidAlchemy","can_mod_post":false,"created_utc":1738274485,"send_replies":true,"parent_id":"t1_ma2m16o","score":7,"author_fullname":"t2_n321yfw5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I just want to control how much yapping so it doesn't blow away my tiny context without giving an answer. \\n\\nDeepSeek themselves are working on it too:\\n\\n&gt; Note that the CoT output can reach up to 32K tokens, and the parameter to control the CoT length (\`reasoning_effort\`) will be available soon.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_ma375v9","is_submitter":true,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I just want to control how much yapping so it doesn&amp;#39;t blow away my tiny context without giving an answer. &lt;/p&gt;\\n\\n&lt;p&gt;DeepSeek themselves are working on it too:&lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;Note that the CoT output can reach up to 32K tokens, and the parameter to control the CoT length (&lt;code&gt;reasoning_effort&lt;/code&gt;) will be available soon.&lt;/p&gt;\\n&lt;/blockquote&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1idseqb","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma375v9/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738274485,"author_flair_text":"llama.cpp","treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":7}}],"before":null}},"user_reports":[],"saved":false,"id":"ma2m16o","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"rusty_fans","can_mod_post":false,"created_utc":1738268644,"send_replies":true,"parent_id":"t3_1idseqb","score":18,"author_fullname":"t2_9ntkbp4y3","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"&gt;Just need to figure out how to short circuit the &lt;think&gt;Blah blah&lt;/think&gt; stuff by injecting a &lt;/think&gt; into the assistant prompt to see if it gives decent results without all the yapping haha...\\n\\nShouldn't you just run Deepseek-3 the same way if you don't want the yapping ? R-1's whole point is the yapping...","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_ma2m16o","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;Just need to figure out how to short circuit the &amp;lt;think&amp;gt;Blah blah&amp;lt;/think&amp;gt; stuff by injecting a &amp;lt;/think&amp;gt; into the assistant prompt to see if it gives decent results without all the yapping haha...&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;Shouldn&amp;#39;t you just run Deepseek-3 the same way if you don&amp;#39;t want the yapping ? R-1&amp;#39;s whole point is the yapping...&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma2m16o/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738268644,"author_flair_text":"llama.cpp","treatment_tags":[],"link_id":"t3_1idseqb","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":18}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"total_awards_received":0,"approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"ups":42,"removal_reason":null,"link_id":"t3_1idseqb","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"ma8g6bc","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"VoidAlchemy","can_mod_post":false,"send_replies":true,"parent_id":"t1_ma5kny4","score":4,"author_fullname":"t2_n321yfw5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Wow thanks for this gem. I replied to that person and asked if they were interested in kicking the tires on R1!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_ma8g6bc","is_submitter":true,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Wow thanks for this gem. I replied to that person and asked if they were interested in kicking the tires on R1!&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1idseqb","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma8g6bc/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738346852,"author_flair_text":"llama.cpp","treatment_tags":[],"created_utc":1738346852,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}}],"before":null}},"user_reports":[],"saved":false,"id":"ma5kny4","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"zerd","can_mod_post":false,"send_replies":true,"parent_id":"t1_ma2h2w0","score":19,"author_fullname":"t2_3mq77","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"This guy is able to read 163GB/s from 12x NVMe SSDs https://forum.level1techs.com/t/threadripper-7980x-asus-pro-ws-trx50-sage-wifi-build/210550/116","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_ma5kny4","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;This guy is able to read 163GB/s from 12x NVMe SSDs &lt;a href=\\"https://forum.level1techs.com/t/threadripper-7980x-asus-pro-ws-trx50-sage-wifi-build/210550/116\\"&gt;https://forum.level1techs.com/t/threadripper-7980x-asus-pro-ws-trx50-sage-wifi-build/210550/116&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1idseqb","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma5kny4/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738304457,"author_flair_text":null,"treatment_tags":[],"created_utc":1738304457,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":19}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"more","data":{"count":3,"name":"t1_ma3ef2r","id":"ma3ef2r","parent_id":"t1_ma3dusb","depth":3,"children":["ma3ef2r"]}}],"before":null}},"user_reports":[],"saved":false,"id":"ma3dusb","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Psychological_Ear393","can_mod_post":false,"send_replies":true,"parent_id":"t1_ma2h2w0","score":9,"author_fullname":"t2_5rdt6qc2","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"It depends heavily on the exact operation, as in read or write, block size, number of threads etc.\\n\\nExcluding any test that is influenced by L3 cache, my 7532 with 8 channels of 3200 real life has:\\n\\n* max write in 1g blocks 64 threads, about 140GiB/s\\n* max read in 1g blocks 64 threads, about 250GiB/s\\n\\nLower or higher threads, and lower block sizes reduces that.  All the way down to 4k blocks and L3 cache takes over and those tests can be ignored (but for reference they are &gt;600)\\n\\nAlso to clarify, Turin is going to have max memory transfer with a 12 CCD CPU, e.g. 9175F or 9565","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_ma3dusb","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It depends heavily on the exact operation, as in read or write, block size, number of threads etc.&lt;/p&gt;\\n\\n&lt;p&gt;Excluding any test that is influenced by L3 cache, my 7532 with 8 channels of 3200 real life has:&lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;max write in 1g blocks 64 threads, about 140GiB/s&lt;/li&gt;\\n&lt;li&gt;max read in 1g blocks 64 threads, about 250GiB/s&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;p&gt;Lower or higher threads, and lower block sizes reduces that.  All the way down to 4k blocks and L3 cache takes over and those tests can be ignored (but for reference they are &amp;gt;600)&lt;/p&gt;\\n\\n&lt;p&gt;Also to clarify, Turin is going to have max memory transfer with a 12 CCD CPU, e.g. 9175F or 9565&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1idseqb","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma3dusb/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738276413,"author_flair_text":null,"treatment_tags":[],"created_utc":1738276413,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":9}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"more","data":{"count":4,"name":"t1_maa62io","id":"maa62io","parent_id":"t1_ma3adrn","depth":3,"children":["maa62io"]}}],"before":null}},"user_reports":[],"saved":false,"id":"ma3adrn","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"ethertype","can_mod_post":false,"send_replies":true,"parent_id":"t1_ma2h2w0","score":17,"author_fullname":"t2_3nfev4","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"You will \\\\*not\\\\* obtain 576 GB/s on a single Turin EPYC \\\\*unless\\\\* it is one of the models with 8 CCDs. \\n\\nThe ones us peasants can afford got 2 CCDs, which nets us 480 GB/s \\\\*if\\\\* you have \\\\*2\\\\* CPUs and both are served with 12 channels of DDR5.\\n\\nThe Turin EPYCs are great CPUs, but there are nuances.....","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_ma3adrn","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You will *not* obtain 576 GB/s on a single Turin EPYC *unless* it is one of the models with 8 CCDs. &lt;/p&gt;\\n\\n&lt;p&gt;The ones us peasants can afford got 2 CCDs, which nets us 480 GB/s *if* you have *2* CPUs and both are served with 12 channels of DDR5.&lt;/p&gt;\\n\\n&lt;p&gt;The Turin EPYCs are great CPUs, but there are nuances.....&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1idseqb","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma3adrn/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738275406,"author_flair_text":null,"treatment_tags":[],"created_utc":1738275406,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":17}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"ma3c5zc","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"elaboratedSalad","can_mod_post":false,"send_replies":true,"parent_id":"t1_ma2h2w0","score":5,"author_fullname":"t2_s2bhbsppd","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"lol, I get 13.5GB/s from a 2 card NVMe array, presumably you'd twice that with 4 drives.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_ma3c5zc","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;lol, I get 13.5GB/s from a 2 card NVMe array, presumably you&amp;#39;d twice that with 4 drives.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1idseqb","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma3c5zc/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738275923,"author_flair_text":null,"treatment_tags":[],"created_utc":1738275923,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}}],"before":null}},"user_reports":[],"saved":false,"id":"ma2h2w0","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":"DELETED","no_follow":true,"author":"[deleted]","can_mod_post":false,"send_replies":true,"parent_id":"t1_ma1rdkq","score":42,"approved_by":null,"report_reasons":null,"all_awardings":[],"subreddit_id":"t5_81eyvm","body":"[removed]","edited":1739202547,"author_flair_css_class":null,"collapsed":true,"downs":0,"is_submitter":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;[removed]&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"associated_award":null,"stickied":false,"subreddit_type":"public","can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":"dark","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma2h2w0/","num_reports":null,"locked":false,"name":"t1_ma2h2w0","created":1738267262,"subreddit":"LocalLLaMA","author_flair_text":null,"treatment_tags":[],"created_utc":1738267262,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":"","collapsed_because_crowd_control":null,"mod_reports":[],"mod_note":null,"distinguished":null}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"ma2opz5","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"_supert_","can_mod_post":false,"send_replies":true,"parent_id":"t1_ma1s7gm","score":8,"author_fullname":"t2_dwj10","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Software raid then, e.g. zfs","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_ma2opz5","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Software raid then, e.g. zfs&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1idseqb","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma2opz5/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738269392,"author_flair_text":null,"treatment_tags":[],"created_utc":1738269392,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":8}},{"kind":"more","data":{"count":2,"name":"t1_ma1swk3","id":"ma1swk3","parent_id":"t1_ma1s7gm","depth":2,"children":["ma1swk3"]}}],"before":null}},"user_reports":[],"saved":false,"id":"ma1s7gm","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"SryUsrNameIsTaken","can_mod_post":false,"created_utc":1738260434,"send_replies":true,"parent_id":"t1_ma1rdkq","score":6,"author_fullname":"t2_8w2fagrd","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Has anyone fixed NVMe raid? It was a pain last time I had to set it up and I think some BIOS don’t like them at all.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_ma1s7gm","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Has anyone fixed NVMe raid? It was a pain last time I had to set it up and I think some BIOS don’t like them at all.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1idseqb","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma1s7gm/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738260434,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"ma5uhiz","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"kovnev","can_mod_post":false,"send_replies":true,"parent_id":"t1_ma24ydu","score":12,"author_fullname":"t2_hhw87t7z","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Upvote for LLM paralympics 😆.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_ma5uhiz","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Upvote for LLM paralympics 😆.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1idseqb","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma5uhiz/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738309938,"author_flair_text":null,"treatment_tags":[],"created_utc":1738309938,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":12}}],"before":null}},"user_reports":[],"saved":false,"id":"ma24ydu","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Reasonable_Flower_72","can_mod_post":false,"send_replies":true,"parent_id":"t1_ma2467o","score":27,"author_fullname":"t2_tsfm03xb","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I know it’s not ideal, but if people are milking 2t/s from it.\\n\\nUsing nvme raid array together with splitting model into more smaller files to help array performance ( it performs much better while reading two different files instead of same one, at least with linux mdadm ) could make this LLM paralympics much more interesting","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_ma24ydu","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I know it’s not ideal, but if people are milking 2t/s from it.&lt;/p&gt;\\n\\n&lt;p&gt;Using nvme raid array together with splitting model into more smaller files to help array performance ( it performs much better while reading two different files instead of same one, at least with linux mdadm ) could make this LLM paralympics much more interesting&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1idseqb","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma24ydu/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738263925,"author_flair_text":null,"treatment_tags":[],"created_utc":1738263925,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":27}},{"kind":"t1","data":{"total_awards_received":0,"approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"ups":2,"removal_reason":null,"link_id":"t3_1idseqb","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"ma2ydyh","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Raikalover","can_mod_post":false,"send_replies":true,"parent_id":"t1_ma2cdn7","score":4,"author_fullname":"t2_5by92yy7","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"If you use the dynamic quant it should fit in ram I think.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_ma2ydyh","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;If you use the dynamic quant it should fit in ram I think.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1idseqb","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma2ydyh/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738272057,"author_flair_text":null,"treatment_tags":[],"created_utc":1738272057,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"more","data":{"count":3,"name":"t1_ma3y6ng","id":"ma3y6ng","parent_id":"t1_ma3qgtk","depth":4,"children":["ma3y6ng"]}}],"before":null}},"user_reports":[],"saved":false,"id":"ma3qgtk","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"cakemates","can_mod_post":false,"send_replies":true,"parent_id":"t1_ma2cdn7","score":3,"author_fullname":"t2_bog7xfmr","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"The full model is 671gb, you need some additional space for context, so lets say 700gb to run the full model, maybe some more.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_ma3qgtk","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The full model is 671gb, you need some additional space for context, so lets say 700gb to run the full model, maybe some more.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1idseqb","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma3qgtk/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738280228,"author_flair_text":null,"treatment_tags":[],"created_utc":1738280228,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"ma2cdn7","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":"DELETED","no_follow":true,"author":"[deleted]","can_mod_post":false,"created_utc":1738265975,"send_replies":true,"parent_id":"t1_ma2467o","score":2,"approved_by":null,"report_reasons":null,"all_awardings":[],"subreddit_id":"t5_81eyvm","body":"[removed]","edited":false,"author_flair_css_class":null,"downs":0,"is_submitter":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;[removed]&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"associated_award":null,"stickied":false,"subreddit_type":"public","can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":"dark","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma2cdn7/","num_reports":null,"locked":false,"name":"t1_ma2cdn7","created":1738265975,"subreddit":"LocalLLaMA","author_flair_text":null,"treatment_tags":[],"collapsed":true,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":"","collapsed_because_crowd_control":null,"mod_reports":[],"mod_note":null,"distinguished":null}}],"before":null}},"user_reports":[],"saved":false,"id":"ma2467o","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"cakemates","can_mod_post":false,"created_utc":1738263709,"send_replies":true,"parent_id":"t1_ma1rdkq","score":3,"author_fullname":"t2_bog7xfmr","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"nvme are too slow for this race, nvme are like horses and this is a airplane race. And the way computer architecture are structured nvme data has to go trough ram to reach the cpu.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_ma2467o","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;nvme are too slow for this race, nvme are like horses and this is a airplane race. And the way computer architecture are structured nvme data has to go trough ram to reach the cpu.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1idseqb","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma2467o/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738263709,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"ma1rdkq","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Reasonable_Flower_72","can_mod_post":false,"created_utc":1738260204,"send_replies":true,"parent_id":"t3_1idseqb","score":48,"author_fullname":"t2_tsfm03xb","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Uhm, guys... what about creating RAID pool from NVMe drives to increase bandwidth further?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_ma1rdkq","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Uhm, guys... what about creating RAID pool from NVMe drives to increase bandwidth further?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma1rdkq/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738260204,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1idseqb","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":48}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"ma49hhq","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Live_Bus7425","can_mod_post":false,"send_replies":true,"parent_id":"t1_ma2op56","score":2,"author_fullname":"t2_81so95e0","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Im not sure. I usually use Kasa Smart Plug with monitoring whenever I want to messure something. 300W would be nice. I hope you are correct.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_ma49hhq","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Im not sure. I usually use Kasa Smart Plug with monitoring whenever I want to messure something. 300W would be nice. I hope you are correct.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1idseqb","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma49hhq/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738286360,"author_flair_text":null,"treatment_tags":[],"created_utc":1738286360,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"more","data":{"count":1,"name":"t1_ma4a8l9","id":"ma4a8l9","parent_id":"t1_ma2op56","depth":2,"children":["ma4a8l9"]}}],"before":null}},"user_reports":[],"saved":false,"id":"ma2op56","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"VoidAlchemy","can_mod_post":false,"created_utc":1738269385,"send_replies":true,"parent_id":"t1_ma1lggg","score":8,"author_fullname":"t2_n321yfw5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Not sure... Wish I had a kill-a-watt jawn to measure at the wall... If I had to speculate wildly I'd say 200W. Supposedly my Toughpower 1350W PSU passive operation lasts up to 300W and that noisy fan was not running.\\n\\nIs there a Linux CLI tool or python app to measure draw easily on a desktop so I can check?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_ma2op56","is_submitter":true,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Not sure... Wish I had a kill-a-watt jawn to measure at the wall... If I had to speculate wildly I&amp;#39;d say 200W. Supposedly my Toughpower 1350W PSU passive operation lasts up to 300W and that noisy fan was not running.&lt;/p&gt;\\n\\n&lt;p&gt;Is there a Linux CLI tool or python app to measure draw easily on a desktop so I can check?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1idseqb","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma2op56/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738269385,"author_flair_text":"llama.cpp","treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":8}}],"before":null}},"user_reports":[],"saved":false,"id":"ma1lggg","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Live_Bus7425","can_mod_post":false,"created_utc":1738258585,"send_replies":true,"parent_id":"t3_1idseqb","score":15,"author_fullname":"t2_81so95e0","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"What is the power draw of your rig during the inference?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_ma1lggg","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;What is the power draw of your rig during the inference?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma1lggg/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738258585,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1idseqb","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":15}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"d2642412-d9ce-11ed-ae30-32b11309f5bd","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"ma365gc","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"VoidAlchemy","can_mod_post":false,"send_replies":true,"parent_id":"t1_ma2ts1z","score":4,"author_fullname":"t2_n321yfw5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"\\\\*EDIT\\\\* Oops I always confuse RAID 0 and 1. RAID 1 is mirroring. I thought RAID 1 would be good given I only care about fast reads? I gotta watch the rest of this \\\\[Level1Techs Quad NVMe Adapter\\\\](https://www.youtube.com/watch?v=3KCaS7EK6Rc) video as Wendell is getting some great read IOPS off that thing.\\n\\nOriginal misspoken post:\\n\\nRight, RAID 0, mirroring 4x drives theoretically could give 4x read performance. But I'm hoping someone else has the hardware to show it does scale linearly enough to hit 4-5 tok/sec!","edited":1738345578,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_ma365gc","is_submitter":true,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;*EDIT* Oops I always confuse RAID 0 and 1. RAID 1 is mirroring. I thought RAID 1 would be good given I only care about fast reads? I gotta watch the rest of this [Level1Techs Quad NVMe Adapter](&lt;a href=\\"https://www.youtube.com/watch?v=3KCaS7EK6Rc\\"&gt;https://www.youtube.com/watch?v=3KCaS7EK6Rc&lt;/a&gt;) video as Wendell is getting some great read IOPS off that thing.&lt;/p&gt;\\n\\n&lt;p&gt;Original misspoken post:&lt;/p&gt;\\n\\n&lt;p&gt;Right, RAID 0, mirroring 4x drives theoretically could give 4x read performance. But I&amp;#39;m hoping someone else has the hardware to show it does scale linearly enough to hit 4-5 tok/sec!&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1idseqb","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma365gc/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738274208,"author_flair_text":"llama.cpp","treatment_tags":[],"created_utc":1738274208,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"more","data":{"count":1,"name":"t1_madhrwj","id":"madhrwj","parent_id":"t1_ma2ts1z","depth":3,"children":["madhrwj"]}}],"before":null}},"user_reports":[],"saved":false,"id":"ma2ts1z","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"ortegaalfredo","can_mod_post":false,"send_replies":true,"parent_id":"t1_ma2m0cy","score":4,"author_fullname":"t2_g177e","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"\\\\&gt; I'd love to find a configuration for 4x that would possibly give even 4\\\\~5 tok/sec maybe???\\n\\nRAID 0","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_ma2ts1z","is_submitter":false,"collapsed":false,"author_flair_richtext":[{"e":"text","t":"Alpaca"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;&amp;gt; I&amp;#39;d love to find a configuration for 4x that would possibly give even 4~5 tok/sec maybe???&lt;/p&gt;\\n\\n&lt;p&gt;RAID 0&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1idseqb","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma2ts1z/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738270779,"author_flair_text":"Alpaca","treatment_tags":[],"created_utc":1738270779,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":"#bd9e9e","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"more","data":{"count":1,"name":"t1_ma4uwpx","id":"ma4uwpx","parent_id":"t1_ma31oh3","depth":3,"children":["ma4uwpx"]}}],"before":null}},"user_reports":[],"saved":false,"id":"ma31oh3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DefNattyBoii","can_mod_post":false,"send_replies":true,"parent_id":"t1_ma2m0cy","score":3,"author_fullname":"t2_150aum","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Would love if you could do some benches with lm-evaluation-harness for GPQA, IFEval etc. I dont frequently see those on quants and the leaderboards take ages to update.\\n  \\nThats good info on swap I will avoid it, basically I had it turned off since I upgraded my mem.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_ma31oh3","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Would love if you could do some benches with lm-evaluation-harness for GPQA, IFEval etc. I dont frequently see those on quants and the leaderboards take ages to update.&lt;/p&gt;\\n\\n&lt;p&gt;Thats good info on swap I will avoid it, basically I had it turned off since I upgraded my mem.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1idseqb","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma31oh3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738272964,"author_flair_text":null,"treatment_tags":[],"created_utc":1738272964,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"ma2m0cy","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"VoidAlchemy","can_mod_post":false,"created_utc":1738268638,"send_replies":true,"parent_id":"t1_ma1qydk","score":13,"author_fullname":"t2_n321yfw5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yes I mention the dynamic quant, check the unsloth blog as they selectively quantize various layers to give okay performance. \\n\\n&gt; By studying DeepSeek R1’s architecture, we managed to selectively quantize certain layers to higher bits (like 4bit) &amp; leave most MoE layers (like those used in GPT-4) to 1.5bit. Naively quantizing all layers breaks the model entirely, causing endless loops &amp; gibberish outputs. Our dynamic quants solve this.\\n\\nCorrect, it is not the same as the full unquantized model, but in limited testing it seems better than any other 30\\\\~70B models I can run locally for some applications like generating \\\\~1000 words of technical or creative writing. Obviously it is slow and low context haha...\\n\\nExactly, I'm using one Crucial T700 2GB (the 1GB is slower). I'd love to find a configuration for 4x that would possibly give even 4\\\\~5 tok/sec maybe???\\n\\nDon't swap though, I tried that, swap is dog slow, thrashes the disks with writes, and my whole system went unstable for~0.3 tok/sec haha...\\n\\nCheers!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_ma2m0cy","is_submitter":true,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yes I mention the dynamic quant, check the unsloth blog as they selectively quantize various layers to give okay performance. &lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;By studying DeepSeek R1’s architecture, we managed to selectively quantize certain layers to higher bits (like 4bit) &amp;amp; leave most MoE layers (like those used in GPT-4) to 1.5bit. Naively quantizing all layers breaks the model entirely, causing endless loops &amp;amp; gibberish outputs. Our dynamic quants solve this.&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;Correct, it is not the same as the full unquantized model, but in limited testing it seems better than any other 30~70B models I can run locally for some applications like generating ~1000 words of technical or creative writing. Obviously it is slow and low context haha...&lt;/p&gt;\\n\\n&lt;p&gt;Exactly, I&amp;#39;m using one Crucial T700 2GB (the 1GB is slower). I&amp;#39;d love to find a configuration for 4x that would possibly give even 4~5 tok/sec maybe???&lt;/p&gt;\\n\\n&lt;p&gt;Don&amp;#39;t swap though, I tried that, swap is dog slow, thrashes the disks with writes, and my whole system went unstable for~0.3 tok/sec haha...&lt;/p&gt;\\n\\n&lt;p&gt;Cheers!&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1idseqb","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma2m0cy/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738268638,"author_flair_text":"llama.cpp","treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":13}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"d2642412-d9ce-11ed-ae30-32b11309f5bd","likes":null,"replies":"","user_reports":[],"saved":false,"id":"ma2tlfg","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"ortegaalfredo","can_mod_post":false,"created_utc":1738270728,"send_replies":true,"parent_id":"t1_ma1qydk","score":7,"author_fullname":"t2_g177e","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"\\\\&gt; This is pretty far from running the full model, which is about 700 GB+, and will give you inferior results. \\n\\nYes I believed the same but just do some tests and see for yourself. There is almost no difference. Huge models lose less quality with quantization than smaller models.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_ma2tlfg","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"Alpaca"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;&amp;gt; This is pretty far from running the full model, which is about 700 GB+, and will give you inferior results. &lt;/p&gt;\\n\\n&lt;p&gt;Yes I believed the same but just do some tests and see for yourself. There is almost no difference. Huge models lose less quality with quantization than smaller models.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1idseqb","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma2tlfg/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738270728,"author_flair_text":"Alpaca","treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":"#bd9e9e","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":7}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"ma2zxvw","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DefNattyBoii","can_mod_post":false,"send_replies":true,"parent_id":"t1_ma2gg4a","score":2,"author_fullname":"t2_150aum","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"For 1k usd you only get the storage setup OP suggests. If you have a beefy PC and enough money you can try it out, worst case you'll have a bunch of 1TB nvme ssds in a beefy array. But its still better to load it into ram. You can get 192 GB on consumer grade - but its not enough to load this quant, needs 212 gb just for the model.\\n  \\nDDR5 high speed memory can go up to 100 GB/s but don't quote me on that","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_ma2zxvw","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;For 1k usd you only get the storage setup OP suggests. If you have a beefy PC and enough money you can try it out, worst case you&amp;#39;ll have a bunch of 1TB nvme ssds in a beefy array. But its still better to load it into ram. You can get 192 GB on consumer grade - but its not enough to load this quant, needs 212 gb just for the model.&lt;/p&gt;\\n\\n&lt;p&gt;DDR5 high speed memory can go up to 100 GB/s but don&amp;#39;t quote me on that&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1idseqb","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma2zxvw/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738272484,"author_flair_text":null,"treatment_tags":[],"created_utc":1738272484,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"more","data":{"count":8,"name":"t1_ma2gvwy","id":"ma2gvwy","parent_id":"t1_ma2gg4a","depth":3,"children":["ma2gvwy"]}}],"before":null}},"user_reports":[],"saved":false,"id":"ma2gg4a","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"mintybadgerme","can_mod_post":false,"send_replies":true,"parent_id":"t1_ma244sr","score":10,"author_fullname":"t2_r6grz","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"For 1K??","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_ma2gg4a","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;For 1K??&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1idseqb","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma2gg4a/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738267088,"author_flair_text":null,"treatment_tags":[],"created_utc":1738267088,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":10}},{"kind":"more","data":{"count":2,"name":"t1_ma2mgjd","id":"ma2mgjd","parent_id":"t1_ma244sr","depth":2,"children":["ma2mgjd"]}}],"before":null}},"user_reports":[],"saved":false,"id":"ma244sr","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"FullstackSensei","can_mod_post":false,"created_utc":1738263698,"send_replies":true,"parent_id":"t1_ma1qydk","score":10,"author_fullname":"t2_17n3nqtj56","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"For 1k you might as well get an Epyc Milan with whatever cheapest Epyc motherboard you can find and 384GB of 3200 ECC DDR4. Everything will fit in RAM and won't need any fiddling with Raid.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_ma244sr","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;For 1k you might as well get an Epyc Milan with whatever cheapest Epyc motherboard you can find and 384GB of 3200 ECC DDR4. Everything will fit in RAM and won&amp;#39;t need any fiddling with Raid.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1idseqb","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma244sr/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738263698,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":10}},{"kind":"more","data":{"count":4,"name":"t1_ma2bw9n","id":"ma2bw9n","parent_id":"t1_ma1qydk","depth":1,"children":["ma2bw9n"]}}],"before":null}},"user_reports":[],"saved":false,"id":"ma1qydk","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"DefNattyBoii","can_mod_post":false,"created_utc":1738260088,"send_replies":true,"parent_id":"t3_1idseqb","score":57,"author_fullname":"t2_150aum","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"You say that you run a full R1 671B model, but yet you pulled the 2.51bit dynamic quant(212GB). This is pretty far from running the full model, which is about 700 GB+, and will give you inferior results. But it still runs at okay speeds, good job on experimenting. I wonder if we stack the ssds into a large acceleration card what speeds we will get.\\n  \\nFour Crucial T705 nvmes put you back about 800 USD and an accelerator card goes around 150-200. So for 1k you can get 60 GBPS in theory, and you can even make a swap for your system to simplify loading it into ram.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_ma1qydk","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You say that you run a full R1 671B model, but yet you pulled the 2.51bit dynamic quant(212GB). This is pretty far from running the full model, which is about 700 GB+, and will give you inferior results. But it still runs at okay speeds, good job on experimenting. I wonder if we stack the ssds into a large acceleration card what speeds we will get.&lt;/p&gt;\\n\\n&lt;p&gt;Four Crucial T705 nvmes put you back about 800 USD and an accelerator card goes around 150-200. So for 1k you can get 60 GBPS in theory, and you can even make a swap for your system to simplify loading it into ram.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma1qydk/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738260088,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1idseqb","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":57}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"total_awards_received":0,"approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"ups":7,"removal_reason":null,"link_id":"t3_1idseqb","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"ma1pji1","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Flashy_Squirrel4745","can_mod_post":false,"send_replies":true,"parent_id":"t1_ma1o8nh","score":10,"author_fullname":"t2_7f3z6oyp","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Probably they mean that the NVMe ssds will probably overheat and shut themselves down. Just add some cooling.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_ma1pji1","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Probably they mean that the NVMe ssds will probably overheat and shut themselves down. Just add some cooling.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1idseqb","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma1pji1/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738259700,"author_flair_text":null,"treatment_tags":[],"created_utc":1738259700,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":10}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"total_awards_received":0,"approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"ups":4,"removal_reason":null,"link_id":"t3_1idseqb","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"more","data":{"count":1,"name":"t1_ma3dh1j","id":"ma3dh1j","parent_id":"t1_ma3cx2d","depth":6,"children":["ma3dh1j"]}}],"before":null}},"user_reports":[],"saved":false,"id":"ma3cx2d","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Xankar","can_mod_post":false,"send_replies":true,"parent_id":"t1_ma33u20","score":3,"author_fullname":"t2_hr6yz","approved_by":null,"mod_note":null,"all_awardings":[],"body":"An asset that's depreciating faster than a new car off the lot.","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_ma3cx2d","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;An asset that&amp;#39;s depreciating faster than a new car off the lot.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1idseqb","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma3cx2d/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738276140,"author_flair_text":null,"treatment_tags":[],"created_utc":1738276140,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"ma33u20","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"More-Acadia2355","can_mod_post":false,"send_replies":true,"parent_id":"t1_ma2kfr9","score":2,"author_fullname":"t2_iom84v7m6","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"It's an asset in that you can resell it so you only pay depreciation.\\n\\nI'm just saying that if you're using it profitably, then renting isn't always the best option, from an accounting perspective. \\n\\nThat's why accountants put computer hardware in the assets column and depreciate the value over time.  ...the cost of depreciation might be less than the rent in the cloud.","edited":false,"author_flair_css_class":null,"name":"t1_ma33u20","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It&amp;#39;s an asset in that you can resell it so you only pay depreciation.&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;m just saying that if you&amp;#39;re using it profitably, then renting isn&amp;#39;t always the best option, from an accounting perspective. &lt;/p&gt;\\n\\n&lt;p&gt;That&amp;#39;s why accountants put computer hardware in the assets column and depreciate the value over time.  ...the cost of depreciation might be less than the rent in the cloud.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1idseqb","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma33u20/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738273565,"author_flair_text":null,"collapsed":false,"created_utc":1738273565,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"ma2kfr9","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"[deleted]","can_mod_post":false,"send_replies":true,"parent_id":"t1_ma2c2ud","score":4,"approved_by":null,"report_reasons":null,"all_awardings":[],"subreddit_id":"t5_81eyvm","body":"Is it though? are you running it 24 hours and making money from it? are you considering how much you are losing from not having that money parked on fixed income or other investments?","edited":false,"downs":0,"author_flair_css_class":null,"collapsed":false,"is_submitter":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Is it though? are you running it 24 hours and making money from it? are you considering how much you are losing from not having that money parked on fixed income or other investments?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"associated_award":null,"stickied":false,"subreddit_type":"public","can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":"dark","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma2kfr9/","num_reports":null,"locked":false,"name":"t1_ma2kfr9","created":1738268201,"subreddit":"LocalLLaMA","author_flair_text":null,"treatment_tags":[],"created_utc":1738268201,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":"","collapsed_because_crowd_control":null,"mod_reports":[],"mod_note":null,"distinguished":null}},{"kind":"more","data":{"count":1,"name":"t1_ma3aw76","id":"ma3aw76","parent_id":"t1_ma2c2ud","depth":3,"children":["ma3aw76"]}}],"before":null}},"user_reports":[],"saved":false,"id":"ma2c2ud","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"More-Acadia2355","can_mod_post":false,"send_replies":true,"parent_id":"t1_ma1o8nh","score":3,"author_fullname":"t2_iom84v7m6","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"The difference is that renting is a COST while having your own equipment is an ASSET.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_ma2c2ud","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The difference is that renting is a COST while having your own equipment is an ASSET.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1idseqb","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma2c2ud/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738265892,"author_flair_text":null,"treatment_tags":[],"created_utc":1738265892,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"more","data":{"count":2,"name":"t1_ma3rdmr","id":"ma3rdmr","parent_id":"t1_ma1o8nh","depth":2,"children":["ma3rdmr"]}}],"before":null}},"user_reports":[],"saved":false,"id":"ma1o8nh","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"[deleted]","can_mod_post":false,"send_replies":true,"parent_id":"t1_ma1nupv","score":7,"approved_by":null,"report_reasons":null,"all_awardings":[],"subreddit_id":"t5_81eyvm","body":"I mean, I don’t understand why. Those model are cool, but if you are going to produce something useful, you probably have the money to rent a proper server to run it.","edited":false,"author_flair_css_class":null,"collapsed":false,"downs":0,"is_submitter":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I mean, I don’t understand why. Those model are cool, but if you are going to produce something useful, you probably have the money to rent a proper server to run it.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"associated_award":null,"stickied":false,"subreddit_type":"public","can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":"dark","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma1o8nh/","num_reports":null,"locked":false,"name":"t1_ma1o8nh","created":1738259346,"subreddit":"LocalLLaMA","author_flair_text":null,"treatment_tags":[],"created_utc":1738259346,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":"","collapsed_because_crowd_control":null,"mod_reports":[],"mod_note":null,"distinguished":null}}],"before":null}},"user_reports":[],"saved":false,"id":"ma1nupv","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"SporksInjected","can_mod_post":false,"created_utc":1738259242,"send_replies":true,"parent_id":"t3_1idseqb","score":26,"author_fullname":"t2_55xlq1cy","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"15 min later all the nvme adapters are gone","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_ma1nupv","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;15 min later all the nvme adapters are gone&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma1nupv/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738259242,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1idseqb","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":26}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"more","data":{"count":1,"name":"t1_ma1vty5","id":"ma1vty5","parent_id":"t1_ma1uqu8","depth":2,"children":["ma1vty5"]}}],"before":null}},"user_reports":[],"saved":false,"id":"ma1uqu8","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Thomas-Lore","can_mod_post":false,"created_utc":1738261136,"send_replies":true,"parent_id":"t1_ma1sy3r","score":5,"author_fullname":"t2_5hobp6m4","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Deepseek v3 might be a better choice, not sure if it is available at those quants though. (I wonder how they compare if you stop R1 from thinking, if they are very similar then loading R1 makes sense, you have a choice then to use think or not.)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_ma1uqu8","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Deepseek v3 might be a better choice, not sure if it is available at those quants though. (I wonder how they compare if you stop R1 from thinking, if they are very similar then loading R1 makes sense, you have a choice then to use think or not.)&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1idseqb","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma1uqu8/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738261136,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"more","data":{"count":1,"name":"t1_ma4uenn","id":"ma4uenn","parent_id":"t1_ma4it43","depth":3,"children":["ma4uenn"]}}],"before":null}},"user_reports":[],"saved":false,"id":"ma4it43","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"coder543","can_mod_post":false,"send_replies":true,"parent_id":"t1_ma2q4s6","score":2,"author_fullname":"t2_fboum","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"reasoning\\\\_effort should just be a matter of adjusting the logit\\\\_bias of the &lt;/think&gt; token so that it becomes more or less likely, depending on how much effort you want the model to apply.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_ma4it43","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;reasoning_effort should just be a matter of adjusting the logit_bias of the &amp;lt;/think&amp;gt; token so that it becomes more or less likely, depending on how much effort you want the model to apply.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1idseqb","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma4it43/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738289468,"author_flair_text":null,"treatment_tags":[],"created_utc":1738289468,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"ma2q4s6","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"VoidAlchemy","can_mod_post":false,"created_utc":1738269782,"send_replies":true,"parent_id":"t1_ma1sy3r","score":5,"author_fullname":"t2_n321yfw5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Sure, but being able to cap the max CoT tokens would be great.\\n\\nThe DeepSeek official API says:\\n\\n\\\\&gt; Note that the CoT output can reach up to 32K tokens, and the parameter to control the CoT length (\`reasoning_effort\`) will be available soon.\\n\\nHopefully a similar \\\\\`reasoning\\\\_effort\\\\\` and \\\\\`strip\\\\_cot\\\\\` behavior will land in \\\\\`litellm\\\\\` etc soon.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_ma2q4s6","is_submitter":true,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Sure, but being able to cap the max CoT tokens would be great.&lt;/p&gt;\\n\\n&lt;p&gt;The DeepSeek official API says:&lt;/p&gt;\\n\\n&lt;p&gt;&amp;gt; Note that the CoT output can reach up to 32K tokens, and the parameter to control the CoT length (&lt;code&gt;reasoning_effort&lt;/code&gt;) will be available soon.&lt;/p&gt;\\n\\n&lt;p&gt;Hopefully a similar \`reasoning_effort\` and \`strip_cot\` behavior will land in \`litellm\` etc soon.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1idseqb","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma2q4s6/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738269782,"author_flair_text":"llama.cpp","treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}}],"before":null}},"user_reports":[],"saved":false,"id":"ma1sy3r","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Firepal64","can_mod_post":false,"created_utc":1738260639,"send_replies":true,"parent_id":"t3_1idseqb","score":23,"author_fullname":"t2_11i4bd","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Using a CoT-oriented model without CoT is a bit of an oxymoron \\\\^\\\\^'","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_ma1sy3r","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Using a CoT-oriented model without CoT is a bit of an oxymoron ^^&amp;#39;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma1sy3r/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738260639,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1idseqb","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":23}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"ma1ltw7","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"JacketHistorical2321","can_mod_post":false,"created_utc":1738258689,"send_replies":true,"parent_id":"t3_1idseqb","score":9,"author_fullname":"t2_bsvkuoyj","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"What is the exact command you used?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_ma1ltw7","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;What is the exact command you used?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma1ltw7/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738258689,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1idseqb","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":9}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"ma3a5ny","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"VoidAlchemy","can_mod_post":false,"created_utc":1738275342,"send_replies":true,"parent_id":"t1_ma2wgce","score":4,"author_fullname":"t2_n321yfw5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"you're doing gods work friend! thanks for the PSA!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_ma3a5ny","is_submitter":true,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;you&amp;#39;re doing gods work friend! thanks for the PSA!&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1idseqb","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma3a5ny/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738275342,"author_flair_text":"llama.cpp","treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}}],"before":null}},"user_reports":[],"saved":false,"id":"ma2wgce","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"ArtifartX","can_mod_post":false,"created_utc":1738271523,"send_replies":true,"parent_id":"t3_1idseqb","score":8,"author_fullname":"t2_nwurczl3","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Link for anyone wading through the distills on huggingface looking for the real R1's he is referring to: https://huggingface.co/unsloth/DeepSeek-R1-GGUF/tree/main","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_ma2wgce","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Link for anyone wading through the distills on huggingface looking for the real R1&amp;#39;s he is referring to: &lt;a href=\\"https://huggingface.co/unsloth/DeepSeek-R1-GGUF/tree/main\\"&gt;https://huggingface.co/unsloth/DeepSeek-R1-GGUF/tree/main&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma2wgce/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738271523,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1idseqb","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":8}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"ma5hljy","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"HelpfulFriendlyOne","can_mod_post":false,"created_utc":1738302912,"send_replies":true,"parent_id":"t1_ma32ifr","score":4,"author_fullname":"t2_rhh5cmkb1","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I'd wait 20 minutes for answers to some questions, like power automate stuff that works (isn't a blatant hallucination).  I'm having trouble doing pretty basic things like referencing the first cell in a table and claude and chatgpt are just hallucinating on every question i ask.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_ma5hljy","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;d wait 20 minutes for answers to some questions, like power automate stuff that works (isn&amp;#39;t a blatant hallucination).  I&amp;#39;m having trouble doing pretty basic things like referencing the first cell in a table and claude and chatgpt are just hallucinating on every question i ask.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1idseqb","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma5hljy/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738302912,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}}],"before":null}},"user_reports":[],"saved":false,"id":"ma32ifr","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"DialDad","can_mod_post":false,"created_utc":1738273197,"send_replies":true,"parent_id":"t3_1idseqb","score":8,"author_fullname":"t2_v5e18g9ii","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I mean, it's cool that we can download these quantized Deepseek R1s and \\\\*run\\\\* them on our (fairly beefy) \\"regular\\" machines, but realistically 2 tokens/second, when you take into account the fairly long \\"thinking\\" COT portions is pretty damn slow.\\n\\nWhen you add more context it slows down a bit too.\\n\\nBasically it's not really usable for \\"real time\\" at all. It's like... ask a question and come back in 20 minutes to see if you have an answer. Pretty neat that we CAN run it, but not super usable for most of us.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_ma32ifr","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I mean, it&amp;#39;s cool that we can download these quantized Deepseek R1s and *run* them on our (fairly beefy) &amp;quot;regular&amp;quot; machines, but realistically 2 tokens/second, when you take into account the fairly long &amp;quot;thinking&amp;quot; COT portions is pretty damn slow.&lt;/p&gt;\\n\\n&lt;p&gt;When you add more context it slows down a bit too.&lt;/p&gt;\\n\\n&lt;p&gt;Basically it&amp;#39;s not really usable for &amp;quot;real time&amp;quot; at all. It&amp;#39;s like... ask a question and come back in 20 minutes to see if you have an answer. Pretty neat that we CAN run it, but not super usable for most of us.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma32ifr/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738273197,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1idseqb","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":8}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"ma4yzq3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"VoidAlchemy","can_mod_post":false,"created_utc":1738295110,"send_replies":true,"parent_id":"t1_ma3gr2i","score":3,"author_fullname":"t2_n321yfw5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Haha I saw Steve on Gamers Nexus did a whole show about it. People scalping them for almost $5090 lmao... Good luck!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_ma4yzq3","is_submitter":true,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Haha I saw Steve on Gamers Nexus did a whole show about it. People scalping them for almost $5090 lmao... Good luck!&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1idseqb","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma4yzq3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738295110,"author_flair_text":"llama.cpp","treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"ma3gr2i","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"PizzaCatAm","can_mod_post":false,"created_utc":1738277268,"send_replies":true,"parent_id":"t3_1idseqb","score":6,"author_fullname":"t2_xr1nq0s3u","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Don’t worry, I have been rushing to get a 5090 for hours without success.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_ma3gr2i","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Don’t worry, I have been rushing to get a 5090 for hours without success.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma3gr2i/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738277268,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1idseqb","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"more","data":{"count":5,"name":"t1_ma5b0av","id":"ma5b0av","parent_id":"t1_ma3smim","depth":1,"children":["ma5b0av"]}}],"before":null}},"user_reports":[],"saved":false,"id":"ma3smim","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"_RealUnderscore_","can_mod_post":false,"created_utc":1738280906,"send_replies":true,"parent_id":"t3_1idseqb","score":7,"author_fullname":"t2_56mwerxh","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Why not use that $1.5k for a workstation motherboard and 512GB RAM?\\n\\n## Lenovo ThinkStation P920\\n(2x hexa-channel, 238GB/s)\\n- **Mobo** $180 https://www.ebay.com/itm/185952378025\\n- **RAM** 16x $25 ($400) https://www.ebay.com/itm/326347210683\\n- **CPU** 2x $20 ($40) https://www.ebay.com/itm/265918939978\\n- **CPU heatsink** 2x $20 ($40) https://www.ebay.com/itm/276182038781\\n\\nAlso:\\n- **PSU** $35 https://www.ebay.com/itm/175589327643 \\n- **Cables** (mobo comes with PDU for GPUs!!)\\n- **Case** (55L tower - https://psref.lenovo.com/syspool/Sys/PDF/ThinkStation/ThinkStation_P920/ThinkStation_P920_Spec.PDF)\\n\\n**Total** $695 + cables + case + other peripherals\\n\\nIf you're lazy, you can also get a prebuilt P720 $280 https://www.ebay.com/itm/405443934239 (2x quad-channel, 158GB/s) then install your own components. CPU hardly matters here, I just chose cheap and powerful for the P920.\\n\\nAlso make sure to enable NUMA in wtv program you're using.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_ma3smim","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Why not use that $1.5k for a workstation motherboard and 512GB RAM?&lt;/p&gt;\\n\\n&lt;h2&gt;Lenovo ThinkStation P920&lt;/h2&gt;\\n\\n&lt;p&gt;(2x hexa-channel, 238GB/s)\\n- &lt;strong&gt;Mobo&lt;/strong&gt; $180 &lt;a href=\\"https://www.ebay.com/itm/185952378025\\"&gt;https://www.ebay.com/itm/185952378025&lt;/a&gt;\\n- &lt;strong&gt;RAM&lt;/strong&gt; 16x $25 ($400) &lt;a href=\\"https://www.ebay.com/itm/326347210683\\"&gt;https://www.ebay.com/itm/326347210683&lt;/a&gt;\\n- &lt;strong&gt;CPU&lt;/strong&gt; 2x $20 ($40) &lt;a href=\\"https://www.ebay.com/itm/265918939978\\"&gt;https://www.ebay.com/itm/265918939978&lt;/a&gt;\\n- &lt;strong&gt;CPU heatsink&lt;/strong&gt; 2x $20 ($40) &lt;a href=\\"https://www.ebay.com/itm/276182038781\\"&gt;https://www.ebay.com/itm/276182038781&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;Also:\\n- &lt;strong&gt;PSU&lt;/strong&gt; $35 &lt;a href=\\"https://www.ebay.com/itm/175589327643\\"&gt;https://www.ebay.com/itm/175589327643&lt;/a&gt; \\n- &lt;strong&gt;Cables&lt;/strong&gt; (mobo comes with PDU for GPUs!!)\\n- &lt;strong&gt;Case&lt;/strong&gt; (55L tower - &lt;a href=\\"https://psref.lenovo.com/syspool/Sys/PDF/ThinkStation/ThinkStation_P920/ThinkStation_P920_Spec.PDF\\"&gt;https://psref.lenovo.com/syspool/Sys/PDF/ThinkStation/ThinkStation_P920/ThinkStation_P920_Spec.PDF&lt;/a&gt;)&lt;/p&gt;\\n\\n&lt;p&gt;&lt;strong&gt;Total&lt;/strong&gt; $695 + cables + case + other peripherals&lt;/p&gt;\\n\\n&lt;p&gt;If you&amp;#39;re lazy, you can also get a prebuilt P720 $280 &lt;a href=\\"https://www.ebay.com/itm/405443934239\\"&gt;https://www.ebay.com/itm/405443934239&lt;/a&gt; (2x quad-channel, 158GB/s) then install your own components. CPU hardly matters here, I just chose cheap and powerful for the P920.&lt;/p&gt;\\n\\n&lt;p&gt;Also make sure to enable NUMA in wtv program you&amp;#39;re using.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma3smim/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738280906,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1idseqb","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":7}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"ma3bw5b","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"KeyPhotojournalist96","can_mod_post":false,"created_utc":1738275846,"send_replies":true,"parent_id":"t3_1idseqb","score":8,"author_fullname":"t2_90xguv94","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I’m going to have to www.downloadmoreram.com","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_ma3bw5b","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I’m going to have to &lt;a href=\\"http://www.downloadmoreram.com\\"&gt;www.downloadmoreram.com&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma3bw5b/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738275846,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1idseqb","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":8}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"ma34f29","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"AD7GD","can_mod_post":false,"send_replies":true,"parent_id":"t1_ma2qpot","score":5,"author_fullname":"t2_gm98s","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"&gt; much more RAM, and faster too.\\n\\nMuch more for sure. Faster is unknown because digits BW is unannounced","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_ma34f29","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;much more RAM, and faster too.&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;Much more for sure. Faster is unknown because digits BW is unannounced&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1idseqb","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma34f29/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738273728,"author_flair_text":null,"treatment_tags":[],"created_utc":1738273728,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"more","data":{"count":1,"name":"t1_maczgui","id":"maczgui","parent_id":"t1_ma5qr7n","depth":3,"children":["maczgui"]}}],"before":null}},"user_reports":[],"saved":false,"id":"ma5qr7n","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Ok_Warning2146","can_mod_post":false,"send_replies":true,"parent_id":"t1_ma2qpot","score":3,"author_fullname":"t2_s6sfw4yy","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"digits is also ARM, so software that can run there is limited. Unless Digits is 546GB/s, you better go for AMD 12 channel RAM solution.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_ma5qr7n","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;digits is also ARM, so software that can run there is limited. Unless Digits is 546GB/s, you better go for AMD 12 channel RAM solution.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1idseqb","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma5qr7n/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738307751,"author_flair_text":null,"treatment_tags":[],"created_utc":1738307751,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"ma2qpot","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Lissanro","can_mod_post":false,"created_utc":1738269941,"send_replies":true,"parent_id":"t1_ma2h91c","score":10,"author_fullname":"t2_fpfao9g","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Digits has just 128GB of slow RAM, for 3K you can buy decent used EPYC platform with much more RAM, and faster too.\\n\\nDigits potentially can work too if you buy several and manage to spread the model across them, but then again, for the same budget you can get far better EPYC platform. Only advantage of Digits, it is mobile and has low power consumption - if it matters, depends on your use case.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_ma2qpot","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Digits has just 128GB of slow RAM, for 3K you can buy decent used EPYC platform with much more RAM, and faster too.&lt;/p&gt;\\n\\n&lt;p&gt;Digits potentially can work too if you buy several and manage to spread the model across them, but then again, for the same budget you can get far better EPYC platform. Only advantage of Digits, it is mobile and has low power consumption - if it matters, depends on your use case.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1idseqb","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma2qpot/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738269941,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":10}}],"before":null}},"user_reports":[],"saved":false,"id":"ma2h91c","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"VancityGaming","can_mod_post":false,"created_utc":1738267310,"send_replies":true,"parent_id":"t3_1idseqb","score":3,"author_fullname":"t2_oeun0","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I'd just bite the bullet and buy Digits at this point rather than building this.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_ma2h91c","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;d just bite the bullet and buy Digits at this point rather than building this.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma2h91c/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738267310,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1idseqb","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"more","data":{"count":1,"name":"t1_ma6p1ng","id":"ma6p1ng","parent_id":"t1_ma4mgwv","depth":1,"children":["ma6p1ng"]}}],"before":null}},"user_reports":[],"saved":false,"id":"ma4mgwv","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"robo_cap","can_mod_post":false,"created_utc":1738290698,"send_replies":true,"parent_id":"t3_1idseqb","score":5,"author_fullname":"t2_9h8my","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"You're never going to find a 5090TI since that doesn't exist.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_ma4mgwv","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You&amp;#39;re never going to find a 5090TI since that doesn&amp;#39;t exist.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma4mgwv/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738290698,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1idseqb","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"ma4z2bq","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"PatientSad2926","can_mod_post":false,"created_utc":1738295136,"send_replies":true,"parent_id":"t3_1idseqb","score":3,"author_fullname":"t2_xi7728uv5","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"The big boys just take the CPU out of the equation with RDMA GPUDirect storage or NVMe over Fabrics","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_ma4z2bq","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The big boys just take the CPU out of the equation with RDMA GPUDirect storage or NVMe over Fabrics&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma4z2bq/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738295136,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1idseqb","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"more","data":{"count":3,"name":"t1_ma1x10i","id":"ma1x10i","parent_id":"t1_ma1wb6e","depth":2,"children":["ma1x10i"]}}],"before":null}},"user_reports":[],"saved":false,"id":"ma1wb6e","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"15f026d6016c482374bf","can_mod_post":false,"created_utc":1738261562,"send_replies":true,"parent_id":"t1_ma1sbfh","score":3,"author_fullname":"t2_9yii3lq8","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Did you actually get cline to work.. with... anything?   I tried several local models, including a qwen2 coder or whatnot, and I think I tried varying 30k and 50k context, but no luck.  \\nAnd if I switch to full Sonnet or Haiku, I hit 1 minute limit caps immediately...","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_ma1wb6e","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Did you actually get cline to work.. with... anything?   I tried several local models, including a qwen2 coder or whatnot, and I think I tried varying 30k and 50k context, but no luck.&lt;br/&gt;\\nAnd if I switch to full Sonnet or Haiku, I hit 1 minute limit caps immediately...&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1idseqb","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma1wb6e/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738261562,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"ma1sbfh","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"neutralpoliticsbot","can_mod_post":false,"created_utc":1738260464,"send_replies":true,"parent_id":"t3_1idseqb","score":12,"author_fullname":"t2_5r63ljr4","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"2k context is useless I need at least 30,000\\n\\nIf you want to code with cline it won’t even run with 2k context","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_ma1sbfh","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;2k context is useless I need at least 30,000&lt;/p&gt;\\n\\n&lt;p&gt;If you want to code with cline it won’t even run with 2k context&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma1sbfh/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738260464,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1idseqb","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":12}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"ma2k25b","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"GT95","can_mod_post":false,"created_utc":1738268097,"send_replies":true,"parent_id":"t1_ma1x7jm","score":2,"author_fullname":"t2_ndro0","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"My gut feeling would be that the bottleneck could be caused by having to move intermediate results between RAM and GPU memory. But I would like someone with actual knowledge of the internals to confirm this.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_ma2k25b","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;My gut feeling would be that the bottleneck could be caused by having to move intermediate results between RAM and GPU memory. But I would like someone with actual knowledge of the internals to confirm this.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1idseqb","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma2k25b/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738268097,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"ma2v5zr","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"VoidAlchemy","can_mod_post":false,"created_utc":1738271166,"send_replies":true,"parent_id":"t1_ma1x7jm","score":1,"author_fullname":"t2_n321yfw5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I have the same question as that makes sense intuitively. I'm need to pin my constants and only change a few variables during bench marking to be sure. e.g. 2k context, same prompts, one with cuda disabled, one with it enabled but no offload, and one with 5 layers offloaded. etc... In anecdotal testing its between 1.3 to 2 tok/sec or so at low context, but I've been fiddling too many bits to give a solid answer.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_ma2v5zr","is_submitter":true,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I have the same question as that makes sense intuitively. I&amp;#39;m need to pin my constants and only change a few variables during bench marking to be sure. e.g. 2k context, same prompts, one with cuda disabled, one with it enabled but no offload, and one with 5 layers offloaded. etc... In anecdotal testing its between 1.3 to 2 tok/sec or so at low context, but I&amp;#39;ve been fiddling too many bits to give a solid answer.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1idseqb","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma2v5zr/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738271166,"author_flair_text":"llama.cpp","treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"ma4x00s","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"VoidAlchemy","can_mod_post":false,"created_utc":1738294388,"send_replies":true,"parent_id":"t1_ma1x7jm","score":1,"author_fullname":"t2_n321yfw5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I ran some proper benchmarks and it is slightly faster with GPU. I had forgotten to document the expert\\\\_used\\\\_count knob and it does effect speed.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_ma4x00s","is_submitter":true,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I ran some proper benchmarks and it is slightly faster with GPU. I had forgotten to document the expert_used_count knob and it does effect speed.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1idseqb","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma4x00s/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738294388,"author_flair_text":"llama.cpp","treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"ma1x7jm","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Legumbrero","can_mod_post":false,"created_utc":1738261806,"send_replies":true,"parent_id":"t3_1idseqb","score":3,"author_fullname":"t2_146dhkvfhd","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Interesting.  Anyone know why that would be faster than running at least some layers in gpu?  Seems like it wouldn't hurt unless it's causing a bottleneck?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_ma1x7jm","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Interesting.  Anyone know why that would be faster than running at least some layers in gpu?  Seems like it wouldn&amp;#39;t hurt unless it&amp;#39;s causing a bottleneck?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma1x7jm/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738261806,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1idseqb","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"more","data":{"count":1,"name":"t1_ma37tqf","id":"ma37tqf","parent_id":"t1_ma2pzpk","depth":2,"children":["ma37tqf"]}}],"before":null}},"user_reports":[],"saved":false,"id":"ma2pzpk","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Lissanro","can_mod_post":false,"created_utc":1738269743,"send_replies":true,"parent_id":"t1_ma21z3x","score":3,"author_fullname":"t2_fpfao9g","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Around 176GB for 64K context due to lack of V cache quantization and flash attention not working (with dynamic 1.73bpw R1 quant).\\n\\nFor small KV cache, around 22GB for 8K context (assuming K cache q4\\\\_0 quantization, and fp16 V cache).","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_ma2pzpk","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Around 176GB for 64K context due to lack of V cache quantization and flash attention not working (with dynamic 1.73bpw R1 quant).&lt;/p&gt;\\n\\n&lt;p&gt;For small KV cache, around 22GB for 8K context (assuming K cache q4_0 quantization, and fp16 V cache).&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1idseqb","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma2pzpk/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738269743,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"ma2vojp","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"VoidAlchemy","can_mod_post":false,"created_utc":1738271309,"send_replies":true,"parent_id":"t1_ma21z3x","score":2,"author_fullname":"t2_n321yfw5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Depends on how much context, but 2k shows this:\\n\`\`\`\\nllama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'q4_0', type_v = 'f16', n_layer = 61, can_shift = 0\\nllama_kv_cache_init:        CPU KV buffer size =  5551.00 MiB\\nllama_init_from_model: KV self size  = 5551.00 MiB, K (q4_0): 1647.00 MiB, V (f16): 3904.00 MiB\\n\`\`\`","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_ma2vojp","is_submitter":true,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Depends on how much context, but 2k shows this:\\n&lt;code&gt;\\nllama_kv_cache_init: kv_size = 2048, offload = 1, type_k = &amp;#39;q4_0&amp;#39;, type_v = &amp;#39;f16&amp;#39;, n_layer = 61, can_shift = 0\\nllama_kv_cache_init:        CPU KV buffer size =  5551.00 MiB\\nllama_init_from_model: KV self size  = 5551.00 MiB, K (q4_0): 1647.00 MiB, V (f16): 3904.00 MiB\\n&lt;/code&gt;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1idseqb","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma2vojp/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738271309,"author_flair_text":"llama.cpp","treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"ma21z3x","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"LetterRip","can_mod_post":false,"created_utc":1738263109,"send_replies":true,"parent_id":"t3_1idseqb","score":3,"author_fullname":"t2_3zb81","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"So how much ram do the KV caches take?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_ma21z3x","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;So how much ram do the KV caches take?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma21z3x/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738263109,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1idseqb","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"more","data":{"count":2,"name":"t1_ma4yvvu","id":"ma4yvvu","parent_id":"t1_ma39oag","depth":1,"children":["ma4yvvu"]}}],"before":null}},"user_reports":[],"saved":false,"id":"ma39oag","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MoneyPowerNexis","can_mod_post":false,"created_utc":1738275204,"send_replies":true,"parent_id":"t3_1idseqb","score":3,"author_fullname":"t2_635g2","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"&gt; what about $1.5k for 4x NVMe SSDs on an expansion card for 2TB \\"VRAM\\" giving theoretical max sequential read \\"memory\\" bandwidth of ~48GB/s?\\n\\nI did something like this with DeepSeek V3 Q8 since I dont have quite enough RAM to fit all the data in RAM so get about 1 t/s compared to about 5 t/s with Q6.\\n\\nI tried this with 4x 1tb orico drives of aliexpress on a bifurcation pci card. Everything is pcie 4.0 and individually those cards do 7.4GB/s. The total cost $420 AUD for the 4 ssds ($96 aud each) and the pci card ($36 aud). So around $261 USD. \\n\\nIn raid 0 I got 26GB/s using ubuntu's built in bandwidth test but found that I got less of a speed increase loading the model from a raid-0 drive then just loading from my data drive and using the ssds as swap.\\n\\nTesting DS-V3 Q8 that bumped up the speed to 2.8 t/s (loading from raid-0 it was 1.8 t/s) I think there could be a couple of reasons swap worked better: less processing overhead (leading to less latency) and better balancing of data across the drives. \\n\\nSince its not such a huge investment I'm tempted to add in another card with another 4 ssds to see how that improves things but I dont expect to see a speedup beyond what I'm getting with smaller quants and 5 t/s is still not an enjoyable speed for me.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_ma39oag","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;what about $1.5k for 4x NVMe SSDs on an expansion card for 2TB &amp;quot;VRAM&amp;quot; giving theoretical max sequential read &amp;quot;memory&amp;quot; bandwidth of ~48GB/s?&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;I did something like this with DeepSeek V3 Q8 since I dont have quite enough RAM to fit all the data in RAM so get about 1 t/s compared to about 5 t/s with Q6.&lt;/p&gt;\\n\\n&lt;p&gt;I tried this with 4x 1tb orico drives of aliexpress on a bifurcation pci card. Everything is pcie 4.0 and individually those cards do 7.4GB/s. The total cost $420 AUD for the 4 ssds ($96 aud each) and the pci card ($36 aud). So around $261 USD. &lt;/p&gt;\\n\\n&lt;p&gt;In raid 0 I got 26GB/s using ubuntu&amp;#39;s built in bandwidth test but found that I got less of a speed increase loading the model from a raid-0 drive then just loading from my data drive and using the ssds as swap.&lt;/p&gt;\\n\\n&lt;p&gt;Testing DS-V3 Q8 that bumped up the speed to 2.8 t/s (loading from raid-0 it was 1.8 t/s) I think there could be a couple of reasons swap worked better: less processing overhead (leading to less latency) and better balancing of data across the drives. &lt;/p&gt;\\n\\n&lt;p&gt;Since its not such a huge investment I&amp;#39;m tempted to add in another card with another 4 ssds to see how that improves things but I dont expect to see a speedup beyond what I&amp;#39;m getting with smaller quants and 5 t/s is still not an enjoyable speed for me.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma39oag/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738275204,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1idseqb","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"ma3ifrt","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Dead_Internet_Theory","can_mod_post":false,"created_utc":1738277765,"send_replies":true,"parent_id":"t3_1idseqb","score":3,"author_fullname":"t2_srolmvkm","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"This sounds like an use case for those crazy fast Optane drives Intel used to peddle as cache.\\n\\nOr some RAID array on a PCIe card.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_ma3ifrt","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;This sounds like an use case for those crazy fast Optane drives Intel used to peddle as cache.&lt;/p&gt;\\n\\n&lt;p&gt;Or some RAID array on a PCIe card.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma3ifrt/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738277765,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1idseqb","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"more","data":{"count":1,"name":"t1_mb2v3n7","id":"mb2v3n7","parent_id":"t1_maeb1p9","depth":2,"children":["mb2v3n7"]}}],"before":null}},"user_reports":[],"saved":false,"id":"maeb1p9","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"VoidAlchemy","can_mod_post":false,"created_utc":1738427416,"send_replies":true,"parent_id":"t1_macy51e","score":3,"author_fullname":"t2_n321yfw5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"So most gaming mobo's currently have 4x DDR5 DIMM slots and if you want to populate all 4x slots then they are not as performant and give lower overall RAM i/o bandwidth. You're better off going up to a server class mobo with a lot more ram i/o controller channels for aggregate bandwidth.\\n\\nCheck out this build [https://threadreaderapp.com/thread/1884244369907278106.html](https://threadreaderapp.com/thread/1884244369907278106.html)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_maeb1p9","is_submitter":true,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;So most gaming mobo&amp;#39;s currently have 4x DDR5 DIMM slots and if you want to populate all 4x slots then they are not as performant and give lower overall RAM i/o bandwidth. You&amp;#39;re better off going up to a server class mobo with a lot more ram i/o controller channels for aggregate bandwidth.&lt;/p&gt;\\n\\n&lt;p&gt;Check out this build &lt;a href=\\"https://threadreaderapp.com/thread/1884244369907278106.html\\"&gt;https://threadreaderapp.com/thread/1884244369907278106.html&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1idseqb","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/maeb1p9/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738427416,"author_flair_text":"llama.cpp","treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"macyov7","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"paulalesius","can_mod_post":false,"created_utc":1738408487,"send_replies":true,"parent_id":"t1_macy51e","score":2,"author_fullname":"t2_dthht","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I'm guessing on Linux you have more options to configure the protocols of 400gb ethernet for direct memory access on other machines, input from somebody that has such a set up would be appreciated as we can only read the documentation and such.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_macyov7","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;m guessing on Linux you have more options to configure the protocols of 400gb ethernet for direct memory access on other machines, input from somebody that has such a set up would be appreciated as we can only read the documentation and such.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1idseqb","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/macyov7/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738408487,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"macy51e","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"paulalesius","can_mod_post":false,"created_utc":1738408167,"send_replies":true,"parent_id":"t3_1idseqb","score":3,"author_fullname":"t2_dthht","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Does anybody know if you can build two gaming machines with tons of RAM and link them together into a bigger one? RDMA? 400gb ethernet?\\n\\nOne would even be able to have a GPU in one and 4x RAID0 using the PCI lanes on the other.\\n\\nOr even 3, cheap gaming CPUs, 192GB DDR5 on each?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_macy51e","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Does anybody know if you can build two gaming machines with tons of RAM and link them together into a bigger one? RDMA? 400gb ethernet?&lt;/p&gt;\\n\\n&lt;p&gt;One would even be able to have a GPU in one and 4x RAID0 using the PCI lanes on the other.&lt;/p&gt;\\n\\n&lt;p&gt;Or even 3, cheap gaming CPUs, 192GB DDR5 on each?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/macy51e/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738408167,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1idseqb","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"ma39p2e","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"VoidAlchemy","can_mod_post":false,"created_utc":1738275210,"send_replies":true,"parent_id":"t1_ma2oglr","score":8,"author_fullname":"t2_n321yfw5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yup.. RAM i/o bandwidth is RAM i/o bandwidth.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_ma39p2e","is_submitter":true,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yup.. RAM i/o bandwidth is RAM i/o bandwidth.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1idseqb","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma39p2e/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738275210,"author_flair_text":"llama.cpp","treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":8}},{"kind":"more","data":{"count":1,"name":"t1_ma4bnt6","id":"ma4bnt6","parent_id":"t1_ma2oglr","depth":1,"children":["ma4bnt6"]}}],"before":null}},"user_reports":[],"saved":false,"id":"ma2oglr","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"ghostinthepoison","can_mod_post":false,"created_utc":1738269319,"send_replies":true,"parent_id":"t3_1idseqb","score":5,"author_fullname":"t2_1ehautwc4x","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Is it possible to run this off of xenon core processors and 512 gigs ram without a gpu?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_ma2oglr","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Is it possible to run this off of xenon core processors and 512 gigs ram without a gpu?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma2oglr/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738269319,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1idseqb","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"total_awards_received":0,"approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"ups":2,"removal_reason":null,"link_id":"t3_1idseqb","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"more","data":{"count":1,"name":"t1_ma2vgif","id":"ma2vgif","parent_id":"t1_ma1yaz2","depth":1,"children":["ma2vgif"]}}],"before":null}},"user_reports":[],"saved":false,"id":"ma1yaz2","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":"DELETED","no_follow":true,"author":"[deleted]","can_mod_post":false,"send_replies":true,"parent_id":"t3_1idseqb","score":2,"approved_by":null,"report_reasons":null,"all_awardings":[],"subreddit_id":"t5_81eyvm","body":"[removed]","edited":false,"downs":0,"author_flair_css_class":null,"collapsed":true,"is_submitter":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;[removed]&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"associated_award":null,"stickied":false,"subreddit_type":"public","can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":"dark","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma1yaz2/","num_reports":null,"locked":false,"name":"t1_ma1yaz2","created":1738262107,"subreddit":"LocalLLaMA","author_flair_text":null,"treatment_tags":[],"created_utc":1738262107,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"","collapsed_because_crowd_control":null,"mod_reports":[],"mod_note":null,"distinguished":null}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"more","data":{"count":1,"name":"t1_ma2w7vl","id":"ma2w7vl","parent_id":"t1_ma23fxf","depth":1,"children":["ma2w7vl"]}}],"before":null}},"user_reports":[],"saved":false,"id":"ma23fxf","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Apprehensive_Dig3462","can_mod_post":false,"created_utc":1738263510,"send_replies":true,"parent_id":"t3_1idseqb","score":2,"author_fullname":"t2_u1j1ve91d","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Did you just calculate the bandwidth or is there a way to check the active current utilized bandwidth while running the model? ","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_ma23fxf","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Did you just calculate the bandwidth or is there a way to check the active current utilized bandwidth while running the model? &lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma23fxf/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738263510,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1idseqb","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"ma28u48","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"siegevjorn","can_mod_post":false,"created_utc":1738264997,"send_replies":true,"parent_id":"t3_1idseqb","score":2,"author_fullname":"t2_vdj76m1r","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"This is awesome. Tell us more!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_ma28u48","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;This is awesome. Tell us more!&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma28u48/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738264997,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1idseqb","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"more","data":{"count":2,"name":"t1_ma392dm","id":"ma392dm","parent_id":"t1_ma2bmld","depth":1,"children":["ma392dm"]}}],"before":null}},"user_reports":[],"saved":false,"id":"ma2bmld","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"xqoe","can_mod_post":false,"created_utc":1738265767,"send_replies":true,"parent_id":"t3_1idseqb","score":2,"author_fullname":"t2_5slsu5xg","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"96GB RAM or nVME?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_ma2bmld","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;96GB RAM or nVME?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma2bmld/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738265767,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1idseqb","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"ma53h5f","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"delvatheus","can_mod_post":false,"send_replies":true,"parent_id":"t1_ma503ie","score":2,"author_fullname":"t2_74h5qptn4","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"That's awesome!\\nI read somewhere that DeepSeek folks did some custom PTX coding to get their inference speeds up. Perhaps, that's something that's still possible in consumer GPUs.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_ma53h5f","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;That&amp;#39;s awesome!\\nI read somewhere that DeepSeek folks did some custom PTX coding to get their inference speeds up. Perhaps, that&amp;#39;s something that&amp;#39;s still possible in consumer GPUs.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1idseqb","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma53h5f/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738296815,"author_flair_text":null,"treatment_tags":[],"created_utc":1738296815,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"ma503ie","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"VoidAlchemy","can_mod_post":false,"created_utc":1738295518,"send_replies":true,"parent_id":"t1_ma2q720","score":2,"author_fullname":"t2_n321yfw5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yup, that is how many folks with 24GB VRAM runt the \\\\~70B IQ4\\\\_XS models by offloading whatever layers don't fit on the GPU VRAM into normal RAM. Works on llama.cpp and downstream projects. Some of the other inference engines are getting there too I believe.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_ma503ie","is_submitter":true,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yup, that is how many folks with 24GB VRAM runt the ~70B IQ4_XS models by offloading whatever layers don&amp;#39;t fit on the GPU VRAM into normal RAM. Works on llama.cpp and downstream projects. Some of the other inference engines are getting there too I believe.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1idseqb","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma503ie/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738295518,"author_flair_text":"llama.cpp","treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"ma2q720","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"delvatheus","can_mod_post":false,"created_utc":1738269799,"send_replies":true,"parent_id":"t3_1idseqb","score":2,"author_fullname":"t2_74h5qptn4","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Is it possible to use normal RAM with GPU?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_ma2q720","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Is it possible to use normal RAM with GPU?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma2q720/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738269799,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1idseqb","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"ma39zmn","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"VoidAlchemy","can_mod_post":false,"created_utc":1738275294,"send_replies":true,"parent_id":"t1_ma2r40t","score":2,"author_fullname":"t2_n321yfw5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"AMD 9950x (no 3d cache)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_ma39zmn","is_submitter":true,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;AMD 9950x (no 3d cache)&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1idseqb","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma39zmn/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738275294,"author_flair_text":"llama.cpp","treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"ma2r40t","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"SpaceCorvette","can_mod_post":false,"created_utc":1738270050,"send_replies":true,"parent_id":"t3_1idseqb","score":2,"author_fullname":"t2_w4yrm","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"What CPU are you using?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_ma2r40t","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;What CPU are you using?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma2r40t/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738270050,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1idseqb","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"ma2teqp","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"thetaFAANG","can_mod_post":false,"created_utc":1738270678,"send_replies":true,"parent_id":"t3_1idseqb","score":2,"author_fullname":"t2_da5i8ajs","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"What about with that 200gb version using that bitnet paper","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_ma2teqp","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;What about with that 200gb version using that bitnet paper&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma2teqp/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738270678,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1idseqb","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"more","data":{"count":2,"name":"t1_ma50duc","id":"ma50duc","parent_id":"t1_ma35lmj","depth":1,"children":["ma50duc"]}}],"before":null}},"user_reports":[],"saved":false,"id":"ma35lmj","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"xignaceh","can_mod_post":false,"created_utc":1738274056,"send_replies":true,"parent_id":"t3_1idseqb","score":2,"author_fullname":"t2_7ec9wuz","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"About the think tags, there is a package that removes these while using stream=true","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_ma35lmj","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;About the think tags, there is a package that removes these while using stream=true&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma35lmj/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738274056,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1idseqb","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"total_awards_received":0,"approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"ups":2,"removal_reason":null,"link_id":"t3_1idseqb","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"more","data":{"count":2,"name":"t1_ma50mj8","id":"ma50mj8","parent_id":"t1_ma37hcf","depth":1,"children":["ma50mj8"]}}],"before":null}},"user_reports":[],"saved":false,"id":"ma37hcf","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"[deleted]","can_mod_post":false,"send_replies":true,"parent_id":"t3_1idseqb","score":2,"approved_by":null,"report_reasons":null,"all_awardings":[],"subreddit_id":"t5_81eyvm","body":"So max with 4 nvme is 48gb/s about. But what about ceph cluster linked with 400GB networking?\\nHmm... actually with ceph cluster having multiple 400GB nics and dozens of nvme 5.0 drives it could achieve ridicilous speeds. Is this the reason why datacenter nvmes are so expensive now? Are large models actually run from nvme clusters?","edited":1738275406,"downs":0,"author_flair_css_class":null,"collapsed":false,"is_submitter":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;So max with 4 nvme is 48gb/s about. But what about ceph cluster linked with 400GB networking?\\nHmm... actually with ceph cluster having multiple 400GB nics and dozens of nvme 5.0 drives it could achieve ridicilous speeds. Is this the reason why datacenter nvmes are so expensive now? Are large models actually run from nvme clusters?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"associated_award":null,"stickied":false,"subreddit_type":"public","can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":"dark","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma37hcf/","num_reports":null,"locked":false,"name":"t1_ma37hcf","created":1738274577,"subreddit":"LocalLLaMA","author_flair_text":null,"treatment_tags":[],"created_utc":1738274577,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"","collapsed_because_crowd_control":null,"mod_reports":[],"mod_note":null,"distinguished":null}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"ma37lrm","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"codematt","can_mod_post":false,"created_utc":1738274611,"send_replies":true,"parent_id":"t3_1idseqb","score":2,"author_fullname":"t2_29f39z0o","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"And then people said wait !!!?? I did not need to spend 20k++ on 4 GPUs/rig to run huge models speedily for inference only ??\\n\\nIt will be way better than this even in 2 weeks, never mind a month from now ⚡️","edited":1738275424,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_ma37lrm","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;And then people said wait !!!?? I did not need to spend 20k++ on 4 GPUs/rig to run huge models speedily for inference only ??&lt;/p&gt;\\n\\n&lt;p&gt;It will be way better than this even in 2 weeks, never mind a month from now ⚡️&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma37lrm/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738274611,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1idseqb","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"total_awards_received":0,"approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"ups":2,"removal_reason":null,"link_id":"t3_1idseqb","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"ma50zqa","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"VoidAlchemy","can_mod_post":false,"created_utc":1738295856,"send_replies":true,"parent_id":"t1_ma3wp1r","score":3,"author_fullname":"t2_n321yfw5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Hey awesome you got it to go! I tweaked a few options and best setup I have is 8k context, 5 layers on GPU, and \\\\\`--override-kv deepseek2.expert\\\\_used\\\\_count=int:4\\\\\` to drop the expert used count down from its default value of 8 (faster inference likely at the cost of quality). That gets me just over 2 tok/sec. Might get you a little more usable room to play around with! Cheers!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_ma50zqa","is_submitter":true,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Hey awesome you got it to go! I tweaked a few options and best setup I have is 8k context, 5 layers on GPU, and \`--override-kv deepseek2.expert_used_count=int:4\` to drop the expert used count down from its default value of 8 (faster inference likely at the cost of quality). That gets me just over 2 tok/sec. Might get you a little more usable room to play around with! Cheers!&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1idseqb","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma50zqa/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738295856,"author_flair_text":"llama.cpp","treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"ma3wp1r","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":"DELETED","no_follow":true,"author":"[deleted]","can_mod_post":false,"send_replies":true,"parent_id":"t3_1idseqb","score":2,"approved_by":null,"report_reasons":null,"all_awardings":[],"subreddit_id":"t5_81eyvm","body":"[removed]","edited":false,"downs":0,"author_flair_css_class":null,"collapsed":true,"is_submitter":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;[removed]&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"associated_award":null,"stickied":false,"subreddit_type":"public","can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":"dark","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma3wp1r/","num_reports":null,"locked":false,"name":"t1_ma3wp1r","created":1738282205,"subreddit":"LocalLLaMA","author_flair_text":null,"treatment_tags":[],"created_utc":1738282205,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"","collapsed_because_crowd_control":null,"mod_reports":[],"mod_note":null,"distinguished":null}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"ma3y1sa","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Thedudely1","can_mod_post":false,"created_utc":1738282643,"send_replies":true,"parent_id":"t3_1idseqb","score":2,"author_fullname":"t2_i305y","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I totally agree I have found that base models of a given size are much more intelligent/creative than distilled models of a similar size. It feels like distilled models kill at benchmarks, but for any original or novel prompts, models that aren't distilled are way better. I came to this conclusion with the SDXL distilled models compared against even SD 1.5 fine tunes, which is smaller than SDXL distil. SDXL Distil creates more coherent images, but they look more \\"Ai-ish\\" and struggles with any prompts that aren't fairly basic. I assume this is one of the reasons the Phi series of models work so well.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_ma3y1sa","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I totally agree I have found that base models of a given size are much more intelligent/creative than distilled models of a similar size. It feels like distilled models kill at benchmarks, but for any original or novel prompts, models that aren&amp;#39;t distilled are way better. I came to this conclusion with the SDXL distilled models compared against even SD 1.5 fine tunes, which is smaller than SDXL distil. SDXL Distil creates more coherent images, but they look more &amp;quot;Ai-ish&amp;quot; and struggles with any prompts that aren&amp;#39;t fairly basic. I assume this is one of the reasons the Phi series of models work so well.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma3y1sa/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738282643,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1idseqb","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"ma42tig","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Vegetable_Sun_9225","can_mod_post":false,"created_utc":1738284185,"send_replies":true,"parent_id":"t3_1idseqb","score":2,"author_fullname":"t2_dsowj79s","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"That's pretty dope.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_ma42tig","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;That&amp;#39;s pretty dope.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma42tig/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738284185,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1idseqb","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"ma4elbm","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Krunkworx","can_mod_post":false,"created_utc":1738288052,"send_replies":true,"parent_id":"t3_1idseqb","score":2,"author_fullname":"t2_9dla0","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"It’s only going to get better. Get rekt ai hw bubble","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_ma4elbm","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It’s only going to get better. Get rekt ai hw bubble&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma4elbm/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738288052,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1idseqb","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"more","data":{"count":1,"name":"t1_ma5eynq","id":"ma5eynq","parent_id":"t1_ma5cyp6","depth":2,"children":["ma5eynq"]}}],"before":null}},"user_reports":[],"saved":false,"id":"ma5cyp6","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"VoidAlchemy","can_mod_post":false,"created_utc":1738300747,"send_replies":true,"parent_id":"t1_ma4xggw","score":2,"author_fullname":"t2_n321yfw5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Not all forms of quantization are the same. Check the unsloth blog for details:\\n\\n&gt; selectively quantize certain layers to higher bits (like 4bit) &amp; leave most MoE layers (like those used in GPT-4) to 1.5bit. Naively quantizing all layers breaks the model entirely, causing endless loops &amp; gibberish outputs. Our dynamic quants solve this.\\n\\nEverything is a trade-off, and it may be possible that even with this level of quant the big R1 model performs better than the smaller distill models. But it is so slow I am not gonna benchmark it haha...","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_ma5cyp6","is_submitter":true,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Not all forms of quantization are the same. Check the unsloth blog for details:&lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;selectively quantize certain layers to higher bits (like 4bit) &amp;amp; leave most MoE layers (like those used in GPT-4) to 1.5bit. Naively quantizing all layers breaks the model entirely, causing endless loops &amp;amp; gibberish outputs. Our dynamic quants solve this.&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;Everything is a trade-off, and it may be possible that even with this level of quant the big R1 model performs better than the smaller distill models. But it is so slow I am not gonna benchmark it haha...&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1idseqb","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma5cyp6/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738300747,"author_flair_text":"llama.cpp","treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"ma4xggw","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MatrixEternal","can_mod_post":false,"created_utc":1738294555,"send_replies":true,"parent_id":"t3_1idseqb","score":2,"author_fullname":"t2_9vfgkwzl","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"You used Q2. I heard that these low quants have quality degradation","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_ma4xggw","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You used Q2. I heard that these low quants have quality degradation&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma4xggw/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738294555,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1idseqb","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"ma5vfvr","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Chromix_","can_mod_post":false,"created_utc":1738310504,"send_replies":true,"parent_id":"t1_ma5bdzw","score":2,"author_fullname":"t2_k7w2h","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"The 64 GB of RAM will be almost fully consumed by the context size when running the model with a reasonable context size that allows some thinking and input of references instead of just \\"think about X for me\\". It'd still work when streaming the model from SSD, yet would likely be a bit faster with more RAM.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_ma5vfvr","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The 64 GB of RAM will be almost fully consumed by the context size when running the model with a reasonable context size that allows some thinking and input of references instead of just &amp;quot;think about X for me&amp;quot;. It&amp;#39;d still work when streaming the model from SSD, yet would likely be a bit faster with more RAM.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1idseqb","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma5vfvr/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738310504,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"ma5bdzw","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Tasty-Masterpiece-22","can_mod_post":false,"created_utc":1738300059,"send_replies":true,"parent_id":"t3_1idseqb","score":2,"author_fullname":"t2_t3bsn0sa","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Do you need 96gb of ram or would 64 be enough?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_ma5bdzw","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Do you need 96gb of ram or would 64 be enough?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma5bdzw/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738300059,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1idseqb","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"ma5ouv5","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"zoidme","can_mod_post":false,"created_utc":1738306702,"send_replies":true,"parent_id":"t3_1idseqb","score":2,"author_fullname":"t2_wpj7q","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"So if I have server rig with ~196gb ddr4 free memory and u2 micron 9200max on pci4.0 is that still good or I need to setup raid 0?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_ma5ouv5","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;So if I have server rig with ~196gb ddr4 free memory and u2 micron 9200max on pci4.0 is that still good or I need to setup raid 0?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma5ouv5/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738306702,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1idseqb","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"more","data":{"count":1,"name":"t1_ma7wffv","id":"ma7wffv","parent_id":"t1_ma7v11f","depth":2,"children":["ma7wffv"]}}],"before":null}},"user_reports":[],"saved":false,"id":"ma7v11f","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"CupcakeVirtual495","can_mod_post":false,"created_utc":1738340977,"send_replies":true,"parent_id":"t1_ma7u3rt","score":2,"author_fullname":"t2_8091ydkc","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I hadn't thought of that. Very interesting if possible even if it means slower processing","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_ma7v11f","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I hadn&amp;#39;t thought of that. Very interesting if possible even if it means slower processing&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1idseqb","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma7v11f/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738340977,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"ma7u3rt","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"mgalbraith81","can_mod_post":false,"created_utc":1738340719,"send_replies":true,"parent_id":"t3_1idseqb","score":2,"author_fullname":"t2_3mcw3sc8","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Do you think its possible to run on virtual memory if the m.2 has high read/write speeds","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_ma7u3rt","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Do you think its possible to run on virtual memory if the m.2 has high read/write speeds&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma7u3rt/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738340719,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1idseqb","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"ma7vxt6","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"SkrakOne","can_mod_post":false,"created_utc":1738341233,"send_replies":true,"parent_id":"t3_1idseqb","score":2,"author_fullname":"t2_18r6jauhws","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I'm wondering how it would work off ddr3... I mean it's slow but would be easy to fit in tmemory. So ddr3 probably faster than nvme even though processors would be pretty slow too","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_ma7vxt6","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;m wondering how it would work off ddr3... I mean it&amp;#39;s slow but would be easy to fit in tmemory. So ddr3 probably faster than nvme even though processors would be pretty slow too&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma7vxt6/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738341233,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1idseqb","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"more","data":{"count":1,"name":"t1_maqbbyr","id":"maqbbyr","parent_id":"t1_manwzub","depth":1,"children":["maqbbyr"]}}],"before":null}},"user_reports":[],"saved":false,"id":"manwzub","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"c0cky","can_mod_post":false,"created_utc":1738547722,"send_replies":true,"parent_id":"t3_1idseqb","score":2,"author_fullname":"t2_3fpot","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Is it possible to run the full unquantized model? Do you have a sense of the hardware required to achieve that?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_manwzub","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Is it possible to run the full unquantized model? Do you have a sense of the hardware required to achieve that?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/manwzub/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738547722,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1idseqb","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"mawgynd","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"SilentLennie","can_mod_post":false,"created_utc":1738665606,"send_replies":true,"parent_id":"t3_1idseqb","score":2,"author_fullname":"t2_4y34t","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"The power of MoE.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_mawgynd","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The power of MoE.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/mawgynd/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738665606,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1idseqb","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"maxwelp","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MikePfunk28","can_mod_post":false,"created_utc":1738685252,"send_replies":true,"parent_id":"t3_1idseqb","score":2,"author_fullname":"t2_48mcxxx1","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I am sure people have mentioned the thunderboltex5 or 4 expansion card?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_maxwelp","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I am sure people have mentioned the thunderboltex5 or 4 expansion card?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/maxwelp/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738685252,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1idseqb","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"more","data":{"count":1,"name":"t1_mb3ly6p","id":"mb3ly6p","parent_id":"t1_mb12p39","depth":1,"children":["mb3ly6p"]}}],"before":null}},"user_reports":[],"saved":false,"id":"mb12p39","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"akumaburn","can_mod_post":false,"created_utc":1738719141,"send_replies":true,"parent_id":"t3_1idseqb","score":2,"author_fullname":"t2_8qi7g","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"u/VoidAlchemy  Can you please share a sample working command. I want to be sure my settings are right.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_mb12p39","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;&lt;a href=\\"/u/VoidAlchemy\\"&gt;u/VoidAlchemy&lt;/a&gt;  Can you please share a sample working command. I want to be sure my settings are right.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/mb12p39/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738719141,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1idseqb","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"more","data":{"count":6,"name":"t1_mc6wm9s","id":"mc6wm9s","parent_id":"t1_mc6aphy","depth":1,"children":["mc6wm9s"]}}],"before":null}},"user_reports":[],"saved":false,"id":"mc6aphy","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"FrederikSchack","can_mod_post":false,"created_utc":1739278227,"send_replies":true,"parent_id":"t3_1idseqb","score":2,"author_fullname":"t2_7z8f9ybx","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Nice, but I think you are going to wear down those NVME drives, NAND memory has limits to their lifetime writes and you'llbe swapping a lot of data. \\n\\nIf it's 2Q, then upgrading to 256 GB to load the entire model would likely be better.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_mc6aphy","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Nice, but I think you are going to wear down those NVME drives, NAND memory has limits to their lifetime writes and you&amp;#39;llbe swapping a lot of data. &lt;/p&gt;\\n\\n&lt;p&gt;If it&amp;#39;s 2Q, then upgrading to 256 GB to load the entire model would likely be better.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/mc6aphy/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1739278227,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1idseqb","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"more","data":{"count":2,"name":"t1_meciva2","id":"meciva2","parent_id":"t1_mebi988","depth":1,"children":["meciva2"]}}],"before":null}},"user_reports":[],"saved":false,"id":"mebi988","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Teacult","can_mod_post":false,"created_utc":1740305014,"send_replies":true,"parent_id":"t3_1idseqb","score":2,"author_fullname":"t2_mv02h","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I really dont agree with you.  \\nIt is really simple to check cpu against gpu. Use low number of layer with high CTX and see the speed difference. Its not just ram. CPU is simply not as effective as GPU multiple cores ...  Core clocks are %50 but core count is 200 times more ... 48 vs 10000.\\n\\nAnother very important aspect is that these GPU's can communicate to each other not via pci-e , via infinity band or other special high speed busses , designed especially for this purpose.\\n\\nMeaning that , You parallalize a model to many GPU's than batch process lots of queries ...\\n\\nSpeed difference is incredible ...\\n\\nDo you really think OpenAI assigns 4 A100 NVIDIA per user per query ?  I bet lots of advanced caching and architectural optimizations going around there ...","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_mebi988","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I really dont agree with you.&lt;br/&gt;\\nIt is really simple to check cpu against gpu. Use low number of layer with high CTX and see the speed difference. Its not just ram. CPU is simply not as effective as GPU multiple cores ...  Core clocks are %50 but core count is 200 times more ... 48 vs 10000.&lt;/p&gt;\\n\\n&lt;p&gt;Another very important aspect is that these GPU&amp;#39;s can communicate to each other not via pci-e , via infinity band or other special high speed busses , designed especially for this purpose.&lt;/p&gt;\\n\\n&lt;p&gt;Meaning that , You parallalize a model to many GPU&amp;#39;s than batch process lots of queries ...&lt;/p&gt;\\n\\n&lt;p&gt;Speed difference is incredible ...&lt;/p&gt;\\n\\n&lt;p&gt;Do you really think OpenAI assigns 4 A100 NVIDIA per user per query ?  I bet lots of advanced caching and architectural optimizations going around there ...&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/mebi988/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1740305014,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1idseqb","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"c00d2408-d9ce-11ed-89c3-96d9f437a7a1","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"more","data":{"count":2,"name":"t1_ma2q93j","id":"ma2q93j","parent_id":"t1_ma21ae1","depth":2,"children":["ma2q93j"]}}],"before":null}},"user_reports":[],"saved":false,"id":"ma21ae1","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"spookperson","can_mod_post":false,"created_utc":1738262921,"send_replies":true,"parent_id":"t1_ma1ti73","score":5,"author_fullname":"t2_is8jf","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Supposedly the M2 Ultra Studio with 192gb of RAM can run the \\"Good\\" quality dynamic quant (ie, not the smallest) at 8k context with 15 tok/sec based on other Reddit threads about the unsloth release ","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_ma21ae1","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"Vicuna"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Supposedly the M2 Ultra Studio with 192gb of RAM can run the &amp;quot;Good&amp;quot; quality dynamic quant (ie, not the smallest) at 8k context with 15 tok/sec based on other Reddit threads about the unsloth release &lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1idseqb","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma21ae1/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738262921,"author_flair_text":"Vicuna","treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":"#9f9286","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"ma2qs7z","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"VoidAlchemy","can_mod_post":false,"send_replies":true,"parent_id":"t1_ma1wr7j","score":2,"author_fullname":"t2_n321yfw5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Check the unsloth blog, a normal q1 or q2 would be awful, but they were selective:\\n\\n\\\\&gt; selectively quantize certain layers to higher bits (like 4bit) &amp; leave most MoE layers","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_ma2qs7z","is_submitter":true,"collapsed":false,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Check the unsloth blog, a normal q1 or q2 would be awful, but they were selective:&lt;/p&gt;\\n\\n&lt;p&gt;&amp;gt; selectively quantize certain layers to higher bits (like 4bit) &amp;amp; leave most MoE layers&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1idseqb","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma2qs7z/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738269961,"author_flair_text":"llama.cpp","treatment_tags":[],"created_utc":1738269961,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"ma1wr7j","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"TakuyaTeng","can_mod_post":false,"created_utc":1738261683,"send_replies":true,"parent_id":"t1_ma1ti73","score":6,"author_fullname":"t2_y4zp8","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Not to mention quantized so aggressively and with virtually no context window lol","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_ma1wr7j","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Not to mention quantized so aggressively and with virtually no context window lol&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1idseqb","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma1wr7j/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738261683,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}}],"before":null}},"user_reports":[],"saved":false,"id":"ma1ti73","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"ThenExtension9196","can_mod_post":false,"created_utc":1738260794,"send_replies":true,"parent_id":"t3_1idseqb","score":5,"author_fullname":"t2_ess5kaos","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"You couldn’t pay me to use an LLM at anything less than 5 tokens per second.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_ma1ti73","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You couldn’t pay me to use an LLM at anything less than 5 tokens per second.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":true,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma1ti73/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738260794,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1idseqb","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"ma1zrib","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"estebansaa","can_mod_post":false,"created_utc":1738262507,"send_replies":true,"parent_id":"t3_1idseqb","score":2,"author_fullname":"t2_m8971","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":" a RAID 0 setup with lots of super fast NVME? very interesting.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_ma1zrib","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;a RAID 0 setup with lots of super fast NVME? very interesting.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma1zrib/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738262507,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1idseqb","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"ma219au","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Pawngeethree","can_mod_post":false,"created_utc":1738262913,"send_replies":true,"parent_id":"t3_1idseqb","score":2,"author_fullname":"t2_18q4v2nuio","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Raid 0 back in style???","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_ma219au","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Raid 0 back in style???&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma219au/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738262913,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1idseqb","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"ma2ydmg","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"VoidAlchemy","can_mod_post":false,"created_utc":1738272054,"send_replies":true,"parent_id":"t1_ma279qk","score":3,"author_fullname":"t2_n321yfw5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Thanks. The system isn't working too hard except for that one SSD. I have enough RAM left to comfortably browse, hack code, etc while it plugs away in the background on small tasks like reformatting emails or writing 1000 word messages or whatever.\\n\\nSure, and I admit I don't use ai for any serious kind of application haha... Cheers!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_ma2ydmg","is_submitter":true,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thanks. The system isn&amp;#39;t working too hard except for that one SSD. I have enough RAM left to comfortably browse, hack code, etc while it plugs away in the background on small tasks like reformatting emails or writing 1000 word messages or whatever.&lt;/p&gt;\\n\\n&lt;p&gt;Sure, and I admit I don&amp;#39;t use ai for any serious kind of application haha... Cheers!&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1idseqb","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma2ydmg/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738272054,"author_flair_text":"llama.cpp","treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"ma279qk","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Captain_Coffee_III","can_mod_post":false,"created_utc":1738264568,"send_replies":true,"parent_id":"t3_1idseqb","score":2,"author_fullname":"t2_33sjvqg2","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"&gt;*P.S. In my opinion this quantized R1 671B beats the pants off any of the distill model toys.* ***While slow and limited in context, it is still likely the best thing available for home users for many applications****.*\\n\\nWut?\\n\\nWhat applications would be suitable for 1-2t/s with the giant overhead of it thinking?  Your setup already consumes an entire system... for basically scraps.\\n\\nThis is definitely a really neat experiment and 100% in the realm of r/LocalLLaMA but not anything to move seriously forward with in any sort of application.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_ma279qk","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;&lt;em&gt;P.S. In my opinion this quantized R1 671B beats the pants off any of the distill model toys.&lt;/em&gt; &lt;strong&gt;&lt;em&gt;While slow and limited in context, it is still likely the best thing available for home users for many applications&lt;/em&gt;&lt;/strong&gt;&lt;em&gt;.&lt;/em&gt;&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;Wut?&lt;/p&gt;\\n\\n&lt;p&gt;What applications would be suitable for 1-2t/s with the giant overhead of it thinking?  Your setup already consumes an entire system... for basically scraps.&lt;/p&gt;\\n\\n&lt;p&gt;This is definitely a really neat experiment and 100% in the realm of &lt;a href=\\"/r/LocalLLaMA\\"&gt;r/LocalLLaMA&lt;/a&gt; but not anything to move seriously forward with in any sort of application.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma279qk/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738264568,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1idseqb","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"ma27ioc","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"And-Bee","can_mod_post":false,"created_utc":1738264635,"send_replies":true,"parent_id":"t3_1idseqb","score":2,"author_fullname":"t2_a81fjhk","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Next step is to have multiple fast NVMe SSDs in parallel and split the files across them for quick access.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_ma27ioc","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Next step is to have multiple fast NVMe SSDs in parallel and split the files across them for quick access.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma27ioc/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738264635,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1idseqb","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"total_awards_received":0,"approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"ups":2,"removal_reason":null,"link_id":"t3_1idseqb","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"ma39k0y","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"VoidAlchemy","can_mod_post":false,"created_utc":1738275170,"send_replies":true,"parent_id":"t1_ma2l9q4","score":3,"author_fullname":"t2_n321yfw5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Hey bud, agreed with you on the ClosedAI doesn't run at home. I'm guessing your 14b is a \\"distill\\" so probably actualy Qwen2.5-14B pretty sure that was supervise fine tuned on output from the real R1 model. Regardless, have fun running all the various open models on your laptop! Cheers!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_ma39k0y","is_submitter":true,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Hey bud, agreed with you on the ClosedAI doesn&amp;#39;t run at home. I&amp;#39;m guessing your 14b is a &amp;quot;distill&amp;quot; so probably actualy Qwen2.5-14B pretty sure that was supervise fine tuned on output from the real R1 model. Regardless, have fun running all the various open models on your laptop! Cheers!&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1idseqb","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma39k0y/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738275170,"author_flair_text":"llama.cpp","treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"ma2l9q4","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"[deleted]","can_mod_post":false,"send_replies":true,"parent_id":"t3_1idseqb","score":2,"approved_by":null,"report_reasons":null,"all_awardings":[],"subreddit_id":"t5_81eyvm","body":"I am running deepseek-r1:14b on a laptop. It works, however slow. But remember, you cannot run ClosedAI's (formerly known as OpenAI) any model on your computer.","edited":false,"downs":0,"author_flair_css_class":null,"collapsed":false,"is_submitter":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I am running deepseek-r1:14b on a laptop. It works, however slow. But remember, you cannot run ClosedAI&amp;#39;s (formerly known as OpenAI) any model on your computer.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"associated_award":null,"stickied":false,"subreddit_type":"public","can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":"dark","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma2l9q4/","num_reports":null,"locked":false,"name":"t1_ma2l9q4","created":1738268434,"subreddit":"LocalLLaMA","author_flair_text":null,"treatment_tags":[],"created_utc":1738268434,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"","collapsed_because_crowd_control":null,"mod_reports":[],"mod_note":null,"distinguished":null}},{"kind":"t1","data":{"total_awards_received":0,"approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"ups":2,"removal_reason":null,"link_id":"t3_1idseqb","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"more","data":{"count":1,"name":"t1_ma50qo8","id":"ma50qo8","parent_id":"t1_ma39jdh","depth":1,"children":["ma50qo8"]}}],"before":null}},"user_reports":[],"saved":false,"id":"ma39jdh","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"[deleted]","can_mod_post":false,"send_replies":true,"parent_id":"t3_1idseqb","score":2,"approved_by":null,"report_reasons":null,"all_awardings":[],"subreddit_id":"t5_81eyvm","body":"Oh fuck, Now I know how to run large models without GPU achieving about 700GB/s read speeds. It would cost a fortune but would have about 10TB of 700Gb/s capacity....\\nSo maybe large companies are not using GPUs for inferencing, but large nvme clusters which can achieve even 1tb/s.","edited":false,"downs":0,"author_flair_css_class":null,"collapsed":false,"is_submitter":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Oh fuck, Now I know how to run large models without GPU achieving about 700GB/s read speeds. It would cost a fortune but would have about 10TB of 700Gb/s capacity....\\nSo maybe large companies are not using GPUs for inferencing, but large nvme clusters which can achieve even 1tb/s.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"associated_award":null,"stickied":false,"subreddit_type":"public","can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":"dark","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma39jdh/","num_reports":null,"locked":false,"name":"t1_ma39jdh","created":1738275165,"subreddit":"LocalLLaMA","author_flair_text":null,"treatment_tags":[],"created_utc":1738275165,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"","collapsed_because_crowd_control":null,"mod_reports":[],"mod_note":null,"distinguished":null}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"ma29r2q","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"siegevjorn","can_mod_post":false,"created_utc":1738265246,"send_replies":true,"parent_id":"t3_1idseqb","score":1,"author_fullname":"t2_vdj76m1r","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"How exactly be 4x 5x4 nvme cards are faster than one 5x4 nvme card? Is this even possible to parallel process tensors in hard drive?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_ma29r2q","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;How exactly be 4x 5x4 nvme cards are faster than one 5x4 nvme card? Is this even possible to parallel process tensors in hard drive?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma29r2q/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738265246,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1idseqb","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"ma39xa7","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"VoidAlchemy","can_mod_post":false,"created_utc":1738275275,"send_replies":true,"parent_id":"t1_ma2owto","score":2,"author_fullname":"t2_n321yfw5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I specifically use 16 threads for my 9950x as using SMT isn't helping in my testing. And yes, good point, I did re-build llama.cpp for CPU-only for some testing.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_ma39xa7","is_submitter":true,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I specifically use 16 threads for my 9950x as using SMT isn&amp;#39;t helping in my testing. And yes, good point, I did re-build llama.cpp for CPU-only for some testing.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1idseqb","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma39xa7/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738275275,"author_flair_text":"llama.cpp","treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"ma2owto","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Chromix_","can_mod_post":false,"created_utc":1738269445,"send_replies":true,"parent_id":"t3_1idseqb","score":1,"author_fullname":"t2_k7w2h","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"There is no need to disable your GPU, just use the llama.cpp CPU-only build with a [higher number of threads](https://www.reddit.com/r/LocalLLaMA/comments/1iczucy/comment/m9w9721/).","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_ma2owto","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;There is no need to disable your GPU, just use the llama.cpp CPU-only build with a &lt;a href=\\"https://www.reddit.com/r/LocalLLaMA/comments/1iczucy/comment/m9w9721/\\"&gt;higher number of threads&lt;/a&gt;.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma2owto/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738269445,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1idseqb","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"more","data":{"count":4,"name":"t1_ma5rgts","id":"ma5rgts","parent_id":"t1_ma30v9z","depth":1,"children":["ma5rgts","ma3ai8e"]}}],"before":null}},"user_reports":[],"saved":false,"id":"ma30v9z","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"sunmast","can_mod_post":false,"created_utc":1738272738,"send_replies":true,"parent_id":"t3_1idseqb","score":1,"author_fullname":"t2_k7mq5","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"So it looks a EPYC with 768GB 12-channel DDR5 memory is the best bang for the buck!?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_ma30v9z","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;So it looks a EPYC with 768GB 12-channel DDR5 memory is the best bang for the buck!?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma30v9z/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738272738,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1idseqb","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"ma3pji0","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Rae_1988","can_mod_post":false,"created_utc":1738279938,"send_replies":true,"parent_id":"t3_1idseqb","score":1,"author_fullname":"t2_4ql3eu2xa","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"can you fine tune / train the models this way too?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_ma3pji0","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;can you fine tune / train the models this way too?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma3pji0/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738279938,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1idseqb","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"ma3ryq2","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Vaddieg","can_mod_post":false,"created_utc":1738280697,"send_replies":true,"parent_id":"t3_1idseqb","score":1,"author_fullname":"t2_10g07z","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Not Deepseek models are killing Nvidia sales, but CPU inference 😂","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_ma3ryq2","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Not Deepseek models are killing Nvidia sales, but CPU inference 😂&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma3ryq2/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738280697,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1idseqb","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"ma5hy06","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AlanCarrOnline","can_mod_post":false,"created_utc":1738303080,"send_replies":true,"parent_id":"t3_1idseqb","score":1,"author_fullname":"t2_ry6xs35o","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"This is incredible. I don't really have the technical knowhow to implement this, but I'm only getting around 2 tps on quanted 70B models, using a 3090. \\n\\nIf you can get the same kind of speeds on a 200+ GB, that's.... well like I said, incredible. \\n\\nI'll save this post for when I have more time and money to get my head and wallet around it!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_ma5hy06","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;This is incredible. I don&amp;#39;t really have the technical knowhow to implement this, but I&amp;#39;m only getting around 2 tps on quanted 70B models, using a 3090. &lt;/p&gt;\\n\\n&lt;p&gt;If you can get the same kind of speeds on a 200+ GB, that&amp;#39;s.... well like I said, incredible. &lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;ll save this post for when I have more time and money to get my head and wallet around it!&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma5hy06/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738303080,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1idseqb","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"ma5ujfc","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ClumsiestSwordLesbo","can_mod_post":false,"created_utc":1738309970,"send_replies":true,"parent_id":"t3_1idseqb","score":1,"author_fullname":"t2_otrai1hdc","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Didn't we with mixtral also do the thing where we played with the amount of activated MOE? There is some kV arg doe that.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_ma5ujfc","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Didn&amp;#39;t we with mixtral also do the thing where we played with the amount of activated MOE? There is some kV arg doe that.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma5ujfc/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738309970,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1idseqb","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"ma5zi8g","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"inteblio","can_mod_post":false,"created_utc":1738312982,"send_replies":true,"parent_id":"t3_1idseqb","score":1,"author_fullname":"t2_bzdh4","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Upping the lame: if you stuff USBs into every socket, fill the sata buss with ssds, PCI-it to the gills... and get as much ram as you can cram... \\n\\nit might be possible to get passable tok/sec for \\"normies\\". \\n\\nI got 1 token per 20 sec using one usb ssd. Which is very impressive (!)\\n\\nI looked at numbers yesterday. Really, you find your slider-position on speed vs cost (both exponential at the extremes). But its amazing that this is possible. And it definitely feels like \\"the start of something\\" if labotomised R1 fizzles out. Which it might not.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_ma5zi8g","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Upping the lame: if you stuff USBs into every socket, fill the sata buss with ssds, PCI-it to the gills... and get as much ram as you can cram... &lt;/p&gt;\\n\\n&lt;p&gt;it might be possible to get passable tok/sec for &amp;quot;normies&amp;quot;. &lt;/p&gt;\\n\\n&lt;p&gt;I got 1 token per 20 sec using one usb ssd. Which is very impressive (!)&lt;/p&gt;\\n\\n&lt;p&gt;I looked at numbers yesterday. Really, you find your slider-position on speed vs cost (both exponential at the extremes). But its amazing that this is possible. And it definitely feels like &amp;quot;the start of something&amp;quot; if labotomised R1 fizzles out. Which it might not.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma5zi8g/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738312982,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1idseqb","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"ma5zwhj","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"minusSeven","can_mod_post":false,"created_utc":1738313229,"send_replies":true,"parent_id":"t3_1idseqb","score":1,"author_fullname":"t2_cg29k","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Does anyone know how to put this in server and enable it through a web so that any browser can access it.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_ma5zwhj","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Does anyone know how to put this in server and enable it through a web so that any browser can access it.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma5zwhj/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738313229,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1idseqb","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"ma6csjl","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"CertainlyBright","can_mod_post":false,"created_utc":1738321195,"send_replies":true,"parent_id":"t3_1idseqb","score":1,"author_fullname":"t2_5xx2c0t2","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"So instead of gpu vram or even ddr ram which operates 100-200GB/s, you're suggesting just flash storage and cpu?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_ma6csjl","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;So instead of gpu vram or even ddr ram which operates 100-200GB/s, you&amp;#39;re suggesting just flash storage and cpu?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma6csjl/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738321195,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1idseqb","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"ma6czwp","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"rdudit","can_mod_post":false,"created_utc":1738321312,"send_replies":true,"parent_id":"t3_1idseqb","score":1,"author_fullname":"t2_mj85m","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I have one of the smaller (70b/32b/14b) DeepSeek R1s running on my MS-A1 8700G 64GB machine.\\n\\nI really thought I needed huge VRAM to do anything, but those models are doing just fine running on my little machine. I was very surprised, but the device is limited to 64GB of RAM, so can never run the big ones, but I'm happy for now.\\n\\nWill try out the unsloth R1 models on my main gaming rig this weekend I think.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_ma6czwp","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I have one of the smaller (70b/32b/14b) DeepSeek R1s running on my MS-A1 8700G 64GB machine.&lt;/p&gt;\\n\\n&lt;p&gt;I really thought I needed huge VRAM to do anything, but those models are doing just fine running on my little machine. I was very surprised, but the device is limited to 64GB of RAM, so can never run the big ones, but I&amp;#39;m happy for now.&lt;/p&gt;\\n\\n&lt;p&gt;Will try out the unsloth R1 models on my main gaming rig this weekend I think.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma6czwp/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738321312,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1idseqb","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"ma6nf3g","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"dondiegorivera","can_mod_post":false,"created_utc":1738326547,"send_replies":true,"parent_id":"t3_1idseqb","score":1,"author_fullname":"t2_6llkg","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"The quality of Unsloth quants are indeed great, I managed to run IQ1_S on a 4090+64GB. Although super slow, quality is was way superior than Qwen 32 Distill. [Documented the test here.](https://rolandpolczer.substack.com/p/deepseek-r1-running-locally)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_ma6nf3g","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The quality of Unsloth quants are indeed great, I managed to run IQ1_S on a 4090+64GB. Although super slow, quality is was way superior than Qwen 32 Distill. &lt;a href=\\"https://rolandpolczer.substack.com/p/deepseek-r1-running-locally\\"&gt;Documented the test here.&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma6nf3g/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738326547,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1idseqb","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"ma6nnr4","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"lookaround314","can_mod_post":false,"created_utc":1738326651,"send_replies":true,"parent_id":"t3_1idseqb","score":1,"author_fullname":"t2_tfx25","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"What's your CPU? Something insane with 40 threads or a normal one?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_ma6nnr4","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;What&amp;#39;s your CPU? Something insane with 40 threads or a normal one?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma6nnr4/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738326651,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1idseqb","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"ma75vyt","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AlRPP","can_mod_post":false,"created_utc":1738333435,"send_replies":true,"parent_id":"t3_1idseqb","score":1,"author_fullname":"t2_1h5xycseks","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I knew there would be a use case for intel optane eventually.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_ma75vyt","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I knew there would be a use case for intel optane eventually.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma75vyt/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738333435,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1idseqb","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"ma78au3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"lblblllb","can_mod_post":false,"created_utc":1738334234,"send_replies":true,"parent_id":"t3_1idseqb","score":1,"author_fullname":"t2_3la1cv1t","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"What CPU are you using?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_ma78au3","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;What CPU are you using?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma78au3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738334234,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1idseqb","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"ma7k3vn","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"iwalkthelonelyroads","can_mod_post":false,"created_utc":1738337866,"send_replies":true,"parent_id":"t3_1idseqb","score":1,"author_fullname":"t2_174n4u","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"you win the internets sir","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_ma7k3vn","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;you win the internets sir&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma7k3vn/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738337866,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1idseqb","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"ma7qw5x","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"JustSellCoveredCalls","can_mod_post":false,"created_utc":1738339814,"send_replies":true,"parent_id":"t3_1idseqb","score":1,"author_fullname":"t2_12oizz0f9s","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"great info thanks","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_ma7qw5x","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;great info thanks&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma7qw5x/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738339814,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1idseqb","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"ma894w8","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"mateusmachadobrandao","can_mod_post":false,"created_utc":1738344891,"send_replies":true,"parent_id":"t3_1idseqb","score":1,"author_fullname":"t2_165b5x","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Optimizing DeepSeek R1 671B Inference on a Local Gaming Rig Without a GPU\\n\\nIntroduction\\n\\nIn the rapidly evolving field of large language models (LLMs), efficient inference on consumer hardware remains a significant challenge. While high-end GPUs like the RTX 5090TI may seem like the obvious solution, recent experiments demonstrate that DeepSeek R1 671B can achieve over 2 tokens per second (tok/sec) on a local gaming rig without a dedicated GPU.\\n\\nThis tutorial outlines the optimal configuration for running DeepSeek R1 671B efficiently using only system RAM and high-speed NVMe storage, highlighting key performance insights and potential hardware optimizations.\\n\\n\\n---\\n\\nHardware &amp; Configuration\\n\\nTested System:\\n\\nCPU: High-performance multi-core processor\\n\\nRAM: 96GB system memory\\n\\nStorage: High-speed PCIe Gen 5 NVMe SSD\\n\\nGPU: Disabled for inference\\n\\n\\nKey Optimization:\\n\\nLoad only the KV cache into RAM\\n\\nAllow llama.cpp to mmap() model files directly from the NVMe SSD\\n\\nLeverage system RAM as a disk cache for active model weights\\n\\n\\nThis configuration enables inference speeds of approximately 2.13 tok/sec with a 2k context while keeping CPU usage below 30% and GPU usage negligible.\\n\\n\\n---\\n\\nBenchmarking &amp; Performance Insights\\n\\nRecent community experiments have confirmed that dynamic quantization of DeepSeek R1 671B significantly enhances performance on consumer hardware. Specifically, the DeepSeek-R1-UD-Q2_K_XL variant achieves:\\n\\n1–2 tok/sec at 2k–16k context\\n\\nUp to 8 concurrent inference slots for increased aggregate throughput\\n\\n\\nIdentifying the Bottleneck\\n\\nDuring testing, the primary bottleneck was NVMe storage performance, rather than CPU or RAM limitations. Key observations include:\\n\\nCPU utilization remained below 30%\\n\\nGPU remained largely idle\\n\\nPower supply fan did not activate, indicating minimal thermal load\\n\\n\\nThese results suggest that storage read speeds are the dominant factor influencing performance in this setup.\\n\\n\\n---\\n\\nOptimizing for Maximum Throughput\\n\\nRather than investing in a $2,000 GPU, a more cost-effective alternative is high-speed NVMe storage expansion, such as:\\n\\n4x NVMe SSDs on a PCIe expansion card (~$1,500)\\n\\n2TB of \\"VRAM-equivalent\\" storage\\n\\nTheoretical max sequential read bandwidth of ~48GB/s\\n\\n\\nThis setup may offer superior price-to-performance benefits, particularly for Mixture of Experts (MoE) models on home rigs. Additionally, if the system does not require a GPU, all 16 PCIe 5.0 lanes on gaming-class motherboards can be dedicated to NVMe storage for further optimization.\\n\\n\\n---\\n\\nFuture Considerations &amp; Community Contributions\\n\\nFurther improvements may be possible by leveraging:\\n\\nHigh-read IOPS NVMe arrays for increased memory bandwidth\\n\\nAssistant prompt modifications to streamline output generation (e.g., reducing unnecessary text using &lt;/think&gt; injections)\\n\\n\\nCommunity members with high-speed storage arrays are encouraged to share their benchmark results. Additionally, discussions with industry experts, such as Wendell from Level1Techs, could provide further insights into hardware optimizations.\\n\\n\\n---\\n\\nConclusion\\n\\nDeepSeek R1 671B can be efficiently run without a GPU by optimizing system RAM and NVMe storage usage. With proper hardware configuration, consumer-grade rigs can achieve usable inference speeds, potentially surpassing the performance of distilled models in certain applications.\\n\\nBy focusing on NVMe storage expansion over GPU investment, home users can achieve cost-effective, high-performance LLM inference while maintaining low power consumption and thermal output.\\n\\nFurther research into high-speed storage arrays and assistant prompt optimizations may unlock even greater performance gains in the future.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_ma894w8","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Optimizing DeepSeek R1 671B Inference on a Local Gaming Rig Without a GPU&lt;/p&gt;\\n\\n&lt;p&gt;Introduction&lt;/p&gt;\\n\\n&lt;p&gt;In the rapidly evolving field of large language models (LLMs), efficient inference on consumer hardware remains a significant challenge. While high-end GPUs like the RTX 5090TI may seem like the obvious solution, recent experiments demonstrate that DeepSeek R1 671B can achieve over 2 tokens per second (tok/sec) on a local gaming rig without a dedicated GPU.&lt;/p&gt;\\n\\n&lt;p&gt;This tutorial outlines the optimal configuration for running DeepSeek R1 671B efficiently using only system RAM and high-speed NVMe storage, highlighting key performance insights and potential hardware optimizations.&lt;/p&gt;\\n\\n&lt;hr/&gt;\\n\\n&lt;p&gt;Hardware &amp;amp; Configuration&lt;/p&gt;\\n\\n&lt;p&gt;Tested System:&lt;/p&gt;\\n\\n&lt;p&gt;CPU: High-performance multi-core processor&lt;/p&gt;\\n\\n&lt;p&gt;RAM: 96GB system memory&lt;/p&gt;\\n\\n&lt;p&gt;Storage: High-speed PCIe Gen 5 NVMe SSD&lt;/p&gt;\\n\\n&lt;p&gt;GPU: Disabled for inference&lt;/p&gt;\\n\\n&lt;p&gt;Key Optimization:&lt;/p&gt;\\n\\n&lt;p&gt;Load only the KV cache into RAM&lt;/p&gt;\\n\\n&lt;p&gt;Allow llama.cpp to mmap() model files directly from the NVMe SSD&lt;/p&gt;\\n\\n&lt;p&gt;Leverage system RAM as a disk cache for active model weights&lt;/p&gt;\\n\\n&lt;p&gt;This configuration enables inference speeds of approximately 2.13 tok/sec with a 2k context while keeping CPU usage below 30% and GPU usage negligible.&lt;/p&gt;\\n\\n&lt;hr/&gt;\\n\\n&lt;p&gt;Benchmarking &amp;amp; Performance Insights&lt;/p&gt;\\n\\n&lt;p&gt;Recent community experiments have confirmed that dynamic quantization of DeepSeek R1 671B significantly enhances performance on consumer hardware. Specifically, the DeepSeek-R1-UD-Q2_K_XL variant achieves:&lt;/p&gt;\\n\\n&lt;p&gt;1–2 tok/sec at 2k–16k context&lt;/p&gt;\\n\\n&lt;p&gt;Up to 8 concurrent inference slots for increased aggregate throughput&lt;/p&gt;\\n\\n&lt;p&gt;Identifying the Bottleneck&lt;/p&gt;\\n\\n&lt;p&gt;During testing, the primary bottleneck was NVMe storage performance, rather than CPU or RAM limitations. Key observations include:&lt;/p&gt;\\n\\n&lt;p&gt;CPU utilization remained below 30%&lt;/p&gt;\\n\\n&lt;p&gt;GPU remained largely idle&lt;/p&gt;\\n\\n&lt;p&gt;Power supply fan did not activate, indicating minimal thermal load&lt;/p&gt;\\n\\n&lt;p&gt;These results suggest that storage read speeds are the dominant factor influencing performance in this setup.&lt;/p&gt;\\n\\n&lt;hr/&gt;\\n\\n&lt;p&gt;Optimizing for Maximum Throughput&lt;/p&gt;\\n\\n&lt;p&gt;Rather than investing in a $2,000 GPU, a more cost-effective alternative is high-speed NVMe storage expansion, such as:&lt;/p&gt;\\n\\n&lt;p&gt;4x NVMe SSDs on a PCIe expansion card (~$1,500)&lt;/p&gt;\\n\\n&lt;p&gt;2TB of &amp;quot;VRAM-equivalent&amp;quot; storage&lt;/p&gt;\\n\\n&lt;p&gt;Theoretical max sequential read bandwidth of ~48GB/s&lt;/p&gt;\\n\\n&lt;p&gt;This setup may offer superior price-to-performance benefits, particularly for Mixture of Experts (MoE) models on home rigs. Additionally, if the system does not require a GPU, all 16 PCIe 5.0 lanes on gaming-class motherboards can be dedicated to NVMe storage for further optimization.&lt;/p&gt;\\n\\n&lt;hr/&gt;\\n\\n&lt;p&gt;Future Considerations &amp;amp; Community Contributions&lt;/p&gt;\\n\\n&lt;p&gt;Further improvements may be possible by leveraging:&lt;/p&gt;\\n\\n&lt;p&gt;High-read IOPS NVMe arrays for increased memory bandwidth&lt;/p&gt;\\n\\n&lt;p&gt;Assistant prompt modifications to streamline output generation (e.g., reducing unnecessary text using &amp;lt;/think&amp;gt; injections)&lt;/p&gt;\\n\\n&lt;p&gt;Community members with high-speed storage arrays are encouraged to share their benchmark results. Additionally, discussions with industry experts, such as Wendell from Level1Techs, could provide further insights into hardware optimizations.&lt;/p&gt;\\n\\n&lt;hr/&gt;\\n\\n&lt;p&gt;Conclusion&lt;/p&gt;\\n\\n&lt;p&gt;DeepSeek R1 671B can be efficiently run without a GPU by optimizing system RAM and NVMe storage usage. With proper hardware configuration, consumer-grade rigs can achieve usable inference speeds, potentially surpassing the performance of distilled models in certain applications.&lt;/p&gt;\\n\\n&lt;p&gt;By focusing on NVMe storage expansion over GPU investment, home users can achieve cost-effective, high-performance LLM inference while maintaining low power consumption and thermal output.&lt;/p&gt;\\n\\n&lt;p&gt;Further research into high-speed storage arrays and assistant prompt optimizations may unlock even greater performance gains in the future.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma894w8/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738344891,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1idseqb","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"ma8ed4a","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"nexusprime2015","can_mod_post":false,"created_utc":1738346342,"send_replies":true,"parent_id":"t3_1idseqb","score":1,"author_fullname":"t2_a6cyw1","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"what do you guys actually do after running these models locally which you cant do on websites?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_ma8ed4a","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;what do you guys actually do after running these models locally which you cant do on websites?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma8ed4a/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738346342,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1idseqb","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"ma8n2n1","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"weespid","can_mod_post":false,"created_utc":1738348782,"send_replies":true,"parent_id":"t3_1idseqb","score":1,"author_fullname":"t2_gggmp","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"You likely want optane drives instead of normal nand flash unless it's pure sequential reads.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_ma8n2n1","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You likely want optane drives instead of normal nand flash unless it&amp;#39;s pure sequential reads.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma8n2n1/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738348782,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1idseqb","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"ma8t3fc","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"novus_nl","can_mod_post":false,"created_utc":1738350467,"send_replies":true,"parent_id":"t3_1idseqb","score":1,"author_fullname":"t2_j73z9","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Still faster than excel starting up.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_ma8t3fc","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Still faster than excel starting up.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma8t3fc/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738350467,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1idseqb","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"ma8zpr3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"tsingtao12","can_mod_post":false,"created_utc":1738352339,"send_replies":true,"parent_id":"t3_1idseqb","score":1,"author_fullname":"t2_cllp2hl","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"time is money.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_ma8zpr3","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;time is money.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma8zpr3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738352339,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1idseqb","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"ma93fox","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"stable-diffusion-69","can_mod_post":false,"created_utc":1738353390,"send_replies":true,"parent_id":"t3_1idseqb","score":1,"author_fullname":"t2_jco7cd7ot","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Interesting.  \\nI am not sure how we can scale this but seems like there is some light at the end of the tunnel.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_ma93fox","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Interesting.&lt;br/&gt;\\nI am not sure how we can scale this but seems like there is some light at the end of the tunnel.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma93fox/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738353390,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1idseqb","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"ma96azl","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"f2466321","can_mod_post":false,"created_utc":1738354208,"send_replies":true,"parent_id":"t3_1idseqb","score":1,"author_fullname":"t2_1vnobagt","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Could i run it somehow on M3 Max with 36gb of ram ? 1tb SSD and very Fast external ssd","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_ma96azl","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Could i run it somehow on M3 Max with 36gb of ram ? 1tb SSD and very Fast external ssd&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/ma96azl/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738354208,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1idseqb","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"maa1y70","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"igniserus","can_mod_post":false,"created_utc":1738363349,"send_replies":true,"parent_id":"t3_1idseqb","score":1,"author_fullname":"t2_3xcil070","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Here's my character prompt which seems to keep the thinking out of display and under control on the 8B. Yes, it's overkill, but it gets interesting results.\\n\\nhttps://preview.redd.it/23yr6lenqege1.png?width=1919&amp;format=png&amp;auto=webp&amp;s=ae367f9eeff8f96cf59d8c0f8a73ea61337b7a5d","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_maa1y70","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Here&amp;#39;s my character prompt which seems to keep the thinking out of display and under control on the 8B. Yes, it&amp;#39;s overkill, but it gets interesting results.&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://preview.redd.it/23yr6lenqege1.png?width=1919&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ae367f9eeff8f96cf59d8c0f8a73ea61337b7a5d\\"&gt;https://preview.redd.it/23yr6lenqege1.png?width=1919&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ae367f9eeff8f96cf59d8c0f8a73ea61337b7a5d&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/maa1y70/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738363349,"media_metadata":{"23yr6lenqege1":{"status":"valid","e":"Image","m":"image/png","p":[{"y":45,"x":108,"u":"https://preview.redd.it/23yr6lenqege1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=fb59b89f4a05b39fe16f95f113298eda2894ef6a"},{"y":91,"x":216,"u":"https://preview.redd.it/23yr6lenqege1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=57526cf4d4b492468efc2eeab86d44a62f7cf909"},{"y":135,"x":320,"u":"https://preview.redd.it/23yr6lenqege1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=4547aa60d28346df84d1f48fc7067073591e1dae"},{"y":270,"x":640,"u":"https://preview.redd.it/23yr6lenqege1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=cd857b662baa135ff2eda6a30e0c74a467e6d1cc"},{"y":405,"x":960,"u":"https://preview.redd.it/23yr6lenqege1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=5d30236e360c693914b9a2469a525305aaebf5eb"},{"y":455,"x":1080,"u":"https://preview.redd.it/23yr6lenqege1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=1fbcdfd55f0ff0c591e3ae6f11d30a5718ef46c7"}],"s":{"y":810,"x":1919,"u":"https://preview.redd.it/23yr6lenqege1.png?width=1919&amp;format=png&amp;auto=webp&amp;s=ae367f9eeff8f96cf59d8c0f8a73ea61337b7a5d"},"id":"23yr6lenqege1"}},"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1idseqb","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"maa4h09","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Infinite-Topic-42","can_mod_post":false,"created_utc":1738364114,"send_replies":true,"parent_id":"t3_1idseqb","score":1,"author_fullname":"t2_1d3wm7vk2a","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Nice!!! Is it possible to hire some custom built reduced (with fewer layers) and quantized version of llm that can be performed by GPU as a draft model for speculative decoding? Does llama.cpp support such thing?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_maa4h09","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Nice!!! Is it possible to hire some custom built reduced (with fewer layers) and quantized version of llm that can be performed by GPU as a draft model for speculative decoding? Does llama.cpp support such thing?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/maa4h09/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738364114,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1idseqb","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"maa93ie","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"apolmig","can_mod_post":false,"created_utc":1738365565,"send_replies":true,"parent_id":"t3_1idseqb","score":2,"author_fullname":"t2_mn0vjud","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"goating","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_maa93ie","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;goating&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/maa93ie/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738365565,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1idseqb","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"mafci8i","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"marcusvispanius","can_mod_post":false,"created_utc":1738438537,"send_replies":true,"parent_id":"t3_1idseqb","score":1,"author_fullname":"t2_h2b5ncwui","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I wonder if Optane would help here, this seems like a low queue depth random access pattern.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_mafci8i","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I wonder if Optane would help here, this seems like a low queue depth random access pattern.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/mafci8i/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738438537,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1idseqb","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"mafllfm","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AReluctantRedditor","can_mod_post":false,"created_utc":1738441300,"send_replies":true,"parent_id":"t3_1idseqb","score":1,"author_fullname":"t2_gr6r2","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I wonder how octane would change this","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_mafllfm","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I wonder how octane would change this&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/mafllfm/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738441300,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1idseqb","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"more","data":{"count":2,"name":"t1_mak1q8i","id":"mak1q8i","parent_id":"t1_mag6iad","depth":1,"children":["mak1q8i"]}}],"before":null}},"user_reports":[],"saved":false,"id":"mag6iad","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ROYCOROI","can_mod_post":false,"created_utc":1738447841,"send_replies":true,"parent_id":"t3_1idseqb","score":1,"author_fullname":"t2_yj1k3","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"This is nice, i’m trying with one GA102 but with poor performance, how can I get some tutorials about this tweak? Can this run with openwebui?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_mag6iad","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;This is nice, i’m trying with one GA102 but with poor performance, how can I get some tutorials about this tweak? Can this run with openwebui?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/mag6iad/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738447841,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1idseqb","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"more","data":{"count":1,"name":"t1_makbtlm","id":"makbtlm","parent_id":"t1_mak266o","depth":1,"children":["makbtlm"]}}],"before":null}},"user_reports":[],"saved":false,"id":"mak266o","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"JohnNguyen_2001","can_mod_post":false,"created_utc":1738506385,"send_replies":true,"parent_id":"t3_1idseqb","score":1,"author_fullname":"t2_liht20p","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"So an Optane would do a job just fine Got it","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_mak266o","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;So an Optane would do a job just fine Got it&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/mak266o/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738506385,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1idseqb","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"mbawueb","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"VoidAlchemy","can_mod_post":false,"created_utc":1738854671,"send_replies":true,"parent_id":"t1_mba865t","score":2,"author_fullname":"t2_n321yfw5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yes indeed. I didn't explain enough, but I am looking for something to ensure that CoT doesn't use my entire limited 8k context haha.. Like if I could say \\"limit CoT tokens to 2k\\" etc. The official DeepSeek API suggests something is coming soon :tm:\\n\\n\\\\&gt; Note that the CoT output can reach up to 32K tokens, and the parameter to control the CoT length (\`reasoning_effort\`) will be available soon.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_mbawueb","is_submitter":true,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yes indeed. I didn&amp;#39;t explain enough, but I am looking for something to ensure that CoT doesn&amp;#39;t use my entire limited 8k context haha.. Like if I could say &amp;quot;limit CoT tokens to 2k&amp;quot; etc. The official DeepSeek API suggests something is coming soon :tm:&lt;/p&gt;\\n\\n&lt;p&gt;&amp;gt; Note that the CoT output can reach up to 32K tokens, and the parameter to control the CoT length (&lt;code&gt;reasoning_effort&lt;/code&gt;) will be available soon.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1idseqb","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/mbawueb/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738854671,"author_flair_text":"llama.cpp","treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"mba865t","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ZachCope","can_mod_post":false,"created_utc":1738846260,"send_replies":true,"parent_id":"t3_1idseqb","score":1,"author_fullname":"t2_dwt3h","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Think tags are obviously part of how the model works!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_mba865t","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Think tags are obviously part of how the model works!&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1idseqb/deepseek_r1_671b_over_2_toksec_without_gpu_on/mba865t/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1738846260,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1idseqb","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"more","data":{"count":19,"name":"t1_ma1rqfr","id":"ma1rqfr","parent_id":"t3_1idseqb","depth":0,"children":["ma1rqfr","ma3e5vq","ma2t46w","ma257bi","ma21eus","ma5qtqt","ma27dpd","ma5bqm8"]}}],"before":null}}]`),n=()=>e.jsx(l,{data:t});export{n as default};
