import{j as e}from"./index-CNyNkRpk.js";import{R as l}from"./RedditPostRenderer-Dza0u9i2.js";import"./index-BUchu_-K.js";const a=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"I've built myself a PC with 2x 3090, because I thought it would be a sweetspot to start with for something twice as capable than a regular single-card PC yet still fitting a regular case. \\n\\nHowever most models still seem to be either targeted at a single card, or at a server. I also likely made a mistake by using an OC-targeted mobo for 4-slots spacing between cards and x8/x8 lanes, but it only has 2x RAM slots, so I can't even shove more RAM into it to run 200gb quants. ","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Are there any quants of larger models 48 VRAM + 96 RAM can run, which are better than just 32B models?","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1m3wogu","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.86,"author_flair_background_color":null,"subreddit_type":"public","ups":14,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_1thcf8mit6","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":14,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1752932911,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;ve built myself a PC with 2x 3090, because I thought it would be a sweetspot to start with for something twice as capable than a regular single-card PC yet still fitting a regular case. &lt;/p&gt;\\n\\n&lt;p&gt;However most models still seem to be either targeted at a single card, or at a server. I also likely made a mistake by using an OC-targeted mobo for 4-slots spacing between cards and x8/x8 lanes, but it only has 2x RAM slots, so I can&amp;#39;t even shove more RAM into it to run 200gb quants. &lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1m3wogu","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"West_Investigator258","discussion_type":null,"num_comments":31,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1m3wogu/are_there_any_quants_of_larger_models_48_vram_96/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1m3wogu/are_there_any_quants_of_larger_models_48_vram_96/","subreddit_subscribers":502030,"created_utc":1752932911,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n40q254","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"fp4guru","can_mod_post":false,"send_replies":true,"parent_id":"t1_n40m6r6","score":2,"author_fullname":"t2_1tp8zldw5g","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"32k is balanced since pp speed is around 50. It takes too long to process anything more than 64k.","edited":false,"author_flair_css_class":null,"name":"t1_n40q254","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;32k is balanced since pp speed is around 50. It takes too long to process anything more than 64k.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1m3wogu","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3wogu/are_there_any_quants_of_larger_models_48_vram_96/n40q254/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752942938,"author_flair_text":null,"collapsed":false,"created_utc":1752942938,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n40m6r6","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Filo0104","can_mod_post":false,"send_replies":true,"parent_id":"t1_n400nh5","score":1,"author_fullname":"t2_1qq10p5wjo","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"how much ctx could you fit? ","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n40m6r6","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;how much ctx could you fit? &lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m3wogu","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3wogu/are_there_any_quants_of_larger_models_48_vram_96/n40m6r6/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752941717,"author_flair_text":null,"treatment_tags":[],"created_utc":1752941717,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n44ddib","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Glittering-Call8746","can_mod_post":false,"send_replies":true,"parent_id":"t1_n400nh5","score":1,"author_fullname":"t2_tqwl6sawb","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Does it work on rocm ? I have 44gb vram","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n44ddib","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Does it work on rocm ? I have 44gb vram&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m3wogu","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3wogu/are_there_any_quants_of_larger_models_48_vram_96/n44ddib/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752990357,"author_flair_text":null,"treatment_tags":[],"created_utc":1752990357,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n400nh5","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"fp4guru","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3zyz02","score":6,"author_fullname":"t2_1tp8zldw5g","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"llamacpp and yes , you have to specify which layer to  CPU. Let me see if I can find a working one from my notes. start with this , adjust to make sure each GPU is at least 75%. --override-tensor  \\"blk\\\\.(?:[7-9]|[1-9][0-8])\\\\.ffn.*=CPU\\"","edited":1752935284,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n400nh5","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;llamacpp and yes , you have to specify which layer to  CPU. Let me see if I can find a working one from my notes. start with this , adjust to make sure each GPU is at least 75%. --override-tensor  &amp;quot;blk.(?:[7-9]|[1-9][0-8]).ffn.*=CPU&amp;quot;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m3wogu","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3wogu/are_there_any_quants_of_larger_models_48_vram_96/n400nh5/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752935006,"author_flair_text":null,"treatment_tags":[],"created_utc":1752935006,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}}],"before":null}},"user_reports":[],"saved":false,"id":"n3zyz02","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"West_Investigator258","can_mod_post":false,"created_utc":1752934460,"send_replies":true,"parent_id":"t1_n3zw5v5","score":0,"author_fullname":"t2_1thcf8mit6","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"What engine would be better for it, and do I need to specify which layers to offload manually, or it handles everything by itself?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3zyz02","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;What engine would be better for it, and do I need to specify which layers to offload manually, or it handles everything by itself?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m3wogu","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3wogu/are_there_any_quants_of_larger_models_48_vram_96/n3zyz02/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752934460,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}},"user_reports":[],"saved":false,"id":"n3zw5v5","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"fp4guru","can_mod_post":false,"created_utc":1752933529,"send_replies":true,"parent_id":"t3_1m3wogu","score":11,"author_fullname":"t2_1tp8zldw5g","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Qwen3 235b q3. You can probably get 10tkps.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3zw5v5","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Qwen3 235b q3. You can probably get 10tkps.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3wogu/are_there_any_quants_of_larger_models_48_vram_96/n3zw5v5/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752933529,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m3wogu","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":11}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n42p82j","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"simracerman","can_mod_post":false,"send_replies":true,"parent_id":"t1_n412zli","score":1,"author_fullname":"t2_vbzgnic","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"The rest of this year and 2026 will tell us more about MoE development to warrant a concern. So far, cleverly built MoE architectures like the Deepseek R1 are quite promising. If R2 improves over the latest patch of R1 by an even 20%, MoE will secure a solid spot and kick out most of the Dense models over 70B.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n42p82j","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The rest of this year and 2026 will tell us more about MoE development to warrant a concern. So far, cleverly built MoE architectures like the Deepseek R1 are quite promising. If R2 improves over the latest patch of R1 by an even 20%, MoE will secure a solid spot and kick out most of the Dense models over 70B.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m3wogu","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3wogu/are_there_any_quants_of_larger_models_48_vram_96/n42p82j/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752965992,"author_flair_text":null,"treatment_tags":[],"created_utc":1752965992,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n412zli","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"eloquentemu","can_mod_post":false,"send_replies":true,"parent_id":"t1_n40x41h","score":3,"author_fullname":"t2_lpdsy","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Oh yeah, Llama3.3 70B is definitely pretty outdated at this point.\\n\\nI mean, it's hard to say until someone makes a new SOTA 70B, I think. Nvidia's tunes do add solid reasoning to the knowledge of L3.3 and seem to out perform Qwen3-32B, for example.  The efficiency of dense 70B is a valid question but you also have to remember that at scale (i.e. with batching) the benefits of MoE in inference largely go away since even small batches start to pull most experts.  Then your Qwen3-235B-A22B ends up giving about ~70B quality but needing 235B of (V)RAM _and_ bandwidth.\\n\\nMoE still saves on training compute though.  This is why I think something like 72B-A13B is very interesting - some research indicates that higher activation rates like that can give (with some tradeoffs) performance similar to dense.  However you still get some MoE training compute benefits while batched runs have fewer total parameters to manage.","edited":1752948272,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n412zli","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Oh yeah, Llama3.3 70B is definitely pretty outdated at this point.&lt;/p&gt;\\n\\n&lt;p&gt;I mean, it&amp;#39;s hard to say until someone makes a new SOTA 70B, I think. Nvidia&amp;#39;s tunes do add solid reasoning to the knowledge of L3.3 and seem to out perform Qwen3-32B, for example.  The efficiency of dense 70B is a valid question but you also have to remember that at scale (i.e. with batching) the benefits of MoE in inference largely go away since even small batches start to pull most experts.  Then your Qwen3-235B-A22B ends up giving about ~70B quality but needing 235B of (V)RAM &lt;em&gt;and&lt;/em&gt; bandwidth.&lt;/p&gt;\\n\\n&lt;p&gt;MoE still saves on training compute though.  This is why I think something like 72B-A13B is very interesting - some research indicates that higher activation rates like that can give (with some tradeoffs) performance similar to dense.  However you still get some MoE training compute benefits while batched runs have fewer total parameters to manage.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m3wogu","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3wogu/are_there_any_quants_of_larger_models_48_vram_96/n412zli/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752946897,"author_flair_text":null,"treatment_tags":[],"created_utc":1752946897,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n40x41h","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"simracerman","can_mod_post":false,"created_utc":1752945124,"send_replies":true,"parent_id":"t1_n40innm","score":5,"author_fullname":"t2_vbzgnic","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Didn’t Mistral 24B come super close on almost everything (except knowledge) to what Llama3.3 70B could do?\\n\\n70B dense will only have general reasoning better than 32B dense or smaller MoE models. That alone is a killer, but also, 70B models are inefficient for the output quality. We only had them briefly as an experiment until the MoEs killed that desire.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n40x41h","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Didn’t Mistral 24B come super close on almost everything (except knowledge) to what Llama3.3 70B could do?&lt;/p&gt;\\n\\n&lt;p&gt;70B dense will only have general reasoning better than 32B dense or smaller MoE models. That alone is a killer, but also, 70B models are inefficient for the output quality. We only had them briefly as an experiment until the MoEs killed that desire.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m3wogu","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3wogu/are_there_any_quants_of_larger_models_48_vram_96/n40x41h/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752945124,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}}],"before":null}},"user_reports":[],"saved":false,"id":"n40innm","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"eloquentemu","can_mod_post":false,"created_utc":1752940612,"send_replies":true,"parent_id":"t3_1m3wogu","score":11,"author_fullname":"t2_lpdsy","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Yeah, I feel like there's a disappointing lack of 70B dense models recently, which were the real selling point for 2x24GB cards last year.\\n\\nOthers have covered dots.llm1 (~80GB at Q4) and Qwen3-235B (132GB at Q4).  Both are good but would spill to the CPU.\\n\\nThe [PanGu Pro](https://huggingface.co/IntervitensInc/pangu-pro-moe-model) from Huawei is a 72B-A13B MoE (~40GB at Q4) but apparently was poorly made or stolen or something.  Which is a real shame because I had been waiting for that exact parameter setup.  I say all this having lazily not tried it, but maybe you'll like it :).\\n\\nNvidia has a few retrains of Llama3.3-70B like [Llama-3.3-Nemotron-Super-49B](https://huggingface.co/nvidia/Llama-3_3-Nemotron-Super-49B-v1) and [Llama-3.3-Nemotron-70B-Reward](https://huggingface.co/nvidia/Llama-3.3-Nemotron-70B-Reward).  I suspect they're more research projects than intended SOTA models, but they do perform well - better than the base for sure - but YMMV if they really perform like you might expect from of, say, a Qwen3-70B.\\n\\nAll that said, did you know HF [lets you search by model size](https://huggingface.co/models?pipeline_tag=text-generation&amp;num_parameters=min:64B,max:256B&amp;library=safetensors&amp;sort=created)? There's a lot of noise there but there's some interesting ones too, like apparently I missed [Kimi-Dev-72B](https://huggingface.co/moonshotai/Kimi-Dev-72B) from last month.  Dunno if it's any good though :)","edited":1752941987,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n40innm","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yeah, I feel like there&amp;#39;s a disappointing lack of 70B dense models recently, which were the real selling point for 2x24GB cards last year.&lt;/p&gt;\\n\\n&lt;p&gt;Others have covered dots.llm1 (~80GB at Q4) and Qwen3-235B (132GB at Q4).  Both are good but would spill to the CPU.&lt;/p&gt;\\n\\n&lt;p&gt;The &lt;a href=\\"https://huggingface.co/IntervitensInc/pangu-pro-moe-model\\"&gt;PanGu Pro&lt;/a&gt; from Huawei is a 72B-A13B MoE (~40GB at Q4) but apparently was poorly made or stolen or something.  Which is a real shame because I had been waiting for that exact parameter setup.  I say all this having lazily not tried it, but maybe you&amp;#39;ll like it :).&lt;/p&gt;\\n\\n&lt;p&gt;Nvidia has a few retrains of Llama3.3-70B like &lt;a href=\\"https://huggingface.co/nvidia/Llama-3_3-Nemotron-Super-49B-v1\\"&gt;Llama-3.3-Nemotron-Super-49B&lt;/a&gt; and &lt;a href=\\"https://huggingface.co/nvidia/Llama-3.3-Nemotron-70B-Reward\\"&gt;Llama-3.3-Nemotron-70B-Reward&lt;/a&gt;.  I suspect they&amp;#39;re more research projects than intended SOTA models, but they do perform well - better than the base for sure - but YMMV if they really perform like you might expect from of, say, a Qwen3-70B.&lt;/p&gt;\\n\\n&lt;p&gt;All that said, did you know HF &lt;a href=\\"https://huggingface.co/models?pipeline_tag=text-generation&amp;amp;num_parameters=min:64B,max:256B&amp;amp;library=safetensors&amp;amp;sort=created\\"&gt;lets you search by model size&lt;/a&gt;? There&amp;#39;s a lot of noise there but there&amp;#39;s some interesting ones too, like apparently I missed &lt;a href=\\"https://huggingface.co/moonshotai/Kimi-Dev-72B\\"&gt;Kimi-Dev-72B&lt;/a&gt; from last month.  Dunno if it&amp;#39;s any good though :)&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3wogu/are_there_any_quants_of_larger_models_48_vram_96/n40innm/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752940612,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m3wogu","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":11}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n44didz","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Glittering-Call8746","can_mod_post":false,"created_utc":1752990428,"send_replies":true,"parent_id":"t1_n41c06m","score":1,"author_fullname":"t2_tqwl6sawb","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"U have 128gb ram and 44gb vram rocm, where do I begin ?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n44didz","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;U have 128gb ram and 44gb vram rocm, where do I begin ?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m3wogu","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3wogu/are_there_any_quants_of_larger_models_48_vram_96/n44didz/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752990428,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n41c06m","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"dionisioalcaraz","can_mod_post":false,"created_utc":1752949725,"send_replies":true,"parent_id":"t3_1m3wogu","score":8,"author_fullname":"t2_rl5alicdt","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"If you can get 128GB RAM you will be able run DeepSeek R1 ubergarm IQ1\\\\_S\\\\_R4 (140G). R1 is amazingly resilient to heavy quantization. Check out table 2 in [https://arxiv.org/pdf/2505.02390v2](https://arxiv.org/pdf/2505.02390v2)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n41c06m","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;If you can get 128GB RAM you will be able run DeepSeek R1 ubergarm IQ1_S_R4 (140G). R1 is amazingly resilient to heavy quantization. Check out table 2 in &lt;a href=\\"https://arxiv.org/pdf/2505.02390v2\\"&gt;https://arxiv.org/pdf/2505.02390v2&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3wogu/are_there_any_quants_of_larger_models_48_vram_96/n41c06m/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752949725,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m3wogu","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":8}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3zuqkz","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"LagOps91","can_mod_post":false,"created_utc":1752933046,"send_replies":true,"parent_id":"t3_1m3wogu","score":6,"author_fullname":"t2_3wi6j7vwh","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"you should be able to run dots.llm1 at good speed with this. i have heard good things about the model. Running the large qwen MoE should also be possible (i did hear dots was better and the large quen MoE isn't so great).","edited":1752933250,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3zuqkz","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;you should be able to run dots.llm1 at good speed with this. i have heard good things about the model. Running the large qwen MoE should also be possible (i did hear dots was better and the large quen MoE isn&amp;#39;t so great).&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3wogu/are_there_any_quants_of_larger_models_48_vram_96/n3zuqkz/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752933046,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m3wogu","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4144ii","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"No_Shape_3423","can_mod_post":false,"created_utc":1752947239,"send_replies":true,"parent_id":"t3_1m3wogu","score":4,"author_fullname":"t2_1mpbnkwidj","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Qwen3 32b Q8/BF16. Nemotron Super 49b Q6/Q8. Qwen3 30b a3b BF16.  Qwen2.5 finetunes like Athene v2 70b Q8.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4144ii","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Qwen3 32b Q8/BF16. Nemotron Super 49b Q6/Q8. Qwen3 30b a3b BF16.  Qwen2.5 finetunes like Athene v2 70b Q8.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3wogu/are_there_any_quants_of_larger_models_48_vram_96/n4144ii/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752947239,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m3wogu","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n40rwm1","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"ForsookComparison","can_mod_post":false,"created_utc":1752943527,"send_replies":true,"parent_id":"t3_1m3wogu","score":4,"author_fullname":"t2_on5es7pe3","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Llama 3.3 70B Q4 beats Qwen3-32B in a lot of tasks.\\n\\nNemotron Super 49B can too, but less-reliably so","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n40rwm1","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Llama 3.3 70B Q4 beats Qwen3-32B in a lot of tasks.&lt;/p&gt;\\n\\n&lt;p&gt;Nemotron Super 49B can too, but less-reliably so&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3wogu/are_there_any_quants_of_larger_models_48_vram_96/n40rwm1/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752943527,"author_flair_text":"llama.cpp","treatment_tags":[],"link_id":"t3_1m3wogu","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n42m13w","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"henfiber","can_mod_post":false,"created_utc":1752964863,"send_replies":true,"parent_id":"t3_1m3wogu","score":3,"author_fullname":"t2_lw9me25","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"You need VRAM/RAM also for longer context, it's useful that you have some space left for that.\\n\\nYou may also run multiple models in parallel, such as speect-to-text, text-to-speect, image generation, architect-coder pairs, vision or omni models etc.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n42m13w","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You need VRAM/RAM also for longer context, it&amp;#39;s useful that you have some space left for that.&lt;/p&gt;\\n\\n&lt;p&gt;You may also run multiple models in parallel, such as speect-to-text, text-to-speect, image generation, architect-coder pairs, vision or omni models etc.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3wogu/are_there_any_quants_of_larger_models_48_vram_96/n42m13w/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752964863,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m3wogu","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4064ep","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"stoppableDissolution","can_mod_post":false,"created_utc":1752936737,"send_replies":true,"parent_id":"t3_1m3wogu","score":2,"author_fullname":"t2_1n0su21k4z","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Um, q4 of llama70 or q6 of nemotron with q8 context?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4064ep","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Um, q4 of llama70 or q6 of nemotron with q8 context?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3wogu/are_there_any_quants_of_larger_models_48_vram_96/n4064ep/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752936737,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m3wogu","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n40u4oq","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"a_beautiful_rhind","can_mod_post":false,"created_utc":1752944219,"send_replies":true,"parent_id":"t1_n40nh11","score":3,"author_fullname":"t2_h5utwre7","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I talked to this model on open router and it was dumb as rocks. Better off with QwQ or 32b.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n40u4oq","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I talked to this model on open router and it was dumb as rocks. Better off with QwQ or 32b.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m3wogu","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3wogu/are_there_any_quants_of_larger_models_48_vram_96/n40u4oq/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752944219,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n40nh11","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"vasileer","can_mod_post":false,"created_utc":1752942122,"send_replies":true,"parent_id":"t3_1m3wogu","score":2,"author_fullname":"t2_730bgdulm","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Hunyuan-A13B-Instruct, with ggufs from unsloth [https://huggingface.co/unsloth/Hunyuan-A13B-Instruct-GGUF](https://huggingface.co/unsloth/Hunyuan-A13B-Instruct-GGUF)\\n\\nhttps://preview.redd.it/x07owftwwudf1.png?width=555&amp;format=png&amp;auto=webp&amp;s=6384859d3f681202aa86deea5a302e40bf6e1faf","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n40nh11","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Hunyuan-A13B-Instruct, with ggufs from unsloth &lt;a href=\\"https://huggingface.co/unsloth/Hunyuan-A13B-Instruct-GGUF\\"&gt;https://huggingface.co/unsloth/Hunyuan-A13B-Instruct-GGUF&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://preview.redd.it/x07owftwwudf1.png?width=555&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6384859d3f681202aa86deea5a302e40bf6e1faf\\"&gt;https://preview.redd.it/x07owftwwudf1.png?width=555&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=6384859d3f681202aa86deea5a302e40bf6e1faf&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3wogu/are_there_any_quants_of_larger_models_48_vram_96/n40nh11/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752942122,"media_metadata":{"x07owftwwudf1":{"status":"valid","e":"Image","m":"image/png","p":[{"y":127,"x":108,"u":"https://preview.redd.it/x07owftwwudf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=9614afae25c2c8aa31cf360ef6a7393806448e03"},{"y":255,"x":216,"u":"https://preview.redd.it/x07owftwwudf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=9c4c493cb0d7b69548be880422c36c73cccdaaf5"},{"y":378,"x":320,"u":"https://preview.redd.it/x07owftwwudf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=c951294cc6b4511b1cfc9817ef1cac57c37e432f"}],"s":{"y":656,"x":555,"u":"https://preview.redd.it/x07owftwwudf1.png?width=555&amp;format=png&amp;auto=webp&amp;s=6384859d3f681202aa86deea5a302e40bf6e1faf"},"id":"x07owftwwudf1"}},"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m3wogu","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n41w63z","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"jacek2023","can_mod_post":false,"created_utc":1752956238,"send_replies":true,"parent_id":"t3_1m3wogu","score":2,"author_fullname":"t2_vqgbql9w","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Of course: llama 4 scout, dots, jamba mini, hunyuan, etc...\\n\\nI have 3x3090 plus 128GB RAM, but RAM is used mostly as a cache to load models faster ;)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n41w63z","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Of course: llama 4 scout, dots, jamba mini, hunyuan, etc...&lt;/p&gt;\\n\\n&lt;p&gt;I have 3x3090 plus 128GB RAM, but RAM is used mostly as a cache to load models faster ;)&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3wogu/are_there_any_quants_of_larger_models_48_vram_96/n41w63z/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752956238,"author_flair_text":"llama.cpp","treatment_tags":[],"link_id":"t3_1m3wogu","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n40ozqn","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ParaboloidalCrest","can_mod_post":false,"created_utc":1752942601,"send_replies":true,"parent_id":"t3_1m3wogu","score":2,"author_fullname":"t2_nc2u4f7","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Maybe run Qwen3-30B-A3B @ **BF16** and don't worry about quantization degradation no more?","edited":1752944032,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n40ozqn","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Maybe run Qwen3-30B-A3B @ &lt;strong&gt;BF16&lt;/strong&gt; and don&amp;#39;t worry about quantization degradation no more?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3wogu/are_there_any_quants_of_larger_models_48_vram_96/n40ozqn/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752942601,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m3wogu","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n40txvv","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"a_beautiful_rhind","can_mod_post":false,"created_utc":1752944160,"send_replies":true,"parent_id":"t3_1m3wogu","score":1,"author_fullname":"t2_h5utwre7","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Smaller quants of mistral-large or command-A might fit.\\n\\nRun MLC on your machine and see what kind of ram bandwidth you get for hybrid inference.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n40txvv","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Smaller quants of mistral-large or command-A might fit.&lt;/p&gt;\\n\\n&lt;p&gt;Run MLC on your machine and see what kind of ram bandwidth you get for hybrid inference.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3wogu/are_there_any_quants_of_larger_models_48_vram_96/n40txvv/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752944160,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m3wogu","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n435d8t","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"chisleu","can_mod_post":false,"send_replies":true,"parent_id":"t1_n42915f","score":2,"author_fullname":"t2_cbxyn","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"gooning. This is localllama. The answer is always gooning.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n435d8t","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;gooning. This is localllama. The answer is always gooning.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m3wogu","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3wogu/are_there_any_quants_of_larger_models_48_vram_96/n435d8t/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752971845,"author_flair_text":null,"treatment_tags":[],"created_utc":1752971845,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n42915f","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"rorowhat","can_mod_post":false,"created_utc":1752960441,"send_replies":true,"parent_id":"t1_n41valu","score":1,"author_fullname":"t2_yq51a","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"What are you using vision for?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n42915f","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;What are you using vision for?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m3wogu","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3wogu/are_there_any_quants_of_larger_models_48_vram_96/n42915f/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752960441,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n41valu","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Pedalnomica","can_mod_post":false,"created_utc":1752955955,"send_replies":true,"parent_id":"t3_1m3wogu","score":1,"author_fullname":"t2_b0d7j6x9","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Qwen 2.5 VL 72B might still be one of the better vision models you can get","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n41valu","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Qwen 2.5 VL 72B might still be one of the better vision models you can get&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3wogu/are_there_any_quants_of_larger_models_48_vram_96/n41valu/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752955955,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m3wogu","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n423k44","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"GL-AI","can_mod_post":false,"send_replies":true,"parent_id":"t1_n421uxh","score":3,"author_fullname":"t2_1sr5yw3yg0","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I think in this instance the non-reasoning model is just learning to use the &lt;think&gt; tags every time. Some of their other models cut out the thinking portion when distilling and just use the answers.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n423k44","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I think in this instance the non-reasoning model is just learning to use the &amp;lt;think&amp;gt; tags every time. Some of their other models cut out the thinking portion when distilling and just use the answers.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m3wogu","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3wogu/are_there_any_quants_of_larger_models_48_vram_96/n423k44/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752958646,"author_flair_text":null,"treatment_tags":[],"created_utc":1752958646,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n45rosk","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"droptableadventures","can_mod_post":false,"send_replies":true,"parent_id":"t1_n421uxh","score":2,"author_fullname":"t2_52zg0eoq","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"There's no fundamental difference in the two models - the \\"reasoning\\" one is just trained to output &lt;think&gt; followed by a discussion of the problem, then &lt;/think&gt; then the actual answer.\\n\\nBoth are just normal output from the LLM. Sometimes the API you're using the LLM through splits them up, and presents them as if they're different, but it's all just ordinary model output.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n45rosk","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;There&amp;#39;s no fundamental difference in the two models - the &amp;quot;reasoning&amp;quot; one is just trained to output &amp;lt;think&amp;gt; followed by a discussion of the problem, then &amp;lt;/think&amp;gt; then the actual answer.&lt;/p&gt;\\n\\n&lt;p&gt;Both are just normal output from the LLM. Sometimes the API you&amp;#39;re using the LLM through splits them up, and presents them as if they&amp;#39;re different, but it&amp;#39;s all just ordinary model output.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m3wogu","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3wogu/are_there_any_quants_of_larger_models_48_vram_96/n45rosk/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753016756,"author_flair_text":null,"treatment_tags":[],"created_utc":1753016756,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n421uxh","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"West_Investigator258","can_mod_post":false,"created_utc":1752958092,"send_replies":true,"parent_id":"t1_n41ykqv","score":1,"author_fullname":"t2_1thcf8mit6","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"How does distilling a reasoning model into a non-reasoning one work? Is it 2 separate calls for think and reply, or embedding think into reply, or just distilling reply and ignoring think?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n421uxh","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;How does distilling a reasoning model into a non-reasoning one work? Is it 2 separate calls for think and reply, or embedding think into reply, or just distilling reply and ignoring think?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m3wogu","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3wogu/are_there_any_quants_of_larger_models_48_vram_96/n421uxh/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752958092,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n41ykqv","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"GL-AI","can_mod_post":false,"created_utc":1752957024,"send_replies":true,"parent_id":"t3_1m3wogu","score":1,"author_fullname":"t2_1sr5yw3yg0","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"OpenBuddy started distilling R1-0528 into Qwen2.5-72B, it might be of interest to you. OpenBuddy/OpenBuddy-R10528DistillQwen-72B-Preview1","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n41ykqv","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;OpenBuddy started distilling R1-0528 into Qwen2.5-72B, it might be of interest to you. OpenBuddy/OpenBuddy-R10528DistillQwen-72B-Preview1&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3wogu/are_there_any_quants_of_larger_models_48_vram_96/n41ykqv/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752957024,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m3wogu","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n441b6c","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"lostnuclues","can_mod_post":false,"created_utc":1752984606,"send_replies":true,"parent_id":"t3_1m3wogu","score":1,"author_fullname":"t2_7spksnox","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"You can still run a 32B with bigger context size and with minimum quantization (fp8) . As I dont see many new 70B model which would have been idle for your setup.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n441b6c","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You can still run a 32B with bigger context size and with minimum quantization (fp8) . As I dont see many new 70B model which would have been idle for your setup.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3wogu/are_there_any_quants_of_larger_models_48_vram_96/n441b6c/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752984606,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m3wogu","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n45h5zm","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Unique_Judgment_1304","can_mod_post":false,"created_utc":1753012424,"send_replies":true,"parent_id":"t3_1m3wogu","score":1,"author_fullname":"t2_1s9hyoxo94","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"For chat, RP and storytelling there are still many good 70b Q4\\\\_K\\\\_M quants at 42.5GB size. 96GB of RAM is fine for a 48GB VRAM rig, you want your RAM to be a bit larger than your VRAM, so if your max RAM is 96GB you might have a problem after getting the 4th 3090.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n45h5zm","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;For chat, RP and storytelling there are still many good 70b Q4_K_M quants at 42.5GB size. 96GB of RAM is fine for a 48GB VRAM rig, you want your RAM to be a bit larger than your VRAM, so if your max RAM is 96GB you might have a problem after getting the 4th 3090.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3wogu/are_there_any_quants_of_larger_models_48_vram_96/n45h5zm/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753012424,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m3wogu","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),n=()=>e.jsx(l,{data:a});export{n as default};
