import{j as e}from"./index-Cd3v0jxz.js";import{R as l}from"./RedditPostRenderer-cI96YLhy.js";import"./index-DGBoKyQm.js";const a=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Less than two weeks Kimi K2's release, Alibaba Qwen's new Qwen3-Coder surpasses it with half the size and double the context window. Despite a significant initial lead, open source models are catching up to closed source and seem to be reaching escape velocity.","link_flair_richtext":[{"e":"text","t":"Discussion"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":95,"top_awarded_type":null,"hide_score":false,"name":"t3_1m7kkyn","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.92,"author_flair_background_color":null,"ups":264,"total_awards_received":0,"media_embed":{},"thumbnail_width":140,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_58qturpl","secure_media":null,"is_reddit_media_domain":true,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Discussion","can_mod_post":false,"score":264,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"https://a.thumbs.redditmedia.com/4F_QBog-_2mH7QRiv8VyzkdamiGlY40D_u3V_zWrFe8.jpg","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"post_hint":"image","content_categories":null,"is_self":false,"subreddit_type":"public","created":1753303228,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"i.redd.it","allow_live_comments":false,"selftext_html":null,"likes":null,"suggested_sort":null,"banned_at_utc":null,"url_overridden_by_dest":"https://i.redd.it/krjfba3oqoef1.jpeg","view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"preview":{"images":[{"source":{"url":"https://preview.redd.it/krjfba3oqoef1.jpeg?auto=webp&amp;s=b6cbfb5587cef2fa66062ecc89fb256764949473","width":1512,"height":1032},"resolutions":[{"url":"https://preview.redd.it/krjfba3oqoef1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=8cf6d39fa1fa4f5683732a0b8993daf74e849afa","width":108,"height":73},{"url":"https://preview.redd.it/krjfba3oqoef1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=75db258338b42adc68e3bb0413ff39fa017bc706","width":216,"height":147},{"url":"https://preview.redd.it/krjfba3oqoef1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=24d9a0eca278a1c3b8db5b848d01205a811bb68d","width":320,"height":218},{"url":"https://preview.redd.it/krjfba3oqoef1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=c50574d0e0fc9f8e0044c2d18d3618b1d155e4e7","width":640,"height":436},{"url":"https://preview.redd.it/krjfba3oqoef1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=a748b89f2ff4d8a9a5d9acb8b0eb069410e02c88","width":960,"height":655},{"url":"https://preview.redd.it/krjfba3oqoef1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e9a2fa720b512765ba2dc5a092f21cccc7eac5f8","width":1080,"height":737}],"variants":{},"id":"SyAH9oAX8vUViOHksDj2yqNlqn4fwNnt93W4G27ThZw"}],"enabled":true},"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"5f921ea4-c7bc-11ed-9c23-3a00622979b4","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"mod_note":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"num_reports":null,"removal_reason":null,"link_flair_background_color":"#646d73","id":"1m7kkyn","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"abdouhlili","discussion_type":null,"num_comments":70,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1m7kkyn/less_than_two_weeks_kimi_k2s_release_alibaba/","stickied":false,"url":"https://i.redd.it/krjfba3oqoef1.jpeg","subreddit_subscribers":504025,"created_utc":1753303228,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4uwttz","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"No_Efficiency_1144","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4tm1rs","score":4,"author_fullname":"t2_1nkj9l14b0","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"https://preview.redd.it/3co3gw3xtref1.jpeg?width=630&amp;format=pjpg&amp;auto=webp&amp;s=b19f91115cfd01afe8656bd85b12ff222d53df2b\\n\\nIf someone trains a neurolens model then did they create the images?\\n\\nIt has a full camera inside the latent space.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4uwttz","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;&lt;a href=\\"https://preview.redd.it/3co3gw3xtref1.jpeg?width=630&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=b19f91115cfd01afe8656bd85b12ff222d53df2b\\"&gt;https://preview.redd.it/3co3gw3xtref1.jpeg?width=630&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=b19f91115cfd01afe8656bd85b12ff222d53df2b&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;If someone trains a neurolens model then did they create the images?&lt;/p&gt;\\n\\n&lt;p&gt;It has a full camera inside the latent space.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m7kkyn","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7kkyn/less_than_two_weeks_kimi_k2s_release_alibaba/n4uwttz/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753340699,"media_metadata":{"3co3gw3xtref1":{"status":"valid","e":"Image","m":"image/jpeg","p":[{"y":64,"x":108,"u":"https://preview.redd.it/3co3gw3xtref1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=9b6a5b8dc7d0fc46c29135177a7754c12bfd2dfe"},{"y":129,"x":216,"u":"https://preview.redd.it/3co3gw3xtref1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=7e4f9031e0738af30600b63da05ba7bcd16a1111"},{"y":192,"x":320,"u":"https://preview.redd.it/3co3gw3xtref1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=ef60dbf55b098e0db16aa93897b3cfa77ff55cc7"}],"s":{"y":378,"x":630,"u":"https://preview.redd.it/3co3gw3xtref1.jpeg?width=630&amp;format=pjpg&amp;auto=webp&amp;s=b19f91115cfd01afe8656bd85b12ff222d53df2b"},"id":"3co3gw3xtref1"}},"author_flair_text":null,"treatment_tags":[],"created_utc":1753340699,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}}],"before":null}},"user_reports":[],"saved":false,"id":"n4tm1rs","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":"LOW_SCORE","no_follow":true,"author":"Yes_but_I_think","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4sb7t8","score":-17,"author_fullname":"t2_rea1qh6m","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"They just pirated the stuff. You are praising as if they created the knowledge.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n4tm1rs","is_submitter":false,"collapsed":true,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;They just pirated the stuff. You are praising as if they created the knowledge.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":"comment score below threshold","distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m7kkyn","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7kkyn/less_than_two_weeks_kimi_k2s_release_alibaba/n4tm1rs/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753320417,"author_flair_text":"llama.cpp","treatment_tags":[],"created_utc":1753320417,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-17}}],"before":null}},"user_reports":[],"saved":false,"id":"n4sb7t8","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"No_Efficiency_1144","can_mod_post":false,"created_utc":1753305126,"send_replies":true,"parent_id":"t1_n4s6aur","score":26,"author_fullname":"t2_1nkj9l14b0","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yeah look at Dalle 3\\n\\n\\nIt’s literally an old school diffusion model (not flow matching) with the original GPT 4 as the text encoder.\\n\\n\\nYet their dataset was so good that to this day it has a very wide range of subjects and strong prompt following.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4sb7t8","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yeah look at Dalle 3&lt;/p&gt;\\n\\n&lt;p&gt;It’s literally an old school diffusion model (not flow matching) with the original GPT 4 as the text encoder.&lt;/p&gt;\\n\\n&lt;p&gt;Yet their dataset was so good that to this day it has a very wide range of subjects and strong prompt following.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m7kkyn","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7kkyn/less_than_two_weeks_kimi_k2s_release_alibaba/n4sb7t8/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753305126,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":26}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4wmc1l","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"-dysangel-","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4uxoc8","score":1,"author_fullname":"t2_12ggykute6","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"That's just the thing. There are a lot of details that are often just totally subjective, and not in requirements. A lot of software engineering is completing the requirements, and sometimes even debugging or changing the requirements if/when they turn out to be impossible or not make sense etc. I kind of get your point, but I think we're already at the place where Claude Code can effectively one shot a \\"complex application\\" if you give very clear specs","edited":false,"author_flair_css_class":null,"name":"t1_n4wmc1l","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;That&amp;#39;s just the thing. There are a lot of details that are often just totally subjective, and not in requirements. A lot of software engineering is completing the requirements, and sometimes even debugging or changing the requirements if/when they turn out to be impossible or not make sense etc. I kind of get your point, but I think we&amp;#39;re already at the place where Claude Code can effectively one shot a &amp;quot;complex application&amp;quot; if you give very clear specs&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1m7kkyn","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":"light","treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7kkyn/less_than_two_weeks_kimi_k2s_release_alibaba/n4wmc1l/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753367235,"author_flair_text":"llama.cpp","collapsed":false,"created_utc":1753367235,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4uxoc8","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"yopla","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4sda9s","score":1,"author_fullname":"t2_3lwth","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Nah. If next year a model can one shot a complex application without hallucinating requirements, libraries and losing track of what it's doing that will become the new bar and what you'll want. \\n\\nYou will not want to continue using those models just like you're not using a clay tablet to write even though it's good enough.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4uxoc8","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Nah. If next year a model can one shot a complex application without hallucinating requirements, libraries and losing track of what it&amp;#39;s doing that will become the new bar and what you&amp;#39;ll want. &lt;/p&gt;\\n\\n&lt;p&gt;You will not want to continue using those models just like you&amp;#39;re not using a clay tablet to write even though it&amp;#39;s good enough.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m7kkyn","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7kkyn/less_than_two_weeks_kimi_k2s_release_alibaba/n4uxoc8/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753341162,"author_flair_text":null,"treatment_tags":[],"created_utc":1753341162,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4skude","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"-dysangel-","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4seqq9","score":8,"author_fullname":"t2_12ggykute6","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I think at some point this is not going to be about the intelligence of the model - it's simply going to be about how effectively we can communicate to the model. Just like real software development teams are limited by how well they can communicate and stay in sync on their goals. I think we're already getting towards this point. With Claude 4.0, I no longer feel like it just doesn't \\"get\\" some things in the same way that Claude 3.5 and Claude 3.7 struggled - I feel like it can do anything that I can explain to it.","edited":false,"author_flair_css_class":null,"name":"t1_n4skude","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I think at some point this is not going to be about the intelligence of the model - it&amp;#39;s simply going to be about how effectively we can communicate to the model. Just like real software development teams are limited by how well they can communicate and stay in sync on their goals. I think we&amp;#39;re already getting towards this point. With Claude 4.0, I no longer feel like it just doesn&amp;#39;t &amp;quot;get&amp;quot; some things in the same way that Claude 3.5 and Claude 3.7 struggled - I feel like it can do anything that I can explain to it.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1m7kkyn","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":"light","treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7kkyn/less_than_two_weeks_kimi_k2s_release_alibaba/n4skude/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753308007,"author_flair_text":"llama.cpp","collapsed":false,"created_utc":1753308007,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":8}}],"before":null}},"user_reports":[],"saved":false,"id":"n4seqq9","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":"LOW_SCORE","no_follow":true,"author":"randombsname1","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4sda9s","score":-7,"author_fullname":"t2_8t0zww56","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":true,"body":"Yeah im not saying these have no utility, and im sure they are good for a lot of tasks, but since I am using them for coding--typically new stacks with limited implementation examples. Then I like to squeeze every lazy drop i can get out of a model. \\n\\nEven Claude Opus I never take the initial code produced. I always iterate over it with documentation, and thus i need the best model available so im not just spinning my wheels longer than needed. \\n\\nWhich means essentially I'll always be looking for SOTA/cutting edge performance. \\n\\nWhich isnt going to come from any Chinese models as long as the entirety of their work is based on U.S. models. Its just not possible to lead when you copy what is actually in the lead, inherently, lol. \\n\\nAgain, I can see great uses for open source models like this. It's just not as exciting for me as new OpenAI, Google, or Anthropic models where everytime they release something it could be a complete game changer as to how workflows are enhanced moving forward.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4seqq9","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yeah im not saying these have no utility, and im sure they are good for a lot of tasks, but since I am using them for coding--typically new stacks with limited implementation examples. Then I like to squeeze every lazy drop i can get out of a model. &lt;/p&gt;\\n\\n&lt;p&gt;Even Claude Opus I never take the initial code produced. I always iterate over it with documentation, and thus i need the best model available so im not just spinning my wheels longer than needed. &lt;/p&gt;\\n\\n&lt;p&gt;Which means essentially I&amp;#39;ll always be looking for SOTA/cutting edge performance. &lt;/p&gt;\\n\\n&lt;p&gt;Which isnt going to come from any Chinese models as long as the entirety of their work is based on U.S. models. Its just not possible to lead when you copy what is actually in the lead, inherently, lol. &lt;/p&gt;\\n\\n&lt;p&gt;Again, I can see great uses for open source models like this. It&amp;#39;s just not as exciting for me as new OpenAI, Google, or Anthropic models where everytime they release something it could be a complete game changer as to how workflows are enhanced moving forward.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":"comment score below threshold","distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m7kkyn","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7kkyn/less_than_two_weeks_kimi_k2s_release_alibaba/n4seqq9/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753306161,"author_flair_text":null,"treatment_tags":[],"created_utc":1753306161,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-7}}],"before":null}},"user_reports":[],"saved":false,"id":"n4sda9s","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"-dysangel-","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4sbm4x","score":19,"author_fullname":"t2_12ggykute6","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"The frontier is rapidly approaching \\"good enough\\" for me. In the same way that I don't care about new generations of phones coming out, if Qwen 3 Coder is as good as Claude 4.0 - I am going to get a LOT of utility out of it for the rest of my life. And I still believe we can get Claude 4 or higher coding ability out of a model that only has 32B params. If we really focus on high quality reasoning and software engineering practices, and leave the more general knowledge to RAG.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n4sda9s","is_submitter":false,"collapsed":false,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The frontier is rapidly approaching &amp;quot;good enough&amp;quot; for me. In the same way that I don&amp;#39;t care about new generations of phones coming out, if Qwen 3 Coder is as good as Claude 4.0 - I am going to get a LOT of utility out of it for the rest of my life. And I still believe we can get Claude 4 or higher coding ability out of a model that only has 32B params. If we really focus on high quality reasoning and software engineering practices, and leave the more general knowledge to RAG.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m7kkyn","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7kkyn/less_than_two_weeks_kimi_k2s_release_alibaba/n4sda9s/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753305732,"author_flair_text":"llama.cpp","treatment_tags":[],"created_utc":1753305732,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":19}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4v5sma","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Orolol","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4uqepx","score":6,"author_fullname":"t2_fbzx9","approved_by":null,"mod_note":null,"all_awardings":[],"body":"MTP also, even if it was only for training purpose.","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n4v5sma","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;MTP also, even if it was only for training purpose.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1m7kkyn","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7kkyn/less_than_two_weeks_kimi_k2s_release_alibaba/n4v5sma/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753345775,"author_flair_text":null,"treatment_tags":[],"created_utc":1753345775,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}}],"before":null}},"user_reports":[],"saved":false,"id":"n4uqepx","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Eelysanio","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4u62aw","score":7,"author_fullname":"t2_nwzlf","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Multi-Head Latent Attention (MLA)","edited":false,"author_flair_css_class":null,"name":"t1_n4uqepx","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Multi-Head Latent Attention (MLA)&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1m7kkyn","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7kkyn/less_than_two_weeks_kimi_k2s_release_alibaba/n4uqepx/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753337256,"author_flair_text":null,"collapsed":false,"created_utc":1753337256,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":7}}],"before":null}},"user_reports":[],"saved":false,"id":"n4u62aw","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"randombsname1","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4u2q3d","score":-4,"author_fullname":"t2_8t0zww56","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"What was the innovation?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4u62aw","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;What was the innovation?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m7kkyn","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7kkyn/less_than_two_weeks_kimi_k2s_release_alibaba/n4u62aw/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753327777,"author_flair_text":null,"treatment_tags":[],"created_utc":1753327777,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-4}}],"before":null}},"user_reports":[],"saved":false,"id":"n4u2q3d","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Orolol","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4sbm4x","score":7,"author_fullname":"t2_fbzx9","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"That's quite false. Deepseek V3 alone was packed with innovation. \\n\\nThey're not frontier only because they currently lack the compute to do so.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n4u2q3d","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;That&amp;#39;s quite false. Deepseek V3 alone was packed with innovation. &lt;/p&gt;\\n\\n&lt;p&gt;They&amp;#39;re not frontier only because they currently lack the compute to do so.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m7kkyn","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7kkyn/less_than_two_weeks_kimi_k2s_release_alibaba/n4u2q3d/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753326437,"author_flair_text":null,"treatment_tags":[],"created_utc":1753326437,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":7}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4t4cfy","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":"LOW_SCORE","no_follow":true,"author":"randombsname1","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4t32kz","score":-6,"author_fullname":"t2_8t0zww56","approved_by":null,"mod_note":null,"all_awardings":[],"body":"Sure, it's fine. It's just doing it based on frontier U.S. LLMs. That's just a fact given jail breaks and the responses we have seen from pretty much all Chinese models.\\n\\nThere isnt any chance that Deepseek was originally trained with 1/10th the resources of U.S. models WITHOUT this being the case by the way. That was a deepseek claim. Not mine. \\n\\nThere isn't any indication that Chinese models are doing anything at the forefront of AI. That's my point. \\n\\nIts cool what they are doing. Which is bringing open source, high-quality models down to a cheap price.\\n\\nI just think it's different than being at the forefront of AI. Seeing as I dont think they have actually achieved anything new or exciting that U.S. frontier models didnt do 6 months prior.","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n4t4cfy","is_submitter":false,"collapsed":true,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Sure, it&amp;#39;s fine. It&amp;#39;s just doing it based on frontier U.S. LLMs. That&amp;#39;s just a fact given jail breaks and the responses we have seen from pretty much all Chinese models.&lt;/p&gt;\\n\\n&lt;p&gt;There isnt any chance that Deepseek was originally trained with 1/10th the resources of U.S. models WITHOUT this being the case by the way. That was a deepseek claim. Not mine. &lt;/p&gt;\\n\\n&lt;p&gt;There isn&amp;#39;t any indication that Chinese models are doing anything at the forefront of AI. That&amp;#39;s my point. &lt;/p&gt;\\n\\n&lt;p&gt;Its cool what they are doing. Which is bringing open source, high-quality models down to a cheap price.&lt;/p&gt;\\n\\n&lt;p&gt;I just think it&amp;#39;s different than being at the forefront of AI. Seeing as I dont think they have actually achieved anything new or exciting that U.S. frontier models didnt do 6 months prior.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":"comment score below threshold","link_id":"t3_1m7kkyn","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7kkyn/less_than_two_weeks_kimi_k2s_release_alibaba/n4t4cfy/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753314289,"author_flair_text":null,"treatment_tags":[],"created_utc":1753314289,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-6}}],"before":null}},"user_reports":[],"saved":false,"id":"n4t32kz","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"YouDontSeemRight","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4sue8f","score":9,"author_fullname":"t2_1b7gjxtue9","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"If OpenAI uses an LLM to generate synthetic datasets is it not okay for them to do the same? It's about curating quality datasets. For sure OpenAI was needed to get going but once the fire is lit its only necessary for gain of function.","edited":false,"author_flair_css_class":null,"name":"t1_n4t32kz","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;If OpenAI uses an LLM to generate synthetic datasets is it not okay for them to do the same? It&amp;#39;s about curating quality datasets. For sure OpenAI was needed to get going but once the fire is lit its only necessary for gain of function.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1m7kkyn","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7kkyn/less_than_two_weeks_kimi_k2s_release_alibaba/n4t32kz/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753313869,"author_flair_text":null,"collapsed":false,"created_utc":1753313869,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":9}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4u91f1","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"randombsname1","can_mod_post":false,"created_utc":1753329013,"send_replies":true,"parent_id":"t1_n4u8tuu","score":1,"author_fullname":"t2_8t0zww56","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Which source do you want? You want the constant references to itself as Claude or Chatgpt? I can provide several. Quickly.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4u91f1","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Which source do you want? You want the constant references to itself as Claude or Chatgpt? I can provide several. Quickly.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7kkyn/less_than_two_weeks_kimi_k2s_release_alibaba/n4u91f1/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753329013,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m7kkyn","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":9,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4u8tuu","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Amazing_Athlete_2265","can_mod_post":false,"created_utc":1753328924,"send_replies":true,"parent_id":"t1_n4u8e5l","score":3,"author_fullname":"t2_1nw9fzb7dt","approved_by":null,"mod_note":null,"all_awardings":[],"body":"I can read. You were asked for a source and didn't provide one.","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n4u8tuu","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I can read. You were asked for a source and didn&amp;#39;t provide one.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7kkyn/less_than_two_weeks_kimi_k2s_release_alibaba/n4u8tuu/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753328924,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m7kkyn","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":8,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n4u8e5l","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"randombsname1","can_mod_post":false,"created_utc":1753328740,"send_replies":true,"parent_id":"t1_n4u8bgt","score":-1,"author_fullname":"t2_8t0zww56","approved_by":null,"mod_note":null,"all_awardings":[],"body":"Oh, so you can't read. \\n\\nK.","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n4u8e5l","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Oh, so you can&amp;#39;t read. &lt;/p&gt;\\n\\n&lt;p&gt;K.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1m7kkyn","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7kkyn/less_than_two_weeks_kimi_k2s_release_alibaba/n4u8e5l/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753328740,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":7,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4u8bgt","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Amazing_Athlete_2265","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4u7fw4","score":6,"author_fullname":"t2_1nw9fzb7dt","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"A simple \\"no\\" would have been sufficient.","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n4u8bgt","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;A simple &amp;quot;no&amp;quot; would have been sufficient.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1m7kkyn","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7kkyn/less_than_two_weeks_kimi_k2s_release_alibaba/n4u8bgt/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753328708,"author_flair_text":null,"treatment_tags":[],"created_utc":1753328708,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":6,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"more","data":{"count":0,"name":"t1__","id":"_","parent_id":"t1_n4unutu","depth":10,"children":[]}}],"before":null}},"user_reports":[],"saved":false,"id":"n4unutu","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"randombsname1","can_mod_post":false,"created_utc":1753335946,"send_replies":true,"parent_id":"t1_n4uni6g","score":0,"author_fullname":"t2_8t0zww56","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Which research paper wasn't just a harp on existing research from any of the 3 big LLM providers-- essentially iterating over the same thing? \\n\\nMore importantly. Which research led them to push frontier models faster than U.S. providers over the last 2 years?\\n\\nNone you say?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4unutu","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Which research paper wasn&amp;#39;t just a harp on existing research from any of the 3 big LLM providers-- essentially iterating over the same thing? &lt;/p&gt;\\n\\n&lt;p&gt;More importantly. Which research led them to push frontier models faster than U.S. providers over the last 2 years?&lt;/p&gt;\\n\\n&lt;p&gt;None you say?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7kkyn/less_than_two_weeks_kimi_k2s_release_alibaba/n4unutu/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753335946,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m7kkyn","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":9,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}},"user_reports":[],"saved":false,"id":"n4uni6g","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"BoJackHorseMan53","can_mod_post":false,"created_utc":1753335766,"send_replies":true,"parent_id":"t1_n4udokh","score":4,"author_fullname":"t2_58t8ty6v","approved_by":null,"mod_note":null,"all_awardings":[],"body":"Only Google releases their research papers.\\n\\nOpenAI has not released a research paper since GPT-3.\\n\\nAnthropic is as closed source as it gets. They only release blog posts.\\n\\nChinese companies on the other hand release all their models along with any new discoveries and new techniques they find.\\n\\nIf you had two brain cells to read those papers, you'd know that they make plenty of new discoveries and open source them.\\n\\nBesides, 50% on AI researchers working in American companies are Chinese immigrants. China has way more of them.","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n4uni6g","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Only Google releases their research papers.&lt;/p&gt;\\n\\n&lt;p&gt;OpenAI has not released a research paper since GPT-3.&lt;/p&gt;\\n\\n&lt;p&gt;Anthropic is as closed source as it gets. They only release blog posts.&lt;/p&gt;\\n\\n&lt;p&gt;Chinese companies on the other hand release all their models along with any new discoveries and new techniques they find.&lt;/p&gt;\\n\\n&lt;p&gt;If you had two brain cells to read those papers, you&amp;#39;d know that they make plenty of new discoveries and open source them.&lt;/p&gt;\\n\\n&lt;p&gt;Besides, 50% on AI researchers working in American companies are Chinese immigrants. China has way more of them.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7kkyn/less_than_two_weeks_kimi_k2s_release_alibaba/n4uni6g/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753335766,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m7kkyn","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":8,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}}],"before":null}},"user_reports":[],"saved":false,"id":"n4udokh","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"randombsname1","can_mod_post":false,"created_utc":1753331050,"send_replies":true,"parent_id":"t1_n4u9hju","score":0,"author_fullname":"t2_8t0zww56","approved_by":null,"mod_note":null,"all_awardings":[],"body":"American companies scrape everything. No one is doubting that whatsoever, and yes. Chatgpt/Google/Claude all probably train off each other's models/outputs as well, but the difference is that they ALSO lead the frontier and are constantly pushing models better than their competitors. Meaning they aren't JUST distilling or training off each other's models.  \\n\\nThat's the difference. \\n\\nI've yet to see any Chinese lab do something equivalent.","edited":1753331819,"gildings":{},"author_flair_css_class":null,"name":"t1_n4udokh","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;American companies scrape everything. No one is doubting that whatsoever, and yes. Chatgpt/Google/Claude all probably train off each other&amp;#39;s models/outputs as well, but the difference is that they ALSO lead the frontier and are constantly pushing models better than their competitors. Meaning they aren&amp;#39;t JUST distilling or training off each other&amp;#39;s models.  &lt;/p&gt;\\n\\n&lt;p&gt;That&amp;#39;s the difference. &lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;ve yet to see any Chinese lab do something equivalent.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1m7kkyn","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7kkyn/less_than_two_weeks_kimi_k2s_release_alibaba/n4udokh/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753331050,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":7,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}},"user_reports":[],"saved":false,"id":"n4u9hju","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"BoJackHorseMan53","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4u7fw4","score":4,"author_fullname":"t2_58t8ty6v","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"So American companies have provided source of their training data?\\n\\nGemini also used to refer to itself as ChatGPT. It's because ChatGPT was first and the internet is polluted with ChatGPT chats. All the proprietary AI companies put the AIs name in the system prompt. But the open source AI labs can't do that, since anyone could run those models.\\n\\nYou seem to be willingly ignorant.","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n4u9hju","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;So American companies have provided source of their training data?&lt;/p&gt;\\n\\n&lt;p&gt;Gemini also used to refer to itself as ChatGPT. It&amp;#39;s because ChatGPT was first and the internet is polluted with ChatGPT chats. All the proprietary AI companies put the AIs name in the system prompt. But the open source AI labs can&amp;#39;t do that, since anyone could run those models.&lt;/p&gt;\\n\\n&lt;p&gt;You seem to be willingly ignorant.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1m7kkyn","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7kkyn/less_than_two_weeks_kimi_k2s_release_alibaba/n4u9hju/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753329204,"author_flair_text":null,"treatment_tags":[],"created_utc":1753329204,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":6,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4vk0ib","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"cheechw","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4u7fw4","score":1,"author_fullname":"t2_595rk","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"You clearly have no idea how LLMs work lol. How tf would DeepSeek know it's name? Where in the training data that it's learning from would it have learned it's name? All the training data it's getting from internet sources associates chatbot/LLM with ChatGPT because it's by far the most popular, and since an LLM's knowledge is derived from it's training data, it associates name of chatbot with ChatGPT. There would be almost no training data in comparison at the time that would have taught it its own name.\\n\\nNormally you'd give the model that context in a system prompt, but if it's an open weight model that anyone can run without system prompts, then are you expecting the DeepSeek or Qwen team to have hard-coded the name in there somewhere? Or to spend resources curating the training data set so that it knows it's name? That would be an absurd thing to ask for.","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n4vk0ib","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You clearly have no idea how LLMs work lol. How tf would DeepSeek know it&amp;#39;s name? Where in the training data that it&amp;#39;s learning from would it have learned it&amp;#39;s name? All the training data it&amp;#39;s getting from internet sources associates chatbot/LLM with ChatGPT because it&amp;#39;s by far the most popular, and since an LLM&amp;#39;s knowledge is derived from it&amp;#39;s training data, it associates name of chatbot with ChatGPT. There would be almost no training data in comparison at the time that would have taught it its own name.&lt;/p&gt;\\n\\n&lt;p&gt;Normally you&amp;#39;d give the model that context in a system prompt, but if it&amp;#39;s an open weight model that anyone can run without system prompts, then are you expecting the DeepSeek or Qwen team to have hard-coded the name in there somewhere? Or to spend resources curating the training data set so that it knows it&amp;#39;s name? That would be an absurd thing to ask for.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1m7kkyn","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7kkyn/less_than_two_weeks_kimi_k2s_release_alibaba/n4vk0ib/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753353564,"author_flair_text":null,"treatment_tags":[],"created_utc":1753353564,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":6,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4w524d","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"YouDontSeemRight","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4u7fw4","score":1,"author_fullname":"t2_1b7gjxtue9","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"No, it indicates that's the most likely response given it was trained off the internet which people dump outputs of those models.","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n4w524d","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;No, it indicates that&amp;#39;s the most likely response given it was trained off the internet which people dump outputs of those models.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1m7kkyn","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7kkyn/less_than_two_weeks_kimi_k2s_release_alibaba/n4w524d/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753361896,"author_flair_text":null,"treatment_tags":[],"created_utc":1753361896,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":6,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4u7fw4","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"randombsname1","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4u464x","score":-1,"author_fullname":"t2_8t0zww56","approved_by":null,"mod_note":null,"all_awardings":[],"body":"The fact it's responses regularly cited itself as Claude or Chatgpt; indicating it was trained off of those models.\\n\\nAlso the fact that all Chinese models, including deepseek have provided fuck all proof of their training claims and/or how they achieved parity with 1/10th of the compute power as they claimed. \\n\\nOr how they have never surpassed SOTA models--which indicates they can only match SOTA. At best. Which is indicative of distilling said models.\\n\\nMeanwhile you have OpenAI, Anthropic, and Google regularly leap frogging each other with substantial increases over previous SOTA models from their competitors. Indicating that they are pushing the frontier. \\n\\nIts like asking, \\"do you have a source for pigs not flying?\\"\\n\\nYeah, fucking reality lol. \\n\\nThat's not how shit works.\\n\\nEverything indicates they are simply distilling models....yet we should believe otherwise......why?","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n4u7fw4","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The fact it&amp;#39;s responses regularly cited itself as Claude or Chatgpt; indicating it was trained off of those models.&lt;/p&gt;\\n\\n&lt;p&gt;Also the fact that all Chinese models, including deepseek have provided fuck all proof of their training claims and/or how they achieved parity with 1/10th of the compute power as they claimed. &lt;/p&gt;\\n\\n&lt;p&gt;Or how they have never surpassed SOTA models--which indicates they can only match SOTA. At best. Which is indicative of distilling said models.&lt;/p&gt;\\n\\n&lt;p&gt;Meanwhile you have OpenAI, Anthropic, and Google regularly leap frogging each other with substantial increases over previous SOTA models from their competitors. Indicating that they are pushing the frontier. &lt;/p&gt;\\n\\n&lt;p&gt;Its like asking, &amp;quot;do you have a source for pigs not flying?&amp;quot;&lt;/p&gt;\\n\\n&lt;p&gt;Yeah, fucking reality lol. &lt;/p&gt;\\n\\n&lt;p&gt;That&amp;#39;s not how shit works.&lt;/p&gt;\\n\\n&lt;p&gt;Everything indicates they are simply distilling models....yet we should believe otherwise......why?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1m7kkyn","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7kkyn/less_than_two_weeks_kimi_k2s_release_alibaba/n4u7fw4/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753328344,"author_flair_text":null,"treatment_tags":[],"created_utc":1753328344,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4u464x","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"BoJackHorseMan53","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4sue8f","score":3,"author_fullname":"t2_58t8ty6v","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"You wanna provide a source bud? Or just gonna talk out your ass?","edited":false,"author_flair_css_class":null,"name":"t1_n4u464x","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You wanna provide a source bud? Or just gonna talk out your ass?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1m7kkyn","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7kkyn/less_than_two_weeks_kimi_k2s_release_alibaba/n4u464x/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753327012,"author_flair_text":null,"collapsed":false,"created_utc":1753327012,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n4sue8f","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":"LOW_SCORE","no_follow":true,"author":"randombsname1","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4stnnb","score":-11,"author_fullname":"t2_8t0zww56","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":true,"body":"They distilled from U.S. models. That's the key detail, lol. \\n\\nThat's been the case since at least the first deepseek. \\n\\nThey also got slightly worse performance with a smaller dataset. Which is exactly what U.S. models show as well.\\n\\nSonnet and Opus don't show huge intelligence differences, but Opus keeps context far better/longer--which is the real differentiator. \\n\\nOtherwise Opus isn't much more intelligent even though it uses a far bigger dataset.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4sue8f","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;They distilled from U.S. models. That&amp;#39;s the key detail, lol. &lt;/p&gt;\\n\\n&lt;p&gt;That&amp;#39;s been the case since at least the first deepseek. &lt;/p&gt;\\n\\n&lt;p&gt;They also got slightly worse performance with a smaller dataset. Which is exactly what U.S. models show as well.&lt;/p&gt;\\n\\n&lt;p&gt;Sonnet and Opus don&amp;#39;t show huge intelligence differences, but Opus keeps context far better/longer--which is the real differentiator. &lt;/p&gt;\\n\\n&lt;p&gt;Otherwise Opus isn&amp;#39;t much more intelligent even though it uses a far bigger dataset.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":"comment score below threshold","distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m7kkyn","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7kkyn/less_than_two_weeks_kimi_k2s_release_alibaba/n4sue8f/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753311055,"author_flair_text":null,"treatment_tags":[],"created_utc":1753311055,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-11}}],"before":null}},"user_reports":[],"saved":false,"id":"n4stnnb","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"YouDontSeemRight","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4sbm4x","score":6,"author_fullname":"t2_1b7gjxtue9","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"They did it better for smaller, therefore, it is frontier and SOTA for the model size. I also highly doubt they rely on US models to product good datasets. They understand what makes a good dataset which is the key detail.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n4stnnb","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;They did it better for smaller, therefore, it is frontier and SOTA for the model size. I also highly doubt they rely on US models to product good datasets. They understand what makes a good dataset which is the key detail.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m7kkyn","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7kkyn/less_than_two_weeks_kimi_k2s_release_alibaba/n4stnnb/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753310815,"author_flair_text":null,"treatment_tags":[],"created_utc":1753310815,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4uzwew","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"nrkishere","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4sbm4x","score":1,"author_fullname":"t2_o66k4w0to","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"if this positively contributes to the society, why should we care? Training a model of this size, even if datasets are available in place is an extremely expensive affair. Very few companies have capital to do that, alibaba is one of them. Since no american companies are giving away weights of any large model, we should appreciate deepseek and alibaba for doing that instead","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n4uzwew","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;if this positively contributes to the society, why should we care? Training a model of this size, even if datasets are available in place is an extremely expensive affair. Very few companies have capital to do that, alibaba is one of them. Since no american companies are giving away weights of any large model, we should appreciate deepseek and alibaba for doing that instead&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m7kkyn","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7kkyn/less_than_two_weeks_kimi_k2s_release_alibaba/n4uzwew/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753342415,"author_flair_text":null,"treatment_tags":[],"created_utc":1753342415,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4sbm4x","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":"LOW_SCORE","no_follow":true,"author":"randombsname1","can_mod_post":false,"created_utc":1753305242,"send_replies":true,"parent_id":"t1_n4s6aur","score":-27,"author_fullname":"t2_8t0zww56","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yep, if you want to see where Chinese models are headed. Just watch American models do it 3 to 6 months earlier. \\n\\nDon't get me wrong, its great that they offer very good performance for a fraction of the cost--but none of this is really at the frontier. Which at present seems to be around 4-6 months windows. \\n\\nThis is why these new Chinese models releases are always just kind of \\"meh\\" -- for me.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4sbm4x","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yep, if you want to see where Chinese models are headed. Just watch American models do it 3 to 6 months earlier. &lt;/p&gt;\\n\\n&lt;p&gt;Don&amp;#39;t get me wrong, its great that they offer very good performance for a fraction of the cost--but none of this is really at the frontier. Which at present seems to be around 4-6 months windows. &lt;/p&gt;\\n\\n&lt;p&gt;This is why these new Chinese models releases are always just kind of &amp;quot;meh&amp;quot; -- for me.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":"comment score below threshold","distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m7kkyn","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7kkyn/less_than_two_weeks_kimi_k2s_release_alibaba/n4sbm4x/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753305242,"author_flair_text":null,"treatment_tags":[],"collapsed":true,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-27}}],"before":null}},"user_reports":[],"saved":false,"id":"n4s6aur","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"nrkishere","can_mod_post":false,"created_utc":1753303735,"send_replies":true,"parent_id":"t3_1m7kkyn","score":72,"author_fullname":"t2_o66k4w0to","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"there's not much magic in the model's architecture. It is all in the dataset. Initially claude and gpt used their custom datasets, which is now being used to create synthetic datasets","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4s6aur","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;there&amp;#39;s not much magic in the model&amp;#39;s architecture. It is all in the dataset. Initially claude and gpt used their custom datasets, which is now being used to create synthetic datasets&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7kkyn/less_than_two_weeks_kimi_k2s_release_alibaba/n4s6aur/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753303735,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m7kkyn","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":72}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4shhc5","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Fantastic-Emu-3819","can_mod_post":false,"created_utc":1753306976,"send_replies":true,"parent_id":"t3_1m7kkyn","score":29,"author_fullname":"t2_1n5r32wumb","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Deepseek R1 0528 score is 68.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4shhc5","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Deepseek R1 0528 score is 68.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7kkyn/less_than_two_weeks_kimi_k2s_release_alibaba/n4shhc5/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753306976,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m7kkyn","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":29}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4tvj6f","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"smealdor","can_mod_post":false,"created_utc":1753323749,"send_replies":true,"parent_id":"t3_1m7kkyn","score":11,"author_fullname":"t2_50nnalwq","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"It's hard to keep up with the progress at this point. Caffeine helps.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4tvj6f","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It&amp;#39;s hard to keep up with the progress at this point. Caffeine helps.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":true,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7kkyn/less_than_two_weeks_kimi_k2s_release_alibaba/n4tvj6f/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753323749,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m7kkyn","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":11}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4vdljr","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AleksHop","can_mod_post":false,"created_utc":1753350222,"send_replies":true,"parent_id":"t3_1m7kkyn","score":1,"author_fullname":"t2_8dnu3hmd","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"no they are not. qwen3-coder results in benchmark is not real :)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4vdljr","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;no they are not. qwen3-coder results in benchmark is not real :)&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7kkyn/less_than_two_weeks_kimi_k2s_release_alibaba/n4vdljr/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753350222,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m7kkyn","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4shc08","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Baldur-Norddahl","can_mod_post":false,"created_utc":1753306931,"send_replies":true,"parent_id":"t1_n4s7m7n","score":25,"author_fullname":"t2_bvqb8ng0","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Training at fp16 because that is better for training. Does not mean it is needed for inference. The fp16 is need for backpropagation due to the need to calculate fine grained gradients. It is just wasting resources to insist on using fp16 for inference at this point.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4shc08","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Training at fp16 because that is better for training. Does not mean it is needed for inference. The fp16 is need for backpropagation due to the need to calculate fine grained gradients. It is just wasting resources to insist on using fp16 for inference at this point.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m7kkyn","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7kkyn/less_than_two_weeks_kimi_k2s_release_alibaba/n4shc08/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753306931,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":25}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4w2zwe","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"CheatCodesOfLife","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4vuiuf","score":2,"author_fullname":"t2_32el727b","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Agreed. Other than that, I never run &gt; FP8.","edited":false,"author_flair_css_class":null,"name":"t1_n4w2zwe","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Agreed. Other than that, I never run &amp;gt; FP8.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1m7kkyn","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7kkyn/less_than_two_weeks_kimi_k2s_release_alibaba/n4w2zwe/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753361210,"author_flair_text":null,"collapsed":false,"created_utc":1753361210,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n4vuiuf","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"GreenTreeAndBlueSky","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4vndca","score":3,"author_fullname":"t2_1p50pl73j2","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Maybe, it's just overall not the case","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4vuiuf","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Maybe, it&amp;#39;s just overall not the case&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m7kkyn","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7kkyn/less_than_two_weeks_kimi_k2s_release_alibaba/n4vuiuf/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753358108,"author_flair_text":null,"treatment_tags":[],"created_utc":1753358108,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n4vndca","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"CheatCodesOfLife","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4saa18","score":-1,"author_fullname":"t2_32el727b","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Try running Orpheus-3b in FP16 vs FP8 and you'll be able to tell with a blind test.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n4vndca","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Try running Orpheus-3b in FP16 vs FP8 and you&amp;#39;ll be able to tell with a blind test.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m7kkyn","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7kkyn/less_than_two_weeks_kimi_k2s_release_alibaba/n4vndca/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753355110,"author_flair_text":null,"treatment_tags":[],"created_utc":1753355110,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4saa18","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"GreenTreeAndBlueSky","can_mod_post":false,"created_utc":1753304853,"send_replies":true,"parent_id":"t1_n4s7m7n","score":18,"author_fullname":"t2_1p50pl73j2","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"It's very rare to see any degradation from fp16 to fp8 though, you would never know in a blind test which is which. Most models trained at fp16 are inferred at fp8 as new gpus support it (or less if quantized for vram space)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4saa18","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It&amp;#39;s very rare to see any degradation from fp16 to fp8 though, you would never know in a blind test which is which. Most models trained at fp16 are inferred at fp8 as new gpus support it (or less if quantized for vram space)&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m7kkyn","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7kkyn/less_than_two_weeks_kimi_k2s_release_alibaba/n4saa18/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753304853,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":18}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4thy29","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"HiddenoO","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4sbu3p","score":6,"author_fullname":"t2_8127x","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"That's a meaningless comparison because there's generally practically no performance degradation when running an FP16 trained model with FP8 during inference.\\n\\nHeck, this whole \\"same/better performance at half the size\\" is extremely misleading because performance never even remotely scales linear with size when quantizing models, and the degradation depends on the actual model. It'd make much more sense to compare performance at specific VRAM footprints and use appropriate quants for each model.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4thy29","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;That&amp;#39;s a meaningless comparison because there&amp;#39;s generally practically no performance degradation when running an FP16 trained model with FP8 during inference.&lt;/p&gt;\\n\\n&lt;p&gt;Heck, this whole &amp;quot;same/better performance at half the size&amp;quot; is extremely misleading because performance never even remotely scales linear with size when quantizing models, and the degradation depends on the actual model. It&amp;#39;d make much more sense to compare performance at specific VRAM footprints and use appropriate quants for each model.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m7kkyn","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7kkyn/less_than_two_weeks_kimi_k2s_release_alibaba/n4thy29/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753318978,"author_flair_text":null,"treatment_tags":[],"created_utc":1753318978,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4sxm7f","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"No_Efficiency_1144","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4sbu3p","score":3,"author_fullname":"t2_1nkj9l14b0","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I see that logic, I used to think of model size that way as well. They are going to perform like their parameter counts though, once both are at FP8.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4sxm7f","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I see that logic, I used to think of model size that way as well. They are going to perform like their parameter counts though, once both are at FP8.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m7kkyn","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7kkyn/less_than_two_weeks_kimi_k2s_release_alibaba/n4sxm7f/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753312076,"author_flair_text":null,"treatment_tags":[],"created_utc":1753312076,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n4sbu3p","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"fallingdowndizzyvr","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4sa7hb","score":9,"author_fullname":"t2_o65i6kx","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"It's not if the model was trained at FP8 and another at FP16. Since that is the full unquantized precision for both.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n4sbu3p","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It&amp;#39;s not if the model was trained at FP8 and another at FP16. Since that is the full unquantized precision for both.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m7kkyn","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7kkyn/less_than_two_weeks_kimi_k2s_release_alibaba/n4sbu3p/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753305306,"author_flair_text":null,"treatment_tags":[],"created_utc":1753305306,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":9}}],"before":null}},"user_reports":[],"saved":false,"id":"n4sa7hb","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"No_Efficiency_1144","can_mod_post":false,"created_utc":1753304833,"send_replies":true,"parent_id":"t1_n4s7m7n","score":22,"author_fullname":"t2_1nkj9l14b0","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Surely it is more misleading to compare FP8 to FP16","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4sa7hb","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Surely it is more misleading to compare FP8 to FP16&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m7kkyn","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7kkyn/less_than_two_weeks_kimi_k2s_release_alibaba/n4sa7hb/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753304833,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":22}}],"before":null}},"user_reports":[],"saved":false,"id":"n4s7m7n","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Few_Painter_5588","can_mod_post":false,"created_utc":1753304101,"send_replies":true,"parent_id":"t3_1m7kkyn","score":12,"author_fullname":"t2_uvgafqnfy","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Half it's size is misleading, at full precision they're nearly using the same amount of VRAM.\\n\\nQwen3 coder = 480B parameters at FP16 = 960GB of memory needed\\n\\nKimi M2 = 1T parameters at FP8 = 1000GB of memory used.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4s7m7n","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Half it&amp;#39;s size is misleading, at full precision they&amp;#39;re nearly using the same amount of VRAM.&lt;/p&gt;\\n\\n&lt;p&gt;Qwen3 coder = 480B parameters at FP16 = 960GB of memory needed&lt;/p&gt;\\n\\n&lt;p&gt;Kimi M2 = 1T parameters at FP8 = 1000GB of memory used.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7kkyn/less_than_two_weeks_kimi_k2s_release_alibaba/n4s7m7n/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753304101,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m7kkyn","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":1,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":12}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4u0not","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"FenderMoon","can_mod_post":false,"created_utc":1753325641,"send_replies":true,"parent_id":"t3_1m7kkyn","score":4,"author_fullname":"t2_f4ibdsc9","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Qwen3-Coder looks great, but it's a 480B MoE (35B active) model, way too large to really run on consumer hardware. \\n\\nCurious if we'll see distilled versions eventually. That'll be great if we can get them in 14B and 32B sizes. I'd love to see them eventually do something in between too (for the folks who can't quite run 32B)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4u0not","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Qwen3-Coder looks great, but it&amp;#39;s a 480B MoE (35B active) model, way too large to really run on consumer hardware. &lt;/p&gt;\\n\\n&lt;p&gt;Curious if we&amp;#39;ll see distilled versions eventually. That&amp;#39;ll be great if we can get them in 14B and 32B sizes. I&amp;#39;d love to see them eventually do something in between too (for the folks who can&amp;#39;t quite run 32B)&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7kkyn/less_than_two_weeks_kimi_k2s_release_alibaba/n4u0not/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753325641,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m7kkyn","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4v1dng","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Stetto","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4s791s","score":2,"author_fullname":"t2_cqodq","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Well, any model lagging behind can use proprietary models to create synthetic training data.\\n\\nThe gap closing is not any surprise.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n4v1dng","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Well, any model lagging behind can use proprietary models to create synthetic training data.&lt;/p&gt;\\n\\n&lt;p&gt;The gap closing is not any surprise.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m7kkyn","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7kkyn/less_than_two_weeks_kimi_k2s_release_alibaba/n4v1dng/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753343256,"author_flair_text":null,"treatment_tags":[],"created_utc":1753343256,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4sa0m7","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":"LOW_SCORE","no_follow":true,"author":"No_Efficiency_1144","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4s791s","score":-12,"author_fullname":"t2_1nkj9l14b0","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Sadly I have a different interpretation.\\n\\n\\nThe trend was that open source would have overtaken closed source by now.\\n\\n\\nHowever O1 came out in September 2024 and since then closed source has been improving twice as fast as before.\\n\\n\\nOn the other side open source has seen less growth rate gains from the reasoning boom.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n4sa0m7","is_submitter":false,"collapsed":true,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Sadly I have a different interpretation.&lt;/p&gt;\\n\\n&lt;p&gt;The trend was that open source would have overtaken closed source by now.&lt;/p&gt;\\n\\n&lt;p&gt;However O1 came out in September 2024 and since then closed source has been improving twice as fast as before.&lt;/p&gt;\\n\\n&lt;p&gt;On the other side open source has seen less growth rate gains from the reasoning boom.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":"comment score below threshold","distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m7kkyn","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7kkyn/less_than_two_weeks_kimi_k2s_release_alibaba/n4sa0m7/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753304779,"author_flair_text":null,"treatment_tags":[],"created_utc":1753304779,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-12}}],"before":null}},"user_reports":[],"saved":false,"id":"n4s791s","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"BZ852","can_mod_post":false,"created_utc":1753304000,"send_replies":true,"parent_id":"t1_n4s5wl5","score":20,"author_fullname":"t2_6h5i414j","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"While true in the absolute metrics, look at it by time.\\n\\nOpen source started a year or more behind, now it's less than a few months.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4s791s","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;While true in the absolute metrics, look at it by time.&lt;/p&gt;\\n\\n&lt;p&gt;Open source started a year or more behind, now it&amp;#39;s less than a few months.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m7kkyn","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7kkyn/less_than_two_weeks_kimi_k2s_release_alibaba/n4s791s/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753304000,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":20}}],"before":null}},"user_reports":[],"saved":false,"id":"n4s5wl5","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"No_Efficiency_1144","can_mod_post":false,"created_utc":1753303623,"send_replies":true,"parent_id":"t3_1m7kkyn","score":2,"author_fullname":"t2_1nkj9l14b0","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"It’s a nice chart but this chart does show closed source moving further away over the course of 2025.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4s5wl5","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It’s a nice chart but this chart does show closed source moving further away over the course of 2025.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7kkyn/less_than_two_weeks_kimi_k2s_release_alibaba/n4s5wl5/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753303623,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m7kkyn","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":1,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4u8dib","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"segmond","can_mod_post":false,"created_utc":1753328732,"send_replies":true,"parent_id":"t1_n4u5ajm","score":2,"author_fullname":"t2_ah13x","approved_by":null,"mod_note":null,"all_awardings":[],"body":"only 60k","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n4u8dib","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;only 60k&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1m7kkyn","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7kkyn/less_than_two_weeks_kimi_k2s_release_alibaba/n4u8dib/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753328732,"author_flair_text":"llama.cpp","treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":7,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n4u5ajm","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"createthiscom","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4u4aoc","score":3,"author_fullname":"t2_ozxxf","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"What context length are you using? I found the full 256k was too much for my hardware. It got faster when I lowered it to a more reasonable 128k.\\n\\nI 1 mil context must be for oligarchs with B200 custers lol","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n4u5ajm","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;What context length are you using? I found the full 256k was too much for my hardware. It got faster when I lowered it to a more reasonable 128k.&lt;/p&gt;\\n\\n&lt;p&gt;I 1 mil context must be for oligarchs with B200 custers lol&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1m7kkyn","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7kkyn/less_than_two_weeks_kimi_k2s_release_alibaba/n4u5ajm/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753327464,"author_flair_text":null,"treatment_tags":[],"created_utc":1753327464,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":6,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n4u4aoc","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"segmond","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4tyjfl","score":2,"author_fullname":"t2_ah13x","approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yup!  Same behavior here.  It's running at half the speed of Kimi for me.  It actually starts out very fast and degrades so quickly. :-(\\n\\n    prompt eval time =   10631.05 ms /   159 tokens (   66.86 ms per token,    14.96 tokens per second)\\n           eval time =   42522.93 ms /   332 tokens (  128.08 ms per token,     7.81 tokens per second)\\n     \\n    prompt eval time =   14331.27 ms /   570 tokens (   25.14 ms per token,    39.77 tokens per second)\\n           eval time =    5979.98 ms /    43 tokens (  139.07 ms per token,     7.19 tokens per second)\\n    \\n    \\n    prompt eval time =    1289.35 ms /    14 tokens (   92.10 ms per token,    10.86 tokens per second)\\n           eval time =   23262.58 ms /   161 tokens (  144.49 ms per token,     6.92 tokens per second)\\n          total time =   24551.94 ms /   175 tokens\\n    \\n    prompt eval time =  557164.88 ms / 12585 tokens (   44.27 ms per token,    22.59 tokens per second)\\n           eval time =  245107.27 ms /   322 tokens (  761.20 ms per token,     1.31 tokens per second)","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n4u4aoc","is_submitter":false,"collapsed":false,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yup!  Same behavior here.  It&amp;#39;s running at half the speed of Kimi for me.  It actually starts out very fast and degrades so quickly. :-(&lt;/p&gt;\\n\\n&lt;pre&gt;&lt;code&gt;prompt eval time =   10631.05 ms /   159 tokens (   66.86 ms per token,    14.96 tokens per second)\\n       eval time =   42522.93 ms /   332 tokens (  128.08 ms per token,     7.81 tokens per second)\\n\\nprompt eval time =   14331.27 ms /   570 tokens (   25.14 ms per token,    39.77 tokens per second)\\n       eval time =    5979.98 ms /    43 tokens (  139.07 ms per token,     7.19 tokens per second)\\n\\n\\nprompt eval time =    1289.35 ms /    14 tokens (   92.10 ms per token,    10.86 tokens per second)\\n       eval time =   23262.58 ms /   161 tokens (  144.49 ms per token,     6.92 tokens per second)\\n      total time =   24551.94 ms /   175 tokens\\n\\nprompt eval time =  557164.88 ms / 12585 tokens (   44.27 ms per token,    22.59 tokens per second)\\n       eval time =  245107.27 ms /   322 tokens (  761.20 ms per token,     1.31 tokens per second)\\n&lt;/code&gt;&lt;/pre&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1m7kkyn","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7kkyn/less_than_two_weeks_kimi_k2s_release_alibaba/n4u4aoc/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753327062,"author_flair_text":"llama.cpp","treatment_tags":[],"created_utc":1753327062,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"body":"Pro tip: use Unsloth’s quants with the Unsloth fork of llama.cpp for good results.","subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4ulmvs","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"__JockY__","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4tyjfl","score":1,"author_fullname":"t2_qf8h7ka8","approved_by":null,"mod_note":null,"all_awardings":[],"author_cakeday":true,"edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n4ulmvs","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Pro tip: use Unsloth’s quants with the Unsloth fork of llama.cpp for good results.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1m7kkyn","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7kkyn/less_than_two_weeks_kimi_k2s_release_alibaba/n4ulmvs/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753334820,"author_flair_text":null,"treatment_tags":[],"created_utc":1753334820,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4tyjfl","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"createthiscom","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4teto5","score":4,"author_fullname":"t2_ozxxf","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I wouldn't be surprised if it's a bug in llama.cpp or a feature that needs to be written. I agree it's odd.","edited":false,"author_flair_css_class":null,"name":"t1_n4tyjfl","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I wouldn&amp;#39;t be surprised if it&amp;#39;s a bug in llama.cpp or a feature that needs to be written. I agree it&amp;#39;s odd.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1m7kkyn","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7kkyn/less_than_two_weeks_kimi_k2s_release_alibaba/n4tyjfl/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753324844,"author_flair_text":null,"collapsed":false,"created_utc":1753324844,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}}],"before":null}},"user_reports":[],"saved":false,"id":"n4teto5","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"segmond","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4t0gh8","score":3,"author_fullname":"t2_ah13x","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"weird, I would imagine it faster since the active parameter is small than kimi.   perhaps the architecture?  i haven't read and contrasted on them.  my download just finished, granted it's for Q4\\\\_K\\\\_XL, will be giving it a drive tonight.  I hope you're wrong.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4teto5","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;weird, I would imagine it faster since the active parameter is small than kimi.   perhaps the architecture?  i haven&amp;#39;t read and contrasted on them.  my download just finished, granted it&amp;#39;s for Q4_K_XL, will be giving it a drive tonight.  I hope you&amp;#39;re wrong.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m7kkyn","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7kkyn/less_than_two_weeks_kimi_k2s_release_alibaba/n4teto5/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753317891,"author_flair_text":"llama.cpp","treatment_tags":[],"created_utc":1753317891,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4tknzy","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"eloquentemu","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4t0gh8","score":2,"author_fullname":"t2_lpdsy","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Keep in mind Kimi has 32B active while Qwen3-Coder is 35B active.  The total size doesn't really affect the speed of these, provided you have enough RAM.  That means Kimi should be very slightly faster at a given quant than Q3C based on bandwidth.  On my machine with small GPU offload they perform about the same at Q4.  Running CPU-only Kimi is about 15% faster.","edited":1753320471,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4tknzy","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Keep in mind Kimi has 32B active while Qwen3-Coder is 35B active.  The total size doesn&amp;#39;t really affect the speed of these, provided you have enough RAM.  That means Kimi should be very slightly faster at a given quant than Q3C based on bandwidth.  On my machine with small GPU offload they perform about the same at Q4.  Running CPU-only Kimi is about 15% faster.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m7kkyn","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7kkyn/less_than_two_weeks_kimi_k2s_release_alibaba/n4tknzy/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753319930,"author_flair_text":null,"treatment_tags":[],"created_utc":1753319930,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n4t0gh8","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"createthiscom","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4sh5ct","score":3,"author_fullname":"t2_ozxxf","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I'm running Kimi-K2-Instruct-GGUF Q4\\\\_K\\\\_XL locally. I switched to Qwen3-Coder-480B-A35B-Instruct-GGUF Q8\\\\_0. It's a smaller file size, but it infers slower on my system for some reason. 14 tok/s instead of kimi's 22 tok/s.\\n\\nEDIT: I like Qwen3-Coder at Q4\\\\_K\\\\_XL a bit more than Q8\\\\_0 on my machine because it's faster. I'm still evaluating.","edited":1753390131,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n4t0gh8","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;m running Kimi-K2-Instruct-GGUF Q4_K_XL locally. I switched to Qwen3-Coder-480B-A35B-Instruct-GGUF Q8_0. It&amp;#39;s a smaller file size, but it infers slower on my system for some reason. 14 tok/s instead of kimi&amp;#39;s 22 tok/s.&lt;/p&gt;\\n\\n&lt;p&gt;EDIT: I like Qwen3-Coder at Q4_K_XL a bit more than Q8_0 on my machine because it&amp;#39;s faster. I&amp;#39;m still evaluating.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m7kkyn","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7kkyn/less_than_two_weeks_kimi_k2s_release_alibaba/n4t0gh8/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753313004,"author_flair_text":null,"treatment_tags":[],"created_utc":1753313004,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n4sh5ct","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"segmond","can_mod_post":false,"created_utc":1753306876,"send_replies":true,"parent_id":"t1_n4sd9ax","score":3,"author_fullname":"t2_ah13x","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"which quant are you running? are you using suggested parameters?  full KV or quantized?   I hope you are wrong, I'm downloading file5 of 6 for my q4.gguf","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4sh5ct","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;which quant are you running? are you using suggested parameters?  full KV or quantized?   I hope you are wrong, I&amp;#39;m downloading file5 of 6 for my q4.gguf&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m7kkyn","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7kkyn/less_than_two_weeks_kimi_k2s_release_alibaba/n4sh5ct/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753306876,"author_flair_text":"llama.cpp","treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4vqhvp","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Amgadoz","can_mod_post":false,"created_utc":1753356469,"send_replies":true,"parent_id":"t1_n4uh0bs","score":1,"author_fullname":"t2_3el21u3z","approved_by":null,"mod_note":null,"all_awardings":[],"body":"You don't know the active params ahead, it's only determined when decoding and it's different for each token generated.","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n4vqhvp","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You don&amp;#39;t know the active params ahead, it&amp;#39;s only determined when decoding and it&amp;#39;s different for each token generated.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1m7kkyn","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7kkyn/less_than_two_weeks_kimi_k2s_release_alibaba/n4vqhvp/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753356469,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":7,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4uh0bs","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"createthiscom","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4uc1hi","score":1,"author_fullname":"t2_ozxxf","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"No just the active params go in the GPU plus a few extra layers.","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n4uh0bs","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;No just the active params go in the GPU plus a few extra layers.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1m7kkyn","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7kkyn/less_than_two_weeks_kimi_k2s_release_alibaba/n4uh0bs/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753332579,"author_flair_text":null,"treatment_tags":[],"created_utc":1753332579,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":6,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4uc1hi","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Ardalok","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4uaszi","score":1,"author_fullname":"t2_vgnewja","approved_by":null,"mod_note":null,"all_awardings":[],"body":"I don't understand, can you really fit the whole model on the GPU? Kimi has fewer active parameters than Qwen, so it's faster overall in any case, but if you offload to the CPU, the difference becomes even larger.","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n4uc1hi","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I don&amp;#39;t understand, can you really fit the whole model on the GPU? Kimi has fewer active parameters than Qwen, so it&amp;#39;s faster overall in any case, but if you offload to the CPU, the difference becomes even larger.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1m7kkyn","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7kkyn/less_than_two_weeks_kimi_k2s_release_alibaba/n4uc1hi/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753330315,"author_flair_text":null,"treatment_tags":[],"created_utc":1753330315,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4uaszi","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"createthiscom","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4u8zgz","score":1,"author_fullname":"t2_ozxxf","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I still don’t really get it as I load the whole MoE into my GPU for both models, then some additional layers ( my blackwell 6000 pro has 96gb VRAM).","edited":false,"author_flair_css_class":null,"name":"t1_n4uaszi","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I still don’t really get it as I load the whole MoE into my GPU for both models, then some additional layers ( my blackwell 6000 pro has 96gb VRAM).&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1m7kkyn","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7kkyn/less_than_two_weeks_kimi_k2s_release_alibaba/n4uaszi/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753329775,"author_flair_text":null,"collapsed":false,"created_utc":1753329775,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4vqna7","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Amgadoz","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4u8zgz","score":1,"author_fullname":"t2_3el21u3z","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"This is true for low-batch-size inference, where we're mostly bandwidth bound. At high batch sizes, we're mostly compute bound so what matters is the FLOPs.","edited":false,"author_flair_css_class":null,"name":"t1_n4vqna7","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;This is true for low-batch-size inference, where we&amp;#39;re mostly bandwidth bound. At high batch sizes, we&amp;#39;re mostly compute bound so what matters is the FLOPs.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1m7kkyn","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7kkyn/less_than_two_weeks_kimi_k2s_release_alibaba/n4vqna7/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753356532,"author_flair_text":null,"collapsed":false,"created_utc":1753356532,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4u8zgz","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Ardalok","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4typso","score":5,"author_fullname":"t2_vgnewja","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I didn’t actually phrase it correctly myself. Here’s what kimi compiled for me:\\n\\n1. Basic rule: when the whole model fits in RAM/VRAM, q4 is slightly slower than q8—a 5–15 % penalty from the extra bit-unpacking instructions.\\n\\n2. What matters is active parameters, not total parameters.\\n\\n   In an MoE, each token only touches k experts, so:  \\n   - the deciding factor is not the 480 B or 1 T total weights,  \\n   - but the 35 GB (q8) or 16 GB (q4) of data that actually travel over PCIe per step.\\n\\n3. In principle, speed depends on the number of active parameters, not the total—even when everything fits in GPU memory.\\n\\n   The throughput of the GPU’s compute units is set by the weights that are being multiplied right now, not by the total volume sitting on the card.\\n\\n4. Bottom line for your pair:\\n\\n   480 B a35B q8 vs. 1 T a32B q4\\n\\n   – q4 ships half as many bytes across the bus;\\n\\n   – the PCIe-bandwidth saving dwarfs the 5–15 % compute overhead.\\n\\n   ⇒ 1 T a32B q4 will be noticeably faster.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4u8zgz","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I didn’t actually phrase it correctly myself. Here’s what kimi compiled for me:&lt;/p&gt;\\n\\n&lt;ol&gt;\\n&lt;li&gt;&lt;p&gt;Basic rule: when the whole model fits in RAM/VRAM, q4 is slightly slower than q8—a 5–15 % penalty from the extra bit-unpacking instructions.&lt;/p&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;p&gt;What matters is active parameters, not total parameters.&lt;/p&gt;\\n\\n&lt;p&gt;In an MoE, each token only touches k experts, so:  &lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;the deciding factor is not the 480 B or 1 T total weights,&lt;br/&gt;&lt;/li&gt;\\n&lt;li&gt;but the 35 GB (q8) or 16 GB (q4) of data that actually travel over PCIe per step.&lt;/li&gt;\\n&lt;/ul&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;p&gt;In principle, speed depends on the number of active parameters, not the total—even when everything fits in GPU memory.&lt;/p&gt;\\n\\n&lt;p&gt;The throughput of the GPU’s compute units is set by the weights that are being multiplied right now, not by the total volume sitting on the card.&lt;/p&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;p&gt;Bottom line for your pair:&lt;/p&gt;\\n\\n&lt;p&gt;480 B a35B q8 vs. 1 T a32B q4&lt;/p&gt;\\n\\n&lt;p&gt;– q4 ships half as many bytes across the bus;&lt;/p&gt;\\n\\n&lt;p&gt;– the PCIe-bandwidth saving dwarfs the 5–15 % compute overhead.&lt;/p&gt;\\n\\n&lt;p&gt;⇒ 1 T a32B q4 will be noticeably faster.&lt;/p&gt;&lt;/li&gt;\\n&lt;/ol&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m7kkyn","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7kkyn/less_than_two_weeks_kimi_k2s_release_alibaba/n4u8zgz/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753328989,"author_flair_text":null,"treatment_tags":[],"created_utc":1753328989,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}}],"before":null}},"user_reports":[],"saved":false,"id":"n4typso","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"createthiscom","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4tjup0","score":0,"author_fullname":"t2_ozxxf","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"So, 8 bit quantized is always slower, even on blackwell, even when the model is smaller? I don't know how that works.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n4typso","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;So, 8 bit quantized is always slower, even on blackwell, even when the model is smaller? I don&amp;#39;t know how that works.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m7kkyn","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7kkyn/less_than_two_weeks_kimi_k2s_release_alibaba/n4typso/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753324908,"author_flair_text":null,"treatment_tags":[],"created_utc":1753324908,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}},"user_reports":[],"saved":false,"id":"n4tjup0","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Ardalok","can_mod_post":false,"created_utc":1753319648,"send_replies":true,"parent_id":"t1_n4sd9ax","score":4,"author_fullname":"t2_vgnewja","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Kimi has fewer active parameters and on top of that it’s 4-bit quantized, so of course it will be faster.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4tjup0","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Kimi has fewer active parameters and on top of that it’s 4-bit quantized, so of course it will be faster.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m7kkyn","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7kkyn/less_than_two_weeks_kimi_k2s_release_alibaba/n4tjup0/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753319648,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}}],"before":null}},"user_reports":[],"saved":false,"id":"n4sd9ax","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"createthiscom","can_mod_post":false,"created_utc":1753305724,"send_replies":true,"parent_id":"t3_1m7kkyn","score":3,"author_fullname":"t2_ozxxf","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"It's slower on my system despite having a smaller size and it doesn't seem as capable. I'm sticking with Kimi for now.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4sd9ax","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It&amp;#39;s slower on my system despite having a smaller size and it doesn&amp;#39;t seem as capable. I&amp;#39;m sticking with Kimi for now.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7kkyn/less_than_two_weeks_kimi_k2s_release_alibaba/n4sd9ax/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753305724,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m7kkyn","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":1,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4uj57o","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"GoldCompetition7722","can_mod_post":false,"created_utc":1753333592,"send_replies":true,"parent_id":"t3_1m7kkyn","score":1,"author_fullname":"t2_banbmed5","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Tokens go brrrr","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4uj57o","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Tokens go brrrr&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7kkyn/less_than_two_weeks_kimi_k2s_release_alibaba/n4uj57o/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753333592,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m7kkyn","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),s=()=>e.jsx(l,{data:a});export{s as default};
