import{j as e}from"./index-BrqAgJWx.js";import{R as t}from"./RedditPostRenderer-chN7TDMj.js";import"./index-DlMtF8rT.js";const a=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Which models offer the best quality-to-performance in terms of prompt adherence and context length for such a usecase? I am currently using NousResearch/Hermes-3-Llama-3.1-8B-GGUF for this task after having failed in trying to get Qwen2.5 7B to give questions from the actual theory text not sections of the book. I am using an RTX 4060 8GB with 16 GB RAM, which severely limits my options but I'd want to use the best I could for my hardware.","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Models for generating QA-pairs from text dataset","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lo1d8t","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.67,"author_flair_background_color":null,"subreddit_type":"public","ups":3,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_giwl2ppl","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":3,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1751268537,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Which models offer the best quality-to-performance in terms of prompt adherence and context length for such a usecase? I am currently using NousResearch/Hermes-3-Llama-3.1-8B-GGUF for this task after having failed in trying to get Qwen2.5 7B to give questions from the actual theory text not sections of the book. I am using an RTX 4060 8GB with 16 GB RAM, which severely limits my options but I&amp;#39;d want to use the best I could for my hardware.&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1lo1d8t","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"Sasikuttan2163","discussion_type":null,"num_comments":8,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lo1d8t/models_for_generating_qapairs_from_text_dataset/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lo1d8t/models_for_generating_qapairs_from_text_dataset/","subreddit_subscribers":492928,"created_utc":1751268537,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0k3zro","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Sasikuttan2163","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0jqbmu","score":1,"author_fullname":"t2_giwl2ppl","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Wow that's actually insane! I'll try to implement something like this, thank you!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0k3zro","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Wow that&amp;#39;s actually insane! I&amp;#39;ll try to implement something like this, thank you!&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lo1d8t","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lo1d8t/models_for_generating_qapairs_from_text_dataset/n0k3zro/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751282010,"author_flair_text":null,"treatment_tags":[],"created_utc":1751282010,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0jqbmu","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"iamnotapuck","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0jo2lv","score":1,"author_fullname":"t2_dyna1211","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"The current gold standard, at least for me, is [Chonkie](https://github.com/chonkie-inc/chonkie). It pretty much does everything you would want automatically (to an extent) dealing with chunking, embedding, and formatting raw text documents effectively. In your use case, I would export the chunks as json, so it easier for the locallm to processes what it wants it to generate. But if you read up on the documentation for Chonkie, it can be very powerful to combine it with embedding and RAG implantation.\\n\\nMy original pipeline is a simple one, but you can get very advanced in the weeds if you want to make sure the questions and answers are higher quality. My more advanced q&amp;a generator is to use Chonkie to segment the raw historical text into a set amount, generally the same size you use or a little smaller, but more overlap, which will then be embedded in a vector DB. Then I have the locallm do a first pass by reading the original chunks to generate questions (usually three). These are stored in a csv or json file. I then have it do a second pass using RAG pipeline. Now that the full document/s are embedded, I can just perform a query for each original question from the chunks I did before. But now since the full document is embedded, I can get a more semenatic result back, taking the top 5 results. Then have the locallm ingest those 5 results, and provide a more detailed answer to the question (than just what the chunk might have had). This provides a higher quality answer with a lower tier LLM.\\n\\nIf you want to look more on the RAG side of this implementation, I know that IBM is doing an amazing job with the granite models that are relatively small and can be used for local hosting. This is my current experimental high advanced one that I am currently trying to figure out on my own :)) ... [IBM agent RAG](https://huggingface.co/ibm-granite/granite-3.3-8b-rag-agent-lib)\\n\\nHope this helps, and if you have any questions, again ask away.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0jqbmu","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The current gold standard, at least for me, is &lt;a href=\\"https://github.com/chonkie-inc/chonkie\\"&gt;Chonkie&lt;/a&gt;. It pretty much does everything you would want automatically (to an extent) dealing with chunking, embedding, and formatting raw text documents effectively. In your use case, I would export the chunks as json, so it easier for the locallm to processes what it wants it to generate. But if you read up on the documentation for Chonkie, it can be very powerful to combine it with embedding and RAG implantation.&lt;/p&gt;\\n\\n&lt;p&gt;My original pipeline is a simple one, but you can get very advanced in the weeds if you want to make sure the questions and answers are higher quality. My more advanced q&amp;amp;a generator is to use Chonkie to segment the raw historical text into a set amount, generally the same size you use or a little smaller, but more overlap, which will then be embedded in a vector DB. Then I have the locallm do a first pass by reading the original chunks to generate questions (usually three). These are stored in a csv or json file. I then have it do a second pass using RAG pipeline. Now that the full document/s are embedded, I can just perform a query for each original question from the chunks I did before. But now since the full document is embedded, I can get a more semenatic result back, taking the top 5 results. Then have the locallm ingest those 5 results, and provide a more detailed answer to the question (than just what the chunk might have had). This provides a higher quality answer with a lower tier LLM.&lt;/p&gt;\\n\\n&lt;p&gt;If you want to look more on the RAG side of this implementation, I know that IBM is doing an amazing job with the granite models that are relatively small and can be used for local hosting. This is my current experimental high advanced one that I am currently trying to figure out on my own :)) ... &lt;a href=\\"https://huggingface.co/ibm-granite/granite-3.3-8b-rag-agent-lib\\"&gt;IBM agent RAG&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;Hope this helps, and if you have any questions, again ask away.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lo1d8t","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lo1d8t/models_for_generating_qapairs_from_text_dataset/n0jqbmu/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751274397,"author_flair_text":null,"treatment_tags":[],"created_utc":1751274397,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0jo2lv","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Sasikuttan2163","can_mod_post":false,"created_utc":1751273021,"send_replies":true,"parent_id":"t1_n0jndkh","score":1,"author_fullname":"t2_giwl2ppl","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Thanks for the descriptive answer! Could you tell me what approach you use for chunking? Right now I'm using a pretty basic langchain RecursiveCharacterTextSplitter with 2000 chunk size and 200 overlap. I realize that the results would vary greatly for different chunk sizes in different models but what have you had the best experience with?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0jo2lv","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thanks for the descriptive answer! Could you tell me what approach you use for chunking? Right now I&amp;#39;m using a pretty basic langchain RecursiveCharacterTextSplitter with 2000 chunk size and 200 overlap. I realize that the results would vary greatly for different chunk sizes in different models but what have you had the best experience with?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lo1d8t","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lo1d8t/models_for_generating_qapairs_from_text_dataset/n0jo2lv/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751273021,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0jndkh","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"iamnotapuck","can_mod_post":false,"created_utc":1751272587,"send_replies":true,"parent_id":"t3_1lo1d8t","score":2,"author_fullname":"t2_dyna1211","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"If just trying to create Q&amp;A pairs, I've found that the specific llm from 7-12B generally perform the same in question and answer generation. More verbose as you increase in parameters. What needs more specificity is the prompt engineering during api requests.\\n\\nMy general pipeline goes something like this:\\n\\nlarge textbook --&gt; chunk into paragraphs (token amounts might vary) --&gt; locallm summarizes chunk --&gt; prompt locallm to generate three questions based on summarization --&gt; prompt locallm to generate three answers based on questions, summarization, &amp; chunk.\\n\\ncsv output: [chunk text][summary][question][answer]\\n\\nThis is helpful to make sure the answers are grounded in the context and not just made up. For human fact checking.\\n\\nMost of my pipeline deals with history texts, so it might not be the same in your use case. I would say it might be less about the model you select, and more about how you construct the pipeline for q&amp;a generation.\\n\\nI've used a intel arc750 gpu with 8GB using LM Studio's api server to run these question and answers format. So your gpu and RAM should be fine, depending on the model quants. But I then would use a local instance of jupyter notebooks to run the python script for requests to LM Studio. \\n\\nHope that helps, and if you need any specific help, just drop me a line.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0jndkh","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;If just trying to create Q&amp;amp;A pairs, I&amp;#39;ve found that the specific llm from 7-12B generally perform the same in question and answer generation. More verbose as you increase in parameters. What needs more specificity is the prompt engineering during api requests.&lt;/p&gt;\\n\\n&lt;p&gt;My general pipeline goes something like this:&lt;/p&gt;\\n\\n&lt;p&gt;large textbook --&amp;gt; chunk into paragraphs (token amounts might vary) --&amp;gt; locallm summarizes chunk --&amp;gt; prompt locallm to generate three questions based on summarization --&amp;gt; prompt locallm to generate three answers based on questions, summarization, &amp;amp; chunk.&lt;/p&gt;\\n\\n&lt;p&gt;csv output: [chunk text][summary][question][answer]&lt;/p&gt;\\n\\n&lt;p&gt;This is helpful to make sure the answers are grounded in the context and not just made up. For human fact checking.&lt;/p&gt;\\n\\n&lt;p&gt;Most of my pipeline deals with history texts, so it might not be the same in your use case. I would say it might be less about the model you select, and more about how you construct the pipeline for q&amp;amp;a generation.&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;ve used a intel arc750 gpu with 8GB using LM Studio&amp;#39;s api server to run these question and answers format. So your gpu and RAM should be fine, depending on the model quants. But I then would use a local instance of jupyter notebooks to run the python script for requests to LM Studio. &lt;/p&gt;\\n\\n&lt;p&gt;Hope that helps, and if you need any specific help, just drop me a line.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lo1d8t/models_for_generating_qapairs_from_text_dataset/n0jndkh/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751272587,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lo1d8t","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0jgzt1","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Sasikuttan2163","can_mod_post":false,"created_utc":1751268639,"send_replies":true,"parent_id":"t3_1lo1d8t","score":1,"author_fullname":"t2_giwl2ppl","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"If you need more details please feel free to ask questions in the comments, I'll try to give the answers.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0jgzt1","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;If you need more details please feel free to ask questions in the comments, I&amp;#39;ll try to give the answers.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lo1d8t/models_for_generating_qapairs_from_text_dataset/n0jgzt1/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751268639,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lo1d8t","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0k48x4","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Sasikuttan2163","can_mod_post":false,"created_utc":1751282133,"send_replies":true,"parent_id":"t1_n0jx8p9","score":1,"author_fullname":"t2_giwl2ppl","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yeah I am using the 4 bit quantised version of Hermes 3 to avoid filling up my whole VRAM. Any resources where I can look into prompts proven to work for this purpose which I can adapt?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0k48x4","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yeah I am using the 4 bit quantised version of Hermes 3 to avoid filling up my whole VRAM. Any resources where I can look into prompts proven to work for this purpose which I can adapt?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lo1d8t","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lo1d8t/models_for_generating_qapairs_from_text_dataset/n0k48x4/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751282133,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0jx8p9","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Longjumpingfish0403","can_mod_post":false,"created_utc":1751278502,"send_replies":true,"parent_id":"t3_1lo1d8t","score":1,"author_fullname":"t2_jarttha4","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"If you're aiming for better performance on RTX 4060, you might want to explore quantized models or explore [GPTQ](https://huggingface.co/blog/4bit-transformers-better-faster-cheaper) for efficiency. Also, try using dynamic chunk sizes based on paragraph structure to maintain context. If your model struggles with prompt adherence, refining prompt templates or experimenting with length constraints in prompts can help. This might boost relevance without heavily taxing your hardware.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0jx8p9","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;If you&amp;#39;re aiming for better performance on RTX 4060, you might want to explore quantized models or explore &lt;a href=\\"https://huggingface.co/blog/4bit-transformers-better-faster-cheaper\\"&gt;GPTQ&lt;/a&gt; for efficiency. Also, try using dynamic chunk sizes based on paragraph structure to maintain context. If your model struggles with prompt adherence, refining prompt templates or experimenting with length constraints in prompts can help. This might boost relevance without heavily taxing your hardware.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lo1d8t/models_for_generating_qapairs_from_text_dataset/n0jx8p9/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751278502,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lo1d8t","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0jya88","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"umtksa","can_mod_post":false,"created_utc":1751279087,"send_replies":true,"parent_id":"t3_1lo1d8t","score":1,"author_fullname":"t2_ut19jt","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Qwen3","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0jya88","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Qwen3&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lo1d8t/models_for_generating_qapairs_from_text_dataset/n0jya88/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751279087,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lo1d8t","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),r=()=>e.jsx(t,{data:a});export{r as default};
