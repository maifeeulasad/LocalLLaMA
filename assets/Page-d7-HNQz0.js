import{j as e}from"./index-CNyNkRpk.js";import{R as l}from"./RedditPostRenderer-Dza0u9i2.js";import"./index-BUchu_-K.js";const a=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Obviously this is a silly question. 4k context is limiting to the point where even dumber models are \\"better\\" for almost any pipeline and use case.\\n\\nBut for those who have been running local LLMs since then, what are you observations (your experience outside of benchmark JPEG's)? What model sizes now beat Llama2-70B in:\\n\\n- instruction following\\n\\n- depth of knowledge \\n\\n- writing skill\\n\\n- coding \\n\\n- logic","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"If you limit context to 4k tokens, which models today beat Llama2-70B from 2 years ago?","link_flair_richtext":[{"e":"text","t":"Discussion"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lzuaa3","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.68,"author_flair_background_color":null,"subreddit_type":"public","ups":8,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_w2gxqd6i2","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Discussion","can_mod_post":false,"score":8,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1752517599,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Obviously this is a silly question. 4k context is limiting to the point where even dumber models are &amp;quot;better&amp;quot; for almost any pipeline and use case.&lt;/p&gt;\\n\\n&lt;p&gt;But for those who have been running local LLMs since then, what are you observations (your experience outside of benchmark JPEG&amp;#39;s)? What model sizes now beat Llama2-70B in:&lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;&lt;p&gt;instruction following&lt;/p&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;p&gt;depth of knowledge &lt;/p&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;p&gt;writing skill&lt;/p&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;p&gt;coding &lt;/p&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;p&gt;logic&lt;/p&gt;&lt;/li&gt;\\n&lt;/ul&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"5f921ea4-c7bc-11ed-9c23-3a00622979b4","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#646d73","id":"1lzuaa3","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"EmPips","discussion_type":null,"num_comments":36,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lzuaa3/if_you_limit_context_to_4k_tokens_which_models/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lzuaa3/if_you_limit_context_to_4k_tokens_which_models/","subreddit_subscribers":499295,"created_utc":1752517599,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n368hkd","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"maverick_soul_143747","can_mod_post":false,"send_replies":true,"parent_id":"t1_n35fndc","score":1,"author_fullname":"t2_1af9q3qa0b","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I have been using this model lately and you just confirmed something I have been experiencing","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n368hkd","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I have been using this model lately and you just confirmed something I have been experiencing&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lzuaa3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzuaa3/if_you_limit_context_to_4k_tokens_which_models/n368hkd/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752537744,"author_flair_text":null,"treatment_tags":[],"created_utc":1752537744,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n35fndc","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"BigRepresentative731","can_mod_post":false,"send_replies":true,"parent_id":"t1_n35e33a","score":1,"author_fullname":"t2_8a78x7h6","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Trust, it is. I have many many many many hours of experience with this model","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n35fndc","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Trust, it is. I have many many many many hours of experience with this model&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lzuaa3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzuaa3/if_you_limit_context_to_4k_tokens_which_models/n35fndc/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752528595,"author_flair_text":null,"treatment_tags":[],"created_utc":1752528595,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n35e33a","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"EmPips","can_mod_post":false,"created_utc":1752528137,"send_replies":true,"parent_id":"t1_n34eqt0","score":2,"author_fullname":"t2_w2gxqd6i2","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Significantly smarter.\\n\\nI don't know if it's on par with knowledge depth though.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n35e33a","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Significantly smarter.&lt;/p&gt;\\n\\n&lt;p&gt;I don&amp;#39;t know if it&amp;#39;s on par with knowledge depth though.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lzuaa3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzuaa3/if_you_limit_context_to_4k_tokens_which_models/n35e33a/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752528137,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n34eqt0","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"BigRepresentative731","can_mod_post":false,"created_utc":1752518057,"send_replies":true,"parent_id":"t3_1lzuaa3","score":12,"author_fullname":"t2_8a78x7h6","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Qwen 2.5 14b","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n34eqt0","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Qwen 2.5 14b&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzuaa3/if_you_limit_context_to_4k_tokens_which_models/n34eqt0/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752518057,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lzuaa3","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":12}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"c07aa42e-51fe-11f0-afcc-462aad931709","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3683w6","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"entsnack","can_mod_post":false,"created_utc":1752537614,"send_replies":true,"parent_id":"t1_n34ntgq","score":2,"author_fullname":"t2_1a48h7vf","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I still find the Llamas *way* better for fine-tunes than any other model. This is despite the Qwens giving me significantly better zero-shot performance.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3683w6","is_submitter":false,"downs":0,"author_flair_richtext":[{"a":":X:","u":"https://emoji.redditmedia.com/tbgegafk739f1_t5_81eyvm/X","e":"emoji"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I still find the Llamas &lt;em&gt;way&lt;/em&gt; better for fine-tunes than any other model. This is despite the Qwens giving me significantly better zero-shot performance.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lzuaa3","unrepliable_reason":null,"author_flair_text_color":"dark","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzuaa3/if_you_limit_context_to_4k_tokens_which_models/n3683w6/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752537614,"author_flair_text":":X:","treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":"transparent","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n34ntgq","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"mikael110","can_mod_post":false,"created_utc":1752520670,"send_replies":true,"parent_id":"t3_1lzuaa3","score":11,"author_fullname":"t2_4amlo","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"If we are talking about vanilla Llama-2 and not a finetune, then pretty much any modern model that is 12B or above will likely beat it on anything other than creative writing.\\n\\nLlama-2 always felt like it was undertrained. It was not very good at instruction following, and it certainly wasn't a fountain of knowledge either. It was also one of the first official instruction models that had been red-teamed to such an extent that it was basically unusable for most tasks.  It was the origin of the whole \\"Refusing to kill a Linux process\\" which was a meme for a bit in this community.\\n\\nThat's part of why very few actually used the official instruct model, and finetunes flourished. I'm pretty sure more finetunes came out of Llama-2 than any other models before or since.\\n\\nCoding was also terrible, it came out before coding was a big focus among LLMs, and it shows. I remember there was a big push to create coding finetunes from it back then because the base model was so bad at it.\\n\\nLlama-2 was a huge deal at the time mostly due to being an open model, at a time when this was not remotely common, and its success ushered in the era of open LLM. So I don't want to give the impression it was entirely bad or anything, it's release was very important. It just hasn't held up performance wise compared to newer models.","edited":1752529135,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n34ntgq","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;If we are talking about vanilla Llama-2 and not a finetune, then pretty much any modern model that is 12B or above will likely beat it on anything other than creative writing.&lt;/p&gt;\\n\\n&lt;p&gt;Llama-2 always felt like it was undertrained. It was not very good at instruction following, and it certainly wasn&amp;#39;t a fountain of knowledge either. It was also one of the first official instruction models that had been red-teamed to such an extent that it was basically unusable for most tasks.  It was the origin of the whole &amp;quot;Refusing to kill a Linux process&amp;quot; which was a meme for a bit in this community.&lt;/p&gt;\\n\\n&lt;p&gt;That&amp;#39;s part of why very few actually used the official instruct model, and finetunes flourished. I&amp;#39;m pretty sure more finetunes came out of Llama-2 than any other models before or since.&lt;/p&gt;\\n\\n&lt;p&gt;Coding was also terrible, it came out before coding was a big focus among LLMs, and it shows. I remember there was a big push to create coding finetunes from it back then because the base model was so bad at it.&lt;/p&gt;\\n\\n&lt;p&gt;Llama-2 was a huge deal at the time mostly due to being an open model, at a time when this was not remotely common, and its success ushered in the era of open LLM. So I don&amp;#39;t want to give the impression it was entirely bad or anything, it&amp;#39;s release was very important. It just hasn&amp;#39;t held up performance wise compared to newer models.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzuaa3/if_you_limit_context_to_4k_tokens_which_models/n34ntgq/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752520670,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lzuaa3","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":11}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n380l1m","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"kaisurniwurer","can_mod_post":false,"send_replies":true,"parent_id":"t1_n35vl4v","score":2,"author_fullname":"t2_qafso","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Could you give me an example of what I'm looking for?","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n380l1m","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Could you give me an example of what I&amp;#39;m looking for?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lzuaa3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzuaa3/if_you_limit_context_to_4k_tokens_which_models/n380l1m/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752564470,"author_flair_text":null,"treatment_tags":[],"created_utc":1752564470,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"7dba5c08-72f1-11ee-9b6f-ca195bc297d4","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n38od76","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Sufficient_Prune3897","can_mod_post":false,"send_replies":true,"parent_id":"t1_n35vl4v","score":1,"author_fullname":"t2_wl169phz5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Alignment isn't the problem, synthetic data is.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n38od76","is_submitter":false,"collapsed":false,"author_flair_richtext":[{"e":"text","t":"Llama 70B"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Alignment isn&amp;#39;t the problem, synthetic data is.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lzuaa3","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzuaa3/if_you_limit_context_to_4k_tokens_which_models/n38od76/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752577655,"author_flair_text":"Llama 70B","treatment_tags":[],"created_utc":1752577655,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n35vl4v","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"No_Efficiency_1144","can_mod_post":false,"created_utc":1752533529,"send_replies":true,"parent_id":"t1_n35c2h5","score":1,"author_fullname":"t2_1nkj9l14b0","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"On huggingface there are models that never had the alignment stage\\n\\n\\nThese would be a good starting point","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n35vl4v","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;On huggingface there are models that never had the alignment stage&lt;/p&gt;\\n\\n&lt;p&gt;These would be a good starting point&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lzuaa3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzuaa3/if_you_limit_context_to_4k_tokens_which_models/n35vl4v/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752533529,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n380uga","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"kaisurniwurer","can_mod_post":false,"created_utc":1752564621,"send_replies":true,"parent_id":"t1_n35c2h5","score":1,"author_fullname":"t2_qafso","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Do you have a preferred model? I actually have a project where I could use a small context model and where the model 'humanness' is quite important.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n380uga","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Do you have a preferred model? I actually have a project where I could use a small context model and where the model &amp;#39;humanness&amp;#39; is quite important.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lzuaa3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzuaa3/if_you_limit_context_to_4k_tokens_which_models/n380uga/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752564621,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n35c2h5","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Flaky_Comedian2012","can_mod_post":false,"created_utc":1752527559,"send_replies":true,"parent_id":"t3_1lzuaa3","score":5,"author_fullname":"t2_1d1v4h15w7","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I have yet to find a single modern model that beats old llama2 based finetunes on just being able to have a human sounding conversation.\\n\\nI give an old model just some example transcript and it will copy the mannerism and writing style perfectly.\\n\\nI can even ask it questions and often characters will just refuse to answer because they do not care or know anything about this topic. With new models even if they are able to handle a few sentences of actually staying in character, it all just goes out of the window when you ask it a question. Then the AI assistant part takes over immediatly. With old models will often act in denial if I even tell them that they are a AI.\\n\\nI really wish someone would make an old school model just with more context.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n35c2h5","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I have yet to find a single modern model that beats old llama2 based finetunes on just being able to have a human sounding conversation.&lt;/p&gt;\\n\\n&lt;p&gt;I give an old model just some example transcript and it will copy the mannerism and writing style perfectly.&lt;/p&gt;\\n\\n&lt;p&gt;I can even ask it questions and often characters will just refuse to answer because they do not care or know anything about this topic. With new models even if they are able to handle a few sentences of actually staying in character, it all just goes out of the window when you ask it a question. Then the AI assistant part takes over immediatly. With old models will often act in denial if I even tell them that they are a AI.&lt;/p&gt;\\n\\n&lt;p&gt;I really wish someone would make an old school model just with more context.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzuaa3/if_you_limit_context_to_4k_tokens_which_models/n35c2h5/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752527559,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lzuaa3","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n38ok5o","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"kaisurniwurer","can_mod_post":false,"send_replies":true,"parent_id":"t1_n38nojb","score":1,"author_fullname":"t2_qafso","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"&gt; L2 could surprise you with a message straight from a redditor\\n\\nI would like to start with a good flavour of \\"old\\" llama that would best pass as a conversation buddy. Maybe a merge or a finetune that was popular at the times when that version was ruling.\\n\\nI will be also looking for miqu model, but those were more popular so are easier to find.\\n\\nI also don't need more than 4k context for my use-case, since I'm working on a system that will not use context for a chat history, so the output quality (conversational) is the main priority. Though it's still too early to tell if it will be reliable enough.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n38ok5o","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;L2 could surprise you with a message straight from a redditor&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;I would like to start with a good flavour of &amp;quot;old&amp;quot; llama that would best pass as a conversation buddy. Maybe a merge or a finetune that was popular at the times when that version was ruling.&lt;/p&gt;\\n\\n&lt;p&gt;I will be also looking for miqu model, but those were more popular so are easier to find.&lt;/p&gt;\\n\\n&lt;p&gt;I also don&amp;#39;t need more than 4k context for my use-case, since I&amp;#39;m working on a system that will not use context for a chat history, so the output quality (conversational) is the main priority. Though it&amp;#39;s still too early to tell if it will be reliable enough.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lzuaa3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzuaa3/if_you_limit_context_to_4k_tokens_which_models/n38ok5o/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752577744,"author_flair_text":null,"treatment_tags":[],"created_utc":1752577744,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n38nojb","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"No_Afternoon_4260","can_mod_post":false,"send_replies":true,"parent_id":"t1_n383rzf","score":1,"author_fullname":"t2_cj9kap4bx","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"What are you looking for? Can you elaborate on what your goal is? I'm not sure I understood","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n38nojb","is_submitter":false,"collapsed":false,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;What are you looking for? Can you elaborate on what your goal is? I&amp;#39;m not sure I understood&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lzuaa3","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzuaa3/if_you_limit_context_to_4k_tokens_which_models/n38nojb/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752577342,"author_flair_text":"llama.cpp","treatment_tags":[],"created_utc":1752577342,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n383rzf","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"kaisurniwurer","can_mod_post":false,"created_utc":1752566288,"send_replies":true,"parent_id":"t1_n35z593","score":1,"author_fullname":"t2_qafso","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"That's what I'm currently looking for, I' working for a solution that makes context a secondary issue.\\n\\nDo you have a suggestion on the model? It doesn't need to be Llama too but that description actually made me want to try it. My goal is a model sounding the most authentic.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n383rzf","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;That&amp;#39;s what I&amp;#39;m currently looking for, I&amp;#39; working for a solution that makes context a secondary issue.&lt;/p&gt;\\n\\n&lt;p&gt;Do you have a suggestion on the model? It doesn&amp;#39;t need to be Llama too but that description actually made me want to try it. My goal is a model sounding the most authentic.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lzuaa3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzuaa3/if_you_limit_context_to_4k_tokens_which_models/n383rzf/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752566288,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n35z593","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"No_Afternoon_4260","can_mod_post":false,"created_utc":1752534677,"send_replies":true,"parent_id":"t3_1lzuaa3","score":3,"author_fullname":"t2_cj9kap4bx","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"It's not about intelligence. These models were more like that \\"it's just your keyboard autocomplet on steroids\\" where modern models are something else.\\n  \\nA mistral small 22B outsmarts it no question, devstral is light years ahead in usefulness for coding because of itsdataset and agentic behavior.\\n\\nYet they both speak like robots where L2 could surprise you with a message straight from a redditor (we lost that in L3 imo)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n35z593","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It&amp;#39;s not about intelligence. These models were more like that &amp;quot;it&amp;#39;s just your keyboard autocomplet on steroids&amp;quot; where modern models are something else.&lt;/p&gt;\\n\\n&lt;p&gt;A mistral small 22B outsmarts it no question, devstral is light years ahead in usefulness for coding because of itsdataset and agentic behavior.&lt;/p&gt;\\n\\n&lt;p&gt;Yet they both speak like robots where L2 could surprise you with a message straight from a redditor (we lost that in L3 imo)&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzuaa3/if_you_limit_context_to_4k_tokens_which_models/n35z593/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752534677,"author_flair_text":"llama.cpp","treatment_tags":[],"link_id":"t3_1lzuaa3","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n35rx0e","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Roubbes","can_mod_post":false,"created_utc":1752532356,"send_replies":true,"parent_id":"t3_1lzuaa3","score":2,"author_fullname":"t2_aoir7erh","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Mistral Small 3.2","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n35rx0e","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Mistral Small 3.2&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzuaa3/if_you_limit_context_to_4k_tokens_which_models/n35rx0e/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752532356,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lzuaa3","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n34hmxm","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"No_Efficiency_1144","can_mod_post":false,"created_utc":1752518885,"send_replies":true,"parent_id":"t1_n34fh00","score":1,"author_fullname":"t2_1nkj9l14b0","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yeah it doesn’t have to be 4k tokens of English\\n\\n\\n4k tokens of a domain specific language or encoded data can be loads","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n34hmxm","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yeah it doesn’t have to be 4k tokens of English&lt;/p&gt;\\n\\n&lt;p&gt;4k tokens of a domain specific language or encoded data can be loads&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lzuaa3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzuaa3/if_you_limit_context_to_4k_tokens_which_models/n34hmxm/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752518885,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n34fh00","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Double_Cause4609","can_mod_post":false,"created_utc":1752518266,"send_replies":true,"parent_id":"t3_1lzuaa3","score":4,"author_fullname":"t2_1kubzxt2ww","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Actually, 4k context is \\\\*a lot\\\\* in the context of a broader system; you'd be surprised at what can be done with 4k context and a carefully engineered setup.\\n\\nRegardless, in my humble opinion:\\n\\nOlmo 2 32B.\\n\\nIt's a pretty remarkable model and really does feel like the mini Claude at home, its only limitation being its context window of 4k (which probably could be alleviated with things like Rope or Alibi if the model were well supported in inference backends).","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n34fh00","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Actually, 4k context is *a lot* in the context of a broader system; you&amp;#39;d be surprised at what can be done with 4k context and a carefully engineered setup.&lt;/p&gt;\\n\\n&lt;p&gt;Regardless, in my humble opinion:&lt;/p&gt;\\n\\n&lt;p&gt;Olmo 2 32B.&lt;/p&gt;\\n\\n&lt;p&gt;It&amp;#39;s a pretty remarkable model and really does feel like the mini Claude at home, its only limitation being its context window of 4k (which probably could be alleviated with things like Rope or Alibi if the model were well supported in inference backends).&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzuaa3/if_you_limit_context_to_4k_tokens_which_models/n34fh00/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752518266,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lzuaa3","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n34ilph","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DinoAmino","can_mod_post":false,"created_utc":1752519161,"send_replies":true,"parent_id":"t3_1lzuaa3","score":2,"author_fullname":"t2_j1v7f","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I'd hazard a subjective guess that anything \\"recent\\" 32B and above are better in all regards due to more training tokens, improved training methods, and higher quality datasets. Codellama 70B was a fine-tune of Llama 2. Since then the only coder fine-tune above 70B was the DS 236B. So I'm assuming later models 70B and above have also been trained on coding datasets. Like Qwen 2.5 32B coder probably was fine-tuned on the same coding datasets used in their 72B. And that coder  certainly beats codellama - codestral 22B did.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n34ilph","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;d hazard a subjective guess that anything &amp;quot;recent&amp;quot; 32B and above are better in all regards due to more training tokens, improved training methods, and higher quality datasets. Codellama 70B was a fine-tune of Llama 2. Since then the only coder fine-tune above 70B was the DS 236B. So I&amp;#39;m assuming later models 70B and above have also been trained on coding datasets. Like Qwen 2.5 32B coder probably was fine-tuned on the same coding datasets used in their 72B. And that coder  certainly beats codellama - codestral 22B did.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzuaa3/if_you_limit_context_to_4k_tokens_which_models/n34ilph/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752519161,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lzuaa3","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n381cxt","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"kaisurniwurer","can_mod_post":false,"send_replies":true,"parent_id":"t1_n34xav5","score":1,"author_fullname":"t2_qafso","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I think it's stuffing the model with information inside the context so that it can use it for creating a better answer or acquire writing style, etc.\\n\\nBasically giving the model examples in lieu of teaching.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n381cxt","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I think it&amp;#39;s stuffing the model with information inside the context so that it can use it for creating a better answer or acquire writing style, etc.&lt;/p&gt;\\n\\n&lt;p&gt;Basically giving the model examples in lieu of teaching.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lzuaa3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzuaa3/if_you_limit_context_to_4k_tokens_which_models/n381cxt/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752564913,"author_flair_text":null,"treatment_tags":[],"created_utc":1752564913,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"c07aa42e-51fe-11f0-afcc-462aad931709","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"c07aa42e-51fe-11f0-afcc-462aad931709","distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3834rg","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"entsnack","can_mod_post":false,"send_replies":true,"parent_id":"t1_n37y1qk","score":0,"author_fullname":"t2_1a48h7vf","approved_by":null,"mod_note":null,"all_awardings":[],"body":"Fries in the bag bro","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n3834rg","is_submitter":false,"collapsed":false,"author_flair_richtext":[{"a":":X:","u":"https://emoji.redditmedia.com/tbgegafk739f1_t5_81eyvm/X","e":"emoji"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Fries in the bag bro&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lzuaa3","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":"dark","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzuaa3/if_you_limit_context_to_4k_tokens_which_models/n3834rg/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752565924,"author_flair_text":":X:","treatment_tags":[],"created_utc":1752565924,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":"transparent","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}},"user_reports":[],"saved":false,"id":"n37y1qk","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DrAlexander","can_mod_post":false,"send_replies":true,"parent_id":"t1_n368fjo","score":2,"author_fullname":"t2_3gchp","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Well, yes. Probably.  \\nTo be honest, I was expecting a reply similar to this, so it's not too bad.\\n\\n  \\nBut I was also hoping for some positive reply as well.  \\nOh well, no matter. I'll read up on it eventually.  \\nCheers!","edited":false,"author_flair_css_class":null,"name":"t1_n37y1qk","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Well, yes. Probably.&lt;br/&gt;\\nTo be honest, I was expecting a reply similar to this, so it&amp;#39;s not too bad.&lt;/p&gt;\\n\\n&lt;p&gt;But I was also hoping for some positive reply as well.&lt;br/&gt;\\nOh well, no matter. I&amp;#39;ll read up on it eventually.&lt;br/&gt;\\nCheers!&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lzuaa3","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzuaa3/if_you_limit_context_to_4k_tokens_which_models/n37y1qk/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752563045,"author_flair_text":null,"collapsed":false,"created_utc":1752563045,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n368fjo","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"entsnack","can_mod_post":false,"send_replies":true,"parent_id":"t1_n34xav5","score":-1,"author_fullname":"t2_1a48h7vf","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"\\\\^ average r/LocalLLaMA user right here","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n368fjo","is_submitter":false,"downs":0,"author_flair_richtext":[{"a":":X:","u":"https://emoji.redditmedia.com/tbgegafk739f1_t5_81eyvm/X","e":"emoji"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;^ average &lt;a href=\\"/r/LocalLLaMA\\"&gt;r/LocalLLaMA&lt;/a&gt; user right here&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lzuaa3","unrepliable_reason":null,"author_flair_text_color":"dark","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzuaa3/if_you_limit_context_to_4k_tokens_which_models/n368fjo/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752537724,"author_flair_text":":X:","treatment_tags":[],"created_utc":1752537724,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":"transparent","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-1}}],"before":null}},"user_reports":[],"saved":false,"id":"n34xav5","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DrAlexander","can_mod_post":false,"send_replies":true,"parent_id":"t1_n34qkgl","score":2,"author_fullname":"t2_3gchp","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"What's context learning?","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n34xav5","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;What&amp;#39;s context learning?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lzuaa3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzuaa3/if_you_limit_context_to_4k_tokens_which_models/n34xav5/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752523411,"author_flair_text":null,"treatment_tags":[],"created_utc":1752523411,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n35vbal","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"No_Efficiency_1144","can_mod_post":false,"send_replies":true,"parent_id":"t1_n34qkgl","score":1,"author_fullname":"t2_1nkj9l14b0","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yeah I agree, too much focus on fine tuning and I fell behind in ICL","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n35vbal","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yeah I agree, too much focus on fine tuning and I fell behind in ICL&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lzuaa3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzuaa3/if_you_limit_context_to_4k_tokens_which_models/n35vbal/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752533443,"author_flair_text":null,"treatment_tags":[],"created_utc":1752533443,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n34qkgl","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Thomas-Lore","can_mod_post":false,"created_utc":1752521465,"send_replies":true,"parent_id":"t1_n34hab2","score":1,"author_fullname":"t2_5hobp6m4","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"You are missing out on in context learning.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n34qkgl","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You are missing out on in context learning.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lzuaa3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzuaa3/if_you_limit_context_to_4k_tokens_which_models/n34qkgl/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752521465,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n34hab2","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"No_Efficiency_1144","can_mod_post":false,"created_utc":1752518787,"send_replies":true,"parent_id":"t3_1lzuaa3","score":1,"author_fullname":"t2_1nkj9l14b0","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Almost all of my usage of LLMs is below 4k token context","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n34hab2","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Almost all of my usage of LLMs is below 4k token context&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzuaa3/if_you_limit_context_to_4k_tokens_which_models/n34hab2/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752518787,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lzuaa3","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n361yb1","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"JC1DA","can_mod_post":false,"created_utc":1752535583,"send_replies":true,"parent_id":"t3_1lzuaa3","score":1,"author_fullname":"t2_gp3kfk8","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Kimi K2","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n361yb1","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Kimi K2&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzuaa3/if_you_limit_context_to_4k_tokens_which_models/n361yb1/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752535583,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lzuaa3","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n34j8ok","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Red_Redditor_Reddit","can_mod_post":false,"created_utc":1752519354,"send_replies":true,"parent_id":"t3_1lzuaa3","score":1,"author_fullname":"t2_8eelmfjg","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"My experience is that the newer models are generally better at everything except writing with human like verbiage from being over trained. Llama 2 also seems to do a better job following some system prompts. Newer models seem to be hard trained to be an AI assistant. If I give a prompt to llama 2 that says \\"this is a conversation between an ant and a user\\", it will hallucinate being an ant. Newer models will insist that it's an AI assistant or at least an ant AI assistant. ","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n34j8ok","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;My experience is that the newer models are generally better at everything except writing with human like verbiage from being over trained. Llama 2 also seems to do a better job following some system prompts. Newer models seem to be hard trained to be an AI assistant. If I give a prompt to llama 2 that says &amp;quot;this is a conversation between an ant and a user&amp;quot;, it will hallucinate being an ant. Newer models will insist that it&amp;#39;s an AI assistant or at least an ant AI assistant. &lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzuaa3/if_you_limit_context_to_4k_tokens_which_models/n34j8ok/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752519354,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lzuaa3","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n34l16b","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AppearanceHeavy6724","can_mod_post":false,"created_utc":1752519885,"send_replies":true,"parent_id":"t3_1lzuaa3","score":1,"author_fullname":"t2_uz37qfx5","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Never tried llama2-70B, but if you offer sample prompt and the produced result, that would make it easier to  answer. I'd think all 24b and bigger models of 2025 will beat llama2 at everything but writing skill, which is hit and miss with modern models.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n34l16b","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Never tried llama2-70B, but if you offer sample prompt and the produced result, that would make it easier to  answer. I&amp;#39;d think all 24b and bigger models of 2025 will beat llama2 at everything but writing skill, which is hit and miss with modern models.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzuaa3/if_you_limit_context_to_4k_tokens_which_models/n34l16b/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752519885,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lzuaa3","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n34gory","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Ravenpest","can_mod_post":false,"created_utc":1752518618,"send_replies":true,"parent_id":"t3_1lzuaa3","score":0,"author_fullname":"t2_21lhc1","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Deepseek (R1 \\\\\\\\ V3) at Q1 at 4k context shits on Llama2 70b any day of the week for whatever task your heart desires.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n34gory","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Deepseek (R1 \\\\ V3) at Q1 at 4k context shits on Llama2 70b any day of the week for whatever task your heart desires.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzuaa3/if_you_limit_context_to_4k_tokens_which_models/n34gory/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752518618,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lzuaa3","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":1,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n34np65","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"a_beautiful_rhind","can_mod_post":false,"created_utc":1752520635,"send_replies":true,"parent_id":"t3_1lzuaa3","score":0,"author_fullname":"t2_h5utwre7","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Plain llama-2 was pretty meh.. what models would beat miqu?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n34np65","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Plain llama-2 was pretty meh.. what models would beat miqu?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzuaa3/if_you_limit_context_to_4k_tokens_which_models/n34np65/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752520635,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lzuaa3","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":1,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n35k50w","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"SillyLilBear","can_mod_post":false,"created_utc":1752529921,"send_replies":true,"parent_id":"t3_1lzuaa3","score":-3,"author_fullname":"t2_wjjtz","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Almost all of them.  Llama models suck.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n35k50w","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Almost all of them.  Llama models suck.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzuaa3/if_you_limit_context_to_4k_tokens_which_models/n35k50w/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752529921,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lzuaa3","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-3}}],"before":null}}]`),r=()=>e.jsx(l,{data:a});export{r as default};
