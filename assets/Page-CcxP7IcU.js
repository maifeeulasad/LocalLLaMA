import{j as e}from"./index-DLSqWzaI.js";import{R as l}from"./RedditPostRenderer-CysRo2D_.js";import"./index-COXiL3Lo.js";const a=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Hi gang, in the use case 1 user total, local chat inference, assume model fits in vram, which engine is faster for tokens/sec for any given prompt?","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Vllm vs. llama.cpp","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":true,"name":"t3_1m1au28","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":1,"author_flair_background_color":null,"subreddit_type":"public","ups":4,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_1l3z4stvkq","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":4,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1752667606,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Hi gang, in the use case 1 user total, local chat inference, assume model fits in vram, which engine is faster for tokens/sec for any given prompt?&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1m1au28","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"Agreeable-Prompt-666","discussion_type":null,"num_comments":5,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1m1au28/vllm_vs_llamacpp/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1m1au28/vllm_vs_llamacpp/","subreddit_subscribers":499773,"created_utc":1752667606,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3fp74r","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"lly0571","can_mod_post":false,"created_utc":1752669206,"send_replies":true,"parent_id":"t3_1m1au28","score":2,"author_fullname":"t2_70vzcleel","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"VLLM could be slightly faster under similar quant levels(eg: int4 AWQ/GPTQ vs Q4\\\\_K\\\\_M GGUF) due to torch.compile and cuda graph.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3fp74r","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;VLLM could be slightly faster under similar quant levels(eg: int4 AWQ/GPTQ vs Q4_K_M GGUF) due to torch.compile and cuda graph.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m1au28/vllm_vs_llamacpp/n3fp74r/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752669206,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m1au28","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3fsdt7","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"No_Afternoon_4260","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3fratg","score":1,"author_fullname":"t2_cj9kap4bx","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Ho that's a hard one because even I don't understand what you are talking about.\\nAfaik you shouldn't pass a reasoning block in the context anyway. It should be removed before the next iteration","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3fsdt7","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Ho that&amp;#39;s a hard one because even I don&amp;#39;t understand what you are talking about.\\nAfaik you shouldn&amp;#39;t pass a reasoning block in the context anyway. It should be removed before the next iteration&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m1au28","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m1au28/vllm_vs_llamacpp/n3fsdt7/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752670326,"author_flair_text":"llama.cpp","treatment_tags":[],"created_utc":1752670326,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3fratg","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Nepherpitu","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3fqnei","score":1,"author_fullname":"t2_plp1w","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Well, try to ask model to reason about parsing of reasoning tag. It will consider closing tag in reasoning output as finish of reasoning. Even worse with &lt;/answer&gt; and hunyuan model - it's stop token, generation will be finished inside of reasoning.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3fratg","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Well, try to ask model to reason about parsing of reasoning tag. It will consider closing tag in reasoning output as finish of reasoning. Even worse with &amp;lt;/answer&amp;gt; and hunyuan model - it&amp;#39;s stop token, generation will be finished inside of reasoning.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m1au28","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m1au28/vllm_vs_llamacpp/n3fratg/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752669953,"author_flair_text":null,"treatment_tags":[],"created_utc":1752669953,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3fqnei","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"No_Afternoon_4260","can_mod_post":false,"created_utc":1752669725,"send_replies":true,"parent_id":"t1_n3fmp4b","score":1,"author_fullname":"t2_cj9kap4bx","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"On my side inference isn't buggy at all with llama.cpp, pretty reliable imho, may be not as optimised and not sure I'd use it in production. Beside that it has interesting features and quants that aren't supported by vllm.\\nThat's for llama.cpp if you put a wrapper around it like ollama then it's something else, easy to use but I wouldn't recommend you miss too much","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3fqnei","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;On my side inference isn&amp;#39;t buggy at all with llama.cpp, pretty reliable imho, may be not as optimised and not sure I&amp;#39;d use it in production. Beside that it has interesting features and quants that aren&amp;#39;t supported by vllm.\\nThat&amp;#39;s for llama.cpp if you put a wrapper around it like ollama then it&amp;#39;s something else, easy to use but I wouldn&amp;#39;t recommend you miss too much&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m1au28","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m1au28/vllm_vs_llamacpp/n3fqnei/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752669725,"author_flair_text":"llama.cpp","treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3fmp4b","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Nepherpitu","can_mod_post":false,"created_utc":1752668275,"send_replies":true,"parent_id":"t3_1m1au28","score":0,"author_fullname":"t2_plp1w","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"int4 AWQ is faster than gguf, vllm inference is less buggy and has better models and features support than llama.cpp.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3fmp4b","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;int4 AWQ is faster than gguf, vllm inference is less buggy and has better models and features support than llama.cpp.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m1au28/vllm_vs_llamacpp/n3fmp4b/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752668275,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m1au28","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}}]`),n=()=>e.jsx(l,{data:a});export{n as default};
