import{j as e}from"./index-CWmJdUH_.js";import{R as l}from"./RedditPostRenderer-D2iunoQ9.js";import"./index-BCg9RP6g.js";const a=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"**Hi everyone,**\\n\\nI’m looking to run an Ollama model locally for building my AI assistant, but my laptop isn’t so powerful. Here are my current specs:\\n\\nDell Latitude 3500\\n\\n8 GB RAM\\n\\nIntel Core i3‑8145U (4 cores)\\n\\nIntel UHD Graphics 620\\n\\nUbuntu 24.04\\n\\nI know these specs aren’t ideal, but I’d love your help figuring out which model would strike the best balance between usability and performance.\\n\\n","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Seeking advice: Which Ollama model should I run on my modest laptop?","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1m13eg0","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.25,"author_flair_background_color":null,"subreddit_type":"public","ups":0,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_1sx560oicu","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":0,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1752640131,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;&lt;strong&gt;Hi everyone,&lt;/strong&gt;&lt;/p&gt;\\n\\n&lt;p&gt;I’m looking to run an Ollama model locally for building my AI assistant, but my laptop isn’t so powerful. Here are my current specs:&lt;/p&gt;\\n\\n&lt;p&gt;Dell Latitude 3500&lt;/p&gt;\\n\\n&lt;p&gt;8 GB RAM&lt;/p&gt;\\n\\n&lt;p&gt;Intel Core i3‑8145U (4 cores)&lt;/p&gt;\\n\\n&lt;p&gt;Intel UHD Graphics 620&lt;/p&gt;\\n\\n&lt;p&gt;Ubuntu 24.04&lt;/p&gt;\\n\\n&lt;p&gt;I know these specs aren’t ideal, but I’d love your help figuring out which model would strike the best balance between usability and performance.&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":true,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1m13eg0","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"AerieExotic342","discussion_type":null,"num_comments":11,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1m13eg0/seeking_advice_which_ollama_model_should_i_run_on/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1m13eg0/seeking_advice_which_ollama_model_should_i_run_on/","subreddit_subscribers":499773,"created_utc":1752640131,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3e9ogq","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AerieExotic342","can_mod_post":false,"created_utc":1752642921,"send_replies":true,"parent_id":"t1_n3e6khj","score":2,"author_fullname":"t2_1sx560oicu","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"yeah i was just on gemini page finding models that will suit me","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3e9ogq","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;yeah i was just on gemini page finding models that will suit me&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m13eg0","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m13eg0/seeking_advice_which_ollama_model_should_i_run_on/n3e9ogq/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752642921,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n3e6khj","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"SomeOrdinaryKangaroo","can_mod_post":false,"created_utc":1752641291,"send_replies":true,"parent_id":"t3_1m13eg0","score":7,"author_fullname":"t2_tu4xlwci2","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Yeah, no\\n\\nEither upgrade the hardware or I suggest you build something using Geminis free API tier, while not local, it is generous and powerful if used right","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3e6khj","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yeah, no&lt;/p&gt;\\n\\n&lt;p&gt;Either upgrade the hardware or I suggest you build something using Geminis free API tier, while not local, it is generous and powerful if used right&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m13eg0/seeking_advice_which_ollama_model_should_i_run_on/n3e6khj/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752641291,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m13eg0","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":7}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3e8d6p","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Waarheid","can_mod_post":false,"created_utc":1752642266,"send_replies":true,"parent_id":"t3_1m13eg0","score":2,"author_fullname":"t2_amsk4","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Look into the Gemma 3n models perhaps.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3e8d6p","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Look into the Gemma 3n models perhaps.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m13eg0/seeking_advice_which_ollama_model_should_i_run_on/n3e8d6p/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752642266,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m13eg0","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"1c60b73a-72f2-11ee-bdcc-8e2a7b94d6a0","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3ehi7b","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"StableLlama","can_mod_post":false,"created_utc":1752646987,"send_replies":true,"parent_id":"t3_1m13eg0","score":1,"author_fullname":"t2_pyjm83bcg","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Look for the \\"tiny\\" model category. It's astonishing what they can do for their size - and it's astonishing how dumb they are in comparison to the bigger models.\\n\\nAs an I/O layer for an assistant they might be well suited. But don't expect much AI magic from them","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3ehi7b","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"textgen web UI"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Look for the &amp;quot;tiny&amp;quot; model category. It&amp;#39;s astonishing what they can do for their size - and it&amp;#39;s astonishing how dumb they are in comparison to the bigger models.&lt;/p&gt;\\n\\n&lt;p&gt;As an I/O layer for an assistant they might be well suited. But don&amp;#39;t expect much AI magic from them&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m13eg0/seeking_advice_which_ollama_model_should_i_run_on/n3ehi7b/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752646987,"author_flair_text":"textgen web UI","treatment_tags":[],"link_id":"t3_1m13eg0","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3eksp4","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"redoubt515","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3ekgsg","score":1,"author_fullname":"t2_kehp8nb59","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Here is the model I was referring to: [https://ollama.com/library/qwen3:0.6b](https://ollama.com/library/qwen3:0.6b)\\n\\nWith more RAM, you would have a bit more breathing room to try out a 3B or 4B model and still have memory leftover for normal usage of your system. More RAM will not improve *speed (unless its a move from single channel to dual channel)*, but it will allow you to load somewhat larger models at lower speeds.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3eksp4","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Here is the model I was referring to: &lt;a href=\\"https://ollama.com/library/qwen3:0.6b\\"&gt;https://ollama.com/library/qwen3:0.6b&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;With more RAM, you would have a bit more breathing room to try out a 3B or 4B model and still have memory leftover for normal usage of your system. More RAM will not improve &lt;em&gt;speed (unless its a move from single channel to dual channel)&lt;/em&gt;, but it will allow you to load somewhat larger models at lower speeds.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m13eg0","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m13eg0/seeking_advice_which_ollama_model_should_i_run_on/n3eksp4/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752648763,"author_flair_text":null,"treatment_tags":[],"created_utc":1752648763,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3ekgsg","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AerieExotic342","can_mod_post":false,"created_utc":1752648586,"send_replies":true,"parent_id":"t1_n3ekact","score":1,"author_fullname":"t2_1sx560oicu","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"yeah i think so i will see if i can upgrade in few days","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3ekgsg","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;yeah i think so i will see if i can upgrade in few days&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m13eg0","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m13eg0/seeking_advice_which_ollama_model_should_i_run_on/n3ekgsg/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752648586,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3ekact","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"redoubt515","can_mod_post":false,"created_utc":1752648492,"send_replies":true,"parent_id":"t3_1m13eg0","score":1,"author_fullname":"t2_kehp8nb59","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Start out with Qwen3-0.6B and if that runs at acceptable speeds you could try moving up to a *slightly* larger model.\\n\\nIs the RAM upgradeable?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3ekact","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Start out with Qwen3-0.6B and if that runs at acceptable speeds you could try moving up to a &lt;em&gt;slightly&lt;/em&gt; larger model.&lt;/p&gt;\\n\\n&lt;p&gt;Is the RAM upgradeable?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m13eg0/seeking_advice_which_ollama_model_should_i_run_on/n3ekact/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752648492,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m13eg0","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3fgxqm","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DigitusDesigner","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3fghz3","score":1,"author_fullname":"t2_1gjiyteyd7","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"To be honest, I don't think it's worth upgrading this laptop since it's quite outdated. I would instead save enough money to buy a new one or build a custom PC to run local LLMs.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3fgxqm","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;To be honest, I don&amp;#39;t think it&amp;#39;s worth upgrading this laptop since it&amp;#39;s quite outdated. I would instead save enough money to buy a new one or build a custom PC to run local LLMs.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m13eg0","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m13eg0/seeking_advice_which_ollama_model_should_i_run_on/n3fgxqm/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752666011,"author_flair_text":null,"treatment_tags":[],"created_utc":1752666011,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3fghz3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AerieExotic342","can_mod_post":false,"created_utc":1752665827,"send_replies":true,"parent_id":"t1_n3favzs","score":1,"author_fullname":"t2_1sx560oicu","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"How about 16 or 24 gb","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3fghz3","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;How about 16 or 24 gb&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m13eg0","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m13eg0/seeking_advice_which_ollama_model_should_i_run_on/n3fghz3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752665827,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3favzs","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DigitusDesigner","can_mod_post":false,"created_utc":1752663322,"send_replies":true,"parent_id":"t3_1m13eg0","score":1,"author_fullname":"t2_1gjiyteyd7","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"8gb ram is big no.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3favzs","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;8gb ram is big no.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m13eg0/seeking_advice_which_ollama_model_should_i_run_on/n3favzs/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752663322,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m13eg0","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3eh2jv","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"NNN_Throwaway2","can_mod_post":false,"created_utc":1752646756,"send_replies":true,"parent_id":"t3_1m13eg0","score":1,"author_fullname":"t2_8rrihts9","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"What is an \\"ollama\\" model?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3eh2jv","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;What is an &amp;quot;ollama&amp;quot; model?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m13eg0/seeking_advice_which_ollama_model_should_i_run_on/n3eh2jv/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752646756,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m13eg0","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),n=()=>e.jsx(l,{data:a});export{n as default};
