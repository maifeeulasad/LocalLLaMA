import{j as e}from"./index-CWmJdUH_.js";import{R as t}from"./RedditPostRenderer-D2iunoQ9.js";import"./index-BCg9RP6g.js";const l=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Hey everyone!\\n\\nI usually rent GPUs from the cloud since I don’t want to make the investment in expensive hardware. Most of the time, I use RunPod when I need extra compute for LLM inference, ComfyUI, or other GPU-heavy tasks.\\n\\nFor LLMs, I personally use text-generation-webui as the backend and either test models directly in the UI or interact with them programmatically via the API. I wanted to give back to the community by brain-dumping all my tips and tricks for getting this up and running.\\n\\nSo here you go, a complete tutorial with a one-click template included:\\n\\n# Source code and instructions:\\n\\n[https://github.com/MattiPaivike/RunPodTextGenWebUI/blob/main/README.md](https://github.com/MattiPaivike/RunPodTextGenWebUI/blob/main/README.md)\\n\\n# RunPod template:\\n\\n[https://console.runpod.io/deploy?template=y11d9xokre&amp;ref=7mxtxxqo](https://console.runpod.io/deploy?template=y11d9xokre&amp;ref=7mxtxxqo)\\n\\nI created a template on RunPod that does about 95% of the work for you. It sets up text-generation-webui and all of its prerequisites. You just need to set a few values, download a model, and you're good to go. The template was inspired by TheBloke's now-deprecated [dockerLLM](https://github.com/TheBlokeAI/dockerLLM) project, which I’ve completely refactored.\\n\\nA quick note: this RunPod template is not intended for production use. I personally use it to experiment or quickly try out a model. For production scenarios, I recommend looking into something like [VLLM](https://github.com/vllm-project/vllm).\\n\\nWhy I use RunPod:\\n\\n*  Relatively cheap – I can get 48 GB VRAM for just $0.40/hour\\n*  Easy multi-GPU support – I can stack cheap GPUs to run big models (like Mistral Large) at a low cost\\n*  Simple templates – very little tinkering needed\\n\\nI see renting GPUs as a solid privacy middle ground. Ideally, I’d run everything locally, but I don’t want to invest in expensive hardware. While I cannot audit RunPod's privacy, I consider it a big step up from relying on API providers (Claude, Google, etc.).\\n\\nThe README/tutorial walks through everything in detail, from setting up RunPod to downloading and loading models and inferencing the model. There is also instructions on calling the API so you can inference it programmatically and connecting to SillyTavern if needed.\\n\\nHave fun!","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Run Large LLMs on RunPod with text-generation-webui – Full Setup Guide + Template","link_flair_richtext":[{"e":"text","t":"Tutorial | Guide"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lsw9vz","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.81,"author_flair_background_color":null,"subreddit_type":"public","ups":15,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_d4l2j","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Tutorial | Guide","can_mod_post":false,"score":15,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"post_hint":"self","content_categories":null,"is_self":true,"mod_note":null,"created":1751790403,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Hey everyone!&lt;/p&gt;\\n\\n&lt;p&gt;I usually rent GPUs from the cloud since I don’t want to make the investment in expensive hardware. Most of the time, I use RunPod when I need extra compute for LLM inference, ComfyUI, or other GPU-heavy tasks.&lt;/p&gt;\\n\\n&lt;p&gt;For LLMs, I personally use text-generation-webui as the backend and either test models directly in the UI or interact with them programmatically via the API. I wanted to give back to the community by brain-dumping all my tips and tricks for getting this up and running.&lt;/p&gt;\\n\\n&lt;p&gt;So here you go, a complete tutorial with a one-click template included:&lt;/p&gt;\\n\\n&lt;h1&gt;Source code and instructions:&lt;/h1&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://github.com/MattiPaivike/RunPodTextGenWebUI/blob/main/README.md\\"&gt;https://github.com/MattiPaivike/RunPodTextGenWebUI/blob/main/README.md&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;h1&gt;RunPod template:&lt;/h1&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://console.runpod.io/deploy?template=y11d9xokre&amp;amp;ref=7mxtxxqo\\"&gt;https://console.runpod.io/deploy?template=y11d9xokre&amp;amp;ref=7mxtxxqo&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;I created a template on RunPod that does about 95% of the work for you. It sets up text-generation-webui and all of its prerequisites. You just need to set a few values, download a model, and you&amp;#39;re good to go. The template was inspired by TheBloke&amp;#39;s now-deprecated &lt;a href=\\"https://github.com/TheBlokeAI/dockerLLM\\"&gt;dockerLLM&lt;/a&gt; project, which I’ve completely refactored.&lt;/p&gt;\\n\\n&lt;p&gt;A quick note: this RunPod template is not intended for production use. I personally use it to experiment or quickly try out a model. For production scenarios, I recommend looking into something like &lt;a href=\\"https://github.com/vllm-project/vllm\\"&gt;VLLM&lt;/a&gt;.&lt;/p&gt;\\n\\n&lt;p&gt;Why I use RunPod:&lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt; Relatively cheap – I can get 48 GB VRAM for just $0.40/hour&lt;/li&gt;\\n&lt;li&gt; Easy multi-GPU support – I can stack cheap GPUs to run big models (like Mistral Large) at a low cost&lt;/li&gt;\\n&lt;li&gt; Simple templates – very little tinkering needed&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;p&gt;I see renting GPUs as a solid privacy middle ground. Ideally, I’d run everything locally, but I don’t want to invest in expensive hardware. While I cannot audit RunPod&amp;#39;s privacy, I consider it a big step up from relying on API providers (Claude, Google, etc.).&lt;/p&gt;\\n\\n&lt;p&gt;The README/tutorial walks through everything in detail, from setting up RunPod to downloading and loading models and inferencing the model. There is also instructions on calling the API so you can inference it programmatically and connecting to SillyTavern if needed.&lt;/p&gt;\\n\\n&lt;p&gt;Have fun!&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"preview":{"images":[{"source":{"url":"https://external-preview.redd.it/4lm8qt3U8kLoO2Fd6yvrls6oC59wE0X3JqIOCIaPvw0.png?auto=webp&amp;s=9af4102d7c46f3c211d2987ea3820b51a803d93e","width":1200,"height":600},"resolutions":[{"url":"https://external-preview.redd.it/4lm8qt3U8kLoO2Fd6yvrls6oC59wE0X3JqIOCIaPvw0.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=dc9ecd3bfc83c2924292457223c42ce5f67b31a0","width":108,"height":54},{"url":"https://external-preview.redd.it/4lm8qt3U8kLoO2Fd6yvrls6oC59wE0X3JqIOCIaPvw0.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=b93f262b9208e3a7ae7d65195067f16b585c4b7d","width":216,"height":108},{"url":"https://external-preview.redd.it/4lm8qt3U8kLoO2Fd6yvrls6oC59wE0X3JqIOCIaPvw0.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=a274c0f4b28e57ac4a08e9436b7c61e2aa840794","width":320,"height":160},{"url":"https://external-preview.redd.it/4lm8qt3U8kLoO2Fd6yvrls6oC59wE0X3JqIOCIaPvw0.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=4b3b0aa648816cf8244e86ca7890a7083130d329","width":640,"height":320},{"url":"https://external-preview.redd.it/4lm8qt3U8kLoO2Fd6yvrls6oC59wE0X3JqIOCIaPvw0.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=3bb9cfc9d83b1e0a52cb8b4edd3378d3115044d7","width":960,"height":480},{"url":"https://external-preview.redd.it/4lm8qt3U8kLoO2Fd6yvrls6oC59wE0X3JqIOCIaPvw0.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b3a5aaf9821feef764da21dde06d50fa9ccf20d0","width":1080,"height":540}],"variants":{},"id":"4lm8qt3U8kLoO2Fd6yvrls6oC59wE0X3JqIOCIaPvw0"}],"enabled":false},"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"449b05a6-bf8e-11ed-b4bd-66961e47bd50","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#0079d3","id":"1lsw9vz","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"abandonedexplorer","discussion_type":null,"num_comments":4,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lsw9vz/run_large_llms_on_runpod_with_textgenerationwebui/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lsw9vz/run_large_llms_on_runpod_with_textgenerationwebui/","subreddit_subscribers":495651,"created_utc":1751790403,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1lv7mx","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"dirtywriter321123","can_mod_post":false,"created_utc":1751792808,"send_replies":true,"parent_id":"t3_1lsw9vz","score":3,"author_fullname":"t2_u21swtl41","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Hey, thanks for doing this! will definitely check it out","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1lv7mx","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Hey, thanks for doing this! will definitely check it out&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lsw9vz/run_large_llms_on_runpod_with_textgenerationwebui/n1lv7mx/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751792808,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lsw9vz","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1ogyna","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"HumbleTech905","can_mod_post":false,"created_utc":1751827864,"send_replies":true,"parent_id":"t3_1lsw9vz","score":2,"author_fullname":"t2_stw6dyd68","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Thanks for sharing 👍","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1ogyna","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thanks for sharing 👍&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lsw9vz/run_large_llms_on_runpod_with_textgenerationwebui/n1ogyna/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751827864,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lsw9vz","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"a8acc9bc-4792-11ee-b77d-c61a47557e59","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1okkd9","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"henk717","can_mod_post":false,"created_utc":1751828979,"send_replies":true,"parent_id":"t3_1lsw9vz","score":1,"author_fullname":"t2_bx8b9","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"KoboldCpp also has official Runpod presence at https://koboldai.org/runpodcpp for those that prefer it . Just customize the env variable with the GGUF using the edit template option (first part of a 00001-of quant is enough, otherwise you can seperate by comma if the multi part is the old style). Extremely easy to get going, runs within minutes on secure cloud as we optimized around the biggest download bottlenecks.","edited":1751830253,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1okkd9","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"KoboldAI"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;KoboldCpp also has official Runpod presence at &lt;a href=\\"https://koboldai.org/runpodcpp\\"&gt;https://koboldai.org/runpodcpp&lt;/a&gt; for those that prefer it . Just customize the env variable with the GGUF using the edit template option (first part of a 00001-of quant is enough, otherwise you can seperate by comma if the multi part is the old style). Extremely easy to get going, runs within minutes on secure cloud as we optimized around the biggest download bottlenecks.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lsw9vz/run_large_llms_on_runpod_with_textgenerationwebui/n1okkd9/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751828979,"author_flair_text":"KoboldAI","treatment_tags":[],"link_id":"t3_1lsw9vz","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#5a74cc","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),r=()=>e.jsx(t,{data:l});export{r as default};
