import{j as e}from"./index-cvG704yx.js";import{R as t}from"./RedditPostRenderer-CBthLTAH.js";import"./index-D-GavSZU.js";const n=JSON.parse('[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Hey everyone,\\n\\nThis weekend I started tinkering with vLLM after a discussion we had over at the [OpenArc](https://github.com/SearchSavior/OpenArc) [discord server](https://discord.gg/Bzz9hax9Jq) last week about getting better performance.\\n\\nBetween vLLM and IPEX documentation they make it easy enough to get things rolling once you are setup; however if you are new to docker/containerization like I was when I got started building a compose from scratch can be hard, and the documentation does not cover that yet it makes deployment cleaner and reproducible.\\n\\n\\n```\\nservices:\\n  ipex-llm-serving:\\n    image: intelanalytics/ipex-llm-serving-xpu:0.8.3-b21\\n    container_name: ipex-vllm\\n    stdin_open: true\\n    tty: true\\n    network_mode: host\\n    devices:\\n      - /dev/dri:/dev/dri\\n    volumes:\\n      - path/to/your/models:/llm/models\\n    environment:\\n      - HTTP_PROXY=\\n      - HTTPS_PROXY=\\n      - http_proxy=\\n      - https_proxy=\\n    restart: unless-stopped\\n```\\n\\nTurns out that most of the cooking to get this running smoothly on multi-GPU requires environment variables that configure oneCCL and oneDNN that I have not figured out yet. Will share an update once I get that sorted, as I\'m eager to test.  \\n\\nIn the meantime, I wanted to share this bare minimum bootstrap for anyone interested. \\n\\nBenchmarks:\\n\\n[SicariusSicariiStuff/Phi-lthy4](https://huggingface.co/SicariusSicariiStuff/Phi-lthy4?not-for-all-audiences=true) @ woq_int4 (which should be close to q4km)\\n\\n1x A770\\nXeon W-2255\\nUbuntu 24.04 6.14.4-061404-generic\\nContext 2048 (~4gb vram to spare)\\n\\n**Serving Benchmark Result**\\nSuccessful requests:                     3000\\n\\nBenchmark duration (s):                  7850.31\\n\\nTotal input tokens:                      3072000\\n\\nTotal generated tokens:                  1536000\\n\\nRequest throughput (req/s):              0.38\\n\\nOutput token throughput (tok/s):         195.66\\n\\nTotal Token throughput (tok/s):          586.98\\n\\n**Time to First Token**\\n\\nMean TTFT (ms):                          3887736.67\\n\\nMedian TTFT (ms):                        3873859.76\\n\\nP99 TTFT (ms):                           7739753.88\\n\\n**Time per Output Token (excl. 1st token)**\\n\\nMean TPOT (ms):                          122.82\\n\\nMedian TPOT (ms):                        111.34\\n\\nP99 TPOT (ms):                           210.83\\n\\n**Inter-token Latency**\\n\\nMean ITL (ms):                           122.90\\n\\nMedian ITL (ms):                         75.30\\n\\nP99 ITL (ms):                            900.24\\n\\n\\n","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Intel GPU vLLM Docker Compose Bootstrap with Phi-lthy4 on A770","link_flair_richtext":[{"e":"text","t":"Resources"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1losjpq","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.7,"author_flair_background_color":null,"subreddit_type":"public","ups":4,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_pw77g8dq","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Resources","can_mod_post":false,"score":4,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"post_hint":"self","content_categories":null,"is_self":true,"mod_note":null,"created":1751343916,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt;\\n\\n&lt;p&gt;This weekend I started tinkering with vLLM after a discussion we had over at the &lt;a href=\\"https://github.com/SearchSavior/OpenArc\\"&gt;OpenArc&lt;/a&gt; &lt;a href=\\"https://discord.gg/Bzz9hax9Jq\\"&gt;discord server&lt;/a&gt; last week about getting better performance.&lt;/p&gt;\\n\\n&lt;p&gt;Between vLLM and IPEX documentation they make it easy enough to get things rolling once you are setup; however if you are new to docker/containerization like I was when I got started building a compose from scratch can be hard, and the documentation does not cover that yet it makes deployment cleaner and reproducible.&lt;/p&gt;\\n\\n&lt;p&gt;&lt;code&gt;\\nservices:\\n  ipex-llm-serving:\\n    image: intelanalytics/ipex-llm-serving-xpu:0.8.3-b21\\n    container_name: ipex-vllm\\n    stdin_open: true\\n    tty: true\\n    network_mode: host\\n    devices:\\n      - /dev/dri:/dev/dri\\n    volumes:\\n      - path/to/your/models:/llm/models\\n    environment:\\n      - HTTP_PROXY=\\n      - HTTPS_PROXY=\\n      - http_proxy=\\n      - https_proxy=\\n    restart: unless-stopped\\n&lt;/code&gt;&lt;/p&gt;\\n\\n&lt;p&gt;Turns out that most of the cooking to get this running smoothly on multi-GPU requires environment variables that configure oneCCL and oneDNN that I have not figured out yet. Will share an update once I get that sorted, as I&amp;#39;m eager to test.  &lt;/p&gt;\\n\\n&lt;p&gt;In the meantime, I wanted to share this bare minimum bootstrap for anyone interested. &lt;/p&gt;\\n\\n&lt;p&gt;Benchmarks:&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://huggingface.co/SicariusSicariiStuff/Phi-lthy4?not-for-all-audiences=true\\"&gt;SicariusSicariiStuff/Phi-lthy4&lt;/a&gt; @ woq_int4 (which should be close to q4km)&lt;/p&gt;\\n\\n&lt;p&gt;1x A770\\nXeon W-2255\\nUbuntu 24.04 6.14.4-061404-generic\\nContext 2048 (~4gb vram to spare)&lt;/p&gt;\\n\\n&lt;p&gt;&lt;strong&gt;Serving Benchmark Result&lt;/strong&gt;\\nSuccessful requests:                     3000&lt;/p&gt;\\n\\n&lt;p&gt;Benchmark duration (s):                  7850.31&lt;/p&gt;\\n\\n&lt;p&gt;Total input tokens:                      3072000&lt;/p&gt;\\n\\n&lt;p&gt;Total generated tokens:                  1536000&lt;/p&gt;\\n\\n&lt;p&gt;Request throughput (req/s):              0.38&lt;/p&gt;\\n\\n&lt;p&gt;Output token throughput (tok/s):         195.66&lt;/p&gt;\\n\\n&lt;p&gt;Total Token throughput (tok/s):          586.98&lt;/p&gt;\\n\\n&lt;p&gt;&lt;strong&gt;Time to First Token&lt;/strong&gt;&lt;/p&gt;\\n\\n&lt;p&gt;Mean TTFT (ms):                          3887736.67&lt;/p&gt;\\n\\n&lt;p&gt;Median TTFT (ms):                        3873859.76&lt;/p&gt;\\n\\n&lt;p&gt;P99 TTFT (ms):                           7739753.88&lt;/p&gt;\\n\\n&lt;p&gt;&lt;strong&gt;Time per Output Token (excl. 1st token)&lt;/strong&gt;&lt;/p&gt;\\n\\n&lt;p&gt;Mean TPOT (ms):                          122.82&lt;/p&gt;\\n\\n&lt;p&gt;Median TPOT (ms):                        111.34&lt;/p&gt;\\n\\n&lt;p&gt;P99 TPOT (ms):                           210.83&lt;/p&gt;\\n\\n&lt;p&gt;&lt;strong&gt;Inter-token Latency&lt;/strong&gt;&lt;/p&gt;\\n\\n&lt;p&gt;Mean ITL (ms):                           122.90&lt;/p&gt;\\n\\n&lt;p&gt;Median ITL (ms):                         75.30&lt;/p&gt;\\n\\n&lt;p&gt;P99 ITL (ms):                            900.24&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"preview":{"images":[{"source":{"url":"https://external-preview.redd.it/NPT2q1kryr57y9RMrGWEOV2MhPhXGETFTmAQPSEtLz4.png?auto=webp&amp;s=c993c6f35cf10a2dbcb9325392860cf200c2feaa","width":1200,"height":600},"resolutions":[{"url":"https://external-preview.redd.it/NPT2q1kryr57y9RMrGWEOV2MhPhXGETFTmAQPSEtLz4.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=bd0eae168a3078f08958617ae694e72aebef94ef","width":108,"height":54},{"url":"https://external-preview.redd.it/NPT2q1kryr57y9RMrGWEOV2MhPhXGETFTmAQPSEtLz4.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=dc4dcf5972f47e5122c59d0f3d18669469dbffb5","width":216,"height":108},{"url":"https://external-preview.redd.it/NPT2q1kryr57y9RMrGWEOV2MhPhXGETFTmAQPSEtLz4.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=fbd0080450353af1547fe0ee1ab75fbd878990e2","width":320,"height":160},{"url":"https://external-preview.redd.it/NPT2q1kryr57y9RMrGWEOV2MhPhXGETFTmAQPSEtLz4.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=b409933f393d7920c346f4c8e6a042beebf78e28","width":640,"height":320},{"url":"https://external-preview.redd.it/NPT2q1kryr57y9RMrGWEOV2MhPhXGETFTmAQPSEtLz4.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=33a90a793e27ea6d3890b1c22f8e749c3aa9de80","width":960,"height":480},{"url":"https://external-preview.redd.it/NPT2q1kryr57y9RMrGWEOV2MhPhXGETFTmAQPSEtLz4.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=7209dd018a6a28a3ad622b34e4df9961beb687ae","width":1080,"height":540}],"variants":{},"id":"NPT2q1kryr57y9RMrGWEOV2MhPhXGETFTmAQPSEtLz4"}],"enabled":false},"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#ccac2b","id":"1losjpq","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"Echo9Zulu-","discussion_type":null,"num_comments":2,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1losjpq/intel_gpu_vllm_docker_compose_bootstrap_with/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1losjpq/intel_gpu_vllm_docker_compose_bootstrap_with/","subreddit_subscribers":493458,"created_utc":1751343916,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0rd4ea","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Echo9Zulu-","can_mod_post":false,"created_utc":1751378415,"send_replies":true,"parent_id":"t1_n0pguhz","score":1,"author_fullname":"t2_pw77g8dq","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I have not found driver performance lackluster across the stack. Llama.cpp Vulkan, Llama.cpp SYCL, Llama.cpp IPEX, OpenVINO, vLLM IPEX.\\n\\nOverall there are other limiting factors which are harder to pin down. Broadly, driver support for AI enables most usecases, much better than it was this time last year in terms of limiting other compatibility. For example, the ipex vllm docs reccomend kernel 6.5 which is why I included that I\'m using 6.14. \\n\\n\\nIn this example TG was 194 t/s but this seems to be a rough average; accounting for dips to under 5 t/s across the whole run might bring it up to 300. So to answer your question, there is probably another test.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0rd4ea","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I have not found driver performance lackluster across the stack. Llama.cpp Vulkan, Llama.cpp SYCL, Llama.cpp IPEX, OpenVINO, vLLM IPEX.&lt;/p&gt;\\n\\n&lt;p&gt;Overall there are other limiting factors which are harder to pin down. Broadly, driver support for AI enables most usecases, much better than it was this time last year in terms of limiting other compatibility. For example, the ipex vllm docs reccomend kernel 6.5 which is why I included that I&amp;#39;m using 6.14. &lt;/p&gt;\\n\\n&lt;p&gt;In this example TG was 194 t/s but this seems to be a rough average; accounting for dips to under 5 t/s across the whole run might bring it up to 300. So to answer your question, there is probably another test.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1losjpq","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1losjpq/intel_gpu_vllm_docker_compose_bootstrap_with/n0rd4ea/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751378415,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0pguhz","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"terminoid_","can_mod_post":false,"created_utc":1751345364,"send_replies":true,"parent_id":"t3_1losjpq","score":2,"author_fullname":"t2_1iu07dnz2i","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"does vLLM make up for the driver\'s lackluster linux performance? i can\'t tell with the numbers you\'ve posted. can you get PP speed and TG speed separately?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0pguhz","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;does vLLM make up for the driver&amp;#39;s lackluster linux performance? i can&amp;#39;t tell with the numbers you&amp;#39;ve posted. can you get PP speed and TG speed separately?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1losjpq/intel_gpu_vllm_docker_compose_bootstrap_with/n0pguhz/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751345364,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1losjpq","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}}]'),o=()=>e.jsx(t,{data:n});export{o as default};
