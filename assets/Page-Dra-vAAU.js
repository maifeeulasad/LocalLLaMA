import{j as t}from"./index-M4edQi1P.js";import{R as l}from"./RedditPostRenderer-CESBGIIy.js";import"./index-DFpL1mt4.js";const e=JSON.parse('[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Hey folks â€” Iâ€™ve been working on a CLI tool called **LoFT (Low-RAM Finetuning Toolkit)**, and I finally have a working release.\\n\\n# ðŸ”§ What it does:\\n\\n* Finetunes open-source LLMs (1â€“3B) like **TinyLlama** using **QLoRA**\\n* Runs entirely on **CPU (MacBook Air 8GB RAM tested)**\\n* Quantizes to **GGUF** format\\n* Runs local inference via **llama.cpp**\\n* All through a clean CLI (`finetune`, `merge`, `quantize`, `chat`)\\n\\n# ðŸ’» Tech Stack:\\n\\n* `transformers`, `peft`, `bitsandbytes`, `datasets`, `llama.cpp`\\n* CLI-based interface built for reproducibility and minimal setup\\n\\n# ðŸ§  Why I built this:\\n\\nI wanted to see if itâ€™s feasible to do **end-to-end finetuning and deployment** of LLMs **without a GPU or cloud setup** â€” for indie hackers, researchers, or hobbyists working on local setups.\\n\\nAnd surprisingly, it works.\\n\\n# ðŸ› ï¸ Coming Soon:\\n\\n* GitHub repo (final touches being made)\\n* Full walkthrough + demo\\n* Support for multi-turn finetuning and inference\\n\\nWould love to hear:\\n\\n* Any feedback from folks doing low-resource model work\\n* Suggestions for models or datasets to support next\\n\\nHappy to tag you once the repo is up.\\n\\nCheers,  \\nDiptanshu","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"[Tool Release] Finetune &amp; Quantize 1â€“3B LLMs on 8GB RAM using LoFT CLI (TinyLlama + QLoRA + llama.cpp)","link_flair_richtext":[{"e":"text","t":"New Model"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":true,"name":"t3_1luiigi","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.91,"author_flair_background_color":null,"subreddit_type":"public","ups":9,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_46jj4viw","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"New Model","can_mod_post":false,"score":9,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1751960180,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Hey folks â€” Iâ€™ve been working on a CLI tool called &lt;strong&gt;LoFT (Low-RAM Finetuning Toolkit)&lt;/strong&gt;, and I finally have a working release.&lt;/p&gt;\\n\\n&lt;h1&gt;ðŸ”§ What it does:&lt;/h1&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;Finetunes open-source LLMs (1â€“3B) like &lt;strong&gt;TinyLlama&lt;/strong&gt; using &lt;strong&gt;QLoRA&lt;/strong&gt;&lt;/li&gt;\\n&lt;li&gt;Runs entirely on &lt;strong&gt;CPU (MacBook Air 8GB RAM tested)&lt;/strong&gt;&lt;/li&gt;\\n&lt;li&gt;Quantizes to &lt;strong&gt;GGUF&lt;/strong&gt; format&lt;/li&gt;\\n&lt;li&gt;Runs local inference via &lt;strong&gt;llama.cpp&lt;/strong&gt;&lt;/li&gt;\\n&lt;li&gt;All through a clean CLI (&lt;code&gt;finetune&lt;/code&gt;, &lt;code&gt;merge&lt;/code&gt;, &lt;code&gt;quantize&lt;/code&gt;, &lt;code&gt;chat&lt;/code&gt;)&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;h1&gt;ðŸ’» Tech Stack:&lt;/h1&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;&lt;code&gt;transformers&lt;/code&gt;, &lt;code&gt;peft&lt;/code&gt;, &lt;code&gt;bitsandbytes&lt;/code&gt;, &lt;code&gt;datasets&lt;/code&gt;, &lt;code&gt;llama.cpp&lt;/code&gt;&lt;/li&gt;\\n&lt;li&gt;CLI-based interface built for reproducibility and minimal setup&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;h1&gt;ðŸ§  Why I built this:&lt;/h1&gt;\\n\\n&lt;p&gt;I wanted to see if itâ€™s feasible to do &lt;strong&gt;end-to-end finetuning and deployment&lt;/strong&gt; of LLMs &lt;strong&gt;without a GPU or cloud setup&lt;/strong&gt; â€” for indie hackers, researchers, or hobbyists working on local setups.&lt;/p&gt;\\n\\n&lt;p&gt;And surprisingly, it works.&lt;/p&gt;\\n\\n&lt;h1&gt;ðŸ› ï¸ Coming Soon:&lt;/h1&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;GitHub repo (final touches being made)&lt;/li&gt;\\n&lt;li&gt;Full walkthrough + demo&lt;/li&gt;\\n&lt;li&gt;Support for multi-turn finetuning and inference&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;p&gt;Would love to hear:&lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;Any feedback from folks doing low-resource model work&lt;/li&gt;\\n&lt;li&gt;Suggestions for models or datasets to support next&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;p&gt;Happy to tag you once the repo is up.&lt;/p&gt;\\n\\n&lt;p&gt;Cheers,&lt;br/&gt;\\nDiptanshu&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"ced98442-f5d3-11ed-b657-66d3b15490c6","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#ffb000","id":"1luiigi","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"diptanshu1991","discussion_type":null,"num_comments":2,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1luiigi/tool_release_finetune_quantize_13b_llms_on_8gb/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1luiigi/tool_release_finetune_quantize_13b_llms_on_8gb/","subreddit_subscribers":496036,"created_utc":1751960180,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1ycjab","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"diptanshu1991","can_mod_post":false,"created_utc":1751964421,"send_replies":true,"parent_id":"t1_n1y9d0z","score":1,"author_fullname":"t2_46jj4viw","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Thank you â€” that means a lot!  \\nTotally agree on model support â€” Phi-2, Zephyr 1.1B, and Gemma 2B are high on the roadmap.  \\nIâ€™ll drop the repo link here in the next post once itâ€™s up (docs + install ready). Would love your feedback once you try it!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1ycjab","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thank you â€” that means a lot!&lt;br/&gt;\\nTotally agree on model support â€” Phi-2, Zephyr 1.1B, and Gemma 2B are high on the roadmap.&lt;br/&gt;\\nIâ€™ll drop the repo link here in the next post once itâ€™s up (docs + install ready). Would love your feedback once you try it!&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1luiigi","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1luiigi/tool_release_finetune_quantize_13b_llms_on_8gb/n1ycjab/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751964421,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1y9d0z","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"amsat","can_mod_post":false,"created_utc":1751962516,"send_replies":true,"parent_id":"t3_1luiigi","score":2,"author_fullname":"t2_3pvxw","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"hi, great tool, exactly what was missing: a clean pipeline for local QLoRA + GGUF on CPU. Suggestions: add support more models: Phi-2, Zephyr 1.1B, Gemma 2B (all QLoRA-ready)\\n\\nDrop the repo when itâ€™s live â€” Iâ€™ll test and share feedback","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1y9d0z","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;hi, great tool, exactly what was missing: a clean pipeline for local QLoRA + GGUF on CPU. Suggestions: add support more models: Phi-2, Zephyr 1.1B, Gemma 2B (all QLoRA-ready)&lt;/p&gt;\\n\\n&lt;p&gt;Drop the repo when itâ€™s live â€” Iâ€™ll test and share feedback&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1luiigi/tool_release_finetune_quantize_13b_llms_on_8gb/n1y9d0z/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751962516,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1luiigi","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}}]'),i=()=>t.jsx(l,{data:e});export{i as default};
