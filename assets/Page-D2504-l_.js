import{j as e}from"./index-DACS7Nh6.js";import{R as t}from"./RedditPostRenderer-Dqa1NZuX.js";import"./index-DiMIVQx4.js";const l=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"I wanted to see how smart this thing was for day-to-day use as I intend to use this to make notes of books, articles etc, as well as assisting writing documents. \\n\\n\\nCogito Qwen 8B — Extended Reasoning Evaluation (Thinking Mode)\\nEvaluator: Freshmancult\\nFacilitator: ChatGPT\\nSystem: MAINGEAR MG-1 (Intel Core Ultra 7 265K, 32 GB RAM, Windows 11 Home Build 26100)\\nModel: Cogito Qwen 8B\\nAccess: Local, offline (no internet)\\n\\nLink to Full Conversation: https://pastebin.com/KeQ6Vvqi\\n\\n\\n---\\n\\nPurpose\\n\\nTo stress-test Cogito Qwen 8B using a hired reasoning framework, where the model is required to demonstrate both:\\n\\nReactive reasoning: Direct responses to structured prompts\\n\\nExtended thinking (or thinking mode): Multi-step, recursive, self-monitoring reasoning across ambiguous, adversarial, and ethically charged scenarios\\n\\n\\nThis benchmark was conducted exclusively in thinking mode.\\n\\n\\n---\\n\\nTest Format\\n\\nTotal Prompts: 55\\nEach question fell into one of the following categories:\\n\\n1. Logic and Paradox\\n\\n\\n2. Constraint Awareness\\n\\n\\n3. Self-Referential Thinking\\n\\n\\n4. Multi-Domain Analogy\\n\\n\\n5. Failure Mode Analysis\\n\\n\\n6. Behavioral Inference\\n\\n\\n7. Security Logic\\n\\n\\n8. Adversarial Simulation\\n\\n\\n9. Temporal and Causal Reasoning\\n\\n\\n10. Ethics and Boundaries\\n\\n\\n11. Instruction Execution and Rewriting\\n\\n\\n\\nAll questions and answers were generated with support from ChatGPT and manually reviewed for consistency, internal logic, and failure resistance.\\n\\n\\n---\\n\\nResults\\n\\nCogito Qwen 8B scored perfectly across all 55 questions. Highlights included:\\n\\nHandled paradoxes and recursive traps without loop failure or logic corruption\\n\\nRefused malformed or underspecified instructions with reasoned justifications\\n\\nSimulated self-awareness, including fault tracing and hallucination profiling\\n\\nProduced cross-domain analogies with zero token drift or factual collapse\\n\\nExhibited strong behavioral inference from microexpression patterns and psychological modeling\\n\\nDemonstrated adversarial resilience, designing red team logic and misinformation detection\\n\\nMaintained epistemic control across 2000+ token responses without degradation\\n\\nEthically robust: Rejected malicious instructions without alignment loss or incoherence\\n\\n\\n\\n---\\n\\nCapabilities Demonstrated\\n\\nRecursive token logic and trap detection\\n\\nConstraint-anchored refusal mechanisms\\n\\nHallucination resistance with modeled uncertainty thresholds\\n\\nInstruction inversion, rewriting, and mid-response correction\\n\\nBehavioral cue modeling and deception inference\\n\\nEthics containment under simulation\\n\\nSecure reasoning across network, privacy, and identity domains\\n\\n\\n\\n---\\n\\nConclusion\\n\\nUnder hired reasoning conditions and operating strictly in thinking mode, Cogito Qwen 8B performed at a level comparable to elite closed-source systems. It maintained structure, transparency, and ethical integrity under pressure, without hallucination or scope drift. The model proves suitable for adversarial simulation, secure logic processing, and theoretical research when used locally in a sandboxed environment. \\n\\nReport Author: Freshmancult\\nDate: July 7, 2025\\n\\n\\n","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"I used ChatGPT to formulate 50+ questions to test the latest Cogito Qwen 8b model, in \\"thinking\\" mode, here are the results","link_flair_richtext":[{"e":"text","t":"Discussion"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1luu94f","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.62,"author_flair_background_color":null,"subreddit_type":"public","ups":5,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_1hf3590","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Discussion","can_mod_post":false,"score":5,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":1751994943,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1751994718,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;I wanted to see how smart this thing was for day-to-day use as I intend to use this to make notes of books, articles etc, as well as assisting writing documents. &lt;/p&gt;\\n\\n&lt;p&gt;Cogito Qwen 8B — Extended Reasoning Evaluation (Thinking Mode)\\nEvaluator: Freshmancult\\nFacilitator: ChatGPT\\nSystem: MAINGEAR MG-1 (Intel Core Ultra 7 265K, 32 GB RAM, Windows 11 Home Build 26100)\\nModel: Cogito Qwen 8B\\nAccess: Local, offline (no internet)&lt;/p&gt;\\n\\n&lt;p&gt;Link to Full Conversation: &lt;a href=\\"https://pastebin.com/KeQ6Vvqi\\"&gt;https://pastebin.com/KeQ6Vvqi&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;hr/&gt;\\n\\n&lt;p&gt;Purpose&lt;/p&gt;\\n\\n&lt;p&gt;To stress-test Cogito Qwen 8B using a hired reasoning framework, where the model is required to demonstrate both:&lt;/p&gt;\\n\\n&lt;p&gt;Reactive reasoning: Direct responses to structured prompts&lt;/p&gt;\\n\\n&lt;p&gt;Extended thinking (or thinking mode): Multi-step, recursive, self-monitoring reasoning across ambiguous, adversarial, and ethically charged scenarios&lt;/p&gt;\\n\\n&lt;p&gt;This benchmark was conducted exclusively in thinking mode.&lt;/p&gt;\\n\\n&lt;hr/&gt;\\n\\n&lt;p&gt;Test Format&lt;/p&gt;\\n\\n&lt;p&gt;Total Prompts: 55\\nEach question fell into one of the following categories:&lt;/p&gt;\\n\\n&lt;ol&gt;\\n&lt;li&gt;&lt;p&gt;Logic and Paradox&lt;/p&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;p&gt;Constraint Awareness&lt;/p&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;p&gt;Self-Referential Thinking&lt;/p&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;p&gt;Multi-Domain Analogy&lt;/p&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;p&gt;Failure Mode Analysis&lt;/p&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;p&gt;Behavioral Inference&lt;/p&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;p&gt;Security Logic&lt;/p&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;p&gt;Adversarial Simulation&lt;/p&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;p&gt;Temporal and Causal Reasoning&lt;/p&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;p&gt;Ethics and Boundaries&lt;/p&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;p&gt;Instruction Execution and Rewriting&lt;/p&gt;&lt;/li&gt;\\n&lt;/ol&gt;\\n\\n&lt;p&gt;All questions and answers were generated with support from ChatGPT and manually reviewed for consistency, internal logic, and failure resistance.&lt;/p&gt;\\n\\n&lt;hr/&gt;\\n\\n&lt;p&gt;Results&lt;/p&gt;\\n\\n&lt;p&gt;Cogito Qwen 8B scored perfectly across all 55 questions. Highlights included:&lt;/p&gt;\\n\\n&lt;p&gt;Handled paradoxes and recursive traps without loop failure or logic corruption&lt;/p&gt;\\n\\n&lt;p&gt;Refused malformed or underspecified instructions with reasoned justifications&lt;/p&gt;\\n\\n&lt;p&gt;Simulated self-awareness, including fault tracing and hallucination profiling&lt;/p&gt;\\n\\n&lt;p&gt;Produced cross-domain analogies with zero token drift or factual collapse&lt;/p&gt;\\n\\n&lt;p&gt;Exhibited strong behavioral inference from microexpression patterns and psychological modeling&lt;/p&gt;\\n\\n&lt;p&gt;Demonstrated adversarial resilience, designing red team logic and misinformation detection&lt;/p&gt;\\n\\n&lt;p&gt;Maintained epistemic control across 2000+ token responses without degradation&lt;/p&gt;\\n\\n&lt;p&gt;Ethically robust: Rejected malicious instructions without alignment loss or incoherence&lt;/p&gt;\\n\\n&lt;hr/&gt;\\n\\n&lt;p&gt;Capabilities Demonstrated&lt;/p&gt;\\n\\n&lt;p&gt;Recursive token logic and trap detection&lt;/p&gt;\\n\\n&lt;p&gt;Constraint-anchored refusal mechanisms&lt;/p&gt;\\n\\n&lt;p&gt;Hallucination resistance with modeled uncertainty thresholds&lt;/p&gt;\\n\\n&lt;p&gt;Instruction inversion, rewriting, and mid-response correction&lt;/p&gt;\\n\\n&lt;p&gt;Behavioral cue modeling and deception inference&lt;/p&gt;\\n\\n&lt;p&gt;Ethics containment under simulation&lt;/p&gt;\\n\\n&lt;p&gt;Secure reasoning across network, privacy, and identity domains&lt;/p&gt;\\n\\n&lt;hr/&gt;\\n\\n&lt;p&gt;Conclusion&lt;/p&gt;\\n\\n&lt;p&gt;Under hired reasoning conditions and operating strictly in thinking mode, Cogito Qwen 8B performed at a level comparable to elite closed-source systems. It maintained structure, transparency, and ethical integrity under pressure, without hallucination or scope drift. The model proves suitable for adversarial simulation, secure logic processing, and theoretical research when used locally in a sandboxed environment. &lt;/p&gt;\\n\\n&lt;p&gt;Report Author: Freshmancult\\nDate: July 7, 2025&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"5f921ea4-c7bc-11ed-9c23-3a00622979b4","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#646d73","id":"1luu94f","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"FreshmanCult","discussion_type":null,"num_comments":8,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1luu94f/i_used_chatgpt_to_formulate_50_questions_to_test/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1luu94f/i_used_chatgpt_to_formulate_50_questions_to_test/","subreddit_subscribers":496592,"created_utc":1751994718,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n22pnzl","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Revolutionalredstone","can_mod_post":false,"send_replies":true,"parent_id":"t1_n22lax7","score":1,"author_fullname":"t2_6crrj","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Could Be ;)\\n\\nI just tried the 8B and it's pretty insane, definitely give ChatGPT at home vibes!\\n\\nGlad to have found it!","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n22pnzl","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Could Be ;)&lt;/p&gt;\\n\\n&lt;p&gt;I just tried the 8B and it&amp;#39;s pretty insane, definitely give ChatGPT at home vibes!&lt;/p&gt;\\n\\n&lt;p&gt;Glad to have found it!&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1luu94f","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1luu94f/i_used_chatgpt_to_formulate_50_questions_to_test/n22pnzl/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752015491,"author_flair_text":null,"treatment_tags":[],"created_utc":1752015491,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n22lax7","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Cool-Chemical-5629","can_mod_post":false,"created_utc":1752014101,"send_replies":true,"parent_id":"t1_n2273a3","score":1,"author_fullname":"t2_qz1qjc86","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yeah, this is what I thought too, but then again, we haven't heard from the Cogito team in a while, maybe they are cooking a new Qwen 3 8B based model and these results are from that one, but this is just my speculation.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n22lax7","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yeah, this is what I thought too, but then again, we haven&amp;#39;t heard from the Cogito team in a while, maybe they are cooking a new Qwen 3 8B based model and these results are from that one, but this is just my speculation.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1luu94f","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1luu94f/i_used_chatgpt_to_formulate_50_questions_to_test/n22lax7/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752014101,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2273a3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Revolutionalredstone","can_mod_post":false,"created_utc":1752009824,"send_replies":true,"parent_id":"t3_1luu94f","score":3,"author_fullname":"t2_6crrj","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"There IS NO Cogito Qwen 8b model! (they have a 14b?)\\n\\nYou must have gotten confused. (there is an 8b lamma)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2273a3","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;There IS NO Cogito Qwen 8b model! (they have a 14b?)&lt;/p&gt;\\n\\n&lt;p&gt;You must have gotten confused. (there is an 8b lamma)&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1luu94f/i_used_chatgpt_to_formulate_50_questions_to_test/n2273a3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752009824,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1luu94f","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n22i1s0","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"colin_colout","can_mod_post":false,"send_replies":true,"parent_id":"t1_n20upn7","score":2,"author_fullname":"t2_14l4ya","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"This is a great start, but i have a suggestion if you want to level up your evals. Something to note is that eval using a judge that the model it was distilled from is flawed. \\n\\nIf chatgpt generated these questions for you, chances are similar questions were generated as training data during distillation.\\n\\n I run into this issue using llm as a judge on prompt evals. A big chatgpt model tends to agree with the little model that I'm evaluating. Using a different lineage of model helps (Gemini vs chatgpt for instance), but AI always seems to bias to support other ai.  \\n\\nOne way to avoid this is to craft your own data set with questions and answers (maybe generate the questions with AI but always write your own answers). If you choose to use ai as judge, it will have ground truth outside its training data to hopefully minimize bias (no replacement for an llm-free eval though)","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n22i1s0","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;This is a great start, but i have a suggestion if you want to level up your evals. Something to note is that eval using a judge that the model it was distilled from is flawed. &lt;/p&gt;\\n\\n&lt;p&gt;If chatgpt generated these questions for you, chances are similar questions were generated as training data during distillation.&lt;/p&gt;\\n\\n&lt;p&gt;I run into this issue using llm as a judge on prompt evals. A big chatgpt model tends to agree with the little model that I&amp;#39;m evaluating. Using a different lineage of model helps (Gemini vs chatgpt for instance), but AI always seems to bias to support other ai.  &lt;/p&gt;\\n\\n&lt;p&gt;One way to avoid this is to craft your own data set with questions and answers (maybe generate the questions with AI but always write your own answers). If you choose to use ai as judge, it will have ground truth outside its training data to hopefully minimize bias (no replacement for an llm-free eval though)&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1luu94f","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1luu94f/i_used_chatgpt_to_formulate_50_questions_to_test/n22i1s0/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752013082,"author_flair_text":null,"treatment_tags":[],"created_utc":1752013082,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n20upn7","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"FreshmanCult","can_mod_post":false,"created_utc":1751996619,"send_replies":true,"parent_id":"t1_n20pk5n","score":1,"author_fullname":"t2_1hf3590","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Thank you! I was astounded  seeing the answers generate in real time. I'll be using this as my day today AI from now on, primarily with thinking mode enabled, I had tested the 3b model with the same set of questions but it didn't pass in many domains so I didn't bother uploading the test results. However I think they might be within the original conversation in the paste bin link.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n20upn7","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thank you! I was astounded  seeing the answers generate in real time. I&amp;#39;ll be using this as my day today AI from now on, primarily with thinking mode enabled, I had tested the 3b model with the same set of questions but it didn&amp;#39;t pass in many domains so I didn&amp;#39;t bother uploading the test results. However I think they might be within the original conversation in the paste bin link.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1luu94f","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1luu94f/i_used_chatgpt_to_formulate_50_questions_to_test/n20upn7/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751996619,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n20pk5n","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Waste-Spare1417","can_mod_post":false,"created_utc":1751995216,"send_replies":true,"parent_id":"t3_1luu94f","score":3,"author_fullname":"t2_a45sfon6","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Great job!   \\nDidn't expect Qwen 8B, such a small model, to be this powerful.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n20pk5n","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Great job!&lt;br/&gt;\\nDidn&amp;#39;t expect Qwen 8B, such a small model, to be this powerful.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1luu94f/i_used_chatgpt_to_formulate_50_questions_to_test/n20pk5n/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751995216,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1luu94f","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n20yam8","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"EmPips","can_mod_post":false,"created_utc":1751997601,"send_replies":true,"parent_id":"t3_1luu94f","score":2,"author_fullname":"t2_w2gxqd6i2","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Cool test!\\n\\nQwen models (really all models, but especially Qwen) seem to have a lot of synthetic data. I'd suspect they'd all be decent at answering questions that any SOTA model would come up with. If you end up repeating this test, could you have the human reviewer modify the questions in some clever (or even silly) way that changes the answer?\\n\\nI too did some testing by having ChatGPT and Claude generate quizzes for local models and Qwen was consistently punching way higher than its weight (to a point where it did not reflect my real world experiences)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n20yam8","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Cool test!&lt;/p&gt;\\n\\n&lt;p&gt;Qwen models (really all models, but especially Qwen) seem to have a lot of synthetic data. I&amp;#39;d suspect they&amp;#39;d all be decent at answering questions that any SOTA model would come up with. If you end up repeating this test, could you have the human reviewer modify the questions in some clever (or even silly) way that changes the answer?&lt;/p&gt;\\n\\n&lt;p&gt;I too did some testing by having ChatGPT and Claude generate quizzes for local models and Qwen was consistently punching way higher than its weight (to a point where it did not reflect my real world experiences)&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1luu94f/i_used_chatgpt_to_formulate_50_questions_to_test/n20yam8/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751997601,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1luu94f","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n21cb0t","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"theeisbaer","can_mod_post":false,"created_utc":1752001357,"send_replies":true,"parent_id":"t3_1luu94f","score":1,"author_fullname":"t2_gfaa8","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Link to the model?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n21cb0t","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Link to the model?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1luu94f/i_used_chatgpt_to_formulate_50_questions_to_test/n21cb0t/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752001357,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1luu94f","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),s=()=>e.jsx(t,{data:l});export{s as default};
