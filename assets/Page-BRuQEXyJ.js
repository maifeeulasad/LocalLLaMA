import{j as e}from"./index-BpC9hjVs.js";import{R as t}from"./RedditPostRenderer-BEo6AnSR.js";import"./index-DwkJHX1_.js";const a=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Curious to hear from others building LLM-based chat apps: Do you implement **prompt caching** to store chat history or previous responses? Or do you send the chat history with each user's prompt?\\n\\nCaching is more expensive to write, but the costs are then net positive if the conversation becomes long, no?\\n\\nWould appreciate your insights — thanks!","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Do you use prompt caching to save chat history in your LLM apps?","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1ltze9d","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.88,"author_flair_background_color":null,"subreddit_type":"public","ups":12,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_8h2i7wiei","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":12,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1751907224,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Curious to hear from others building LLM-based chat apps: Do you implement &lt;strong&gt;prompt caching&lt;/strong&gt; to store chat history or previous responses? Or do you send the chat history with each user&amp;#39;s prompt?&lt;/p&gt;\\n\\n&lt;p&gt;Caching is more expensive to write, but the costs are then net positive if the conversation becomes long, no?&lt;/p&gt;\\n\\n&lt;p&gt;Would appreciate your insights — thanks!&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1ltze9d","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"Physical_Ad9040","discussion_type":null,"num_comments":9,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1ltze9d/do_you_use_prompt_caching_to_save_chat_history_in/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1ltze9d/do_you_use_prompt_caching_to_save_chat_history_in/","subreddit_subscribers":496034,"created_utc":1751907224,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1wp4un","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"SkyFeistyLlama8","can_mod_post":false,"created_utc":1751937430,"send_replies":true,"parent_id":"t1_n1u6r7r","score":2,"author_fullname":"t2_1hgbaqgbnq","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"llama-server keeps a KV cache for specific slots so you can carry on with a long conversation without each new input requiring reprocessing of all previous inputs. That's only active as long as the server is running.\\n\\nTerminate the server and that cache gets cleared from RAM, so if you restart the same conversation later the entire chat history has to be included as part of the context and reprocessed.\\n\\nI don't know of any open source inference stack that can save KV caches to disk and reload from that. It would be the equivalent of saving a VM's state to a memory dump file and then reloading from that dump file into RAM the next time the VM is fired up again.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1wp4un","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;llama-server keeps a KV cache for specific slots so you can carry on with a long conversation without each new input requiring reprocessing of all previous inputs. That&amp;#39;s only active as long as the server is running.&lt;/p&gt;\\n\\n&lt;p&gt;Terminate the server and that cache gets cleared from RAM, so if you restart the same conversation later the entire chat history has to be included as part of the context and reprocessed.&lt;/p&gt;\\n\\n&lt;p&gt;I don&amp;#39;t know of any open source inference stack that can save KV caches to disk and reload from that. It would be the equivalent of saving a VM&amp;#39;s state to a memory dump file and then reloading from that dump file into RAM the next time the VM is fired up again.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ltze9d","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltze9d/do_you_use_prompt_caching_to_save_chat_history_in/n1wp4un/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751937430,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n1u6r7r","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Awwtifishal","can_mod_post":false,"created_utc":1751908026,"send_replies":true,"parent_id":"t3_1ltze9d","score":16,"author_fullname":"t2_1d96a8k10t","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"With typical LLM APIs (local or not) the whole conversation is sent on each request, regardless of \\"prompt caching\\" support. Usually \\"prompt caching\\" means storing the KV cache even if it's just temporarily, so you don't have to re-process the whole KV data on each request, and it's pretty common. For example llama.cpp and llama.cpp based projects all do it by default: they reuse the KV cache up to the point it changes or it ends. So for example editing the last message is much faster than editing the first message or the system message.\\n\\nNote that LLM inferencing doesn't distinguish between \\"messages\\", it's just one big string of text to autocomplete (usually just to let the \\"assistant\\" part of the conversation be generated and stop when this part ends). So it's not just the \\"prompt\\" what is cached, but everything.\\n\\nThen if you e.g. store the conversation server-side and just send each message individually, that's a different feature, meant for saving in bandwidth although it's usually not much so it may not be worth it.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1u6r7r","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;With typical LLM APIs (local or not) the whole conversation is sent on each request, regardless of &amp;quot;prompt caching&amp;quot; support. Usually &amp;quot;prompt caching&amp;quot; means storing the KV cache even if it&amp;#39;s just temporarily, so you don&amp;#39;t have to re-process the whole KV data on each request, and it&amp;#39;s pretty common. For example llama.cpp and llama.cpp based projects all do it by default: they reuse the KV cache up to the point it changes or it ends. So for example editing the last message is much faster than editing the first message or the system message.&lt;/p&gt;\\n\\n&lt;p&gt;Note that LLM inferencing doesn&amp;#39;t distinguish between &amp;quot;messages&amp;quot;, it&amp;#39;s just one big string of text to autocomplete (usually just to let the &amp;quot;assistant&amp;quot; part of the conversation be generated and stop when this part ends). So it&amp;#39;s not just the &amp;quot;prompt&amp;quot; what is cached, but everything.&lt;/p&gt;\\n\\n&lt;p&gt;Then if you e.g. store the conversation server-side and just send each message individually, that&amp;#39;s a different feature, meant for saving in bandwidth although it&amp;#39;s usually not much so it may not be worth it.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltze9d/do_you_use_prompt_caching_to_save_chat_history_in/n1u6r7r/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751908026,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1ltze9d","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":16}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1udlcj","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Flaky_Comedian2012","can_mod_post":false,"created_utc":1751910026,"send_replies":true,"parent_id":"t3_1ltze9d","score":3,"author_fullname":"t2_1d1v4h15w7","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I just send it all as it is already cached by default in the backend. It only has to process whatever extra tokens that has been added to the context.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1udlcj","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I just send it all as it is already cached by default in the backend. It only has to process whatever extra tokens that has been added to the context.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltze9d/do_you_use_prompt_caching_to_save_chat_history_in/n1udlcj/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751910026,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1ltze9d","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"2b12e2b8-fdc0-11ee-9a03-6e2f48afd456","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1uk1q0","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Former-Ad-5757","can_mod_post":false,"created_utc":1751912001,"send_replies":true,"parent_id":"t3_1ltze9d","score":3,"author_fullname":"t2_ihsdiwk6k","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"What do you mean with chat history? The history of the current chat, or the complete chat history of the user?\\n\\nCurrent chat you have to send with each users prompt to get a chat, without that it is just a oneshot answering machine without memory.\\n\\nUser chat history can’t really be send with each prompt as it is too long, if you want to achieve this then you need to summarize a chat and then in the next chat you send the summary, or you use rag to insert a previous summary. This is where chat history functions come into play, for example I summarize my chats, then I summarize my 7 days old summaries again and summarize monthly etc. This way I lose precision over time but I keep my context within size and I have toolcalls to get every summary or original chat if I need it","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1uk1q0","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"Llama 3"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;What do you mean with chat history? The history of the current chat, or the complete chat history of the user?&lt;/p&gt;\\n\\n&lt;p&gt;Current chat you have to send with each users prompt to get a chat, without that it is just a oneshot answering machine without memory.&lt;/p&gt;\\n\\n&lt;p&gt;User chat history can’t really be send with each prompt as it is too long, if you want to achieve this then you need to summarize a chat and then in the next chat you send the summary, or you use rag to insert a previous summary. This is where chat history functions come into play, for example I summarize my chats, then I summarize my 7 days old summaries again and summarize monthly etc. This way I lose precision over time but I keep my context within size and I have toolcalls to get every summary or original chat if I need it&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltze9d/do_you_use_prompt_caching_to_save_chat_history_in/n1uk1q0/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751912001,"author_flair_text":"Llama 3","treatment_tags":[],"link_id":"t3_1ltze9d","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#c7b594","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1u5oev","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"picturpoet","can_mod_post":false,"created_utc":1751907715,"send_replies":true,"parent_id":"t3_1ltze9d","score":1,"author_fullname":"t2_naay5","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"following","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1u5oev","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;following&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltze9d/do_you_use_prompt_caching_to_save_chat_history_in/n1u5oev/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751907715,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1ltze9d","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1udyuw","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Some-Cauliflower4902","can_mod_post":false,"created_utc":1751910140,"send_replies":true,"parent_id":"t3_1ltze9d","score":1,"author_fullname":"t2_1e2tnqudlj","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"My hobby project sends the whole chat history. Would like to learn other ways.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1udyuw","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;My hobby project sends the whole chat history. Would like to learn other ways.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltze9d/do_you_use_prompt_caching_to_save_chat_history_in/n1udyuw/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751910140,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1ltze9d","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1uutuv","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"a_chatbot","can_mod_post":false,"created_utc":1751915495,"send_replies":true,"parent_id":"t3_1ltze9d","score":1,"author_fullname":"t2_u1kj3zgg","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"The reason why I don't:  \\n1. Chatbot instructions and 'memories' usually go before the conversation history in a prompt, and that can be dynamic depending on context.  \\n2. To maximize amount of conversation used as close to the context limit without mid-sentence cutoffs.   \\n3. Running locally, it doesn't make a whole lot of difference in speed at least for me.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1uutuv","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The reason why I don&amp;#39;t:&lt;br/&gt;\\n1. Chatbot instructions and &amp;#39;memories&amp;#39; usually go before the conversation history in a prompt, and that can be dynamic depending on context.&lt;br/&gt;\\n2. To maximize amount of conversation used as close to the context limit without mid-sentence cutoffs.&lt;br/&gt;\\n3. Running locally, it doesn&amp;#39;t make a whole lot of difference in speed at least for me.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltze9d/do_you_use_prompt_caching_to_save_chat_history_in/n1uutuv/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751915495,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1ltze9d","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1vy177","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Agreeable-Prompt-666","can_mod_post":false,"created_utc":1751928409,"send_replies":true,"parent_id":"t3_1ltze9d","score":1,"author_fullname":"t2_1l3z4stvkq","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I'm not too sure what best practice is for a chat/rag LLM on llama.cpp with kvcache=true usage... right now I'm only automatically using it on the first message, primarily to cache the system prompt... Whats the proper way?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1vy177","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;m not too sure what best practice is for a chat/rag LLM on llama.cpp with kvcache=true usage... right now I&amp;#39;m only automatically using it on the first message, primarily to cache the system prompt... Whats the proper way?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltze9d/do_you_use_prompt_caching_to_save_chat_history_in/n1vy177/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751928409,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1ltze9d","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1wzrz3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"drdailey","can_mod_post":false,"created_utc":1751941089,"send_replies":true,"parent_id":"t3_1ltze9d","score":1,"author_fullname":"t2_15f7qa","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Yes.  But not for true prompt caching.  Occasionally may save some tokens… mostly for future automation.  I save every chat, tool use, response.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1wzrz3","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yes.  But not for true prompt caching.  Occasionally may save some tokens… mostly for future automation.  I save every chat, tool use, response.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":true,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltze9d/do_you_use_prompt_caching_to_save_chat_history_in/n1wzrz3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751941089,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1ltze9d","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),s=()=>e.jsx(t,{data:a});export{s as default};
