import{j as e}from"./index-cvG704yx.js";import{R as t}from"./RedditPostRenderer-CBthLTAH.js";import"./index-D-GavSZU.js";const l=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"You know that feeling when you have to explain the same story to five different people?\\n\\nThat’s been my experience with LLMs so far.\\n\\nI’ll start a convo with ChatGPT, hit a wall or I am dissatisfied, and switch to Claude for better capabilities. Suddenly, I’m back at square one, explaining *everything* again.\\n\\nI’ve tried keeping a doc with my context and asking one LLM to help prep for the next. It gets the job done to an extent, but it’s still far from ideal.\\n\\nSo, I built Windo - a universal context window that lets you share the same context across different LLMs.\\n\\n# How it works\\n\\n# Context adding\\n\\n* By pulling LLMs discussions on the go\\n* Manually, by uploading files, text, screenshots, voice notes\\n* By connecting data sources (Notion, Linear, Slack...) via MCP\\n\\n# Context filtering/preparation\\n\\n* Noise removal\\n* A local LLM filters public/private data, so we send only “public” data to the server\\n\\nWe are considering a local first approach. However, with the current state of local models, we can’t run everything locally; for now we are aiming for a partially local approach but our end goal is to have it fully local.\\n\\n# Context management\\n\\n* Context indexing in vector DB\\n* We make sense of the indexed data (context understanding) by generating project artifacts (overview, target users, goals…) to give models a quick summary, not to overwhelm them with a data dump.\\n* Context splitting into separate spaces based on projects, tasks, initiatives… giving the user granular control and permissions over what to share with different models and agents.\\n\\n# Context retrieval\\n\\n* User triggers context retrieval on any model\\n* Based on the user’s current work, we prepare the needed context, compressed adequately to not overload the target model’s context window.\\n* Or, the LLMs retrieve what they need via MCP (for models that support it), as Windo acts as an MCP server as well.\\n\\nWindo is like your AI’s USB stick for memory. Plug it into any LLM, and pick up where you left off.\\n\\nRight now, we’re testing with early users. If that sounds like something you need, I can share with you the website in the DMs if you ask. Looking for your feedback. Thanks.","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"How to use the same context across LLMs and Agents","link_flair_richtext":[{"e":"text","t":"Discussion"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1m2inuu","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.88,"author_flair_background_color":null,"subreddit_type":"public","ups":7,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_9blylofs","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Discussion","can_mod_post":false,"score":7,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1752784710,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;You know that feeling when you have to explain the same story to five different people?&lt;/p&gt;\\n\\n&lt;p&gt;That’s been my experience with LLMs so far.&lt;/p&gt;\\n\\n&lt;p&gt;I’ll start a convo with ChatGPT, hit a wall or I am dissatisfied, and switch to Claude for better capabilities. Suddenly, I’m back at square one, explaining &lt;em&gt;everything&lt;/em&gt; again.&lt;/p&gt;\\n\\n&lt;p&gt;I’ve tried keeping a doc with my context and asking one LLM to help prep for the next. It gets the job done to an extent, but it’s still far from ideal.&lt;/p&gt;\\n\\n&lt;p&gt;So, I built Windo - a universal context window that lets you share the same context across different LLMs.&lt;/p&gt;\\n\\n&lt;h1&gt;How it works&lt;/h1&gt;\\n\\n&lt;h1&gt;Context adding&lt;/h1&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;By pulling LLMs discussions on the go&lt;/li&gt;\\n&lt;li&gt;Manually, by uploading files, text, screenshots, voice notes&lt;/li&gt;\\n&lt;li&gt;By connecting data sources (Notion, Linear, Slack...) via MCP&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;h1&gt;Context filtering/preparation&lt;/h1&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;Noise removal&lt;/li&gt;\\n&lt;li&gt;A local LLM filters public/private data, so we send only “public” data to the server&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;p&gt;We are considering a local first approach. However, with the current state of local models, we can’t run everything locally; for now we are aiming for a partially local approach but our end goal is to have it fully local.&lt;/p&gt;\\n\\n&lt;h1&gt;Context management&lt;/h1&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;Context indexing in vector DB&lt;/li&gt;\\n&lt;li&gt;We make sense of the indexed data (context understanding) by generating project artifacts (overview, target users, goals…) to give models a quick summary, not to overwhelm them with a data dump.&lt;/li&gt;\\n&lt;li&gt;Context splitting into separate spaces based on projects, tasks, initiatives… giving the user granular control and permissions over what to share with different models and agents.&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;h1&gt;Context retrieval&lt;/h1&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;User triggers context retrieval on any model&lt;/li&gt;\\n&lt;li&gt;Based on the user’s current work, we prepare the needed context, compressed adequately to not overload the target model’s context window.&lt;/li&gt;\\n&lt;li&gt;Or, the LLMs retrieve what they need via MCP (for models that support it), as Windo acts as an MCP server as well.&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;p&gt;Windo is like your AI’s USB stick for memory. Plug it into any LLM, and pick up where you left off.&lt;/p&gt;\\n\\n&lt;p&gt;Right now, we’re testing with early users. If that sounds like something you need, I can share with you the website in the DMs if you ask. Looking for your feedback. Thanks.&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"5f921ea4-c7bc-11ed-9c23-3a00622979b4","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#646d73","id":"1m2inuu","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"Imad-aka","discussion_type":null,"num_comments":2,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1m2inuu/how_to_use_the_same_context_across_llms_and_agents/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1m2inuu/how_to_use_the_same_context_across_llms_and_agents/","subreddit_subscribers":500896,"created_utc":1752784710,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3pk9bc","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Imad-aka","can_mod_post":false,"created_utc":1752789049,"send_replies":true,"parent_id":"t1_n3pfp23","score":2,"author_fullname":"t2_9blylofs","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"This is not an MCP, it uses it as one of the methods of getting and sharing context. That's all ;)","edited":1752790113,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3pk9bc","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;This is not an MCP, it uses it as one of the methods of getting and sharing context. That&amp;#39;s all ;)&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2inuu","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2inuu/how_to_use_the_same_context_across_llms_and_agents/n3pk9bc/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752789049,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n3pfp23","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Secure_Reflection409","can_mod_post":false,"created_utc":1752787687,"send_replies":true,"parent_id":"t3_1m2inuu","score":1,"author_fullname":"t2_by77ogdhr","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Ever since someone mentioned they'd built a new version of MCP, we've had MCP threads every day :D","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3pfp23","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Ever since someone mentioned they&amp;#39;d built a new version of MCP, we&amp;#39;ve had MCP threads every day :D&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2inuu/how_to_use_the_same_context_across_llms_and_agents/n3pfp23/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752787687,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m2inuu","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),s=()=>e.jsx(t,{data:l});export{s as default};
