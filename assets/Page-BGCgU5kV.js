import{j as e}from"./index-Dh2YTDbC.js";import{R as l}from"./RedditPostRenderer-BwWe7STC.js";import"./index-D7FMfiLd.js";const a=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Pretty much the title!\\n\\nDoes anyone have examples of llama.cpp being used in a form of enterprise/business context successfully?\\n\\nI see vLLM used at scale everywhere, so it would be cool to see any use cases that leverage laptops/lower-end hardware towards their benefit!","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Using llama.cpp in an enterprise?","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lp5obe","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.83,"author_flair_background_color":null,"subreddit_type":"public","ups":4,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_2eugqr60","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":4,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1751386101,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Pretty much the title!&lt;/p&gt;\\n\\n&lt;p&gt;Does anyone have examples of llama.cpp being used in a form of enterprise/business context successfully?&lt;/p&gt;\\n\\n&lt;p&gt;I see vLLM used at scale everywhere, so it would be cool to see any use cases that leverage laptops/lower-end hardware towards their benefit!&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1lp5obe","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"Careless-Car_","discussion_type":null,"num_comments":23,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lp5obe/using_llamacpp_in_an_enterprise/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lp5obe/using_llamacpp_in_an_enterprise/","subreddit_subscribers":493457,"created_utc":1751386101,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0uwcvq","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"0xFatWhiteMan","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0sopj2","score":1,"author_fullname":"t2_pz2dkuedp","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"ollama, and llama.cpp that its based can rely solely on cpu. vllm requires cuda.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0uwcvq","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;ollama, and llama.cpp that its based can rely solely on cpu. vllm requires cuda.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lp5obe","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp5obe/using_llamacpp_in_an_enterprise/n0uwcvq/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751415870,"author_flair_text":null,"treatment_tags":[],"created_utc":1751415870,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0v82ks","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MDT-49","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0sopj2","score":1,"author_fullname":"t2_h8yrica5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"When it comes to standardization of LLM inference, then llama.cpp is definitely used. Probably because it's kinda runs on anything, although not always in the most optimal way, and it supports most models and architectures. GGUF also makes things easier when it comes to standardization. \\n\\nFor example, it's used in llamafile and also in Docker Model Runner. There are GPU cloud services that offer \\"scale to zero\\" containers for AI inference based on Docker Models.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0v82ks","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;When it comes to standardization of LLM inference, then llama.cpp is definitely used. Probably because it&amp;#39;s kinda runs on anything, although not always in the most optimal way, and it supports most models and architectures. GGUF also makes things easier when it comes to standardization. &lt;/p&gt;\\n\\n&lt;p&gt;For example, it&amp;#39;s used in llamafile and also in Docker Model Runner. There are GPU cloud services that offer &amp;quot;scale to zero&amp;quot; containers for AI inference based on Docker Models.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lp5obe","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp5obe/using_llamacpp_in_an_enterprise/n0v82ks/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751420054,"author_flair_text":null,"treatment_tags":[],"created_utc":1751420054,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0sopj2","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Careless-Car_","can_mod_post":false,"created_utc":1751391805,"send_replies":true,"parent_id":"t1_n0shaai","score":1,"author_fullname":"t2_2eugqr60","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Right, for centralized inference you need vLLM or related.\\n\\nBut the concept of using llama.cpp in some enterprise context could be a standardization for the processes involved in running local LLMs","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0sopj2","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Right, for centralized inference you need vLLM or related.&lt;/p&gt;\\n\\n&lt;p&gt;But the concept of using llama.cpp in some enterprise context could be a standardization for the processes involved in running local LLMs&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lp5obe","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp5obe/using_llamacpp_in_an_enterprise/n0sopj2/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751391805,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0shaai","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"mikkel1156","can_mod_post":false,"created_utc":1751389814,"send_replies":true,"parent_id":"t3_1lp5obe","score":2,"author_fullname":"t2_p2pcn","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"If you are going enterprise then Kubernetes and either vLLM and SGLang might be your best bet. My org is still in early stages of looking into AI, but this is what I gathered.\\n\\nI wouldnt use laptops or low-end hardware for entreprise.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0shaai","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;If you are going enterprise then Kubernetes and either vLLM and SGLang might be your best bet. My org is still in early stages of looking into AI, but this is what I gathered.&lt;/p&gt;\\n\\n&lt;p&gt;I wouldnt use laptops or low-end hardware for entreprise.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp5obe/using_llamacpp_in_an_enterprise/n0shaai/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751389814,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lp5obe","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0szsqt","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Careless-Car_","can_mod_post":false,"created_utc":1751394877,"send_replies":true,"parent_id":"t1_n0swzru","score":1,"author_fullname":"t2_2eugqr60","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Or one model to one user, but any paradigm really","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0szsqt","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Or one model to one user, but any paradigm really&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lp5obe","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp5obe/using_llamacpp_in_an_enterprise/n0szsqt/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751394877,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0swzru","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"No_Afternoon_4260","can_mod_post":false,"created_utc":1751394086,"send_replies":true,"parent_id":"t3_1lp5obe","score":2,"author_fullname":"t2_cj9kap4bx","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Are you referring to using each piece of equipment as a RPC llama.cpp server?  \\nElse yeah vllm or sglang really","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0swzru","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Are you referring to using each piece of equipment as a RPC llama.cpp server?&lt;br/&gt;\\nElse yeah vllm or sglang really&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp5obe/using_llamacpp_in_an_enterprise/n0swzru/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751394086,"author_flair_text":"llama.cpp","treatment_tags":[],"link_id":"t3_1lp5obe","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0vbpw9","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Careless-Car_","can_mod_post":false,"created_utc":1751421316,"send_replies":true,"parent_id":"t1_n0t1bna","score":1,"author_fullname":"t2_2eugqr60","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yes, this!\\n\\nIf people already have the hardware, why not is exactly the question!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0vbpw9","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yes, this!&lt;/p&gt;\\n\\n&lt;p&gt;If people already have the hardware, why not is exactly the question!&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lp5obe","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp5obe/using_llamacpp_in_an_enterprise/n0vbpw9/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751421316,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0t1bna","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Conscious_Cut_6144","can_mod_post":false,"created_utc":1751395307,"send_replies":true,"parent_id":"t3_1lp5obe","score":2,"author_fullname":"t2_9hl4ymvj","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"So you have 100 workstations, you fire up Qwen3 30B-3A on all of them and run your batch jobs at night on them? Say you get 15 T/s each, that's 1500 T/s.\\n\\nI think I would rather get 1 GPU instead of trying to deal with 100 workstations but sure I guess why not?  \\nI'm sure someone has tried it.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0t1bna","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;So you have 100 workstations, you fire up Qwen3 30B-3A on all of them and run your batch jobs at night on them? Say you get 15 T/s each, that&amp;#39;s 1500 T/s.&lt;/p&gt;\\n\\n&lt;p&gt;I think I would rather get 1 GPU instead of trying to deal with 100 workstations but sure I guess why not?&lt;br/&gt;\\nI&amp;#39;m sure someone has tried it.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp5obe/using_llamacpp_in_an_enterprise/n0t1bna/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751395307,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lp5obe","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0vel4v","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Careless-Car_","can_mod_post":false,"created_utc":1751422304,"send_replies":true,"parent_id":"t1_n0vdfvd","score":1,"author_fullname":"t2_2eugqr60","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"“At my work” - this is what I am (poorly) asking for!\\n\\nIf ollama/llama.cpp is being used in any enterprise/work context, inclusive of prototyping!\\n\\nAny chance you’d like to expand on what your dev workflow looks like, ollama -&gt; vLLM and how you ship to production?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0vel4v","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;“At my work” - this is what I am (poorly) asking for!&lt;/p&gt;\\n\\n&lt;p&gt;If ollama/llama.cpp is being used in any enterprise/work context, inclusive of prototyping!&lt;/p&gt;\\n\\n&lt;p&gt;Any chance you’d like to expand on what your dev workflow looks like, ollama -&amp;gt; vLLM and how you ship to production?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp5obe/using_llamacpp_in_an_enterprise/n0vel4v/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751422304,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lp5obe","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":9,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0vdfvd","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"0xFatWhiteMan","can_mod_post":false,"created_utc":1751421912,"send_replies":true,"parent_id":"t1_n0vcxst","score":2,"author_fullname":"t2_pz2dkuedp","approved_by":null,"mod_note":null,"all_awardings":[],"body":"I'm lost to what you are asking. I am happily prototyping ollama at my work, using our rather underpowered servers.","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n0vdfvd","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;m lost to what you are asking. I am happily prototyping ollama at my work, using our rather underpowered servers.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp5obe/using_llamacpp_in_an_enterprise/n0vdfvd/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751421912,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lp5obe","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":8,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n0vcxst","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Careless-Car_","can_mod_post":false,"created_utc":1751421739,"send_replies":true,"parent_id":"t1_n0vchy0","score":1,"author_fullname":"t2_2eugqr60","approved_by":null,"mod_note":null,"all_awardings":[],"body":"Nah not 4070s, but they could hand out Macs to their users/developers and higher-end laptops and workstations with GPUs that vLLM couldn’t utilize.\\n\\nSpecifically for those users, some permutation of llama.cpp would enable them to run these models with no dependency on a central/cloud LLM (aside from the privacy benefits)","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n0vcxst","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Nah not 4070s, but they could hand out Macs to their users/developers and higher-end laptops and workstations with GPUs that vLLM couldn’t utilize.&lt;/p&gt;\\n\\n&lt;p&gt;Specifically for those users, some permutation of llama.cpp would enable them to run these models with no dependency on a central/cloud LLM (aside from the privacy benefits)&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lp5obe","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp5obe/using_llamacpp_in_an_enterprise/n0vcxst/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751421739,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":7,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0vchy0","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"0xFatWhiteMan","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0vc41b","score":1,"author_fullname":"t2_pz2dkuedp","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I don't know any enterprise that have 4070s on devices, or even any gpus - just sitting around.","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n0vchy0","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I don&amp;#39;t know any enterprise that have 4070s on devices, or even any gpus - just sitting around.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lp5obe","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp5obe/using_llamacpp_in_an_enterprise/n0vchy0/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751421589,"author_flair_text":null,"treatment_tags":[],"created_utc":1751421589,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":6,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0vc41b","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Careless-Car_","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0vb1sa","score":1,"author_fullname":"t2_2eugqr60","approved_by":null,"mod_note":null,"all_awardings":[],"body":"They will work fantastically well, but are enterprises going to scale out ollama to all of their user devices/locations, or just switch to some central GPU cluster?\\n\\nMost have been doing the latter, I want to see if anyone is doing that ollama/llama.cpp scale out","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n0vc41b","is_submitter":true,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;They will work fantastically well, but are enterprises going to scale out ollama to all of their user devices/locations, or just switch to some central GPU cluster?&lt;/p&gt;\\n\\n&lt;p&gt;Most have been doing the latter, I want to see if anyone is doing that ollama/llama.cpp scale out&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lp5obe","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp5obe/using_llamacpp_in_an_enterprise/n0vc41b/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751421453,"author_flair_text":null,"treatment_tags":[],"created_utc":1751421453,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0vb1sa","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"0xFatWhiteMan","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0v56wj","score":1,"author_fullname":"t2_pz2dkuedp","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"ollama or ramalama will work great on that","edited":false,"author_flair_css_class":null,"name":"t1_n0vb1sa","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;ollama or ramalama will work great on that&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lp5obe","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp5obe/using_llamacpp_in_an_enterprise/n0vb1sa/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751421085,"author_flair_text":null,"collapsed":false,"created_utc":1751421085,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0v56wj","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Careless-Car_","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0v404z","score":1,"author_fullname":"t2_2eugqr60","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"A Mac GPU, a 4070, any consumer GPU, etc.\\n\\nReally anything lower than a Nvidia L40s","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0v56wj","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;A Mac GPU, a 4070, any consumer GPU, etc.&lt;/p&gt;\\n\\n&lt;p&gt;Really anything lower than a Nvidia L40s&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lp5obe","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp5obe/using_llamacpp_in_an_enterprise/n0v56wj/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751419025,"author_flair_text":null,"treatment_tags":[],"created_utc":1751419025,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0v404z","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"0xFatWhiteMan","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0v2w3f","score":1,"author_fullname":"t2_pz2dkuedp","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Ok what specifically ?","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0v404z","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Ok what specifically ?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lp5obe","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp5obe/using_llamacpp_in_an_enterprise/n0v404z/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751418602,"author_flair_text":null,"treatment_tags":[],"created_utc":1751418602,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0v2w3f","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Careless-Car_","can_mod_post":false,"created_utc":1751418208,"send_replies":true,"parent_id":"t1_n0uw7dv","score":1,"author_fullname":"t2_2eugqr60","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Not low, just lower than what vLLM and others support the most","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0v2w3f","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Not low, just lower than what vLLM and others support the most&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lp5obe","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp5obe/using_llamacpp_in_an_enterprise/n0v2w3f/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751418208,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0uw7dv","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"0xFatWhiteMan","can_mod_post":false,"created_utc":1751415815,"send_replies":true,"parent_id":"t3_1lp5obe","score":1,"author_fullname":"t2_pz2dkuedp","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"you just can't use low end hardware. I tried, any model under about 6b is pretty dumb and unuseable imo. And anything bigger needs some decent metal","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0uw7dv","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;you just can&amp;#39;t use low end hardware. I tried, any model under about 6b is pretty dumb and unuseable imo. And anything bigger needs some decent metal&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp5obe/using_llamacpp_in_an_enterprise/n0uw7dv/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751415815,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lp5obe","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0te1ns","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"commanderthot","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0sofp4","score":2,"author_fullname":"t2_2zjjpdrm","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"If you’re using Mac’s, an alternative clustering framework is exo, which can scale up a lot depending on how many Mac’s you’re willing to invest in, as well as thunderbolt cables.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0te1ns","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;If you’re using Mac’s, an alternative clustering framework is exo, which can scale up a lot depending on how many Mac’s you’re willing to invest in, as well as thunderbolt cables.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lp5obe","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp5obe/using_llamacpp_in_an_enterprise/n0te1ns/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751398963,"author_flair_text":null,"treatment_tags":[],"created_utc":1751398963,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n0sofp4","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Careless-Car_","can_mod_post":false,"created_utc":1751391731,"send_replies":true,"parent_id":"t1_n0sd7je","score":1,"author_fullname":"t2_2eugqr60","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Exactly! But if a business hands out, let’s say M series Macs or some laptop with an integrated GPU as workstations, I can see a use case for having some IT team provide centralize llama.cpp packages and models for local use. \\n\\nThat’s just a theory, but I’d love to see any real implementation and/or trials","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0sofp4","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Exactly! But if a business hands out, let’s say M series Macs or some laptop with an integrated GPU as workstations, I can see a use case for having some IT team provide centralize llama.cpp packages and models for local use. &lt;/p&gt;\\n\\n&lt;p&gt;That’s just a theory, but I’d love to see any real implementation and/or trials&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lp5obe","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp5obe/using_llamacpp_in_an_enterprise/n0sofp4/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751391731,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0sd7je","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"LinkSea8324","can_mod_post":false,"created_utc":1751388703,"send_replies":true,"parent_id":"t3_1lp5obe","score":1,"author_fullname":"t2_152zyn72n4","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"llama.cpp has terrible performance drop when you got parallel users cf https://github.com/ggml-org/llama.cpp/issues/10860","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0sd7je","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;llama.cpp has terrible performance drop when you got parallel users cf &lt;a href=\\"https://github.com/ggml-org/llama.cpp/issues/10860\\"&gt;https://github.com/ggml-org/llama.cpp/issues/10860&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp5obe/using_llamacpp_in_an_enterprise/n0sd7je/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751388703,"author_flair_text":"llama.cpp","treatment_tags":[],"link_id":"t3_1lp5obe","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),o=()=>e.jsx(l,{data:a});export{o as default};
