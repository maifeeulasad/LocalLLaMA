import{j as e}from"./index-Bu7qcPAU.js";import{R as l}from"./RedditPostRenderer-CbHA7O5q.js";import"./index-BKgbfxhf.js";const t=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"As title states I tried to find the way to use Jan AI with ollama available local models but I didn't found the working way.\\n\\nAfter lot of trial and error I found working way forwared and document in a blog post\\n\\n[Jan.AI with Ollama (working solution)](https://developers.knowivate.com/@kheersagar/jan-ai-with-ollama-working-solution)\\n\\nEdit 1:\\n\\n&gt;Why would you use another API server in an API server? That's redundant.Â \\n\\nYes, it's redundant.\\n\\nBut in case of my senario\\n\\nI already have lot of downloaded local llms in my system via ollama.\\n\\nNow when I installed Jan AI then I saw I can either download llms from there application or I can connect with other local/online provider.\\n\\nBut for me it's really hard to download data from internet. Anything above 800MB is nightmare for me.\\n\\nI have already struggled to download llms by going 200\\\\~250km away from my village to city stay 2\\\\~3 days there and download the large models in my another system\\n\\nthen from another system move models to my main system then make it working.\\n\\nSo it's really costly for me to do it again to just use Jan AI.\\n\\nAlso I thought if there is other providers option exist in Jan AI then why not ollama.\\n\\nSo I tried to find working way and when checked there github issue there I found ollama is not supported because ollama doesn't have Open AI compatible api but ollama have.\\n\\nFor me hardware, compute etc doesn't matter in this senario but downloading the large file matters.\\n\\nWhenever I try to find any solution then I simply getÂ **Just download it from here**,Â **Just download this tool**,Â **just get this from hf**Â etc which I cannot\\n\\n&gt;Jan\\\\[.\\\\]ai consumes openai-compatible apis. Ollama has an openai-compatible api. What is the problem\\n\\nBut when you try to add ollama endpoint normally, then it doesn't work","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Jan.AI with Ollama (working solution)","link_flair_richtext":[{"e":"text","t":"Resources"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lsoflk","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.47,"author_flair_background_color":null,"subreddit_type":"public","ups":0,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_1b8utegv8t","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Resources","can_mod_post":false,"score":0,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":1751789621,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"post_hint":"self","content_categories":null,"is_self":true,"mod_note":null,"created":1751762120,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;As title states I tried to find the way to use Jan AI with ollama available local models but I didn&amp;#39;t found the working way.&lt;/p&gt;\\n\\n&lt;p&gt;After lot of trial and error I found working way forwared and document in a blog post&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://developers.knowivate.com/@kheersagar/jan-ai-with-ollama-working-solution\\"&gt;Jan.AI with Ollama (working solution)&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;Edit 1:&lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;Why would you use another API server in an API server? That&amp;#39;s redundant.Â &lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;Yes, it&amp;#39;s redundant.&lt;/p&gt;\\n\\n&lt;p&gt;But in case of my senario&lt;/p&gt;\\n\\n&lt;p&gt;I already have lot of downloaded local llms in my system via ollama.&lt;/p&gt;\\n\\n&lt;p&gt;Now when I installed Jan AI then I saw I can either download llms from there application or I can connect with other local/online provider.&lt;/p&gt;\\n\\n&lt;p&gt;But for me it&amp;#39;s really hard to download data from internet. Anything above 800MB is nightmare for me.&lt;/p&gt;\\n\\n&lt;p&gt;I have already struggled to download llms by going 200~250km away from my village to city stay 2~3 days there and download the large models in my another system&lt;/p&gt;\\n\\n&lt;p&gt;then from another system move models to my main system then make it working.&lt;/p&gt;\\n\\n&lt;p&gt;So it&amp;#39;s really costly for me to do it again to just use Jan AI.&lt;/p&gt;\\n\\n&lt;p&gt;Also I thought if there is other providers option exist in Jan AI then why not ollama.&lt;/p&gt;\\n\\n&lt;p&gt;So I tried to find working way and when checked there github issue there I found ollama is not supported because ollama doesn&amp;#39;t have Open AI compatible api but ollama have.&lt;/p&gt;\\n\\n&lt;p&gt;For me hardware, compute etc doesn&amp;#39;t matter in this senario but downloading the large file matters.&lt;/p&gt;\\n\\n&lt;p&gt;Whenever I try to find any solution then I simply getÂ &lt;strong&gt;Just download it from here&lt;/strong&gt;,Â &lt;strong&gt;Just download this tool&lt;/strong&gt;,Â &lt;strong&gt;just get this from hf&lt;/strong&gt;Â etc which I cannot&lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;Jan[.]ai consumes openai-compatible apis. Ollama has an openai-compatible api. What is the problem&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;But when you try to add ollama endpoint normally, then it doesn&amp;#39;t work&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":true,"is_crosspostable":false,"pinned":false,"over_18":false,"preview":{"images":[{"source":{"url":"https://external-preview.redd.it/91QifmmkYXf4kont-M3pku_SgKmkEq-VLJGmQTJzr_I.png?auto=webp&amp;s=afb36909de3fdf0851321ac502e2cce712b98c60","width":500,"height":500},"resolutions":[{"url":"https://external-preview.redd.it/91QifmmkYXf4kont-M3pku_SgKmkEq-VLJGmQTJzr_I.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=3c478b3adde7eeada968ed24f96e70509dcbda47","width":108,"height":108},{"url":"https://external-preview.redd.it/91QifmmkYXf4kont-M3pku_SgKmkEq-VLJGmQTJzr_I.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=41118f6fe36315cf0dbd19388a1a22c9d4d07793","width":216,"height":216},{"url":"https://external-preview.redd.it/91QifmmkYXf4kont-M3pku_SgKmkEq-VLJGmQTJzr_I.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=2d6edb2be32708b5dd197c38934d1d63c949e3a9","width":320,"height":320}],"variants":{},"id":"91QifmmkYXf4kont-M3pku_SgKmkEq-VLJGmQTJzr_I"}],"enabled":false},"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#ccac2b","id":"1lsoflk","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"InsideResolve4517","discussion_type":null,"num_comments":15,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lsoflk/janai_with_ollama_working_solution/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lsoflk/janai_with_ollama_working_solution/","subreddit_subscribers":495396,"created_utc":1751762120,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"fe89e94a-13f2-11f0-a9de-6262c74956cf","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"fe89e94a-13f2-11f0-a9de-6262c74956cf","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1katjb","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Asleep-Ratio7535","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1k8zkx","score":2,"author_fullname":"t2_1lfyddwf0c","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Thanks for explanation. I suddenly realize Jan is using gguf, while ollama's gguf isn't called gguf.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n1katjb","is_submitter":false,"collapsed":false,"author_flair_richtext":[{"e":"text","t":"Llama 4"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thanks for explanation. I suddenly realize Jan is using gguf, while ollama&amp;#39;s gguf isn&amp;#39;t called gguf.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lsoflk","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lsoflk/janai_with_ollama_working_solution/n1katjb/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751764629,"author_flair_text":"Llama 4","treatment_tags":[],"created_utc":1751764629,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":"#b0ae9b","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"fe89e94a-13f2-11f0-a9de-6262c74956cf","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1mezro","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Viktor_Cat_U","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1leaz0","score":1,"author_fullname":"t2_r77qw","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Oh I misunderstood it and thought the gguf problem was referring to jan.ai, I have also tried ollama through the transformer lab and indeed it has a problem with just loading gguf model I have already downloaded. I might just try out lm studio instead which seems to provide similar interface and functions","edited":1751803907,"author_flair_css_class":null,"name":"t1_n1mezro","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Oh I misunderstood it and thought the gguf problem was referring to jan.ai, I have also tried ollama through the transformer lab and indeed it has a problem with just loading gguf model I have already downloaded. I might just try out lm studio instead which seems to provide similar interface and functions&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lsoflk","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lsoflk/janai_with_ollama_working_solution/n1mezro/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751803699,"author_flair_text":null,"collapsed":false,"created_utc":1751803699,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1leaz0","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Asleep-Ratio7535","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1l3hw5","score":2,"author_fullname":"t2_1lfyddwf0c","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"oh, I used Jan, it uses normal gguf, his problem is from ollama which doesn't have .gguf I guess.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1leaz0","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"Llama 4"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;oh, I used Jan, it uses normal gguf, his problem is from ollama which doesn&amp;#39;t have .gguf I guess.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lsoflk","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lsoflk/janai_with_ollama_working_solution/n1leaz0/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751782834,"author_flair_text":"Llama 4","treatment_tags":[],"created_utc":1751782834,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":"#b0ae9b","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n1l3hw5","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Viktor_Cat_U","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1k8zkx","score":1,"author_fullname":"t2_r77qw","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Damn I was gonna try out jan.ai cuz of MCP I don't know if I still want to after reading that they dont play well with existing gguf files ðŸ¤”","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n1l3hw5","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Damn I was gonna try out jan.ai cuz of MCP I don&amp;#39;t know if I still want to after reading that they dont play well with existing gguf files ðŸ¤”&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lsoflk","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lsoflk/janai_with_ollama_working_solution/n1l3hw5/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751777082,"author_flair_text":null,"treatment_tags":[],"created_utc":1751777082,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1lo4oj","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"InsideResolve4517","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1k8zkx","score":1,"author_fullname":"t2_1b8utegv8t","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"&gt;The whole post is a low quality, self-promo for OP's blog. And I strongly wonder how literally the one and only correct way to do this, via the exposed OpenAI compatible API, was a trial-and-error, hard to accomplish feat for OP.\\n\\nI am really sorry if the post feels like low quality, self-promo. But my intention is to share the knowledge and things on which I already struggled.\\n\\nAnd in some senario it's really great way like in my case I cannot afford to download the llms I already have downloaded via ollama. And when I tried to find way I didn't found any, so it's like nightmare for me to download again &amp; after trial and error I found the working way. So I tried to share with everyone.\\n\\n&gt;But all that aside, Jan.ai is its own can of worms that doesn't work like all other inference engines. You can't just point it to your $HF\\\\_HOME or whatever folder full of gguf files. They have their own format, and set folder on C-disk user %appdata%. So if someone has 1 TB of model files already downloaded and they desperately wanted to use Jan.ai as a front end, I would absolutely rather use an external inference engine than try to manually migrate model files or re-download them to get them all into Jan.ai folder structure format.\\n\\nInteresting! If will fail to run using ollama then my next plan is to point out and try to make working, but thank you!\\n\\n&gt;Again, it still begs the question of why to do literally any of this, with Ollama or Jan .ai at all.\\n\\nI already have ollama &amp; it just works for me like my personal assistant, ollama exposes endpoint to use in another application, it's just like plug and play but don't have GUI except GUI. And jan ai I recently heard about it. I also heard about llmstudio but I haven't tried it yet","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n1lo4oj","is_submitter":true,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;The whole post is a low quality, self-promo for OP&amp;#39;s blog. And I strongly wonder how literally the one and only correct way to do this, via the exposed OpenAI compatible API, was a trial-and-error, hard to accomplish feat for OP.&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;I am really sorry if the post feels like low quality, self-promo. But my intention is to share the knowledge and things on which I already struggled.&lt;/p&gt;\\n\\n&lt;p&gt;And in some senario it&amp;#39;s really great way like in my case I cannot afford to download the llms I already have downloaded via ollama. And when I tried to find way I didn&amp;#39;t found any, so it&amp;#39;s like nightmare for me to download again &amp;amp; after trial and error I found the working way. So I tried to share with everyone.&lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;But all that aside, Jan.ai is its own can of worms that doesn&amp;#39;t work like all other inference engines. You can&amp;#39;t just point it to your $HF_HOME or whatever folder full of gguf files. They have their own format, and set folder on C-disk user %appdata%. So if someone has 1 TB of model files already downloaded and they desperately wanted to use Jan.ai as a front end, I would absolutely rather use an external inference engine than try to manually migrate model files or re-download them to get them all into Jan.ai folder structure format.&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;Interesting! If will fail to run using ollama then my next plan is to point out and try to make working, but thank you!&lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;Again, it still begs the question of why to do literally any of this, with Ollama or Jan .ai at all.&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;I already have ollama &amp;amp; it just works for me like my personal assistant, ollama exposes endpoint to use in another application, it&amp;#39;s just like plug and play but don&amp;#39;t have GUI except GUI. And jan ai I recently heard about it. I also heard about llmstudio but I haven&amp;#39;t tried it yet&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lsoflk","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lsoflk/janai_with_ollama_working_solution/n1lo4oj/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751788540,"author_flair_text":null,"treatment_tags":[],"created_utc":1751788540,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1loiku","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"InsideResolve4517","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1k8zkx","score":1,"author_fullname":"t2_1b8utegv8t","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"And in ollama gguf works\\n\\nI have one model from HF which works smoothly\\n\\nI have \`hf[.]co/QuantFactory/Qwen2.5-Coder-7B-Instruct-GGUF:latest\`\\n\\nbut I think not all hf models can to runned (I tried 2\\\\~3 months ago)\\n\\nwhen I tried Qwen2.5-Coder-7B directly then It didn't worked but from \`QuantFactory\` it worked","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n1loiku","is_submitter":true,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;And in ollama gguf works&lt;/p&gt;\\n\\n&lt;p&gt;I have one model from HF which works smoothly&lt;/p&gt;\\n\\n&lt;p&gt;I have &lt;code&gt;hf[.]co/QuantFactory/Qwen2.5-Coder-7B-Instruct-GGUF:latest&lt;/code&gt;&lt;/p&gt;\\n\\n&lt;p&gt;but I think not all hf models can to runned (I tried 2~3 months ago)&lt;/p&gt;\\n\\n&lt;p&gt;when I tried Qwen2.5-Coder-7B directly then It didn&amp;#39;t worked but from &lt;code&gt;QuantFactory&lt;/code&gt; it worked&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lsoflk","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lsoflk/janai_with_ollama_working_solution/n1loiku/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751788765,"author_flair_text":null,"treatment_tags":[],"created_utc":1751788765,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1k8zkx","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Marksta","can_mod_post":false,"created_utc":1751763891,"send_replies":true,"parent_id":"t1_n1k6tyk","score":12,"author_fullname":"t2_559a1","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"The whole post is a low quality, self-promo for OP's blog. And I strongly wonder how literally the one and only correct way to do this, via the exposed OpenAI compatible API, was a trial-and-error, hard to accomplish feat for OP.\\n\\nBut all that aside, Jan.ai is its own can of worms that doesn't work like all other inference engines. You can't just point it to your $HF_HOME or whatever folder full of gguf files. They have their own format, and set folder on C-disk user %appdata%. So if someone has 1 TB of model files already downloaded and they desperately wanted to use Jan.ai as a front end, I would absolutely rather use an external inference engine than try to manually migrate model files or re-download them to get them all into Jan.ai folder structure format.\\n\\nAgain, it still begs the question of why to do literally any of this, with Ollama or Jan.ai at all.","edited":1751764072,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1k8zkx","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The whole post is a low quality, self-promo for OP&amp;#39;s blog. And I strongly wonder how literally the one and only correct way to do this, via the exposed OpenAI compatible API, was a trial-and-error, hard to accomplish feat for OP.&lt;/p&gt;\\n\\n&lt;p&gt;But all that aside, Jan.ai is its own can of worms that doesn&amp;#39;t work like all other inference engines. You can&amp;#39;t just point it to your $HF_HOME or whatever folder full of gguf files. They have their own format, and set folder on C-disk user %appdata%. So if someone has 1 TB of model files already downloaded and they desperately wanted to use Jan.ai as a front end, I would absolutely rather use an external inference engine than try to manually migrate model files or re-download them to get them all into Jan.ai folder structure format.&lt;/p&gt;\\n\\n&lt;p&gt;Again, it still begs the question of why to do literally any of this, with Ollama or Jan.ai at all.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lsoflk","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lsoflk/janai_with_ollama_working_solution/n1k8zkx/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751763891,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":12}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1le0iz","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"defmans7","can_mod_post":false,"created_utc":1751782669,"send_replies":true,"parent_id":"t1_n1k6tyk","score":3,"author_fullname":"t2_mh59k","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I use Jan for the common interface, I run ollama AND a llama.cpp setup on a separate machine. Ability to swap different models for different tasks, and have a common API setup is good. \\n\\nI don't have to worry about models taking up hard drive space on my main work machine. \\n\\nI used to run Jan server when I was starting out with local models, but quickly found that it's not as flexible or customisable as llama.cpp with swap, or just ollama itself.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1le0iz","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I use Jan for the common interface, I run ollama AND a llama.cpp setup on a separate machine. Ability to swap different models for different tasks, and have a common API setup is good. &lt;/p&gt;\\n\\n&lt;p&gt;I don&amp;#39;t have to worry about models taking up hard drive space on my main work machine. &lt;/p&gt;\\n\\n&lt;p&gt;I used to run Jan server when I was starting out with local models, but quickly found that it&amp;#39;s not as flexible or customisable as llama.cpp with swap, or just ollama itself.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lsoflk","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lsoflk/janai_with_ollama_working_solution/n1le0iz/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751782669,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1lmftj","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"InsideResolve4517","can_mod_post":false,"created_utc":1751787555,"send_replies":true,"parent_id":"t1_n1k6tyk","score":2,"author_fullname":"t2_1b8utegv8t","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yes, it's redundant.\\n\\nBut in case of my senario\\n\\nI already have lot of downloaded local llms in my system via ollama.\\n\\nNow when I installed Jan AI then I saw I can either download llms from there application or I can connect with other local/online provider.\\n\\nBut for me it's really hard to download data from internet. Anything above 800MB is nightmare for me.\\n\\nI have already struggled to download llms by going 200\\\\~250km away from my village to city stay 2\\\\~3 days there and download the large models in my another system\\n\\nthen from another system move models to my main system then make it working.\\n\\nSo it's really costly for me to do it again to just use Jan AI.\\n\\nAlso I thought if there is other providers option exist in Jan AI then why not ollama.\\n\\nSo I tried to find working way and when checked there github issue there I found ollama is not supported because ollama doesn't have Open AI compatible api but ollama have.\\n\\n  \\nFor me hardware, compute etc doesn't matter in this senario but downloading the large file matters.\\n\\n  \\nWhenever I try to find any solution then I simply get **Just download it from here**, **Just download this tool**, **just get this from hf** etc which I cannot","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1lmftj","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yes, it&amp;#39;s redundant.&lt;/p&gt;\\n\\n&lt;p&gt;But in case of my senario&lt;/p&gt;\\n\\n&lt;p&gt;I already have lot of downloaded local llms in my system via ollama.&lt;/p&gt;\\n\\n&lt;p&gt;Now when I installed Jan AI then I saw I can either download llms from there application or I can connect with other local/online provider.&lt;/p&gt;\\n\\n&lt;p&gt;But for me it&amp;#39;s really hard to download data from internet. Anything above 800MB is nightmare for me.&lt;/p&gt;\\n\\n&lt;p&gt;I have already struggled to download llms by going 200~250km away from my village to city stay 2~3 days there and download the large models in my another system&lt;/p&gt;\\n\\n&lt;p&gt;then from another system move models to my main system then make it working.&lt;/p&gt;\\n\\n&lt;p&gt;So it&amp;#39;s really costly for me to do it again to just use Jan AI.&lt;/p&gt;\\n\\n&lt;p&gt;Also I thought if there is other providers option exist in Jan AI then why not ollama.&lt;/p&gt;\\n\\n&lt;p&gt;So I tried to find working way and when checked there github issue there I found ollama is not supported because ollama doesn&amp;#39;t have Open AI compatible api but ollama have.&lt;/p&gt;\\n\\n&lt;p&gt;For me hardware, compute etc doesn&amp;#39;t matter in this senario but downloading the large file matters.&lt;/p&gt;\\n\\n&lt;p&gt;Whenever I try to find any solution then I simply get &lt;strong&gt;Just download it from here&lt;/strong&gt;, &lt;strong&gt;Just download this tool&lt;/strong&gt;, &lt;strong&gt;just get this from hf&lt;/strong&gt; etc which I cannot&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lsoflk","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lsoflk/janai_with_ollama_working_solution/n1lmftj/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751787555,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n1k6tyk","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Asleep-Ratio7535","can_mod_post":false,"created_utc":1751763020,"send_replies":true,"parent_id":"t3_1lsoflk","score":12,"author_fullname":"t2_1lfyddwf0c","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Why would you use another API server in an API server? That's redundant.Â ","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1k6tyk","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"Llama 4"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Why would you use another API server in an API server? That&amp;#39;s redundant.Â &lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lsoflk/janai_with_ollama_working_solution/n1k6tyk/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751763020,"author_flair_text":"Llama 4","treatment_tags":[],"link_id":"t3_1lsoflk","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#b0ae9b","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":12}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1lpt15","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"InsideResolve4517","can_mod_post":false,"created_utc":1751789519,"send_replies":true,"parent_id":"t1_n1lfbat","score":1,"author_fullname":"t2_1b8utegv8t","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"yes!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1lpt15","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;yes!&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lsoflk","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lsoflk/janai_with_ollama_working_solution/n1lpt15/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751789519,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1lfbat","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"lothariusdark","can_mod_post":false,"created_utc":1751783408,"send_replies":true,"parent_id":"t3_1lsoflk","score":4,"author_fullname":"t2_idhb522c","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Thats all he wants to say in his blog:\\n\\n&gt;  \\nGo toÂ **Settings &gt; Model Providers**\\n\\n&gt;ClickÂ **OpenAI**\\n\\n&gt;InÂ **API Key**Â field addÂ \`ollama\`Â as api key &amp; in Base URL enterÂ [\`http://localhost:11434/v1\`](http://localhost:11434/v1)Â equivalent endpoint.\\n\\n&gt;InÂ **Models**Â list you can see many already available models of OpenAI you can keep it or delete it (but it's optional)\\n\\n&gt;InÂ **Models**Â there is and option to add new model (there is plus icon), so click on it and add you local ollama model name in my case it wasÂ \`qwen2.5-coder:14b\`Â (to see available models with exact name runÂ \`ollama list\`Â command in terminal)\\n\\n&gt;Save the model (optional: After saving you will see your model inÂ **Models**Â list you can click edit and enable tool calling if your model support it and you want it)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1lfbat","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thats all he wants to say in his blog:&lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;Go toÂ &lt;strong&gt;Settings &amp;gt; Model Providers&lt;/strong&gt;&lt;/p&gt;\\n\\n&lt;p&gt;ClickÂ &lt;strong&gt;OpenAI&lt;/strong&gt;&lt;/p&gt;\\n\\n&lt;p&gt;InÂ &lt;strong&gt;API Key&lt;/strong&gt;Â field addÂ &lt;code&gt;ollama&lt;/code&gt;Â as api key &amp;amp; in Base URL enterÂ &lt;a href=\\"http://localhost:11434/v1\\"&gt;&lt;code&gt;http://localhost:11434/v1&lt;/code&gt;&lt;/a&gt;Â equivalent endpoint.&lt;/p&gt;\\n\\n&lt;p&gt;InÂ &lt;strong&gt;Models&lt;/strong&gt;Â list you can see many already available models of OpenAI you can keep it or delete it (but it&amp;#39;s optional)&lt;/p&gt;\\n\\n&lt;p&gt;InÂ &lt;strong&gt;Models&lt;/strong&gt;Â there is and option to add new model (there is plus icon), so click on it and add you local ollama model name in my case it wasÂ &lt;code&gt;qwen2.5-coder:14b&lt;/code&gt;Â (to see available models with exact name runÂ &lt;code&gt;ollama list&lt;/code&gt;Â command in terminal)&lt;/p&gt;\\n\\n&lt;p&gt;Save the model (optional: After saving you will see your model inÂ &lt;strong&gt;Models&lt;/strong&gt;Â list you can click edit and enable tool calling if your model support it and you want it)&lt;/p&gt;\\n&lt;/blockquote&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lsoflk/janai_with_ollama_working_solution/n1lfbat/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751783408,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lsoflk","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1kej3o","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"webitube","can_mod_post":false,"created_utc":1751766147,"send_replies":true,"parent_id":"t3_1lsoflk","score":2,"author_fullname":"t2_71eku","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"It looks interesting enough, but I think I'll wait till you get near the end of your current roadmap.  \\nThere isn't enough that is working yet to be really useful right now. (Maybe Q4 2025?)  \\nAlso, it would be nice to see an option to make it easy to backup and restore the databases.\\n\\nI'll stick with OpenWebUI for now.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1kej3o","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It looks interesting enough, but I think I&amp;#39;ll wait till you get near the end of your current roadmap.&lt;br/&gt;\\nThere isn&amp;#39;t enough that is working yet to be really useful right now. (Maybe Q4 2025?)&lt;br/&gt;\\nAlso, it would be nice to see an option to make it easy to backup and restore the databases.&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;ll stick with OpenWebUI for now.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lsoflk/janai_with_ollama_working_solution/n1kej3o/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751766147,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lsoflk","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1lpvim","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"InsideResolve4517","can_mod_post":false,"created_utc":1751789561,"send_replies":true,"parent_id":"t1_n1kkhjy","score":1,"author_fullname":"t2_1b8utegv8t","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"But when you try to add ollama endpoint normally, then it doesn't work","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1lpvim","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;But when you try to add ollama endpoint normally, then it doesn&amp;#39;t work&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lsoflk","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lsoflk/janai_with_ollama_working_solution/n1lpvim/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751789561,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1kkhjy","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"emprahsFury","can_mod_post":false,"created_utc":1751768586,"send_replies":false,"parent_id":"t3_1lsoflk","score":2,"author_fullname":"t2_177r8n","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Jan.ai consumes openai-compatible apis. Ollama has an openai-compatible api. What is the problem","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1kkhjy","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Jan.ai consumes openai-compatible apis. Ollama has an openai-compatible api. What is the problem&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lsoflk/janai_with_ollama_working_solution/n1kkhjy/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751768586,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lsoflk","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}}]`),r=()=>e.jsx(l,{data:t});export{r as default};
