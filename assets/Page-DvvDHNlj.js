import{j as e}from"./index-BxgxThME.js";import{R as t}from"./RedditPostRenderer-BL_SOtuv.js";import"./index--Az3yIKM.js";const n=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Basically it's a step above 'prompt engineering ' \\n\\nThe prompt is for the moment, the specific input. \\n\\n'Context engineering' is setting up for the moment. \\n\\nThink about it as building a movie - the background, the details etc. That would be the context framing. The prompt would be when the actors come in and say their one line. \\n\\nSame thing for context engineering. You're building the set for the LLM to come in and say they're one line. \\n\\nThis is a lot more detailed way of framing the LLM over saying \\"Act as a Meta Prompt Master and develop a badass prompt....\\" \\n\\nYou have to understand Linguistics Programming (I wrote an article on it, link in bio)\\n\\nSince English is the new coding language, users have to understand Linguistics a little more than the average bear. \\n\\nThe Linguistics Compression is the important aspect of this \\"Context Engineering\\" to save tokens so your context frame doesn't fill up the entire context window. \\n\\nIf you do not use your word choices correctly, you can easily fill up a context window and not get the results you're looking for. Linguistics compression reduces the amount of tokens while maintaining maximum information Density. \\n\\nAnd that's why I say it's a step above prompt engineering. I create digital notebooks for my prompts. Now I have a name for them - Context Engineering Notebooks... \\n\\nAs an example, I have a digital writing notebook that has seven or eight tabs, and 20 pages in a Google document. Most of the pages are samples of my writing, I have a tab dedicated to resources, best practices, etc. this writing notebook serve as a context notebook for the LLM in terms of producing an output similar to my writing style. So I've created an environment a resources for the llm to pull from. The result is an output that's probably 80% my style, my tone, my specific word choices, etc.\\n\\n","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"What Is Context Engineering? My Thoughts..","link_flair_richtext":[{"e":"text","t":"Discussion"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lnsqkl","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.25,"author_flair_background_color":null,"subreddit_type":"public","ups":0,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_1o7sgurmvt","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Discussion","can_mod_post":false,"score":0,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1751239544,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Basically it&amp;#39;s a step above &amp;#39;prompt engineering &amp;#39; &lt;/p&gt;\\n\\n&lt;p&gt;The prompt is for the moment, the specific input. &lt;/p&gt;\\n\\n&lt;p&gt;&amp;#39;Context engineering&amp;#39; is setting up for the moment. &lt;/p&gt;\\n\\n&lt;p&gt;Think about it as building a movie - the background, the details etc. That would be the context framing. The prompt would be when the actors come in and say their one line. &lt;/p&gt;\\n\\n&lt;p&gt;Same thing for context engineering. You&amp;#39;re building the set for the LLM to come in and say they&amp;#39;re one line. &lt;/p&gt;\\n\\n&lt;p&gt;This is a lot more detailed way of framing the LLM over saying &amp;quot;Act as a Meta Prompt Master and develop a badass prompt....&amp;quot; &lt;/p&gt;\\n\\n&lt;p&gt;You have to understand Linguistics Programming (I wrote an article on it, link in bio)&lt;/p&gt;\\n\\n&lt;p&gt;Since English is the new coding language, users have to understand Linguistics a little more than the average bear. &lt;/p&gt;\\n\\n&lt;p&gt;The Linguistics Compression is the important aspect of this &amp;quot;Context Engineering&amp;quot; to save tokens so your context frame doesn&amp;#39;t fill up the entire context window. &lt;/p&gt;\\n\\n&lt;p&gt;If you do not use your word choices correctly, you can easily fill up a context window and not get the results you&amp;#39;re looking for. Linguistics compression reduces the amount of tokens while maintaining maximum information Density. &lt;/p&gt;\\n\\n&lt;p&gt;And that&amp;#39;s why I say it&amp;#39;s a step above prompt engineering. I create digital notebooks for my prompts. Now I have a name for them - Context Engineering Notebooks... &lt;/p&gt;\\n\\n&lt;p&gt;As an example, I have a digital writing notebook that has seven or eight tabs, and 20 pages in a Google document. Most of the pages are samples of my writing, I have a tab dedicated to resources, best practices, etc. this writing notebook serve as a context notebook for the LLM in terms of producing an output similar to my writing style. So I&amp;#39;ve created an environment a resources for the llm to pull from. The result is an output that&amp;#39;s probably 80% my style, my tone, my specific word choices, etc.&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":true,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"5f921ea4-c7bc-11ed-9c23-3a00622979b4","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#646d73","id":"1lnsqkl","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"Lumpy-Ad-173","discussion_type":null,"num_comments":3,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lnsqkl/what_is_context_engineering_my_thoughts/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lnsqkl/what_is_context_engineering_my_thoughts/","subreddit_subscribers":492929,"created_utc":1751239544,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0i26os","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"codeprimate","can_mod_post":false,"created_utc":1751245255,"send_replies":true,"parent_id":"t3_1lnsqkl","score":1,"author_fullname":"t2_3cev3","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I will often ask the LLM to “Rewrite using maximum semantic density while prioritizing full informational retention. Prefer clear and straightforward statements and simple conditional clauses.”\\n\\nThis will reduce token count","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0i26os","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I will often ask the LLM to “Rewrite using maximum semantic density while prioritizing full informational retention. Prefer clear and straightforward statements and simple conditional clauses.”&lt;/p&gt;\\n\\n&lt;p&gt;This will reduce token count&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsqkl/what_is_context_engineering_my_thoughts/n0i26os/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751245255,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnsqkl","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0ilw24","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"klam997","can_mod_post":false,"created_utc":1751252880,"send_replies":true,"parent_id":"t3_1lnsqkl","score":1,"author_fullname":"t2_fpop6","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"So it's still prompt engineering except you instruct them to use semantic shorthands along with unrestricted use of symbols and notations to ensure intent and messages are passed on. There's already papers on this.\\n\\nThis is nothing new. You just reinvented the wheel and slap on your brand.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0ilw24","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;So it&amp;#39;s still prompt engineering except you instruct them to use semantic shorthands along with unrestricted use of symbols and notations to ensure intent and messages are passed on. There&amp;#39;s already papers on this.&lt;/p&gt;\\n\\n&lt;p&gt;This is nothing new. You just reinvented the wheel and slap on your brand.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsqkl/what_is_context_engineering_my_thoughts/n0ilw24/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751252880,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnsqkl","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0jpb9s","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"kantydir","can_mod_post":false,"created_utc":1751273780,"send_replies":true,"parent_id":"t3_1lnsqkl","score":1,"author_fullname":"t2_rf2xb","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Context engineering is about providing the LLM with all the info it needs to answer the query accurately. The difference with plain prompt engineering is the tools you  use to \\"supercharge\\" the prompt with the relevant info in a efficient way.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0jpb9s","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Context engineering is about providing the LLM with all the info it needs to answer the query accurately. The difference with plain prompt engineering is the tools you  use to &amp;quot;supercharge&amp;quot; the prompt with the relevant info in a efficient way.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnsqkl/what_is_context_engineering_my_thoughts/n0jpb9s/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751273780,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnsqkl","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),i=()=>e.jsx(t,{data:n});export{i as default};
