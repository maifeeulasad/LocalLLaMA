import{j as e}from"./index-BpC9hjVs.js";import{R as l}from"./RedditPostRenderer-BEo6AnSR.js";import"./index-DwkJHX1_.js";const t=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"I want to use flagship models for coding, without worrying that some personal/business specific data leaks to cloud. Was thinking maybe there is a solution that would do something like this:\\n\\nlocal model:\\n\\n* detects personal or business specific data in prompts,\\n* creates mapping dictionary\\n* warns if replace is not feasible\\n\\nproxy app:\\n\\n* executes string replace according to rules in dictionary\\n* routes requests to cloud LLM api\\n* passes LLM warnings to user\\n\\nEDIT: The solution should serve OpenAI compatible API, replacing data and routing requests to cloud behind the scenes.","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Using local LLM for anonymizing prompts before sending to cloud LLM - are there any open source solutions?","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lsv7j1","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.67,"author_flair_background_color":null,"subreddit_type":"public","ups":4,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_1maalib5ze","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":4,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":1751799203,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1751786038,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;I want to use flagship models for coding, without worrying that some personal/business specific data leaks to cloud. Was thinking maybe there is a solution that would do something like this:&lt;/p&gt;\\n\\n&lt;p&gt;local model:&lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;detects personal or business specific data in prompts,&lt;/li&gt;\\n&lt;li&gt;creates mapping dictionary&lt;/li&gt;\\n&lt;li&gt;warns if replace is not feasible&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;p&gt;proxy app:&lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;executes string replace according to rules in dictionary&lt;/li&gt;\\n&lt;li&gt;routes requests to cloud LLM api&lt;/li&gt;\\n&lt;li&gt;passes LLM warnings to user&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;p&gt;EDIT: The solution should serve OpenAI compatible API, replacing data and routing requests to cloud behind the scenes.&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1lsv7j1","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"cesarean722","discussion_type":null,"num_comments":7,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lsv7j1/using_local_llm_for_anonymizing_prompts_before/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lsv7j1/using_local_llm_for_anonymizing_prompts_before/","subreddit_subscribers":495651,"created_utc":1751786038,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1mfzw6","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Decaf_GT","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1m556n","score":2,"author_fullname":"t2_r98e1eogc","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I'm not actually sure about this. Ironically though, it's probably something that'd be really great to ask any of the big LLMs! They might be able to help provide more contextually relevant guidance on if it's possible and how to do it. It really seems like it should be.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n1mfzw6","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;m not actually sure about this. Ironically though, it&amp;#39;s probably something that&amp;#39;d be really great to ask any of the big LLMs! They might be able to help provide more contextually relevant guidance on if it&amp;#39;s possible and how to do it. It really seems like it should be.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lsv7j1","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lsv7j1/using_local_llm_for_anonymizing_prompts_before/n1mfzw6/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751804145,"author_flair_text":null,"treatment_tags":[],"created_utc":1751804145,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n1m556n","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"cesarean722","can_mod_post":false,"created_utc":1751798773,"send_replies":true,"parent_id":"t1_n1lywz6","score":1,"author_fullname":"t2_1maalib5ze","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I don't have experience with N8N. Is it doable, for example when I want to use Roo code agent, I use local OpenAI compatible api, and the local LLM can be configured to use N8N workflows behind the scenes?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1m556n","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I don&amp;#39;t have experience with N8N. Is it doable, for example when I want to use Roo code agent, I use local OpenAI compatible api, and the local LLM can be configured to use N8N workflows behind the scenes?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lsv7j1","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lsv7j1/using_local_llm_for_anonymizing_prompts_before/n1m556n/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751798773,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1lywz6","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Decaf_GT","can_mod_post":false,"created_utc":1751795103,"send_replies":true,"parent_id":"t3_1lsv7j1","score":3,"author_fullname":"t2_r98e1eogc","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I mean, isn't this more or less what an agentic workflow would be for? Like N8N? You should be able to control it pretty finely with that.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1lywz6","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I mean, isn&amp;#39;t this more or less what an agentic workflow would be for? Like N8N? You should be able to control it pretty finely with that.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lsv7j1/using_local_llm_for_anonymizing_prompts_before/n1lywz6/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751795103,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lsv7j1","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1om3ka","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Square-Onion-1825","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1o0bmt","score":3,"author_fullname":"t2_1mkh7x2yxn","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I'm sorry, I won't be able to because it was developed for a client and they own the rights to it now because of the contract agreement. But its not hard to ask claude to create this for you. Claude is really good at creating python code.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n1om3ka","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;m sorry, I won&amp;#39;t be able to because it was developed for a client and they own the rights to it now because of the contract agreement. But its not hard to ask claude to create this for you. Claude is really good at creating python code.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lsv7j1","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lsv7j1/using_local_llm_for_anonymizing_prompts_before/n1om3ka/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751829450,"author_flair_text":null,"treatment_tags":[],"created_utc":1751829450,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n1o0bmt","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"bhupesh-g","can_mod_post":false,"created_utc":1751822824,"send_replies":true,"parent_id":"t1_n1m07ci","score":1,"author_fullname":"t2_182usrqcei","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"hey, if you dont mind, can you share that code?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1o0bmt","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;hey, if you dont mind, can you share that code?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lsv7j1","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lsv7j1/using_local_llm_for_anonymizing_prompts_before/n1o0bmt/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751822824,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1m07ci","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Square-Onion-1825","can_mod_post":false,"created_utc":1751795869,"send_replies":true,"parent_id":"t3_1lsv7j1","score":3,"author_fullname":"t2_1mkh7x2yxn","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I had claude create python scripts to anonymize my data before feeding it to the llm. i have a mapping file so it can decrypt the resultant report the LLM creates. However, anonymization is not fool proof. you can test it out by asking an LLM what company the data belongs to when you give the anonymized data set to it. It will most likely guess who the company is that the data belongs to.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1m07ci","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I had claude create python scripts to anonymize my data before feeding it to the llm. i have a mapping file so it can decrypt the resultant report the LLM creates. However, anonymization is not fool proof. you can test it out by asking an LLM what company the data belongs to when you give the anonymized data set to it. It will most likely guess who the company is that the data belongs to.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lsv7j1/using_local_llm_for_anonymizing_prompts_before/n1m07ci/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751795869,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lsv7j1","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}}]`),r=()=>e.jsx(l,{data:t});export{r as default};
