import{j as e}from"./index-BxgxThME.js";import{R as a}from"./RedditPostRenderer-BL_SOtuv.js";import"./index--Az3yIKM.js";const l=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Abstract:\\n\\nReasoning, the process of devising and executing complex goal-oriented action sequences, remains a critical challenge in AI. Current large language models (LLMs) primarily employ Chain-of-Thought (CoT) techniques, which suffer from brittle task decomposition, extensive data requirements, and high latency. Inspired by the hierarchical and multi-timescale processing in the human brain, we propose the Hierarchical Reasoning Model (HRM), a novel recurrent architecture that attains significant computational depth while maintaining both training stability and efficiency. HRM executes sequential reasoning tasks in a single forward pass without explicit supervision of the intermediate process, through two interdependent recurrent modules: a high-level module responsible for slow, abstract planning, and a low-level module handling rapid, detailed computations. With only 27 million parameters, HRM achieves exceptional performance on complex reasoning tasks using only 1000 training samples. The model operates without pre-training or CoT data, yet achieves nearly perfect performance on challenging tasks including complex Sudoku puzzles and optimal path finding in large mazes. Furthermore, HRM outperforms much larger models with significantly longer context windows on the Abstraction and Reasoning Corpus (ARC), a key benchmark for measuring artificial general intelligence capabilities. These results underscore HRM's potential as a transformative advancement toward universal computation and general-purpose reasoning systems.","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"[2506.21734] Hierarchical Reasoning Model","link_flair_richtext":[{"e":"text","t":"Discussion"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lo84yj","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.88,"author_flair_background_color":null,"subreddit_type":"public","ups":25,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_1pr7hwh6t5","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Discussion","can_mod_post":false,"score":25,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"default","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":false,"mod_note":null,"created":1751291680,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"arxiv.org","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Abstract:&lt;/p&gt;\\n\\n&lt;p&gt;Reasoning, the process of devising and executing complex goal-oriented action sequences, remains a critical challenge in AI. Current large language models (LLMs) primarily employ Chain-of-Thought (CoT) techniques, which suffer from brittle task decomposition, extensive data requirements, and high latency. Inspired by the hierarchical and multi-timescale processing in the human brain, we propose the Hierarchical Reasoning Model (HRM), a novel recurrent architecture that attains significant computational depth while maintaining both training stability and efficiency. HRM executes sequential reasoning tasks in a single forward pass without explicit supervision of the intermediate process, through two interdependent recurrent modules: a high-level module responsible for slow, abstract planning, and a low-level module handling rapid, detailed computations. With only 27 million parameters, HRM achieves exceptional performance on complex reasoning tasks using only 1000 training samples. The model operates without pre-training or CoT data, yet achieves nearly perfect performance on challenging tasks including complex Sudoku puzzles and optimal path finding in large mazes. Furthermore, HRM outperforms much larger models with significantly longer context windows on the Abstraction and Reasoning Corpus (ARC), a key benchmark for measuring artificial general intelligence capabilities. These results underscore HRM&amp;#39;s potential as a transformative advancement toward universal computation and general-purpose reasoning systems.&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"url_overridden_by_dest":"https://arxiv.org/abs/2506.21734","view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"5f921ea4-c7bc-11ed-9c23-3a00622979b4","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#646d73","id":"1lo84yj","is_robot_indexable":true,"num_duplicates":1,"report_reasons":null,"author":"absolooot1","discussion_type":null,"num_comments":10,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lo84yj/250621734_hierarchical_reasoning_model/","stickied":false,"url":"https://arxiv.org/abs/2506.21734","subreddit_subscribers":493243,"created_utc":1751291680,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"d2642412-d9ce-11ed-ae30-32b11309f5bd","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0ljni7","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"LagOps91","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0ld1c7","score":3,"author_fullname":"t2_3wi6j7vwh","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"well yes, there have been plenty of those. but the question is if any of it actually scales.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0ljni7","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;well yes, there have been plenty of those. but the question is if any of it actually scales.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lo84yj","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lo84yj/250621734_hierarchical_reasoning_model/n0ljni7/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751299564,"author_flair_text":null,"treatment_tags":[],"created_utc":1751299564,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n0ld1c7","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Everlier","can_mod_post":false,"created_utc":1751297655,"send_replies":true,"parent_id":"t1_n0ku721","score":2,"author_fullname":"t2_o7p5m","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"That's a PoC for long-term horizon planning, applying LLMs is yet to happen","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0ld1c7","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"Alpaca"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;That&amp;#39;s a PoC for long-term horizon planning, applying LLMs is yet to happen&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lo84yj","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lo84yj/250621734_hierarchical_reasoning_model/n0ld1c7/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751297655,"author_flair_text":"Alpaca","treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":"#bd9e9e","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n0ku721","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"LagOps91","can_mod_post":false,"created_utc":1751292021,"send_replies":true,"parent_id":"t3_1lo84yj","score":7,"author_fullname":"t2_3wi6j7vwh","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"\\"27 million parameters\\" ... you mean billions, right?\\n\\nwith such a tiny model it doesn't really show that any of it can scale. not doing any pre-training and only training on 1000 samples is quite sus as well.\\n\\nthat seems to be significantly too little to learn about language, let alone to allow the model to generalize to any meaningful degree.\\n\\ni'll give the paper a read, but this abstract leaves me extremely sceptical.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0ku721","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;&amp;quot;27 million parameters&amp;quot; ... you mean billions, right?&lt;/p&gt;\\n\\n&lt;p&gt;with such a tiny model it doesn&amp;#39;t really show that any of it can scale. not doing any pre-training and only training on 1000 samples is quite sus as well.&lt;/p&gt;\\n\\n&lt;p&gt;that seems to be significantly too little to learn about language, let alone to allow the model to generalize to any meaningful degree.&lt;/p&gt;\\n\\n&lt;p&gt;i&amp;#39;ll give the paper a read, but this abstract leaves me extremely sceptical.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lo84yj/250621734_hierarchical_reasoning_model/n0ku721/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751292021,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lo84yj","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":7}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0l6fx5","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"LagOps91","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0l4p5b","score":4,"author_fullname":"t2_3wi6j7vwh","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"yeah but still, it's a highly task-specialized model (which doesn't need to be large since it's not a general model!). i think they would need to make at least a small language model (0.5b or something) and compare it with transformer models of the same size.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0l6fx5","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;yeah but still, it&amp;#39;s a highly task-specialized model (which doesn&amp;#39;t need to be large since it&amp;#39;s not a general model!). i think they would need to make at least a small language model (0.5b or something) and compare it with transformer models of the same size.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lo84yj","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lo84yj/250621734_hierarchical_reasoning_model/n0l6fx5/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751295731,"author_flair_text":null,"treatment_tags":[],"created_utc":1751295731,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}}],"before":null}},"user_reports":[],"saved":false,"id":"n0l4p5b","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Lazy-Pattern-5171","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0kw1sv","score":1,"author_fullname":"t2_1lyjk8is25","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"This is what I was wondering as well. However they did mention that for a more complete test set they created transformations of the original sudoku dataset samples by randomizing, coloring, etc to make a novel dataset with similar data that they used for training and their Sudoku experiment results are from this set it seems.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0l4p5b","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;This is what I was wondering as well. However they did mention that for a more complete test set they created transformations of the original sudoku dataset samples by randomizing, coloring, etc to make a novel dataset with similar data that they used for training and their Sudoku experiment results are from this set it seems.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lo84yj","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lo84yj/250621734_hierarchical_reasoning_model/n0l4p5b/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751295221,"author_flair_text":null,"treatment_tags":[],"created_utc":1751295221,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0kw1sv","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"LagOps91","can_mod_post":false,"created_utc":1751292605,"send_replies":true,"parent_id":"t1_n0ktn83","score":10,"author_fullname":"t2_3wi6j7vwh","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"well... they do state that they train the model on the example data only. so it's not even really a language model or anything, but a task-specific (\\"narrow\\") AI model.\\n\\n\\"In the Abstraction and Reasoning Corpus (ARC) AGI Challenge 27,28,29 - a benchmark of inductive reasoning - HRM, trained from scratch with only the official dataset (\\\\~1000 examples), with only 27M parameters and a 30x30 grid context (900 tokens), achieves a performance of 40.3%, which substantially surpasses leading CoT-based models like o3-mini-high (34.5%) and Claude 3.7 8K context (21.2%)\\"","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0kw1sv","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;well... they do state that they train the model on the example data only. so it&amp;#39;s not even really a language model or anything, but a task-specific (&amp;quot;narrow&amp;quot;) AI model.&lt;/p&gt;\\n\\n&lt;p&gt;&amp;quot;In the Abstraction and Reasoning Corpus (ARC) AGI Challenge 27,28,29 - a benchmark of inductive reasoning - HRM, trained from scratch with only the official dataset (~1000 examples), with only 27M parameters and a 30x30 grid context (900 tokens), achieves a performance of 40.3%, which substantially surpasses leading CoT-based models like o3-mini-high (34.5%) and Claude 3.7 8K context (21.2%)&amp;quot;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lo84yj","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lo84yj/250621734_hierarchical_reasoning_model/n0kw1sv/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751292605,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":10}}],"before":null}},"user_reports":[],"saved":false,"id":"n0ktn83","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"absolooot1","can_mod_post":false,"created_utc":1751291849,"send_replies":true,"parent_id":"t3_1lo84yj","score":2,"author_fullname":"t2_1pr7hwh6t5","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"The paper doesn't discuss limitations of this new HRM architecture, but whatever they may be, I think that given its SOTA performance at a mere 27 million parameters, they will be solved in future iterations. I might be missing something, but this looks like a milestone in AI development.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0ktn83","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The paper doesn&amp;#39;t discuss limitations of this new HRM architecture, but whatever they may be, I think that given its SOTA performance at a mere 27 million parameters, they will be solved in future iterations. I might be missing something, but this looks like a milestone in AI development.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lo84yj/250621734_hierarchical_reasoning_model/n0ktn83/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751291849,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lo84yj","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0opmgt","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DFructonucleotide","can_mod_post":false,"created_utc":1751334550,"send_replies":true,"parent_id":"t3_1lo84yj","score":1,"author_fullname":"t2_4klay4un","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Just read how they evaluated ARC-AGI. That's outright cheating. They were pretty honest about that though.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0opmgt","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Just read how they evaluated ARC-AGI. That&amp;#39;s outright cheating. They were pretty honest about that though.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lo84yj/250621734_hierarchical_reasoning_model/n0opmgt/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751334550,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lo84yj","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0pdity","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Dizzy-Ad6103","can_mod_post":false,"created_utc":1751343829,"send_replies":true,"parent_id":"t1_n0pdgvn","score":1,"author_fullname":"t2_a8020hb2","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"result in the paper \\n\\nhttps://preview.redd.it/7b3dr55dw6af1.png?width=775&amp;format=png&amp;auto=webp&amp;s=764b0385a0af02db5d2eb195864b91f522c8f812","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0pdity","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;result in the paper &lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://preview.redd.it/7b3dr55dw6af1.png?width=775&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=764b0385a0af02db5d2eb195864b91f522c8f812\\"&gt;https://preview.redd.it/7b3dr55dw6af1.png?width=775&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=764b0385a0af02db5d2eb195864b91f522c8f812&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lo84yj","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lo84yj/250621734_hierarchical_reasoning_model/n0pdity/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751343829,"media_metadata":{"7b3dr55dw6af1":{"status":"valid","e":"Image","m":"image/png","p":[{"y":40,"x":108,"u":"https://preview.redd.it/7b3dr55dw6af1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=18902161c73022bd40047ed67b312024ca9d3493"},{"y":80,"x":216,"u":"https://preview.redd.it/7b3dr55dw6af1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=fec3611edb13f33f552114dd1c928e00117d8c7c"},{"y":119,"x":320,"u":"https://preview.redd.it/7b3dr55dw6af1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=adba07c8be15ce11ab77c2e84e2f5b185de8b524"},{"y":238,"x":640,"u":"https://preview.redd.it/7b3dr55dw6af1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=3452e045bac8c255c62cbb1b05a836635bce6c55"}],"s":{"y":289,"x":775,"u":"https://preview.redd.it/7b3dr55dw6af1.png?width=775&amp;format=png&amp;auto=webp&amp;s=764b0385a0af02db5d2eb195864b91f522c8f812"},"id":"7b3dr55dw6af1"}},"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0pdgvn","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Dizzy-Ad6103","can_mod_post":false,"created_utc":1751343805,"send_replies":true,"parent_id":"t3_1lo84yj","score":1,"author_fullname":"t2_a8020hb2","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"https://preview.redd.it/oige9s83w6af1.png?width=1045&amp;format=png&amp;auto=webp&amp;s=35352a544d7c0a595ef004dbdfe0b91ba7a2a5ac\\n\\nthe result in paper is not Comprehensive, here is arc agi leader broad [https://arcprize.org/leaderboard](https://arcprize.org/leaderboard)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0pdgvn","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;&lt;a href=\\"https://preview.redd.it/oige9s83w6af1.png?width=1045&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=35352a544d7c0a595ef004dbdfe0b91ba7a2a5ac\\"&gt;https://preview.redd.it/oige9s83w6af1.png?width=1045&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=35352a544d7c0a595ef004dbdfe0b91ba7a2a5ac&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;the result in paper is not Comprehensive, here is arc agi leader broad &lt;a href=\\"https://arcprize.org/leaderboard\\"&gt;https://arcprize.org/leaderboard&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lo84yj/250621734_hierarchical_reasoning_model/n0pdgvn/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751343805,"media_metadata":{"oige9s83w6af1":{"status":"valid","e":"Image","m":"image/png","p":[{"y":84,"x":108,"u":"https://preview.redd.it/oige9s83w6af1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=c1c6eee95b4a0323184cf7617794d30aa3884334"},{"y":168,"x":216,"u":"https://preview.redd.it/oige9s83w6af1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=5cd7725f05711bf4c8df067a1d1dcaaf2cfe3c25"},{"y":249,"x":320,"u":"https://preview.redd.it/oige9s83w6af1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=c99ecd50c61a1a061e6dc690bf8501a1c8d6c476"},{"y":499,"x":640,"u":"https://preview.redd.it/oige9s83w6af1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=624097ee4636fea2112f292f2d8784dcd9c36be2"},{"y":749,"x":960,"u":"https://preview.redd.it/oige9s83w6af1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=8b6528b4b1e9ad1d447bc3f7b0c32ca6d4bc8bfe"}],"s":{"y":816,"x":1045,"u":"https://preview.redd.it/oige9s83w6af1.png?width=1045&amp;format=png&amp;auto=webp&amp;s=35352a544d7c0a595ef004dbdfe0b91ba7a2a5ac"},"id":"oige9s83w6af1"}},"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lo84yj","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),o=()=>e.jsx(a,{data:l});export{o as default};
