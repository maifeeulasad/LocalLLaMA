import{j as e}from"./index-VJ_Q0xWX.js";import{R as t}from"./RedditPostRenderer-DG3v0tMm.js";import"./index-CyVJZhk_.js";const l=JSON.parse('[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Are the scrapers usually part of the model or is it an MCP server? How did scrapers change after ai? Deep research is probably one of the most useful things I’ve used, if I run it locally with openwebui and the search integration (like ddg) how does it get the data from sites?","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"How are local or online models scraping? Is it different from search?","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lniut8","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.8,"author_flair_background_color":null,"subreddit_type":"public","ups":6,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_12s3hn4y0b","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":6,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1751214496,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Are the scrapers usually part of the model or is it an MCP server? How did scrapers change after ai? Deep research is probably one of the most useful things I’ve used, if I run it locally with openwebui and the search integration (like ddg) how does it get the data from sites?&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1lniut8","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"InsideYork","discussion_type":null,"num_comments":3,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lniut8/how_are_local_or_online_models_scraping_is_it/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lniut8/how_are_local_or_online_models_scraping_is_it/","subreddit_subscribers":492929,"created_utc":1751214496,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0kb8rf","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Evening_Ad6637","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0gi1ck","score":1,"author_fullname":"t2_p45er6oo","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"It has never something to do with the model itself - besides that the model could have explicitly been trained to call functions.\\n\\nCalling functions means, that instead of train the model this way:\\n\\n\\n```\\nstart of system:\\nprompt: you are a friendly assistant Blablah\\nend of system:\\n\\nstart of user:\\nrequest: How is the weather in moscow?\\nend of user:\\n\\nstart of assistant:\\nresponse: I don’t have the ability to tell you anything about realtime weather data etc blablah\\nend of assistant:\\n\\n```\\n\\n\\nIt was trained this way:\\n\\n\\n```\\nstart of system:\\nprompt: you are a friendly assistant and have the following tools -&gt; weather (arguments: city), websearch (arguments: words)\\nend of system:\\n\\nstart of user:\\nrequest: How is the weather in moscow?\\nend of user:\\n\\nstart of assistant:\\n\\nfunction_call_weather: true\\nfunction_call_weather_arg: moscow\\n\\nfunction_call_websearch: false\\nfunction_call_ websearch_arg:\\n\\nresponse: Of course, I can tell you what the weather is in moscow\\nend of assistant:\\n\\nauto_insert_call_result\\n\\n```\\n\\n\\nThe model itself has no idea what happens in the backend. It is up to you to decide what should happen when certain triggers are initiated by the model. Whether you forward the query to ddg or searx or weatherapi.com etc. is entirely up to you/the software developer.\\n\\n\\nMCP is much more advanced than the simple example above and it\'s also a little hacky, allowing a back and forth for the model by incorporating or exploiting the user message structure. MCP also offers more than just function calls.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0kb8rf","is_submitter":false,"collapsed":false,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It has never something to do with the model itself - besides that the model could have explicitly been trained to call functions.&lt;/p&gt;\\n\\n&lt;p&gt;Calling functions means, that instead of train the model this way:&lt;/p&gt;\\n\\n&lt;p&gt;```\\nstart of system:\\nprompt: you are a friendly assistant Blablah\\nend of system:&lt;/p&gt;\\n\\n&lt;p&gt;start of user:\\nrequest: How is the weather in moscow?\\nend of user:&lt;/p&gt;\\n\\n&lt;p&gt;start of assistant:\\nresponse: I don’t have the ability to tell you anything about realtime weather data etc blablah\\nend of assistant:&lt;/p&gt;\\n\\n&lt;p&gt;```&lt;/p&gt;\\n\\n&lt;p&gt;It was trained this way:&lt;/p&gt;\\n\\n&lt;p&gt;```\\nstart of system:\\nprompt: you are a friendly assistant and have the following tools -&amp;gt; weather (arguments: city), websearch (arguments: words)\\nend of system:&lt;/p&gt;\\n\\n&lt;p&gt;start of user:\\nrequest: How is the weather in moscow?\\nend of user:&lt;/p&gt;\\n\\n&lt;p&gt;start of assistant:&lt;/p&gt;\\n\\n&lt;p&gt;function_call_weather: true\\nfunction_call_weather_arg: moscow&lt;/p&gt;\\n\\n&lt;p&gt;function&lt;em&gt;call_websearch: false\\nfunction_call&lt;/em&gt; websearch_arg:&lt;/p&gt;\\n\\n&lt;p&gt;response: Of course, I can tell you what the weather is in moscow\\nend of assistant:&lt;/p&gt;\\n\\n&lt;p&gt;auto_insert_call_result&lt;/p&gt;\\n\\n&lt;p&gt;```&lt;/p&gt;\\n\\n&lt;p&gt;The model itself has no idea what happens in the backend. It is up to you to decide what should happen when certain triggers are initiated by the model. Whether you forward the query to ddg or searx or weatherapi.com etc. is entirely up to you/the software developer.&lt;/p&gt;\\n\\n&lt;p&gt;MCP is much more advanced than the simple example above and it&amp;#39;s also a little hacky, allowing a back and forth for the model by incorporating or exploiting the user message structure. MCP also offers more than just function calls.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lniut8","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lniut8/how_are_local_or_online_models_scraping_is_it/n0kb8rf/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751285234,"author_flair_text":"llama.cpp","treatment_tags":[],"created_utc":1751285234,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0gi1ck","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"InsideYork","can_mod_post":false,"created_utc":1751225990,"send_replies":true,"parent_id":"t1_n0g1s1w","score":1,"author_fullname":"t2_12s3hn4y0b","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I’m asking about how it’s usually implemented. Im wondering if deep research more a special sauce on the scraper or if it is more of the model. Maybe performance drastically improved from a higher quality scraper.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0gi1ck","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I’m asking about how it’s usually implemented. Im wondering if deep research more a special sauce on the scraper or if it is more of the model. Maybe performance drastically improved from a higher quality scraper.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lniut8","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lniut8/how_are_local_or_online_models_scraping_is_it/n0gi1ck/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751225990,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0g1s1w","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"SM8085","can_mod_post":false,"created_utc":1751220775,"send_replies":true,"parent_id":"t3_1lniut8","score":3,"author_fullname":"t2_14vikjao97","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"&gt;Are the scrapers usually part of the model or is it an MCP server?\\n\\nAll the models I know use tool/function calling, which include MCP servers.\\n\\n&gt;if I run it locally with openwebui and the search integration (like ddg) how does it get the data from sites?\\n\\nThere can be many implementations, are you asking about a specific one?\\n\\nIn general I would expect it to do a search with ddg/some search engine, pick the top N results, and then fetch those pages and clean the HTML for inference.  If it\'s written in Python then Python has their [requests](https://requests.readthedocs.io/en/latest/) library for downloading things from the web.  Then they have things like [BeautifulSoup](https://beautiful-soup-4.readthedocs.io/en/latest/) to clean up the HTML.  If the tool/MCP server is written in a different language they would simply do something similar in that language.  Fetch the web-thing, parse the text/etc., feed it to the bot somehow.\\n\\nThe logic of how they present the pages to the bot may differ in different ways.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0g1s1w","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;Are the scrapers usually part of the model or is it an MCP server?&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;All the models I know use tool/function calling, which include MCP servers.&lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;if I run it locally with openwebui and the search integration (like ddg) how does it get the data from sites?&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;There can be many implementations, are you asking about a specific one?&lt;/p&gt;\\n\\n&lt;p&gt;In general I would expect it to do a search with ddg/some search engine, pick the top N results, and then fetch those pages and clean the HTML for inference.  If it&amp;#39;s written in Python then Python has their &lt;a href=\\"https://requests.readthedocs.io/en/latest/\\"&gt;requests&lt;/a&gt; library for downloading things from the web.  Then they have things like &lt;a href=\\"https://beautiful-soup-4.readthedocs.io/en/latest/\\"&gt;BeautifulSoup&lt;/a&gt; to clean up the HTML.  If the tool/MCP server is written in a different language they would simply do something similar in that language.  Fetch the web-thing, parse the text/etc., feed it to the bot somehow.&lt;/p&gt;\\n\\n&lt;p&gt;The logic of how they present the pages to the bot may differ in different ways.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lniut8/how_are_local_or_online_models_scraping_is_it/n0g1s1w/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751220775,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lniut8","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}}]'),o=()=>e.jsx(t,{data:l});export{o as default};
