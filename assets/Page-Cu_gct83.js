import{j as e}from"./index-BOnf-UhU.js";import{R as a}from"./RedditPostRenderer-Ce39qICS.js";import"./index-CDase6VD.js";const t=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Hi all,\\n\\nI recently digitized most of my documents as PDFs and set up Paperless-NGX to provide a searchable web-based database. Since the documents are analyzed with OCR, the search in Paperless-NGX for certain keywords works really well to identify documents related to e.g. car repairs.\\n\\n  \\nNow I'm trying to use a local LLM to ask it questions about my said car repairs, e.g. \\"which parts were repaired in 2024 and how much did they cost each? what is the phone number of the shop?\\"\\n\\nI'm running ollama locally on a Win11 host machine an can run e.g. qwen3:32b or mixtral:8x7b at sufficient speeds.  \\nI want this local LLM to search my PDFs and provide me with answers.\\n\\nI tried setting up paperless-ai which kinda works for \\"tagging\\" the documents, but the RAG chat function really is all over the place. It analyzes each and every document before giving an answer but the answers are all over the place. When asked about car repairs it also gives information about my motorcycle or random stuff about my telecom provider...\\n\\nSecond i tried setting up AnythingLLM where documents can be loaded into the \\"Workspace\\" and this works well when only single documents are loaded. However, if I load 300 PDFs (which are not OCR scanned because they're not based on paperless-ngx) it also fails miserably.\\n\\n  \\nIs there any recommened setup for my use case?\\n\\nThe goal is to ask questions about car/bike repairs, internet/power contracts or concert tickets.\\n\\n  \\nTx in advance for any help.","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Local PDF Database searchable with ollama - best setup?","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lutlfx","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.75,"author_flair_background_color":null,"subreddit_type":"public","ups":2,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_11dmsl","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":2,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1751993192,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\\n\\n&lt;p&gt;I recently digitized most of my documents as PDFs and set up Paperless-NGX to provide a searchable web-based database. Since the documents are analyzed with OCR, the search in Paperless-NGX for certain keywords works really well to identify documents related to e.g. car repairs.&lt;/p&gt;\\n\\n&lt;p&gt;Now I&amp;#39;m trying to use a local LLM to ask it questions about my said car repairs, e.g. &amp;quot;which parts were repaired in 2024 and how much did they cost each? what is the phone number of the shop?&amp;quot;&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;m running ollama locally on a Win11 host machine an can run e.g. qwen3:32b or mixtral:8x7b at sufficient speeds.&lt;br/&gt;\\nI want this local LLM to search my PDFs and provide me with answers.&lt;/p&gt;\\n\\n&lt;p&gt;I tried setting up paperless-ai which kinda works for &amp;quot;tagging&amp;quot; the documents, but the RAG chat function really is all over the place. It analyzes each and every document before giving an answer but the answers are all over the place. When asked about car repairs it also gives information about my motorcycle or random stuff about my telecom provider...&lt;/p&gt;\\n\\n&lt;p&gt;Second i tried setting up AnythingLLM where documents can be loaded into the &amp;quot;Workspace&amp;quot; and this works well when only single documents are loaded. However, if I load 300 PDFs (which are not OCR scanned because they&amp;#39;re not based on paperless-ngx) it also fails miserably.&lt;/p&gt;\\n\\n&lt;p&gt;Is there any recommened setup for my use case?&lt;/p&gt;\\n\\n&lt;p&gt;The goal is to ask questions about car/bike repairs, internet/power contracts or concert tickets.&lt;/p&gt;\\n\\n&lt;p&gt;Tx in advance for any help.&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1lutlfx","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"540Flair","discussion_type":null,"num_comments":2,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lutlfx/local_pdf_database_searchable_with_ollama_best/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lutlfx/local_pdf_database_searchable_with_ollama_best/","subreddit_subscribers":496592,"created_utc":1751993192,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n24bc8y","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"f3llowtraveler","can_mod_post":false,"created_utc":1752035879,"send_replies":true,"parent_id":"t3_1lutlfx","score":1,"author_fullname":"t2_ny8giydrf","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Check out EagleFiler. Not sure if it's available for Windows but just look for a similar app.\\n\\nFor the scanner / OCR check out the ScanSnap from Fujitsu. It can scan a pile of papers, double-sided, OCR, and drop it into a folder for EagleFiler to import.\\n\\nThe UI is really nice, searchable. You can use AI to make a similar app pretty easily I'd assume, since the hard part is already handled by the ScanSnap.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n24bc8y","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Check out EagleFiler. Not sure if it&amp;#39;s available for Windows but just look for a similar app.&lt;/p&gt;\\n\\n&lt;p&gt;For the scanner / OCR check out the ScanSnap from Fujitsu. It can scan a pile of papers, double-sided, OCR, and drop it into a folder for EagleFiler to import.&lt;/p&gt;\\n\\n&lt;p&gt;The UI is really nice, searchable. You can use AI to make a similar app pretty easily I&amp;#39;d assume, since the hard part is already handled by the ScanSnap.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lutlfx/local_pdf_database_searchable_with_ollama_best/n24bc8y/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752035879,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lutlfx","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),r=()=>e.jsx(a,{data:t});export{r as default};
