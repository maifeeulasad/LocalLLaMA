import{j as e}from"./index-BgwOAK4-.js";import{R as a}from"./RedditPostRenderer-BOBjDTFu.js";import"./index-BL22wVg5.js";const t=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Hi everyone,\\n\\nI recently reimplemented Google's open-source LLMs Gemma 1, Gemma 2, and Gemma 3 from scratch as part of my learning journey into LLM architectures.\\n\\nThis was a deep dive into transformer internals and helped me understand the core mechanisms behind large models. I read and followed the official papers:\\n- [Gemma 1](https://arxiv.org/pdf/2403.08295)\\n- [Gemma 2](https://arxiv.org/pdf/2408.00118)\\n- [Gemma 3](https://arxiv.org/pdf/2406.07101) (multimodal vision)\\n\\nThis was a purely educational reimplementation.\\n\\nI also shared this on LinkedIn with more details if you're curious:\\n🔗 [LinkedIn post here](https://www.linkedin.com/posts/satyam-mishra-a827b0325_llm-nlp-gemma-activity-7348017348030713857-Qa1-?utm_source=share&amp;utm_medium=member_desktop)\\n\\nI'm now planning to add more LLMs (e.g., Mistral, LLaMA, Phi) to the repo and build a learning-oriented repo for students and researchers.\\n\\nWould love any feedback, suggestions, or advice on what model to reimplement next!\\n\\nThanks 🙏","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Reimplementing an LLM from Scratch","link_flair_richtext":[{"e":"text","t":"Resources"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lv7s0r","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.88,"author_flair_background_color":null,"subreddit_type":"public","ups":12,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_6qpq9avr5","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Resources","can_mod_post":false,"score":12,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1752029046,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;\\n\\n&lt;p&gt;I recently reimplemented Google&amp;#39;s open-source LLMs Gemma 1, Gemma 2, and Gemma 3 from scratch as part of my learning journey into LLM architectures.&lt;/p&gt;\\n\\n&lt;p&gt;This was a deep dive into transformer internals and helped me understand the core mechanisms behind large models. I read and followed the official papers:\\n- &lt;a href=\\"https://arxiv.org/pdf/2403.08295\\"&gt;Gemma 1&lt;/a&gt;\\n- &lt;a href=\\"https://arxiv.org/pdf/2408.00118\\"&gt;Gemma 2&lt;/a&gt;\\n- &lt;a href=\\"https://arxiv.org/pdf/2406.07101\\"&gt;Gemma 3&lt;/a&gt; (multimodal vision)&lt;/p&gt;\\n\\n&lt;p&gt;This was a purely educational reimplementation.&lt;/p&gt;\\n\\n&lt;p&gt;I also shared this on LinkedIn with more details if you&amp;#39;re curious:\\n🔗 &lt;a href=\\"https://www.linkedin.com/posts/satyam-mishra-a827b0325_llm-nlp-gemma-activity-7348017348030713857-Qa1-?utm_source=share&amp;amp;utm_medium=member_desktop\\"&gt;LinkedIn post here&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;m now planning to add more LLMs (e.g., Mistral, LLaMA, Phi) to the repo and build a learning-oriented repo for students and researchers.&lt;/p&gt;\\n\\n&lt;p&gt;Would love any feedback, suggestions, or advice on what model to reimplement next!&lt;/p&gt;\\n\\n&lt;p&gt;Thanks 🙏&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":true,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#ccac2b","id":"1lv7s0r","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"CodingWithSatyam","discussion_type":null,"num_comments":1,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lv7s0r/reimplementing_an_llm_from_scratch/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lv7s0r/reimplementing_an_llm_from_scratch/","subreddit_subscribers":497025,"created_utc":1752029046,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n251msz","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Languages_Learner","can_mod_post":false,"created_utc":1752049559,"send_replies":true,"parent_id":"t3_1lv7s0r","score":2,"author_fullname":"t2_v9x8tm7u","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Thanks for great work. Hope that you will share inference code for quantatized versions of gemma 1, gemma 2, gemma 3. It would be especially good if inference code will be written not in python but in some other programming language.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n251msz","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thanks for great work. Hope that you will share inference code for quantatized versions of gemma 1, gemma 2, gemma 3. It would be especially good if inference code will be written not in python but in some other programming language.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lv7s0r/reimplementing_an_llm_from_scratch/n251msz/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752049559,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lv7s0r","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}}]`),s=()=>e.jsx(a,{data:t});export{s as default};
