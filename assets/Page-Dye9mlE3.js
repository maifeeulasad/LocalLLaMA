import{j as e}from"./index-F0NXdzZX.js";import{R as l}from"./RedditPostRenderer-CoEZB1dt.js";import"./index-DrtravXm.js";const t=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"What would be the maximum B to use on this config (with RAM offload of course)","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Maximum parameters for this 4050 RTX 6GB vram with 32GB RAM","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1m2w3i3","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.5,"author_flair_background_color":null,"subreddit_type":"public","ups":0,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_8hi4bk25","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":0,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1752824358,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;What would be the maximum B to use on this config (with RAM offload of course)&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":true,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1m2w3i3","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"Accomplished_Mark_10","discussion_type":null,"num_comments":14,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1m2w3i3/maximum_parameters_for_this_4050_rtx_6gb_vram/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1m2w3i3/maximum_parameters_for_this_4050_rtx_6gb_vram/","subreddit_subscribers":501233,"created_utc":1752824358,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3s4ujr","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"LagOps91","can_mod_post":false,"created_utc":1752826655,"send_replies":true,"parent_id":"t1_n3s4fe0","score":2,"author_fullname":"t2_3wi6j7vwh","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"fully agree on using this or the bert 4.5 small MoE. you will make the most out of your hardware and get usable speed with a relatively smart model.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3s4ujr","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;fully agree on using this or the bert 4.5 small MoE. you will make the most out of your hardware and get usable speed with a relatively smart model.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2w3i3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2w3i3/maximum_parameters_for_this_4050_rtx_6gb_vram/n3s4ujr/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752826655,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n3s4fe0","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"kironlau","can_mod_post":false,"created_utc":1752826416,"send_replies":true,"parent_id":"t3_1m2w3i3","score":4,"author_fullname":"t2_tb0dz2ds","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I suggest Qwen3-30B-A3B.  \\nThe performance is more or less between a 10\\\\~14B model, but the speed is like 3\\\\~4B model.  \\n  \\nQ4 (I suggest IQ4XS) is about 16GB in size (+20% context size, 18 gb), using CUDA but offloading all layers to CPU. (the inference speed highly depends on you ram speed)\\n\\nIn this case, the model will still be inferenced by GPU, but not much vram used. It's stilled using up a little bit, in win 11. \\n\\nOverride tensor to GPU will get higher speed, not much in mainline llama.cpp, but it depends on your system config.\\n\\nIf you're using linux (or wsl), you main try to use ik\\\\_llama.cpp, it is optimized to run MOE model.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3s4fe0","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I suggest Qwen3-30B-A3B.&lt;br/&gt;\\nThe performance is more or less between a 10~14B model, but the speed is like 3~4B model.  &lt;/p&gt;\\n\\n&lt;p&gt;Q4 (I suggest IQ4XS) is about 16GB in size (+20% context size, 18 gb), using CUDA but offloading all layers to CPU. (the inference speed highly depends on you ram speed)&lt;/p&gt;\\n\\n&lt;p&gt;In this case, the model will still be inferenced by GPU, but not much vram used. It&amp;#39;s stilled using up a little bit, in win 11. &lt;/p&gt;\\n\\n&lt;p&gt;Override tensor to GPU will get higher speed, not much in mainline llama.cpp, but it depends on your system config.&lt;/p&gt;\\n\\n&lt;p&gt;If you&amp;#39;re using linux (or wsl), you main try to use ik_llama.cpp, it is optimized to run MOE model.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2w3i3/maximum_parameters_for_this_4050_rtx_6gb_vram/n3s4fe0/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752826416,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m2w3i3","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3uerxy","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Admirable-Star7088","can_mod_post":false,"created_utc":1752856904,"send_replies":true,"parent_id":"t1_n3ucioz","score":2,"author_fullname":"t2_qhlcbiy3k","approved_by":null,"mod_note":null,"all_awardings":[],"body":"Prompt processing seem to be as fast as traditional transformer models, at least in my very limited testings :P Maybe there is a difference, but in that case it was not noticeably for me. Or maybe it's just noticeable with **very** long context.","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n3uerxy","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Prompt processing seem to be as fast as traditional transformer models, at least in my very limited testings :P Maybe there is a difference, but in that case it was not noticeably for me. Or maybe it&amp;#39;s just noticeable with &lt;strong&gt;very&lt;/strong&gt; long context.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2w3i3/maximum_parameters_for_this_4050_rtx_6gb_vram/n3uerxy/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752856904,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m2w3i3","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":8,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n3ucioz","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"LagOps91","can_mod_post":false,"created_utc":1752856250,"send_replies":true,"parent_id":"t1_n3ubgpm","score":1,"author_fullname":"t2_3wi6j7vwh","approved_by":null,"mod_note":null,"all_awardings":[],"body":"oof even if you just continue? yeah that's rough. how fast is promp processing then? is it fast too?","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n3ucioz","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;oof even if you just continue? yeah that&amp;#39;s rough. how fast is promp processing then? is it fast too?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1m2w3i3","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2w3i3/maximum_parameters_for_this_4050_rtx_6gb_vram/n3ucioz/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752856250,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":7,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3ubgpm","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Admirable-Star7088","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3stw72","score":2,"author_fullname":"t2_qhlcbiy3k","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I tested with a larger chunk of text, and yes, it does seem to re-compute everything when you regenerate or when you simply continue the chat with additional messages. \\n\\nSo if you work with large context, this is currently a drawback.","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n3ubgpm","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I tested with a larger chunk of text, and yes, it does seem to re-compute everything when you regenerate or when you simply continue the chat with additional messages. &lt;/p&gt;\\n\\n&lt;p&gt;So if you work with large context, this is currently a drawback.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1m2w3i3","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2w3i3/maximum_parameters_for_this_4050_rtx_6gb_vram/n3ubgpm/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752855944,"author_flair_text":null,"treatment_tags":[],"created_utc":1752855944,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":6,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n3stw72","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"LagOps91","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3soue4","score":1,"author_fullname":"t2_3wi6j7vwh","approved_by":null,"mod_note":null,"all_awardings":[],"body":"how is it with context? if i understand it correctly, editing / regenerating responses etc. causes a full re-compute (due to the hidden state, i assume).","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n3stw72","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;how is it with context? if i understand it correctly, editing / regenerating responses etc. causes a full re-compute (due to the hidden state, i assume).&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1m2w3i3","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2w3i3/maximum_parameters_for_this_4050_rtx_6gb_vram/n3stw72/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752839456,"author_flair_text":null,"treatment_tags":[],"created_utc":1752839456,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3soue4","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Admirable-Star7088","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3skasc","score":2,"author_fullname":"t2_qhlcbiy3k","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yes, it appears the \\"hybrid Transformer-Mamba\\" architecture that Jamba uses is way faster for CPU interference than transformer only.\\n\\nFrom my limited testings so far, the model seems pretty powerful, apart from some unusual reasoning quirks now and then, but I don't know if this is related to the Mamba architecture or just the model/training data itself.\\n\\nEither way, this could be pretty much revolutionary for CPU/RAM users - If things keep looking this good. The \\"hybrid Transformer-Mamba\\" might be the future of LLMs.","edited":false,"author_flair_css_class":null,"name":"t1_n3soue4","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yes, it appears the &amp;quot;hybrid Transformer-Mamba&amp;quot; architecture that Jamba uses is way faster for CPU interference than transformer only.&lt;/p&gt;\\n\\n&lt;p&gt;From my limited testings so far, the model seems pretty powerful, apart from some unusual reasoning quirks now and then, but I don&amp;#39;t know if this is related to the Mamba architecture or just the model/training data itself.&lt;/p&gt;\\n\\n&lt;p&gt;Either way, this could be pretty much revolutionary for CPU/RAM users - If things keep looking this good. The &amp;quot;hybrid Transformer-Mamba&amp;quot; might be the future of LLMs.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1m2w3i3","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2w3i3/maximum_parameters_for_this_4050_rtx_6gb_vram/n3soue4/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752837321,"author_flair_text":null,"collapsed":false,"created_utc":1752837321,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n3skasc","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"LagOps91","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3s76ap","score":1,"author_fullname":"t2_3wi6j7vwh","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"oh i'm surprised that Jamba is this usable. didn't really try the model yet since fast forwarding / context shift are not (yet?) supported in koboldcpp.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3skasc","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;oh i&amp;#39;m surprised that Jamba is this usable. didn&amp;#39;t really try the model yet since fast forwarding / context shift are not (yet?) supported in koboldcpp.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2w3i3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2w3i3/maximum_parameters_for_this_4050_rtx_6gb_vram/n3skasc/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752835213,"author_flair_text":null,"treatment_tags":[],"created_utc":1752835213,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3ud4ko","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Admirable-Star7088","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3st4za","score":1,"author_fullname":"t2_qhlcbiy3k","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"A MoE is a technique where a computer processes only smaller portions of an AI model at a time, rather than handling the entire model simultaneously. \\n\\nFor example, if a model consists of 70 billion parameters, the computer only processes 10 billion of those parameters at any given moment. This reduces the amount of data the RAM and CPU need to manage.","edited":false,"author_flair_css_class":null,"name":"t1_n3ud4ko","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;A MoE is a technique where a computer processes only smaller portions of an AI model at a time, rather than handling the entire model simultaneously. &lt;/p&gt;\\n\\n&lt;p&gt;For example, if a model consists of 70 billion parameters, the computer only processes 10 billion of those parameters at any given moment. This reduces the amount of data the RAM and CPU need to manage.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1m2w3i3","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2w3i3/maximum_parameters_for_this_4050_rtx_6gb_vram/n3ud4ko/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752856428,"author_flair_text":null,"collapsed":false,"created_utc":1752856428,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3st4za","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Jawzper","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3s76ap","score":1,"author_fullname":"t2_gebwv","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Any chance you can explain why MoE is better for speed when using CPU? Trying to learn.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3st4za","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Any chance you can explain why MoE is better for speed when using CPU? Trying to learn.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2w3i3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2w3i3/maximum_parameters_for_this_4050_rtx_6gb_vram/n3st4za/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752839152,"author_flair_text":null,"treatment_tags":[],"created_utc":1752839152,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3s76ap","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Admirable-Star7088","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3s4ynu","score":1,"author_fullname":"t2_qhlcbiy3k","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I didn't include speed since OP only asked about RAM and maximum model.\\n\\nFor better speed, MoE is a good option. If you want a dense model, I'd recommend Jamba 1.7 52b. For comparison, with my currently relatively low-clocked RAM (3600 Mhz):\\n\\nNemotron 51b (with 16GB VRAM offload): \\\\~1.6 t/s  \\nJamba 1.7 52b (pure RAM, no VRAM offload): \\\\~4.9 t/s  \\nJamba 1.7 52b (with 16GB VRAM offload): \\\\~6 t/s\\n\\nIf you higher you RAM speed to \\\\~5000 MHz or even \\\\~6000 MHz you would get a few more t/s. (my ram supports 6400 MHz but I can't use the max speed in my current setup).","edited":1752828204,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3s76ap","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I didn&amp;#39;t include speed since OP only asked about RAM and maximum model.&lt;/p&gt;\\n\\n&lt;p&gt;For better speed, MoE is a good option. If you want a dense model, I&amp;#39;d recommend Jamba 1.7 52b. For comparison, with my currently relatively low-clocked RAM (3600 Mhz):&lt;/p&gt;\\n\\n&lt;p&gt;Nemotron 51b (with 16GB VRAM offload): ~1.6 t/s&lt;br/&gt;\\nJamba 1.7 52b (pure RAM, no VRAM offload): ~4.9 t/s&lt;br/&gt;\\nJamba 1.7 52b (with 16GB VRAM offload): ~6 t/s&lt;/p&gt;\\n\\n&lt;p&gt;If you higher you RAM speed to ~5000 MHz or even ~6000 MHz you would get a few more t/s. (my ram supports 6400 MHz but I can&amp;#39;t use the max speed in my current setup).&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2w3i3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2w3i3/maximum_parameters_for_this_4050_rtx_6gb_vram/n3s76ap/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752827990,"author_flair_text":null,"treatment_tags":[],"created_utc":1752827990,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3s4ynu","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"LagOps91","can_mod_post":false,"created_utc":1752826721,"send_replies":true,"parent_id":"t1_n3s43hn","score":2,"author_fullname":"t2_3wi6j7vwh","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"while this is correct, it would be dreadfully slow and i just can't recommend it. a much better alternative would be small MoE models.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3s4ynu","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;while this is correct, it would be dreadfully slow and i just can&amp;#39;t recommend it. a much better alternative would be small MoE models.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2w3i3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2w3i3/maximum_parameters_for_this_4050_rtx_6gb_vram/n3s4ynu/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752826721,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n3s43hn","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Admirable-Star7088","can_mod_post":false,"created_utc":1752826227,"send_replies":true,"parent_id":"t3_1m2w3i3","score":1,"author_fullname":"t2_qhlcbiy3k","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"For reference, when I load the following models in LM Studio at Q5\\\\_K\\\\_M and 8192 context:\\n\\n* Llama 3.3 70b: \\\\~51GB RAM.\\n* Nemotron 51b: \\\\~35GB RAM.\\n* GLM 4 32b: \\\\~22GB RAM.\\n\\nWith your 38GB total RAM, you could fit a \\\\~50b model, at least at a relatively low context. However, if I max the context on Nemotron 51b from 8192 to 131072, it uses a whopping 73GB of my RAM, up from 35GB.\\n\\nAs mentioned, these were all at Q5. You could try the slightly lower quant Q4 (i.e, Q4\\\\_K\\\\_M) if you want to fit slightly more parameters and/or context. The quality drop should in most cases not be very noticeable at Q4, if at all.\\n\\nI hoped this gave you a general idea to your question.","edited":1752826755,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3s43hn","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;For reference, when I load the following models in LM Studio at Q5_K_M and 8192 context:&lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;Llama 3.3 70b: ~51GB RAM.&lt;/li&gt;\\n&lt;li&gt;Nemotron 51b: ~35GB RAM.&lt;/li&gt;\\n&lt;li&gt;GLM 4 32b: ~22GB RAM.&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;p&gt;With your 38GB total RAM, you could fit a ~50b model, at least at a relatively low context. However, if I max the context on Nemotron 51b from 8192 to 131072, it uses a whopping 73GB of my RAM, up from 35GB.&lt;/p&gt;\\n\\n&lt;p&gt;As mentioned, these were all at Q5. You could try the slightly lower quant Q4 (i.e, Q4_K_M) if you want to fit slightly more parameters and/or context. The quality drop should in most cases not be very noticeable at Q4, if at all.&lt;/p&gt;\\n\\n&lt;p&gt;I hoped this gave you a general idea to your question.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2w3i3/maximum_parameters_for_this_4050_rtx_6gb_vram/n3s43hn/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752826227,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m2w3i3","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3sh3sr","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"GPTshop_ai","can_mod_post":false,"created_utc":1752833563,"send_replies":true,"parent_id":"t3_1m2w3i3","score":-1,"author_fullname":"t2_rkmud0isr","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Decent LLMs need a least 96GB of VRAM.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3sh3sr","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Decent LLMs need a least 96GB of VRAM.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2w3i3/maximum_parameters_for_this_4050_rtx_6gb_vram/n3sh3sr/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752833563,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m2w3i3","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-1}}],"before":null}}]`),s=()=>e.jsx(l,{data:t});export{s as default};
