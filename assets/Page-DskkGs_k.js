import{j as e}from"./index-CWmJdUH_.js";import{R as l}from"./RedditPostRenderer-D2iunoQ9.js";import"./index-BCg9RP6g.js";const a=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Long story short I won 2 sticks of 32 GB DDR5 ram but I only have a gaming laptop, and I have always wanted to build a PC.\\ncan I skip buying a GPU for now and put my unbelievable 64GBs to use with a CPU and run LLMs and STT models from it, in terms of loading the models I know that I will be able to load bigger models than any GPU I would ever buy anytime soon, but my question is will the CPU provide reasonable inference speed? do you have any recommendations for a CPU that maybe has a good NPU or do I just buy a powerful and new CPU blindly? I am not very experienced in running AI workloads on CPU and I would appreciate any correction or input about your past experiences or any tests you might have done recently.","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Enough resources for light AI workloads?","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lzzka4","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.6,"author_flair_background_color":null,"subreddit_type":"public","ups":1,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_1m0sp5gn6a","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":1,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1752529491,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Long story short I won 2 sticks of 32 GB DDR5 ram but I only have a gaming laptop, and I have always wanted to build a PC.\\ncan I skip buying a GPU for now and put my unbelievable 64GBs to use with a CPU and run LLMs and STT models from it, in terms of loading the models I know that I will be able to load bigger models than any GPU I would ever buy anytime soon, but my question is will the CPU provide reasonable inference speed? do you have any recommendations for a CPU that maybe has a good NPU or do I just buy a powerful and new CPU blindly? I am not very experienced in running AI workloads on CPU and I would appreciate any correction or input about your past experiences or any tests you might have done recently.&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":true,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1lzzka4","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"EyasDBoi_i","discussion_type":null,"num_comments":12,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lzzka4/enough_resources_for_light_ai_workloads/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lzzka4/enough_resources_for_light_ai_workloads/","subreddit_subscribers":499295,"created_utc":1752529491,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n38cov7","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"EyasDBoi_i","can_mod_post":false,"created_utc":1752571598,"send_replies":true,"parent_id":"t1_n35p0tt","score":1,"author_fullname":"t2_1m0sp5gn6a","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"i need to put my ddr5 ram to use, it is not selling at all ATM Jordan has a horrible pc parts market at the moment. im stuck with these sticks and i gotta use em","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n38cov7","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;i need to put my ddr5 ram to use, it is not selling at all ATM Jordan has a horrible pc parts market at the moment. im stuck with these sticks and i gotta use em&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lzzka4","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzzka4/enough_resources_for_light_ai_workloads/n38cov7/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752571598,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n35p0tt","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Highwaytothebeach","can_mod_post":false,"created_utc":1752531437,"send_replies":true,"parent_id":"t3_1lzzka4","score":3,"author_fullname":"t2_1qychuraq9","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Just go for  a mini PC with 64 - 128 GB RAM and  occulink or a usb 4 so you can add a GPU if you wish and save energy.  Lpddr5 rocks, at more than 8000 mhz,  and there is already standard for lpddr6 in place. I expect much sooner a mini pc with 1 TB lpddr will be available on the market than old fashioned pc...","edited":1752532291,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n35p0tt","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Just go for  a mini PC with 64 - 128 GB RAM and  occulink or a usb 4 so you can add a GPU if you wish and save energy.  Lpddr5 rocks, at more than 8000 mhz,  and there is already standard for lpddr6 in place. I expect much sooner a mini pc with 1 TB lpddr will be available on the market than old fashioned pc...&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzzka4/enough_resources_for_light_ai_workloads/n35p0tt/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752531437,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lzzka4","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n35m2t2","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"LA_rent_Aficionado","can_mod_post":false,"send_replies":true,"parent_id":"t1_n35lc54","score":2,"author_fullname":"t2_t8zbiflk","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Translation will be a standard LLM so no difference there, as far as I know TTS models are much lighter weight so you may have more success - mostly everything uses transformers type architecture (vision, text, etc) so size will be driving factor behind performance.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n35m2t2","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Translation will be a standard LLM so no difference there, as far as I know TTS models are much lighter weight so you may have more success - mostly everything uses transformers type architecture (vision, text, etc) so size will be driving factor behind performance.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lzzka4","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzzka4/enough_resources_for_light_ai_workloads/n35m2t2/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752530512,"author_flair_text":null,"treatment_tags":[],"created_utc":1752530512,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n35lc54","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"EyasDBoi_i","can_mod_post":false,"created_utc":1752530285,"send_replies":true,"parent_id":"t1_n35kk1j","score":1,"author_fullname":"t2_1m0sp5gn6a","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"what about other types of models, Speech to text, transcription, translation... stuff like that...","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n35lc54","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;what about other types of models, Speech to text, transcription, translation... stuff like that...&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lzzka4","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzzka4/enough_resources_for_light_ai_workloads/n35lc54/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752530285,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n35kk1j","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"LA_rent_Aficionado","can_mod_post":false,"created_utc":1752530047,"send_replies":true,"parent_id":"t3_1lzzka4","score":2,"author_fullname":"t2_t8zbiflk","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Not any any decent sized model, RAM is already slow for interface and on a gaming PC you’re generally limited with memory channels. You could probably get away with a 8B or smaller model but it will crawl along without a GPU.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n35kk1j","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Not any any decent sized model, RAM is already slow for interface and on a gaming PC you’re generally limited with memory channels. You could probably get away with a 8B or smaller model but it will crawl along without a GPU.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzzka4/enough_resources_for_light_ai_workloads/n35kk1j/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752530047,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lzzka4","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n35o59s","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"EyasDBoi_i","can_mod_post":false,"created_utc":1752531161,"send_replies":true,"parent_id":"t1_n35m2el","score":1,"author_fullname":"t2_1m0sp5gn6a","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"thats great to know, thanks 🙏","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n35o59s","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;thats great to know, thanks 🙏&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lzzka4","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzzka4/enough_resources_for_light_ai_workloads/n35o59s/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752531161,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n35m2el","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"riklaunim","can_mod_post":false,"created_utc":1752530509,"send_replies":true,"parent_id":"t3_1lzzka4","score":2,"author_fullname":"t2_y4m3","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Like you can load 30B model or somewhat larger and make it run slowly and compare with smaller ones for example. Not practical but doable - people are using AMD Strix Point or Intel Arrow Lake and older for this.\\n\\nIf the CPU has decent iGPU then you can also try with iGPU - BIOS should have an option to select how much RAM goes for iGPU (usually up to 50% of your RAM) - but this option is not always present.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n35m2el","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Like you can load 30B model or somewhat larger and make it run slowly and compare with smaller ones for example. Not practical but doable - people are using AMD Strix Point or Intel Arrow Lake and older for this.&lt;/p&gt;\\n\\n&lt;p&gt;If the CPU has decent iGPU then you can also try with iGPU - BIOS should have an option to select how much RAM goes for iGPU (usually up to 50% of your RAM) - but this option is not always present.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzzka4/enough_resources_for_light_ai_workloads/n35m2el/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752530509,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lzzka4","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n35vj31","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"jamaalwakamaal","can_mod_post":false,"created_utc":1752533511,"send_replies":true,"parent_id":"t3_1lzzka4","score":1,"author_fullname":"t2_alyeos2m","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"For reference: Fore transcription Faster-Whisper base and medium (slightly delay) run fine on DDR4 2133MHz.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n35vj31","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;For reference: Fore transcription Faster-Whisper base and medium (slightly delay) run fine on DDR4 2133MHz.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzzka4/enough_resources_for_light_ai_workloads/n35vj31/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752533511,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lzzka4","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n38gftk","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"kaisurniwurer","can_mod_post":false,"send_replies":true,"parent_id":"t1_n38ckbh","score":2,"author_fullname":"t2_qafso","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"For LLM you need a specific GPU (ones with a lot of VRAM) and those might not be the best value for gaming. Just keep that in mind.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n38gftk","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;For LLM you need a specific GPU (ones with a lot of VRAM) and those might not be the best value for gaming. Just keep that in mind.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lzzka4","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzzka4/enough_resources_for_light_ai_workloads/n38gftk/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752573699,"author_flair_text":null,"treatment_tags":[],"created_utc":1752573699,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n38ckbh","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"EyasDBoi_i","can_mod_post":false,"created_utc":1752571526,"send_replies":true,"parent_id":"t1_n389ypq","score":1,"author_fullname":"t2_1m0sp5gn6a","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"even though this ends up being a horrible move I'm not very worried because I will buy a GPU once I save up enough anyways. I was just curious if I could utilize the huge amount of RAM sitting in my drawer XD. thanks for the suggestion I will try it once I have a build ready","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n38ckbh","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;even though this ends up being a horrible move I&amp;#39;m not very worried because I will buy a GPU once I save up enough anyways. I was just curious if I could utilize the huge amount of RAM sitting in my drawer XD. thanks for the suggestion I will try it once I have a build ready&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lzzka4","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzzka4/enough_resources_for_light_ai_workloads/n38ckbh/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752571526,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n389ypq","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"kaisurniwurer","can_mod_post":false,"created_utc":1752570000,"send_replies":true,"parent_id":"t3_1lzzka4","score":1,"author_fullname":"t2_qafso","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"For RAM you want the biggest MoE model with the smallest experts. To my knowlegde, Qwen 30B A3B should be your best bet. Try it first before you invest in hardware. The only thing that will be different is the speed. I started with mistral at 5t/s and wanted more, and only then I invested in hardware.\\n\\nhttps://huggingface.co/mradermacher/Qwen3-30B-A3B-Base-GGUF\\n\\nYou can load way bigger models with 64GB (up to ~100B) but it will be really slow on RAM/CPU.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n389ypq","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;For RAM you want the biggest MoE model with the smallest experts. To my knowlegde, Qwen 30B A3B should be your best bet. Try it first before you invest in hardware. The only thing that will be different is the speed. I started with mistral at 5t/s and wanted more, and only then I invested in hardware.&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://huggingface.co/mradermacher/Qwen3-30B-A3B-Base-GGUF\\"&gt;https://huggingface.co/mradermacher/Qwen3-30B-A3B-Base-GGUF&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;You can load way bigger models with 64GB (up to ~100B) but it will be really slow on RAM/CPU.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzzka4/enough_resources_for_light_ai_workloads/n389ypq/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752570000,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lzzka4","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),n=()=>e.jsx(l,{data:a});export{n as default};
