import{j as e}from"./index-cvG704yx.js";import{R as a}from"./RedditPostRenderer-CBthLTAH.js";import"./index-D-GavSZU.js";const l=JSON.parse('[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"https://preview.redd.it/sye5ssnxv9af1.png?width=3456&amp;format=png&amp;auto=webp&amp;s=5d312de9207cf7cd5edd21029849b10a3b23bbb5\\n\\nhttps://preview.redd.it/dy46sdb5x9af1.png?width=3456&amp;format=png&amp;auto=webp&amp;s=afe062e1e653c8467fb97741a2b2591a467e2c3d\\n\\n  \\nI successfully ran LoRA training on an NVIDIA Jetson AGX Orin 64GB. Both 8-bit and FP16 modes are working. I\'m currently training the Qwen 2.5 7B model. Although the process is slow, it\'s sufficient for my needs since there\'s no urgency.","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"LoRA training on NVIDIA Jetson AGX Orin 64GB","link_flair_richtext":[{"e":"text","t":"Discussion"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":76,"top_awarded_type":null,"hide_score":false,"media_metadata":{"sye5ssnxv9af1":{"status":"valid","e":"Image","m":"image/png","p":[{"y":59,"x":108,"u":"https://preview.redd.it/sye5ssnxv9af1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=711696091c7304a0549e96995c934067ef53a7dc"},{"y":118,"x":216,"u":"https://preview.redd.it/sye5ssnxv9af1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=ee1e8944d72f5afe647a1bbc05af02fca1627983"},{"y":175,"x":320,"u":"https://preview.redd.it/sye5ssnxv9af1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=2b4f6e0b5a00b579f04daade616e39355de3af5e"},{"y":350,"x":640,"u":"https://preview.redd.it/sye5ssnxv9af1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=999d16893012a66bdda12f495a708a4d6c82d3f1"},{"y":525,"x":960,"u":"https://preview.redd.it/sye5ssnxv9af1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=d17f9f48eba2d5e61912f4d2ab7e22718d62e4bc"},{"y":590,"x":1080,"u":"https://preview.redd.it/sye5ssnxv9af1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a9d391e99cf38775fccac0b22f6bff9c10a99751"}],"s":{"y":1890,"x":3456,"u":"https://preview.redd.it/sye5ssnxv9af1.png?width=3456&amp;format=png&amp;auto=webp&amp;s=5d312de9207cf7cd5edd21029849b10a3b23bbb5"},"id":"sye5ssnxv9af1"},"dy46sdb5x9af1":{"status":"valid","e":"Image","m":"image/png","p":[{"y":59,"x":108,"u":"https://preview.redd.it/dy46sdb5x9af1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=1ecb1ccd4f568548485d79959726f54ec8f52d32"},{"y":118,"x":216,"u":"https://preview.redd.it/dy46sdb5x9af1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=a56daea4f447896c50dce08250a0431aefe5b156"},{"y":175,"x":320,"u":"https://preview.redd.it/dy46sdb5x9af1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=5641334620aad4011d11d45b29e1b1ae3cac1119"},{"y":350,"x":640,"u":"https://preview.redd.it/dy46sdb5x9af1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=cef9f30183e48f4b60c60a6fc1a8189b6856b583"},{"y":525,"x":960,"u":"https://preview.redd.it/dy46sdb5x9af1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=a94db803c6a9a6e9b3099755c12c821829eb5e08"},{"y":590,"x":1080,"u":"https://preview.redd.it/dy46sdb5x9af1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=753d780417d6c416a50c00ff1d065269fb808a76"}],"s":{"y":1890,"x":3456,"u":"https://preview.redd.it/dy46sdb5x9af1.png?width=3456&amp;format=png&amp;auto=webp&amp;s=afe062e1e653c8467fb97741a2b2591a467e2c3d"},"id":"dy46sdb5x9af1"}},"name":"t3_1lp37v0","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.9,"author_flair_background_color":null,"subreddit_type":"public","ups":15,"total_awards_received":0,"media_embed":{},"thumbnail_width":140,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_im30t","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Discussion","can_mod_post":false,"score":15,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"https://b.thumbs.redditmedia.com/oh8UHfFWv9AwkzOmulxotDC0dlTYauybGEDMEiEkogE.jpg","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1751380368,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;&lt;a href=\\"https://preview.redd.it/sye5ssnxv9af1.png?width=3456&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5d312de9207cf7cd5edd21029849b10a3b23bbb5\\"&gt;https://preview.redd.it/sye5ssnxv9af1.png?width=3456&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=5d312de9207cf7cd5edd21029849b10a3b23bbb5&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://preview.redd.it/dy46sdb5x9af1.png?width=3456&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=afe062e1e653c8467fb97741a2b2591a467e2c3d\\"&gt;https://preview.redd.it/dy46sdb5x9af1.png?width=3456&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=afe062e1e653c8467fb97741a2b2591a467e2c3d&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;I successfully ran LoRA training on an NVIDIA Jetson AGX Orin 64GB. Both 8-bit and FP16 modes are working. I&amp;#39;m currently training the Qwen 2.5 7B model. Although the process is slow, it&amp;#39;s sufficient for my needs since there&amp;#39;s no urgency.&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"5f921ea4-c7bc-11ed-9c23-3a00622979b4","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#646d73","id":"1lp37v0","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"ahstanin","discussion_type":null,"num_comments":7,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lp37v0/lora_training_on_nvidia_jetson_agx_orin_64gb/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lp37v0/lora_training_on_nvidia_jetson_agx_orin_64gb/","subreddit_subscribers":493458,"created_utc":1751380368,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0tmk7v","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"IrisColt","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0tmc2b","score":2,"author_fullname":"t2_c2f558x","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Thanks for the insight!","edited":false,"author_flair_css_class":null,"name":"t1_n0tmk7v","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thanks for the insight!&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lp37v0","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp37v0/lora_training_on_nvidia_jetson_agx_orin_64gb/n0tmk7v/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751401406,"author_flair_text":null,"collapsed":false,"created_utc":1751401406,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n0tmc2b","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ahstanin","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0tluo6","score":2,"author_fullname":"t2_im30t","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I would say 6x slower than my regular training on the H200 GPU but pretty close with the RTX 3090.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0tmc2b","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I would say 6x slower than my regular training on the H200 GPU but pretty close with the RTX 3090.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lp37v0","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp37v0/lora_training_on_nvidia_jetson_agx_orin_64gb/n0tmc2b/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751401342,"author_flair_text":null,"treatment_tags":[],"created_utc":1751401342,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n0tluo6","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"IrisColt","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0s58if","score":1,"author_fullname":"t2_c2f558x","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Is that fast or slow? Genuinely interested.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0tluo6","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Is that fast or slow? Genuinely interested.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lp37v0","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp37v0/lora_training_on_nvidia_jetson_agx_orin_64gb/n0tluo6/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751401208,"author_flair_text":null,"treatment_tags":[],"created_utc":1751401208,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0s58if","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ahstanin","can_mod_post":false,"created_utc":1751386459,"send_replies":true,"parent_id":"t1_n0rxisq","score":3,"author_fullname":"t2_im30t","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I used a dataset of 1000 conversations, and each conversation has around 1200 tokens.  \\nOne adapter training took 2 hours and 30 minutes on learning rates \\\\`1e-5\\\\` and \\\\`5e-6\\\\`. Also had \\\\`max\\\\_seq\\\\_length 4096\\\\`","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0s58if","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I used a dataset of 1000 conversations, and each conversation has around 1200 tokens.&lt;br/&gt;\\nOne adapter training took 2 hours and 30 minutes on learning rates `1e-5` and `5e-6`. Also had `max_seq_length 4096`&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lp37v0","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp37v0/lora_training_on_nvidia_jetson_agx_orin_64gb/n0s58if/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751386459,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n0rxisq","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MKU64","can_mod_post":false,"created_utc":1751384322,"send_replies":true,"parent_id":"t3_1lp37v0","score":3,"author_fullname":"t2_wn7it","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Awesome! How much time did it took you and did you do it with a dataset of lots of tokens?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0rxisq","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Awesome! How much time did it took you and did you do it with a dataset of lots of tokens?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp37v0/lora_training_on_nvidia_jetson_agx_orin_64gb/n0rxisq/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751384322,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lp37v0","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0t9n20","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ahstanin","can_mod_post":false,"created_utc":1751397679,"send_replies":true,"parent_id":"t1_n0t2e7c","score":1,"author_fullname":"t2_im30t","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I have one RTX 3090 and one RTX 5090, but using the Jetson because I don\'t have any other use for this at this moment.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0t9n20","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I have one RTX 3090 and one RTX 5090, but using the Jetson because I don&amp;#39;t have any other use for this at this moment.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lp37v0","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp37v0/lora_training_on_nvidia_jetson_agx_orin_64gb/n0t9n20/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751397679,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0t2e7c","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Caffdy","can_mod_post":false,"created_utc":1751395610,"send_replies":true,"parent_id":"t3_1lp37v0","score":1,"author_fullname":"t2_ql2vu0wz","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"are you using the Jetson for LoRA training because of the memory capacity of because you don\'t have a GPU in hand?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0t2e7c","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;are you using the Jetson for LoRA training because of the memory capacity of because you don&amp;#39;t have a GPU in hand?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp37v0/lora_training_on_nvidia_jetson_agx_orin_64gb/n0t2e7c/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751395610,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lp37v0","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]'),s=()=>e.jsx(a,{data:l});export{s as default};
