import{j as e}from"./index-M4edQi1P.js";import{R as l}from"./RedditPostRenderer-CESBGIIy.js";import"./index-DFpL1mt4.js";const t=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Investigating this idea myself, and noting it down. Thought I'd post it as a discussion in case people have roasts/suggestions before I revisit it. I'll research all this myself but if anyone wants to criticize or correct me, that would be welcome\\n\\nCould be done on any platform that has plug and play for Node.js?\\n\\nIs the cost of Microsoft or Amazon cloud hosted LLMs cheaper than this idea?\\n\\nMy big hangup on AI based APIs is tying it to yet another API account with or without spending limits. So far, I've hosted open source llama and gemma locally, but I haven't done anything networking with it. I've configured many a VPS but haven't done any AI based APIs.","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"What do you think of self-hosting a small LLM on a VPS or abstracted container, calling it externally for simple AI agents/API calls? Cheaper or more expensive than bigger models?","link_flair_richtext":[{"e":"text","t":"Discussion"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1m5djms","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.67,"author_flair_background_color":null,"subreddit_type":"public","ups":1,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_jzw36c","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Discussion","can_mod_post":false,"score":1,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1753087056,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Investigating this idea myself, and noting it down. Thought I&amp;#39;d post it as a discussion in case people have roasts/suggestions before I revisit it. I&amp;#39;ll research all this myself but if anyone wants to criticize or correct me, that would be welcome&lt;/p&gt;\\n\\n&lt;p&gt;Could be done on any platform that has plug and play for Node.js?&lt;/p&gt;\\n\\n&lt;p&gt;Is the cost of Microsoft or Amazon cloud hosted LLMs cheaper than this idea?&lt;/p&gt;\\n\\n&lt;p&gt;My big hangup on AI based APIs is tying it to yet another API account with or without spending limits. So far, I&amp;#39;ve hosted open source llama and gemma locally, but I haven&amp;#39;t done anything networking with it. I&amp;#39;ve configured many a VPS but haven&amp;#39;t done any AI based APIs.&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":true,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"5f921ea4-c7bc-11ed-9c23-3a00622979b4","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#646d73","id":"1m5djms","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"angry_cactus","discussion_type":null,"num_comments":7,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1m5djms/what_do_you_think_of_selfhosting_a_small_llm_on_a/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1m5djms/what_do_you_think_of_selfhosting_a_small_llm_on_a/","subreddit_subscribers":502516,"created_utc":1753087056,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4b4cfj","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"mtmttuan","can_mod_post":false,"created_utc":1753087451,"send_replies":true,"parent_id":"t3_1m5djms","score":2,"author_fullname":"t2_6mjqz0at","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"1. It's smaller model hence worse performance.\\n2. Typically you would want a GPU, and GPUs aren't cheap. Unless you constantly push it to the limit, I doubt that your solution would be cheaper than API based.\\n3. Well chances are your self hosted model will be much slower than many big API models, even when you have a GPU. On CPU (VPS CPU hence not have the luxury of multi channels RAM) inference speed will be so slow that you might just give up.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4b4cfj","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;ol&gt;\\n&lt;li&gt;It&amp;#39;s smaller model hence worse performance.&lt;/li&gt;\\n&lt;li&gt;Typically you would want a GPU, and GPUs aren&amp;#39;t cheap. Unless you constantly push it to the limit, I doubt that your solution would be cheaper than API based.&lt;/li&gt;\\n&lt;li&gt;Well chances are your self hosted model will be much slower than many big API models, even when you have a GPU. On CPU (VPS CPU hence not have the luxury of multi channels RAM) inference speed will be so slow that you might just give up.&lt;/li&gt;\\n&lt;/ol&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5djms/what_do_you_think_of_selfhosting_a_small_llm_on_a/n4b4cfj/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753087451,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m5djms","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4bgtzh","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DepthHour1669","can_mod_post":false,"created_utc":1753094418,"send_replies":true,"parent_id":"t1_n4b5nk3","score":1,"author_fullname":"t2_t6glzswk","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yep, this is the real correct answer. It makes sense to host your own server for 2 reasons:\\n\\n- Privacy\\n- Processing a LOT of tokens per hour. \\n\\nIf you're doing just a thousand queries per month, don't bother with your own server. Just use [openrouter free models](https://openrouter.ai/models/?q=free&amp;order=top-weekly) like everyone else.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4bgtzh","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yep, this is the real correct answer. It makes sense to host your own server for 2 reasons:&lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;Privacy&lt;/li&gt;\\n&lt;li&gt;Processing a LOT of tokens per hour. &lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;p&gt;If you&amp;#39;re doing just a thousand queries per month, don&amp;#39;t bother with your own server. Just use &lt;a href=\\"https://openrouter.ai/models/?q=free&amp;amp;order=top-weekly\\"&gt;openrouter free models&lt;/a&gt; like everyone else.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m5djms","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5djms/what_do_you_think_of_selfhosting_a_small_llm_on_a/n4bgtzh/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753094418,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4b5nk3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Round_Mixture_7541","can_mod_post":false,"created_utc":1753088220,"send_replies":true,"parent_id":"t3_1m5djms","score":2,"author_fullname":"t2_114cnblv7x","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Depends on your purpose. I'm e.g. burning ~40M tokens/hour, for me having to pay per token wouldn't make much sense.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4b5nk3","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Depends on your purpose. I&amp;#39;m e.g. burning ~40M tokens/hour, for me having to pay per token wouldn&amp;#39;t make much sense.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5djms/what_do_you_think_of_selfhosting_a_small_llm_on_a/n4b5nk3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753088220,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m5djms","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4b4tlo","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"spaceman_","can_mod_post":false,"created_utc":1753087732,"send_replies":true,"parent_id":"t3_1m5djms","score":1,"author_fullname":"t2_9neub","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Depends on how fast it has to be and how small the model can be for your application.\\n\\n\\nMost realistic use cases will be better off benefiting from the economy of scale from larger inference api providers. ","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4b4tlo","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Depends on how fast it has to be and how small the model can be for your application.&lt;/p&gt;\\n\\n&lt;p&gt;Most realistic use cases will be better off benefiting from the economy of scale from larger inference api providers. &lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5djms/what_do_you_think_of_selfhosting_a_small_llm_on_a/n4b4tlo/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753087732,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m5djms","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4bgkz9","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DepthHour1669","can_mod_post":false,"created_utc":1753094290,"send_replies":true,"parent_id":"t1_n4bebj6","score":1,"author_fullname":"t2_t6glzswk","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I’ve tried it. You get 4 cpu cores and 24gb ram.\\n\\nIt’s a great free VPS but a shitty AI server. Don’t bother. It is possible to run Qwen3-30b Q4 on it, but that’s the only model that’d I’d recommend running. And it’s still very slow.\\n\\nDon't bother using that for AI. Instead, just use openrouter free tier: \\nhttps://openrouter.ai/models/?q=free&amp;order=top-weekly","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4bgkz9","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I’ve tried it. You get 4 cpu cores and 24gb ram.&lt;/p&gt;\\n\\n&lt;p&gt;It’s a great free VPS but a shitty AI server. Don’t bother. It is possible to run Qwen3-30b Q4 on it, but that’s the only model that’d I’d recommend running. And it’s still very slow.&lt;/p&gt;\\n\\n&lt;p&gt;Don&amp;#39;t bother using that for AI. Instead, just use openrouter free tier: \\n&lt;a href=\\"https://openrouter.ai/models/?q=free&amp;amp;order=top-weekly\\"&gt;https://openrouter.ai/models/?q=free&amp;amp;order=top-weekly&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m5djms","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5djms/what_do_you_think_of_selfhosting_a_small_llm_on_a/n4bgkz9/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753094290,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4bebj6","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DirectCurrent_","can_mod_post":false,"created_utc":1753093129,"send_replies":true,"parent_id":"t3_1m5djms","score":1,"author_fullname":"t2_vl1u9ir8","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"If you can get a slot, you can get a free oracle vps that has compute for free. I haven’t tried it yet, but they have it on their “forever free” tier:\\n\\nOCI Ampere A1 Compute instances (Arm processor): All tenancies get the first 3,000 OCPU hours and 18,000 GB hours per month for free for VM instances using the VM.Standard.A1.Flex shape, which has an Arm processor. For Always Free tenancies, this is equivalent to 4 OCPUs and 24 GB of memory.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4bebj6","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;If you can get a slot, you can get a free oracle vps that has compute for free. I haven’t tried it yet, but they have it on their “forever free” tier:&lt;/p&gt;\\n\\n&lt;p&gt;OCI Ampere A1 Compute instances (Arm processor): All tenancies get the first 3,000 OCPU hours and 18,000 GB hours per month for free for VM instances using the VM.Standard.A1.Flex shape, which has an Arm processor. For Always Free tenancies, this is equivalent to 4 OCPUs and 24 GB of memory.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5djms/what_do_you_think_of_selfhosting_a_small_llm_on_a/n4bebj6/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753093129,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m5djms","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4bgnq5","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DepthHour1669","can_mod_post":false,"created_utc":1753094329,"send_replies":true,"parent_id":"t3_1m5djms","score":1,"author_fullname":"t2_t6glzswk","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"It’s much cheaper to use the OpenAI api instead of running your own server, unless you’re doing millions of tokens all the time.\\n\\nWhat you’re saying is… in theory possible, but on the cheap side it’s slow and bad quality (you can run a free VPS that does Q4 Qwen3 30b at like 1 token/sec). But Q4 Qwen3 30b is not smart. \\n\\nIf you want to run something smart, like Qwen3-235b, or Hunyuan A13b, or Deepseek R1, you will need to spend a LOT of money per hour on VPS.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4bgnq5","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It’s much cheaper to use the OpenAI api instead of running your own server, unless you’re doing millions of tokens all the time.&lt;/p&gt;\\n\\n&lt;p&gt;What you’re saying is… in theory possible, but on the cheap side it’s slow and bad quality (you can run a free VPS that does Q4 Qwen3 30b at like 1 token/sec). But Q4 Qwen3 30b is not smart. &lt;/p&gt;\\n\\n&lt;p&gt;If you want to run something smart, like Qwen3-235b, or Hunyuan A13b, or Deepseek R1, you will need to spend a LOT of money per hour on VPS.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5djms/what_do_you_think_of_selfhosting_a_small_llm_on_a/n4bgnq5/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753094329,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m5djms","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),r=()=>e.jsx(l,{data:t});export{r as default};
