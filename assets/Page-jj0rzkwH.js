import{j as e}from"./index-DACS7Nh6.js";import{R as l}from"./RedditPostRenderer-Dqa1NZuX.js";import"./index-DiMIVQx4.js";const a=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"TLDR: for a small overhead of additional trained parameters, you can get 2.5-5x more tokens per second.","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"A new paper from Apple shows you can tack on Multi-Token Prediction to any LLM with no loss in quality","link_flair_richtext":[{"e":"text","t":"News"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1m3vqom","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.97,"author_flair_background_color":null,"subreddit_type":"public","ups":343,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_7kg5p","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"News","can_mod_post":false,"score":343,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"default","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":false,"mod_note":null,"created":1752930223,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"arxiv.org","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;TLDR: for a small overhead of additional trained parameters, you can get 2.5-5x more tokens per second.&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"url_overridden_by_dest":"https://arxiv.org/abs/2507.11851","view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#cc3600","id":"1m3vqom","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"Kooshi_Govno","discussion_type":null,"num_comments":27,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1m3vqom/a_new_paper_from_apple_shows_you_can_tack_on/","stickied":false,"url":"https://arxiv.org/abs/2507.11851","subreddit_subscribers":501526,"created_utc":1752930223,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n40hfvh","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"LagOps91","can_mod_post":false,"send_replies":true,"parent_id":"t1_n40e0ai","score":1,"author_fullname":"t2_3wi6j7vwh","approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yeah that makes no sense to me either.","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n40hfvh","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yeah that makes no sense to me either.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1m3vqom","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3vqom/a_new_paper_from_apple_shows_you_can_tack_on/n40hfvh/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752940234,"author_flair_text":null,"treatment_tags":[],"created_utc":1752940234,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n40e0ai","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Kooshi_Govno","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3zv0go","score":4,"author_fullname":"t2_7kg5p","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"That's fair. Yeah in that case... why the hell isn't it available?","edited":false,"author_flair_css_class":null,"name":"t1_n40e0ai","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;That&amp;#39;s fair. Yeah in that case... why the hell isn&amp;#39;t it available?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1m3vqom","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3vqom/a_new_paper_from_apple_shows_you_can_tack_on/n40e0ai/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752939179,"author_flair_text":null,"collapsed":false,"created_utc":1752939179,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}}],"before":null}},"user_reports":[],"saved":false,"id":"n3zv0go","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"LagOps91","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3zqa18","score":7,"author_fullname":"t2_3wi6j7vwh","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"it would be the same quality for inference. it's effectively a built in draft model. if the prediction is wrong / not confirmed by the full model, it gets rejected.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3zv0go","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;it would be the same quality for inference. it&amp;#39;s effectively a built in draft model. if the prediction is wrong / not confirmed by the full model, it gets rejected.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m3vqom","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3vqom/a_new_paper_from_apple_shows_you_can_tack_on/n3zv0go/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752933138,"author_flair_text":null,"treatment_tags":[],"created_utc":1752933138,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":7}}],"before":null}},"user_reports":[],"saved":false,"id":"n3zqa18","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Kooshi_Govno","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3zoqpk","score":15,"author_fullname":"t2_7kg5p","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Agreed, though their implementation was kindof odd. It only used minimal parameters at the very end of the model for the extra tokens, so there was quality loss. It makes sense that it would create some better gradients for training, but they wanted maximum quality for inference.\\n\\nApple's strategy includes some self-correction and seems to use more of the internal state of the model to pull out better predictions.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3zqa18","is_submitter":true,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Agreed, though their implementation was kindof odd. It only used minimal parameters at the very end of the model for the extra tokens, so there was quality loss. It makes sense that it would create some better gradients for training, but they wanted maximum quality for inference.&lt;/p&gt;\\n\\n&lt;p&gt;Apple&amp;#39;s strategy includes some self-correction and seems to use more of the internal state of the model to pull out better predictions.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m3vqom","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3vqom/a_new_paper_from_apple_shows_you_can_tack_on/n3zqa18/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752931471,"author_flair_text":null,"treatment_tags":[],"created_utc":1752931471,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":15}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n42ilhf","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"InsideYork","can_mod_post":false,"send_replies":true,"parent_id":"t1_n41fd6m","score":1,"author_fullname":"t2_12s3hn4y0b","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Activate it and see. Maybe you can use both for even faster tokens","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n42ilhf","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Activate it and see. Maybe you can use both for even faster tokens&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1m3vqom","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3vqom/a_new_paper_from_apple_shows_you_can_tack_on/n42ilhf/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752963662,"author_flair_text":null,"treatment_tags":[],"created_utc":1752963662,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":6,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n41fd6m","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"LagOps91","can_mod_post":false,"send_replies":true,"parent_id":"t1_n41e5j0","score":3,"author_fullname":"t2_3wi6j7vwh","approved_by":null,"mod_note":null,"all_awardings":[],"body":"80% is quite high and i'm confident it gives a speedup. a builtin draft model will be more accurate than using an external draft model. speculative decoding was also not great for me, but here it should work much better.","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n41fd6m","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;80% is quite high and i&amp;#39;m confident it gives a speedup. a builtin draft model will be more accurate than using an external draft model. speculative decoding was also not great for me, but here it should work much better.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1m3vqom","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3vqom/a_new_paper_from_apple_shows_you_can_tack_on/n41fd6m/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752950812,"author_flair_text":null,"treatment_tags":[],"created_utc":1752950812,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n41e5j0","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"InsideYork","can_mod_post":false,"send_replies":true,"parent_id":"t1_n413tkw","score":2,"author_fullname":"t2_12s3hn4y0b","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Multi token prediction in this paper is lossless. 80% accuracy is terrible especially if it needs to be corrected. Speculative drafts have slowed down my models when the accuracy is low.","edited":false,"author_flair_css_class":null,"name":"t1_n41e5j0","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Multi token prediction in this paper is lossless. 80% accuracy is terrible especially if it needs to be corrected. Speculative drafts have slowed down my models when the accuracy is low.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1m3vqom","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3vqom/a_new_paper_from_apple_shows_you_can_tack_on/n41e5j0/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752950419,"author_flair_text":null,"collapsed":false,"created_utc":1752950419,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n413tkw","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"LagOps91","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4114lx","score":2,"author_fullname":"t2_3wi6j7vwh","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"no, their paper said it has about 80% prediction accuracy. that's pretty damned good.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n413tkw","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;no, their paper said it has about 80% prediction accuracy. that&amp;#39;s pretty damned good.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m3vqom","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3vqom/a_new_paper_from_apple_shows_you_can_tack_on/n413tkw/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752947147,"author_flair_text":null,"treatment_tags":[],"created_utc":1752947147,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n4114lx","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"InsideYork","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3zoqpk","score":2,"author_fullname":"t2_12s3hn4y0b","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Their implementation is probably different. It sometimes writes in Chinese, so it may have been more error prone than worth using.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n4114lx","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Their implementation is probably different. It sometimes writes in Chinese, so it may have been more error prone than worth using.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m3vqom","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3vqom/a_new_paper_from_apple_shows_you_can_tack_on/n4114lx/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752946336,"author_flair_text":null,"treatment_tags":[],"created_utc":1752946336,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n3zoqpk","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"LagOps91","can_mod_post":false,"created_utc":1752930908,"send_replies":true,"parent_id":"t1_n3zoc02","score":36,"author_fullname":"t2_3wi6j7vwh","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"on that note, it has always bugged me that V3/R1 come with multi-token prediction, but apparently it was only meant for training purposes... but why tho? isn't it effectively a free speed gain?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3zoqpk","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;on that note, it has always bugged me that V3/R1 come with multi-token prediction, but apparently it was only meant for training purposes... but why tho? isn&amp;#39;t it effectively a free speed gain?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m3vqom","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3vqom/a_new_paper_from_apple_shows_you_can_tack_on/n3zoqpk/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752930908,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":36}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n42wfkq","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"squired","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4153qs","score":1,"author_fullname":"t2_3k4yf","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I think we can legitimate do it. I was playing with this a few months ago with Wan2.1 and the quant didn't matter much (for generation, not training). I have a couple projects I'm still wading through, but if you take a look at it before I circle back, please feel free to dm me, for emotional support if nothing else! I'm going to need a mathematician to make an attempt, if you happen to know one? I understand the processes involved and can do the coding, but my stochastic calculus is *very* weak.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n42wfkq","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I think we can legitimate do it. I was playing with this a few months ago with Wan2.1 and the quant didn&amp;#39;t matter much (for generation, not training). I have a couple projects I&amp;#39;m still wading through, but if you take a look at it before I circle back, please feel free to dm me, for emotional support if nothing else! I&amp;#39;m going to need a mathematician to make an attempt, if you happen to know one? I understand the processes involved and can do the coding, but my stochastic calculus is &lt;em&gt;very&lt;/em&gt; weak.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m3vqom","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3vqom/a_new_paper_from_apple_shows_you_can_tack_on/n42wfkq/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752968545,"author_flair_text":null,"treatment_tags":[],"created_utc":1752968545,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4153qs","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Expensive-Apricot-25","can_mod_post":false,"created_utc":1752947536,"send_replies":true,"parent_id":"t1_n3zoc02","score":1,"author_fullname":"t2_idqkwio0","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"it seems very promising, but I have a feeling it will be sensitive to quantization","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4153qs","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;it seems very promising, but I have a feeling it will be sensitive to quantization&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m3vqom","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3vqom/a_new_paper_from_apple_shows_you_can_tack_on/n4153qs/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752947536,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3zoc02","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"LagOps91","can_mod_post":false,"created_utc":1752930760,"send_replies":true,"parent_id":"t3_1m3vqom","score":93,"author_fullname":"t2_3wi6j7vwh","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"that sounds amazing! i really hope something like that can become the standard or that there is a way to for a community-made tool to add it to models and train it. a speed increase like that can turn \\"too slow to be useable\\" into \\"works fine for me\\" for a lot of larger models with gpu/cpu hybrid inference.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3zoc02","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;that sounds amazing! i really hope something like that can become the standard or that there is a way to for a community-made tool to add it to models and train it. a speed increase like that can turn &amp;quot;too slow to be useable&amp;quot; into &amp;quot;works fine for me&amp;quot; for a lot of larger models with gpu/cpu hybrid inference.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3vqom/a_new_paper_from_apple_shows_you_can_tack_on/n3zoc02/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752930760,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m3vqom","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":93}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3ztkw1","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Chromix_","can_mod_post":false,"created_utc":1752932644,"send_replies":true,"parent_id":"t3_1m3vqom","score":25,"author_fullname":"t2_k7w2h","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Yes please!\\n\\nIt should be easy to add support for this for those who train the model. Yet it can also be added afterwards, you \\"just\\" need 50k SFT iterations on 8x A100 GPUs to make it possible.\\n\\nA decent speedup can be achieved with less than 1% memory overhead at inference time - so it's basically free. Going for higher memory overhead like 5% comes with greatly diminishing returns - not worth it.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3ztkw1","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yes please!&lt;/p&gt;\\n\\n&lt;p&gt;It should be easy to add support for this for those who train the model. Yet it can also be added afterwards, you &amp;quot;just&amp;quot; need 50k SFT iterations on 8x A100 GPUs to make it possible.&lt;/p&gt;\\n\\n&lt;p&gt;A decent speedup can be achieved with less than 1% memory overhead at inference time - so it&amp;#39;s basically free. Going for higher memory overhead like 5% comes with greatly diminishing returns - not worth it.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3vqom/a_new_paper_from_apple_shows_you_can_tack_on/n3ztkw1/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752932644,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m3vqom","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":25}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n40g9dr","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Kooshi_Govno","can_mod_post":false,"created_utc":1752939868,"send_replies":true,"parent_id":"t1_n40a5lu","score":1,"author_fullname":"t2_7kg5p","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"EAGLE-3 looks very impressive. I guess it's a matter of which technique is easiest to train and has the lowest RAM overhead when considering what gets adopted at the consumer level.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n40g9dr","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;EAGLE-3 looks very impressive. I guess it&amp;#39;s a matter of which technique is easiest to train and has the lowest RAM overhead when considering what gets adopted at the consumer level.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m3vqom","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3vqom/a_new_paper_from_apple_shows_you_can_tack_on/n40g9dr/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752939868,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n40a5lu","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"FullstackSensei","can_mod_post":false,"created_utc":1752937991,"send_replies":true,"parent_id":"t3_1m3vqom","score":18,"author_fullname":"t2_17n3nqtj56","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Multi-token generation has been explored quite a bit over the past couple of years with several published implementations like [EAGLE (now in V3)](https://github.com/SafeAILab/EAGLE), [Medusa](https://github.com/FasterDecoding/Medusa) and [Hydra](https://github.com/zankner/Hydra), to name a few.\\n\\nThe challenge with most of these approaches is collecting a representative data set to perform the tuning required for multiple token prediction. Maybe somebody like the Unsloth team can do it using the same dataset they use in their dynamic quant 2.0?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n40a5lu","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Multi-token generation has been explored quite a bit over the past couple of years with several published implementations like &lt;a href=\\"https://github.com/SafeAILab/EAGLE\\"&gt;EAGLE (now in V3)&lt;/a&gt;, &lt;a href=\\"https://github.com/FasterDecoding/Medusa\\"&gt;Medusa&lt;/a&gt; and &lt;a href=\\"https://github.com/zankner/Hydra\\"&gt;Hydra&lt;/a&gt;, to name a few.&lt;/p&gt;\\n\\n&lt;p&gt;The challenge with most of these approaches is collecting a representative data set to perform the tuning required for multiple token prediction. Maybe somebody like the Unsloth team can do it using the same dataset they use in their dynamic quant 2.0?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3vqom/a_new_paper_from_apple_shows_you_can_tack_on/n40a5lu/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752937991,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m3vqom","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":18}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3zumn3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"AltruisticList6000","can_mod_post":false,"created_utc":1752933008,"send_replies":true,"parent_id":"t3_1m3vqom","score":17,"author_fullname":"t2_hnjq9xn4a","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"That would be interesting if it translates to RAM performance too. So a bigger 32b+ model shared between VRAM and RAM (for example 16gb VRAM) that would normally generate only 4-6t/s could do 15-18t/s or even more with this making the generation speed very good and usable. It would make larger models way more usable on low VRAM. It is very exciting.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3zumn3","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;That would be interesting if it translates to RAM performance too. So a bigger 32b+ model shared between VRAM and RAM (for example 16gb VRAM) that would normally generate only 4-6t/s could do 15-18t/s or even more with this making the generation speed very good and usable. It would make larger models way more usable on low VRAM. It is very exciting.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3vqom/a_new_paper_from_apple_shows_you_can_tack_on/n3zumn3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752933008,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m3vqom","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":17}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n40e1j7","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"fullouterjoin","can_mod_post":false,"created_utc":1752939190,"send_replies":true,"parent_id":"t1_n407cue","score":5,"author_fullname":"t2_406sj","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Speed up means more tps or less Wh/token. Apple did this for the higher token rate and the battery (and data center) power saved.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n40e1j7","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Speed up means more tps or less Wh/token. Apple did this for the higher token rate and the battery (and data center) power saved.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m3vqom","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3vqom/a_new_paper_from_apple_shows_you_can_tack_on/n40e1j7/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752939190,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}}],"before":null}},"user_reports":[],"saved":false,"id":"n407cue","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"ArchdukeofHyperbole","can_mod_post":false,"created_utc":1752937123,"send_replies":true,"parent_id":"t3_1m3vqom","score":9,"author_fullname":"t2_1p41v97q5d","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"2.5-5 times speedup sounds great. \\n\\nLlama 70B on my pc would go from 0.2 tps to like 0.5-1 tps, still not great. \\n\\nMistral 24B would go from 2tps to 5-10 tps, very useable for me. \\n\\nPossibly qwen3 30B would go from 10 tps to 25-50 which is more like the speed I get when fully offloading an 8B model. If I'm understanding it anyway, this sounds really awesome. \\n\\nOh, and I guess fully offloaded 8B model would go from about 30tps to 75-150 tps 🫨","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n407cue","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;2.5-5 times speedup sounds great. &lt;/p&gt;\\n\\n&lt;p&gt;Llama 70B on my pc would go from 0.2 tps to like 0.5-1 tps, still not great. &lt;/p&gt;\\n\\n&lt;p&gt;Mistral 24B would go from 2tps to 5-10 tps, very useable for me. &lt;/p&gt;\\n\\n&lt;p&gt;Possibly qwen3 30B would go from 10 tps to 25-50 which is more like the speed I get when fully offloading an 8B model. If I&amp;#39;m understanding it anyway, this sounds really awesome. &lt;/p&gt;\\n\\n&lt;p&gt;Oh, and I guess fully offloaded 8B model would go from about 30tps to 75-150 tps 🫨&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3vqom/a_new_paper_from_apple_shows_you_can_tack_on/n407cue/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752937123,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m3vqom","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":9}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n409xtf","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"popecostea","can_mod_post":false,"created_utc":1752937925,"send_replies":true,"parent_id":"t1_n4085em","score":17,"author_fullname":"t2_a8xhk1ib","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"My understanding is that this can be done post-training for any model, by adding a little something to that model, you don’t require to train a new separate model for the speculative decoding.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n409xtf","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;My understanding is that this can be done post-training for any model, by adding a little something to that model, you don’t require to train a new separate model for the speculative decoding.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m3vqom","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3vqom/a_new_paper_from_apple_shows_you_can_tack_on/n409xtf/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752937925,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":17}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n40cx58","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Kooshi_Govno","can_mod_post":false,"created_utc":1752938843,"send_replies":true,"parent_id":"t1_n4085em","score":9,"author_fullname":"t2_7kg5p","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I wasn't familiar with the details of speculative decoding, so I skimmed this article: https://pytorch.org/blog/hitchhikers-guide-speculative-decoding/\\n\\nIt looks like there are two common ways to do it, one is to use a distilled speculator model, like using a 7B model to speculate for the 70B of the same family.\\n\\nThat's fairly inefficient compared to this apple paper, or the other method mentioned in the article.\\n\\nThe other method mentioned training speculator heads directly on the existing model, which is more efficient and performant. This sounds very similar to this Apple paper, and even found similar speedups of 2x for text and 3x for code.\\n\\nDepending on exactly how those speculator heads are trained, this Apple paper's method could be more user friendly, as the speculator could be distributed similarly to a LoRA, and plug into compatible models.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n40cx58","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I wasn&amp;#39;t familiar with the details of speculative decoding, so I skimmed this article: &lt;a href=\\"https://pytorch.org/blog/hitchhikers-guide-speculative-decoding/\\"&gt;https://pytorch.org/blog/hitchhikers-guide-speculative-decoding/&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;It looks like there are two common ways to do it, one is to use a distilled speculator model, like using a 7B model to speculate for the 70B of the same family.&lt;/p&gt;\\n\\n&lt;p&gt;That&amp;#39;s fairly inefficient compared to this apple paper, or the other method mentioned in the article.&lt;/p&gt;\\n\\n&lt;p&gt;The other method mentioned training speculator heads directly on the existing model, which is more efficient and performant. This sounds very similar to this Apple paper, and even found similar speedups of 2x for text and 3x for code.&lt;/p&gt;\\n\\n&lt;p&gt;Depending on exactly how those speculator heads are trained, this Apple paper&amp;#39;s method could be more user friendly, as the speculator could be distributed similarly to a LoRA, and plug into compatible models.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m3vqom","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3vqom/a_new_paper_from_apple_shows_you_can_tack_on/n40cx58/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752938843,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":9}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n41508r","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Expensive-Apricot-25","can_mod_post":false,"created_utc":1752947507,"send_replies":true,"parent_id":"t1_n4085em","score":3,"author_fullname":"t2_idqkwio0","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"less overhead.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n41508r","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;less overhead.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m3vqom","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3vqom/a_new_paper_from_apple_shows_you_can_tack_on/n41508r/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752947507,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n42r1hx","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"towelpluswater","can_mod_post":false,"created_utc":1752966637,"send_replies":true,"parent_id":"t1_n4085em","score":2,"author_fullname":"t2_hpc5s","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Way more accurate in theory because it was trained with a mask target to optimize during training. You can’t retrofit it to any old model (at least not from the way the paper’s authors implemented that I saw). Makes sense though. Especially for Apple with on device models. Also doesn’t need a separate draft model which also increases accuracy since same model. Differs from Eagle in that it’s not using random SFT of a prediction head.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n42r1hx","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Way more accurate in theory because it was trained with a mask target to optimize during training. You can’t retrofit it to any old model (at least not from the way the paper’s authors implemented that I saw). Makes sense though. Especially for Apple with on device models. Also doesn’t need a separate draft model which also increases accuracy since same model. Differs from Eagle in that it’s not using random SFT of a prediction head.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m3vqom","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3vqom/a_new_paper_from_apple_shows_you_can_tack_on/n42r1hx/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752966637,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n42un3r","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"milesper","can_mod_post":false,"created_utc":1752967907,"send_replies":true,"parent_id":"t1_n4085em","score":2,"author_fullname":"t2_n6mxv","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Well yeah it’s a totally different mechanism?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n42un3r","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Well yeah it’s a totally different mechanism?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m3vqom","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3vqom/a_new_paper_from_apple_shows_you_can_tack_on/n42un3r/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752967907,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n4085em","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"MrKingold","can_mod_post":false,"created_utc":1752937367,"send_replies":true,"parent_id":"t3_1m3vqom","score":10,"author_fullname":"t2_p8xr7r6g0","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Is there any difference between this and speculative decoding, which has been with us, I don't know, may be since 2023?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4085em","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Is there any difference between this and speculative decoding, which has been with us, I don&amp;#39;t know, may be since 2023?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3vqom/a_new_paper_from_apple_shows_you_can_tack_on/n4085em/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752937367,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m3vqom","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":10}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n43b1if","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Kooshi_Govno","can_mod_post":false,"created_utc":1752973997,"send_replies":true,"parent_id":"t1_n42vnh4","score":1,"author_fullname":"t2_7kg5p","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Brother you sound like a 1B LLM on meth. You doing ok?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n43b1if","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Brother you sound like a 1B LLM on meth. You doing ok?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m3vqom","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3vqom/a_new_paper_from_apple_shows_you_can_tack_on/n43b1if/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752973997,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n42vnh4","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"squired","can_mod_post":false,"created_utc":1752968266,"send_replies":true,"parent_id":"t3_1m3vqom","score":0,"author_fullname":"t2_3k4yf","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Pardon my language, but we motherfucking called it!!!!\\n\\nI was working with a dude several months ago on effectively this, leveraging early coherence to map and/or divine later steps/tokens for Wan2.1. Unfortunately, I didn't have the math chops to complete the stochastic calculus and he got caught up with work after becoming discouraged when Veo3 dropped (it was just so damn good!).\\n\\nVery gratifying and assuring that we weren't just crazy and pushing bits for kicks! We were onto something legitimately big!\\n\\n*In a few words: the calculus involved in traversing the latent space allows you to predict the ending at the outset, sort of like graphing out a function to see the \\"big picture\\".* \\n\\nBut we were missing the forest for the trees, just as many here may be as well. We hadn't even considered the parallelism benefits.. Think of normal generation like hiking up a winding mountain path with the goal of taking one picture for every 10 steps. You have to follow the trail, counting your steps along the way, to get your pictures. But if you have a map, you can send 2 or 2000 people out, giving them each one leg to walk. Collectively, every step is still trodden, but all at once, provided enough hikers. Early coherence affords you the map so that you can assign x GPUs to each segment. These are the kind of speed explosions that define breakthroughs. Big deal!! And if Apple is publishing this, it means the other houses already have it. Veo3 makes a lot more sense now as does Gemini's context window.\\n\\nIf any other AI tourists are reading this, keep banging that code ya'll! Here we go!!","edited":1752969084,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n42vnh4","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Pardon my language, but we motherfucking called it!!!!&lt;/p&gt;\\n\\n&lt;p&gt;I was working with a dude several months ago on effectively this, leveraging early coherence to map and/or divine later steps/tokens for Wan2.1. Unfortunately, I didn&amp;#39;t have the math chops to complete the stochastic calculus and he got caught up with work after becoming discouraged when Veo3 dropped (it was just so damn good!).&lt;/p&gt;\\n\\n&lt;p&gt;Very gratifying and assuring that we weren&amp;#39;t just crazy and pushing bits for kicks! We were onto something legitimately big!&lt;/p&gt;\\n\\n&lt;p&gt;&lt;em&gt;In a few words: the calculus involved in traversing the latent space allows you to predict the ending at the outset, sort of like graphing out a function to see the &amp;quot;big picture&amp;quot;.&lt;/em&gt; &lt;/p&gt;\\n\\n&lt;p&gt;But we were missing the forest for the trees, just as many here may be as well. We hadn&amp;#39;t even considered the parallelism benefits.. Think of normal generation like hiking up a winding mountain path with the goal of taking one picture for every 10 steps. You have to follow the trail, counting your steps along the way, to get your pictures. But if you have a map, you can send 2 or 2000 people out, giving them each one leg to walk. Collectively, every step is still trodden, but all at once, provided enough hikers. Early coherence affords you the map so that you can assign x GPUs to each segment. These are the kind of speed explosions that define breakthroughs. Big deal!! And if Apple is publishing this, it means the other houses already have it. Veo3 makes a lot more sense now as does Gemini&amp;#39;s context window.&lt;/p&gt;\\n\\n&lt;p&gt;If any other AI tourists are reading this, keep banging that code ya&amp;#39;ll! Here we go!!&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3vqom/a_new_paper_from_apple_shows_you_can_tack_on/n42vnh4/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752968266,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m3vqom","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}}]`),n=()=>e.jsx(l,{data:a});export{n as default};
