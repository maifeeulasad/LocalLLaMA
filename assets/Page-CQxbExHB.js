import{j as e}from"./index-Bu7qcPAU.js";import{R as l}from"./RedditPostRenderer-CbHA7O5q.js";import"./index-BKgbfxhf.js";const t=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Hello folks,\\n\\nI'm building a new PC that will also be used for running local LLMs. \\n\\nI would like the possibility of using a decent LLM for programming work. Someone recommended:\\n* buying a motherboard with 2 PCI Express 16x slots \\n* buying 2 \\"cheaper\\" identical 16GB CPUs\\n* splitting the model to run on both of them (for a total of 32GB).\\n\\nHowever, they mentioned 2 caveats:\\n\\n1. Is it hard to do the LLM split on multiple GPUs? Do all models support this?\\n\\n2. Inference would then run on just 1 GPU, computing wise. Would this cause a huge slowdown?\\n\\n3. Apparently a lot of consumer grade motherboards actually don't have enough bandwidth for 2 16x GPUs at the same time and silently downgrade them to 8x each. Do you have recommendations for motherboards which don't do this downgrade (compatible with AMD Ryzen 9 7900X)?","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Motherboard with 2 PCI Express running at full 16x/16x","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1m3y0m8","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.5,"author_flair_background_color":null,"subreddit_type":"public","ups":0,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_9a80o","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":0,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1752936433,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Hello folks,&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;m building a new PC that will also be used for running local LLMs. &lt;/p&gt;\\n\\n&lt;p&gt;I would like the possibility of using a decent LLM for programming work. Someone recommended:\\n* buying a motherboard with 2 PCI Express 16x slots \\n* buying 2 &amp;quot;cheaper&amp;quot; identical 16GB CPUs\\n* splitting the model to run on both of them (for a total of 32GB).&lt;/p&gt;\\n\\n&lt;p&gt;However, they mentioned 2 caveats:&lt;/p&gt;\\n\\n&lt;ol&gt;\\n&lt;li&gt;&lt;p&gt;Is it hard to do the LLM split on multiple GPUs? Do all models support this?&lt;/p&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;p&gt;Inference would then run on just 1 GPU, computing wise. Would this cause a huge slowdown?&lt;/p&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;p&gt;Apparently a lot of consumer grade motherboards actually don&amp;#39;t have enough bandwidth for 2 16x GPUs at the same time and silently downgrade them to 8x each. Do you have recommendations for motherboards which don&amp;#39;t do this downgrade (compatible with AMD Ryzen 9 7900X)?&lt;/p&gt;&lt;/li&gt;\\n&lt;/ol&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":true,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1m3y0m8","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"oblio-","discussion_type":null,"num_comments":24,"send_replies":false,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1m3y0m8/motherboard_with_2_pci_express_running_at_full/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1m3y0m8/motherboard_with_2_pci_express_running_at_full/","subreddit_subscribers":502030,"created_utc":1752936433,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n40kqvf","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ClearApartment2627","can_mod_post":false,"created_utc":1752941266,"send_replies":true,"parent_id":"t1_n407i46","score":3,"author_fullname":"t2_1p0o7y7278","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"The MSI would bei PCIe 4, though. Blackwell cards are PCIe 5 cards, which gives them twice the throughput per lane.\\n4 lanes PCIe 4 are all you need afaik. You can run the model tensor parallel with Exllama or VLLM, then you utilize both GPUs and get roughly twice the performance of one Card.\\nJust look up tensor parallelism.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n40kqvf","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The MSI would bei PCIe 4, though. Blackwell cards are PCIe 5 cards, which gives them twice the throughput per lane.\\n4 lanes PCIe 4 are all you need afaik. You can run the model tensor parallel with Exllama or VLLM, then you utilize both GPUs and get roughly twice the performance of one Card.\\nJust look up tensor parallelism.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m3y0m8","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3y0m8/motherboard_with_2_pci_express_running_at_full/n40kqvf/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752941266,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n417lh7","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"AICatgirls","can_mod_post":false,"send_replies":true,"parent_id":"t1_n40r34n","score":4,"author_fullname":"t2_v5i06lqf","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"You can. The limitation with mining rigs is pcie lanes, which come from the CPU, not the number of pcie slots on the mobo (since we can add splitters if need be).","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n417lh7","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You can. The limitation with mining rigs is pcie lanes, which come from the CPU, not the number of pcie slots on the mobo (since we can add splitters if need be).&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m3y0m8","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3y0m8/motherboard_with_2_pci_express_running_at_full/n417lh7/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752948321,"author_flair_text":null,"treatment_tags":[],"created_utc":1752948321,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"2b12e2b8-fdc0-11ee-9a03-6e2f48afd456","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n45q2ie","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"kryptkpr","can_mod_post":false,"send_replies":true,"parent_id":"t1_n40r34n","score":3,"author_fullname":"t2_30i1a","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I run all my cards x8x8, which is basically no penalty.\\n\\nIf you drop to x4x4x4x4 it hurts around 20%, still mostly ok.\\n\\nx1 works in a pinch but the signal integrity of those risers is very poor, better to spend $30 on the SFF-8611 kit and run x4.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n45q2ie","is_submitter":false,"collapsed":false,"author_flair_richtext":[{"e":"text","t":"Llama 3"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I run all my cards x8x8, which is basically no penalty.&lt;/p&gt;\\n\\n&lt;p&gt;If you drop to x4x4x4x4 it hurts around 20%, still mostly ok.&lt;/p&gt;\\n\\n&lt;p&gt;x1 works in a pinch but the signal integrity of those risers is very poor, better to spend $30 on the SFF-8611 kit and run x4.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m3y0m8","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3y0m8/motherboard_with_2_pci_express_running_at_full/n45q2ie/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753016137,"author_flair_text":"Llama 3","treatment_tags":[],"created_utc":1753016137,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":"#c7b594","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n415xbp","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"smayonak","can_mod_post":false,"send_replies":true,"parent_id":"t1_n40w0d5","score":1,"author_fullname":"t2_3xyf2","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Sorry i asked a weird question. I'd bet it's possible to hook up two GPUs to a single PCIe but there'd be a large performance penalty","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n415xbp","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Sorry i asked a weird question. I&amp;#39;d bet it&amp;#39;s possible to hook up two GPUs to a single PCIe but there&amp;#39;d be a large performance penalty&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m3y0m8","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3y0m8/motherboard_with_2_pci_express_running_at_full/n415xbp/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752947786,"author_flair_text":null,"treatment_tags":[],"created_utc":1752947786,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n40w0d5","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"thepriceisright__","can_mod_post":false,"send_replies":true,"parent_id":"t1_n40r34n","score":2,"author_fullname":"t2_x22mr","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I don’t know. Maybe it is?","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n40w0d5","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I don’t know. Maybe it is?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":true,"can_gild":false,"link_id":"t3_1m3y0m8","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3y0m8/motherboard_with_2_pci_express_running_at_full/n40w0d5/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752944795,"author_flair_text":null,"treatment_tags":[],"created_utc":1752944795,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n40r34n","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"smayonak","can_mod_post":false,"created_utc":1752943267,"send_replies":true,"parent_id":"t1_n407i46","score":3,"author_fullname":"t2_3xyf2","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Thanks for explaining. A question that arises from this, is: if you don't need the full x16 to do GPU inference, then why can't we use a riser cable and bifurcation to connect two GPUs to a single PCIe like crypto miners do?","edited":1752947685,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n40r34n","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thanks for explaining. A question that arises from this, is: if you don&amp;#39;t need the full x16 to do GPU inference, then why can&amp;#39;t we use a riser cable and bifurcation to connect two GPUs to a single PCIe like crypto miners do?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m3y0m8","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3y0m8/motherboard_with_2_pci_express_running_at_full/n40r34n/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752943267,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n41tcqg","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"oblio-","can_mod_post":false,"send_replies":true,"parent_id":"t1_n40lc5n","score":1,"author_fullname":"t2_9a80o","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Ah, interesting. One comment was saying that during prompt processing there's a spike, so you probably want 16x for at least one GPU, but I guess the second one can be at 4x or whatever.\\n\\nI'm not going to train LLMs (at most I want to add my local content/code; but it's not like that's going to be \\"big data\\"), I mostly want inference.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n41tcqg","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Ah, interesting. One comment was saying that during prompt processing there&amp;#39;s a spike, so you probably want 16x for at least one GPU, but I guess the second one can be at 4x or whatever.&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;m not going to train LLMs (at most I want to add my local content/code; but it&amp;#39;s not like that&amp;#39;s going to be &amp;quot;big data&amp;quot;), I mostly want inference.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m3y0m8","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3y0m8/motherboard_with_2_pci_express_running_at_full/n41tcqg/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752955329,"author_flair_text":null,"treatment_tags":[],"created_utc":1752955329,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n40lc5n","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"thepriceisright__","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4089uu","score":8,"author_fullname":"t2_x22mr","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"You don’t need all of that bandwidth for LLMs because you are loading everything into vram once and the processing is happening in the card, unlike gaming where the CPU has to push data to the card continuously.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n40lc5n","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You don’t need all of that bandwidth for LLMs because you are loading everything into vram once and the processing is happening in the card, unlike gaming where the CPU has to push data to the card continuously.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":true,"can_gild":false,"link_id":"t3_1m3y0m8","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3y0m8/motherboard_with_2_pci_express_running_at_full/n40lc5n/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752941452,"author_flair_text":null,"treatment_tags":[],"created_utc":1752941452,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":8}}],"before":null}},"user_reports":[],"saved":false,"id":"n4089uu","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"oblio-","can_mod_post":false,"created_utc":1752937406,"send_replies":true,"parent_id":"t1_n407i46","score":0,"author_fullname":"t2_9a80o","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"&gt; That being said, you don’t really need the full lanes so the next biggest issue will be card clearance. \\n\\nSo 16x should be decent-ish in terms of LLM throughput? My (wishful) thinking would be to use the LLM for some kind of IDE autocompletion backed by a local LLM.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4089uu","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;That being said, you don’t really need the full lanes so the next biggest issue will be card clearance. &lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;So 16x should be decent-ish in terms of LLM throughput? My (wishful) thinking would be to use the LLM for some kind of IDE autocompletion backed by a local LLM.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m3y0m8","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3y0m8/motherboard_with_2_pci_express_running_at_full/n4089uu/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752937406,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}},"user_reports":[],"saved":false,"id":"n407i46","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"thepriceisright__","can_mod_post":false,"created_utc":1752937168,"send_replies":true,"parent_id":"t3_1m3y0m8","score":13,"author_fullname":"t2_x22mr","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"If you want the full x16 speed on both slots you’ll need a server motherboard and an Epyc, Threadripper, or. Xeon to get the PCIe lanes you’ll need. \\n\\nEven the Asus X670E GODLIKE only has one x16 slot actually running at full x16. The other two are at x8 and x4. \\n\\nThat being said, you don’t really need the full lanes so the next biggest issue will be card clearance. \\n\\nCheck out the MSI MAG B550 Tomahawk. It has two x16 slots far enough apart to fit big cards, supports up to 128GB of ram, and is around $150. \\n\\nhttps://www.msi.com/Motherboard/MAG-B550-TOMAHAWK","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n407i46","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;If you want the full x16 speed on both slots you’ll need a server motherboard and an Epyc, Threadripper, or. Xeon to get the PCIe lanes you’ll need. &lt;/p&gt;\\n\\n&lt;p&gt;Even the Asus X670E GODLIKE only has one x16 slot actually running at full x16. The other two are at x8 and x4. &lt;/p&gt;\\n\\n&lt;p&gt;That being said, you don’t really need the full lanes so the next biggest issue will be card clearance. &lt;/p&gt;\\n\\n&lt;p&gt;Check out the MSI MAG B550 Tomahawk. It has two x16 slots far enough apart to fit big cards, supports up to 128GB of ram, and is around $150. &lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://www.msi.com/Motherboard/MAG-B550-TOMAHAWK\\"&gt;https://www.msi.com/Motherboard/MAG-B550-TOMAHAWK&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":true,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3y0m8/motherboard_with_2_pci_express_running_at_full/n407i46/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752937168,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m3y0m8","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":13}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n40922c","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"jacek2023","can_mod_post":false,"created_utc":1752937650,"send_replies":true,"parent_id":"t3_1m3y0m8","score":5,"author_fullname":"t2_vqgbql9w","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"There is no problem with splitting the model.\\n\\nThere is a problem how to fit two GPUs into computer.\\n\\nI use open frame.\\n\\nAnd three 3090.\\n\\nAlso there is a lot of misinformation on reddit about motherboards.\\nYou don't need expensive one to use LLMs if you can put your model into GPUs.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n40922c","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;There is no problem with splitting the model.&lt;/p&gt;\\n\\n&lt;p&gt;There is a problem how to fit two GPUs into computer.&lt;/p&gt;\\n\\n&lt;p&gt;I use open frame.&lt;/p&gt;\\n\\n&lt;p&gt;And three 3090.&lt;/p&gt;\\n\\n&lt;p&gt;Also there is a lot of misinformation on reddit about motherboards.\\nYou don&amp;#39;t need expensive one to use LLMs if you can put your model into GPUs.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3y0m8/motherboard_with_2_pci_express_running_at_full/n40922c/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752937650,"author_flair_text":"llama.cpp","treatment_tags":[],"link_id":"t3_1m3y0m8","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n40c76l","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"stoppableDissolution","can_mod_post":false,"send_replies":true,"parent_id":"t1_n40avd3","score":5,"author_fullname":"t2_1n0su21k4z","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"If you are running something like llamacpp (ie, layer split) the only thing that is transferred between gpus is a single tensor that is kilobytes big, once per token. There is an inherent speed hit for going 2x gpus because of latency, but otheteise bandwidth is mostly irrelevant, you will be fine even on 3.0x1 mining riser (loading will take forever tho)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n40c76l","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;If you are running something like llamacpp (ie, layer split) the only thing that is transferred between gpus is a single tensor that is kilobytes big, once per token. There is an inherent speed hit for going 2x gpus because of latency, but otheteise bandwidth is mostly irrelevant, you will be fine even on 3.0x1 mining riser (loading will take forever tho)&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m3y0m8","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3y0m8/motherboard_with_2_pci_express_running_at_full/n40c76l/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752938621,"author_flair_text":null,"treatment_tags":[],"created_utc":1752938621,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}}],"before":null}},"user_reports":[],"saved":false,"id":"n40avd3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"RottenPingu1","can_mod_post":false,"send_replies":true,"parent_id":"t1_n40ab54","score":2,"author_fullname":"t2_vr2wdzpw","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Thank you for replying.\\nTo be honest, I'm just using my setup to run assistants. No fine tuning etc...\\nBut...I am interested in speed...","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n40avd3","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thank you for replying.\\nTo be honest, I&amp;#39;m just using my setup to run assistants. No fine tuning etc...\\nBut...I am interested in speed...&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m3y0m8","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3y0m8/motherboard_with_2_pci_express_running_at_full/n40avd3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752938209,"author_flair_text":null,"treatment_tags":[],"created_utc":1752938209,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n40ab54","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"stoppableDissolution","can_mod_post":false,"created_utc":1752938037,"send_replies":true,"parent_id":"t1_n408wla","score":6,"author_fullname":"t2_1n0su21k4z","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Almost none, thats how I have it set up. It could affect tensor parallel if you are running vllm, but thats about it.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n40ab54","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Almost none, thats how I have it set up. It could affect tensor parallel if you are running vllm, but thats about it.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m3y0m8","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3y0m8/motherboard_with_2_pci_express_running_at_full/n40ab54/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752938037,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}}],"before":null}},"user_reports":[],"saved":false,"id":"n408wla","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"RottenPingu1","can_mod_post":false,"created_utc":1752937603,"send_replies":true,"parent_id":"t3_1m3y0m8","score":3,"author_fullname":"t2_vr2wdzpw","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I'm wondering what kind of performance hit I'd take using two GPUs, one in a X16 and the other in X4.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n408wla","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;m wondering what kind of performance hit I&amp;#39;d take using two GPUs, one in a X16 and the other in X4.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3y0m8/motherboard_with_2_pci_express_running_at_full/n408wla/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752937603,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m3y0m8","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n40n8cd","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Threatening-Silence-","can_mod_post":false,"created_utc":1752942046,"send_replies":true,"parent_id":"t3_1m3y0m8","score":2,"author_fullname":"t2_15wqsifdjf","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"To echo what others have said, pipeline parallel inference uses just about zero picie bandwidth once the model is loaded. \\n\\nTensor parallel and training are a different story. \\n\\nBut if you just want to run inference you're fine with one card x16 and the others less. \\n\\nDo make sure you have at least one card running at full x16 though because it gets pegged during prompt processing. Set that one as your \`--main-gpu\` in llama-cpp.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n40n8cd","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;To echo what others have said, pipeline parallel inference uses just about zero picie bandwidth once the model is loaded. &lt;/p&gt;\\n\\n&lt;p&gt;Tensor parallel and training are a different story. &lt;/p&gt;\\n\\n&lt;p&gt;But if you just want to run inference you&amp;#39;re fine with one card x16 and the others less. &lt;/p&gt;\\n\\n&lt;p&gt;Do make sure you have at least one card running at full x16 though because it gets pegged during prompt processing. Set that one as your &lt;code&gt;--main-gpu&lt;/code&gt; in llama-cpp.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3y0m8/motherboard_with_2_pci_express_running_at_full/n40n8cd/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752942046,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m3y0m8","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n41tgc3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"vulcan4d","can_mod_post":false,"created_utc":1752955362,"send_replies":true,"parent_id":"t3_1m3y0m8","score":2,"author_fullname":"t2_a5y20","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"This is why I rock the i9-9800x on a 2066 super micro board.  The board gives me 4 full length pcie slots and the CPU gives 44pcie lanes which is much more than the typical consumer stuff.  Not as high as enterprise. It it is a good middle ground.  Still not enough to get all running x16 but two GPUs easily.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n41tgc3","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;This is why I rock the i9-9800x on a 2066 super micro board.  The board gives me 4 full length pcie slots and the CPU gives 44pcie lanes which is much more than the typical consumer stuff.  Not as high as enterprise. It it is a good middle ground.  Still not enough to get all running x16 but two GPUs easily.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3y0m8/motherboard_with_2_pci_express_running_at_full/n41tgc3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752955362,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m3y0m8","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n422mb1","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Maximum-SandwichF","can_mod_post":false,"created_utc":1752958338,"send_replies":true,"parent_id":"t3_1m3y0m8","score":2,"author_fullname":"t2_lyj35mhq","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"get epyc 7002","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n422mb1","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;get epyc 7002&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3y0m8/motherboard_with_2_pci_express_running_at_full/n422mb1/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752958338,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m3y0m8","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n42cduj","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"chub0ka","can_mod_post":false,"created_utc":1752961557,"send_replies":true,"parent_id":"t3_1m3y0m8","score":2,"author_fullname":"t2_8mgnhbh3f","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"X8 works just fine believe me","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n42cduj","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;X8 works just fine believe me&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3y0m8/motherboard_with_2_pci_express_running_at_full/n42cduj/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752961557,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m3y0m8","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n40od0e","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"FieldProgrammable","can_mod_post":false,"created_utc":1752942402,"send_replies":true,"parent_id":"t3_1m3y0m8","score":3,"author_fullname":"t2_moet0t","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Consumer CPUs only have 24 PCIE lanes. 4 of these are usually dedicated to the motherboard chipset, 4 to the M.2 slot and 16 to the first PCIE slot. So how you think you can get 32 lanes out of a Ryzen is hard to fathom.\\n\\nIf you want more lanes you will need a server CPU and motherboard.\\n\\nBudget motherboards will wire all other PCIE slots to the chipset, so it is effectively daisy chained and forced to share bandwidth to the CPU with your USB, SATA etc.\\n\\nMore expensive boards will wire the second slot to the CPU and have the first slot auto bifurcate to 8+8 when the second slot is populated. Even for boards that only have a single x16 slot to the CPU, a decent BIOS can support bifurcation risers to split that slot.\\n\\nMany budget GPUs now only use 8 lanes anyway, so you may not need to lose any lanes in a dual budget GPU setup.\\n\\nWhether you need loads of PCIE bandwidth depends on the task you are doing. Training needs huge bandwidth, pipelined inference very little, tensor parallel inference something in between.\\n\\nIf you are building a dual GPU system then considering PCIE lanes is certainly important and worth factoring into your choice of motherboard, but whether you really need PCIE5x16 on two slots for inference is a highly dubious claim.\\n\\nMulti GPU is generally well supported on LLM backends the same is not true for other popular AI applications like txt2img.","edited":1752942655,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n40od0e","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Consumer CPUs only have 24 PCIE lanes. 4 of these are usually dedicated to the motherboard chipset, 4 to the M.2 slot and 16 to the first PCIE slot. So how you think you can get 32 lanes out of a Ryzen is hard to fathom.&lt;/p&gt;\\n\\n&lt;p&gt;If you want more lanes you will need a server CPU and motherboard.&lt;/p&gt;\\n\\n&lt;p&gt;Budget motherboards will wire all other PCIE slots to the chipset, so it is effectively daisy chained and forced to share bandwidth to the CPU with your USB, SATA etc.&lt;/p&gt;\\n\\n&lt;p&gt;More expensive boards will wire the second slot to the CPU and have the first slot auto bifurcate to 8+8 when the second slot is populated. Even for boards that only have a single x16 slot to the CPU, a decent BIOS can support bifurcation risers to split that slot.&lt;/p&gt;\\n\\n&lt;p&gt;Many budget GPUs now only use 8 lanes anyway, so you may not need to lose any lanes in a dual budget GPU setup.&lt;/p&gt;\\n\\n&lt;p&gt;Whether you need loads of PCIE bandwidth depends on the task you are doing. Training needs huge bandwidth, pipelined inference very little, tensor parallel inference something in between.&lt;/p&gt;\\n\\n&lt;p&gt;If you are building a dual GPU system then considering PCIE lanes is certainly important and worth factoring into your choice of motherboard, but whether you really need PCIE5x16 on two slots for inference is a highly dubious claim.&lt;/p&gt;\\n\\n&lt;p&gt;Multi GPU is generally well supported on LLM backends the same is not true for other popular AI applications like txt2img.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3y0m8/motherboard_with_2_pci_express_running_at_full/n40od0e/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752942402,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m3y0m8","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n48bdpc","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"b0tbuilder","can_mod_post":false,"created_utc":1753045473,"send_replies":true,"parent_id":"t3_1m3y0m8","score":1,"author_fullname":"t2_bxf9roj","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"1. It is not a function of the model but what you use to run it.  2.  No, compute and memory across both.  3 probably not the issue you think it is.  2x Radeon VII across 2 occulink ports with only 4 pcie lanes.\\n\\nhttps://preview.redd.it/9veho848g3ef1.jpeg?width=5712&amp;format=pjpg&amp;auto=webp&amp;s=a057a4c209e187e61f2b272f657742782b8b959e","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n48bdpc","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;ol&gt;\\n&lt;li&gt;It is not a function of the model but what you use to run it.  2.  No, compute and memory across both.  3 probably not the issue you think it is.  2x Radeon VII across 2 occulink ports with only 4 pcie lanes.&lt;/li&gt;\\n&lt;/ol&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://preview.redd.it/9veho848g3ef1.jpeg?width=5712&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=a057a4c209e187e61f2b272f657742782b8b959e\\"&gt;https://preview.redd.it/9veho848g3ef1.jpeg?width=5712&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=a057a4c209e187e61f2b272f657742782b8b959e&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3y0m8/motherboard_with_2_pci_express_running_at_full/n48bdpc/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753045473,"media_metadata":{"9veho848g3ef1":{"status":"valid","e":"Image","m":"image/jpeg","p":[{"y":81,"x":108,"u":"https://preview.redd.it/9veho848g3ef1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=768e4f6138a39e57c11ef8de49641dfd99478b22"},{"y":162,"x":216,"u":"https://preview.redd.it/9veho848g3ef1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=da665b5b900ce9cf2780d416f65f1b97b808ee55"},{"y":240,"x":320,"u":"https://preview.redd.it/9veho848g3ef1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=559c0ab80e7f2ae7ffc8cdec5ccd9b507cc2331b"},{"y":480,"x":640,"u":"https://preview.redd.it/9veho848g3ef1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=926934e50746aa773b4a4ebb72b362d06637173e"},{"y":720,"x":960,"u":"https://preview.redd.it/9veho848g3ef1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=927fc0d24ded6a08108a5d1f3fc59a9543ccb078"},{"y":810,"x":1080,"u":"https://preview.redd.it/9veho848g3ef1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=1dd11627189c6fbbfa5e4fa32852e6178d48d81f"}],"s":{"y":4284,"x":5712,"u":"https://preview.redd.it/9veho848g3ef1.jpeg?width=5712&amp;format=pjpg&amp;auto=webp&amp;s=a057a4c209e187e61f2b272f657742782b8b959e"},"id":"9veho848g3ef1"}},"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m3y0m8","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n40fetd","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Ok-Internal9317","can_mod_post":false,"created_utc":1752939606,"send_replies":true,"parent_id":"t3_1m3y0m8","score":1,"author_fullname":"t2_77yd9w74","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I'm so happy that I brought 4x x16 and 2x x8 board with m.2, lack of pcie is PAIN","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n40fetd","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;m so happy that I brought 4x x16 and 2x x8 board with m.2, lack of pcie is PAIN&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3y0m8/motherboard_with_2_pci_express_running_at_full/n40fetd/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752939606,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m3y0m8","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n41mas9","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"oblio-","can_mod_post":false,"created_utc":1752953049,"send_replies":true,"parent_id":"t1_n40qplx","score":1,"author_fullname":"t2_9a80o","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Life is complicated and technology advances. I configured my first rackable server 15 years ago and my last one about 8 years ago 🙂\\n\\n\\nI just don't have time to dive deep into this as my time is very limited due to work and personal constraints. Ergo asking here, where people have managed to plug a lot of gaps in about 1 hour of research.\\n\\n\\nI'm fairy sure that for about 95% of what I'll use this computer, I'm set.","edited":1752955380,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n41mas9","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Life is complicated and technology advances. I configured my first rackable server 15 years ago and my last one about 8 years ago 🙂&lt;/p&gt;\\n\\n&lt;p&gt;I just don&amp;#39;t have time to dive deep into this as my time is very limited due to work and personal constraints. Ergo asking here, where people have managed to plug a lot of gaps in about 1 hour of research.&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;m fairy sure that for about 95% of what I&amp;#39;ll use this computer, I&amp;#39;m set.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m3y0m8","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3y0m8/motherboard_with_2_pci_express_running_at_full/n41mas9/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752953049,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n40qplx","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Xamanthas","can_mod_post":false,"created_utc":1752943148,"send_replies":true,"parent_id":"t3_1m3y0m8","score":-4,"author_fullname":"t2_e6bnx","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"This is really blunt but its needed advice:\\n\\nIts called servers and if you dont already know this you really shouldnt be spending anything on hardware yet.","edited":1752946658,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n40qplx","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;This is really blunt but its needed advice:&lt;/p&gt;\\n\\n&lt;p&gt;Its called servers and if you dont already know this you really shouldnt be spending anything on hardware yet.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3y0m8/motherboard_with_2_pci_express_running_at_full/n40qplx/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752943148,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m3y0m8","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-4}}],"before":null}}]`),n=()=>e.jsx(l,{data:t});export{n as default};
