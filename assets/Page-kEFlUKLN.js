import{j as e}from"./index-CqAPCjw5.js";import{R as l}from"./RedditPostRenderer-4oBDAtGr.js";import"./index-D3Sdy_Op.js";const t=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"We introduce **EXAONE 4.0**, which integrates a **Non-reasoning mode** and **Reasoning mode** to achieve both the excellent usability of [EXAONE 3.5](https://github.com/LG-AI-EXAONE/EXAONE-3.5) and the advanced reasoning abilities of [EXAONE Deep](https://github.com/LG-AI-EXAONE/EXAONE-Deep). To pave the way for the agentic AI era, EXAONE 4.0 incorporates essential features such as agentic tool use, and its multilingual capabilities are extended to support Spanish in addition to English and Korean.\\n\\nThe EXAONE 4.0 model series consists of two sizes: a mid-size **32B** model optimized for high performance, and a small-size **1.2B** model designed for on-device applications.\\n\\nIn the EXAONE 4.0 architecture, we apply new architectural changes compared to previous EXAONE models as below:\\n\\n1. **Hybrid Attention**: For the 32B model, we adopt hybrid attention scheme, which combines *Local attention (sliding window attention)* with *Global attention (full attention)* in a 3:1 ratio. We do not use RoPE (Rotary Positional Embedding) for global attention for better global context understanding.\\n2. **QK-Reorder-Norm**: We reorder the LayerNorm position from the traditional Pre-LN scheme by applying LayerNorm directly to the attention and MLP outputs, and we add RMS normalization right after the Q and K projection. It helps yield better performance on downstream tasks despite consuming more computation.\\n\\n[https://huggingface.co/LGAI-EXAONE/EXAONE-4.0-32B-GGUF](https://huggingface.co/LGAI-EXAONE/EXAONE-4.0-32B-GGUF)\\n\\n[https://huggingface.co/LGAI-EXAONE/EXAONE-4.0-1.2B-GGUF](https://huggingface.co/LGAI-EXAONE/EXAONE-4.0-1.2B-GGUF)\\n\\n","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"support for EXAONE 4.0 model architecture has been merged into llama.cpp","link_flair_richtext":[{"e":"text","t":"New Model"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":70,"top_awarded_type":null,"hide_score":false,"name":"t3_1m31z4z","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.95,"author_flair_background_color":"#bbbdbf","ups":102,"total_awards_received":0,"media_embed":{},"thumbnail_width":140,"author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","is_original_content":false,"author_fullname":"t2_vqgbql9w","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"New Model","can_mod_post":false,"score":102,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"https://external-preview.redd.it/S3ENLEeG2S1qRdSN-s_xJNZnxYIxW1L4lI691Agwkuo.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=087614e53193eca8f16794a001127ac893e70f4a","edited":false,"author_flair_css_class":null,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"gildings":{},"post_hint":"link","content_categories":null,"is_self":false,"subreddit_type":"public","created":1752844334,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"richtext","domain":"github.com","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;We introduce &lt;strong&gt;EXAONE 4.0&lt;/strong&gt;, which integrates a &lt;strong&gt;Non-reasoning mode&lt;/strong&gt; and &lt;strong&gt;Reasoning mode&lt;/strong&gt; to achieve both the excellent usability of &lt;a href=\\"https://github.com/LG-AI-EXAONE/EXAONE-3.5\\"&gt;EXAONE 3.5&lt;/a&gt; and the advanced reasoning abilities of &lt;a href=\\"https://github.com/LG-AI-EXAONE/EXAONE-Deep\\"&gt;EXAONE Deep&lt;/a&gt;. To pave the way for the agentic AI era, EXAONE 4.0 incorporates essential features such as agentic tool use, and its multilingual capabilities are extended to support Spanish in addition to English and Korean.&lt;/p&gt;\\n\\n&lt;p&gt;The EXAONE 4.0 model series consists of two sizes: a mid-size &lt;strong&gt;32B&lt;/strong&gt; model optimized for high performance, and a small-size &lt;strong&gt;1.2B&lt;/strong&gt; model designed for on-device applications.&lt;/p&gt;\\n\\n&lt;p&gt;In the EXAONE 4.0 architecture, we apply new architectural changes compared to previous EXAONE models as below:&lt;/p&gt;\\n\\n&lt;ol&gt;\\n&lt;li&gt;&lt;strong&gt;Hybrid Attention&lt;/strong&gt;: For the 32B model, we adopt hybrid attention scheme, which combines &lt;em&gt;Local attention (sliding window attention)&lt;/em&gt; with &lt;em&gt;Global attention (full attention)&lt;/em&gt; in a 3:1 ratio. We do not use RoPE (Rotary Positional Embedding) for global attention for better global context understanding.&lt;/li&gt;\\n&lt;li&gt;&lt;strong&gt;QK-Reorder-Norm&lt;/strong&gt;: We reorder the LayerNorm position from the traditional Pre-LN scheme by applying LayerNorm directly to the attention and MLP outputs, and we add RMS normalization right after the Q and K projection. It helps yield better performance on downstream tasks despite consuming more computation.&lt;/li&gt;\\n&lt;/ol&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://huggingface.co/LGAI-EXAONE/EXAONE-4.0-32B-GGUF\\"&gt;https://huggingface.co/LGAI-EXAONE/EXAONE-4.0-32B-GGUF&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://huggingface.co/LGAI-EXAONE/EXAONE-4.0-1.2B-GGUF\\"&gt;https://huggingface.co/LGAI-EXAONE/EXAONE-4.0-1.2B-GGUF&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"url_overridden_by_dest":"https://github.com/ggml-org/llama.cpp/pull/14630","view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"preview":{"images":[{"source":{"url":"https://external-preview.redd.it/S3ENLEeG2S1qRdSN-s_xJNZnxYIxW1L4lI691Agwkuo.png?auto=webp&amp;s=61ddce86131a3020b1b6014ee2d5a634dbd3bbce","width":1200,"height":600},"resolutions":[{"url":"https://external-preview.redd.it/S3ENLEeG2S1qRdSN-s_xJNZnxYIxW1L4lI691Agwkuo.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=bddc4a7497f5680e1abffba8fc5ae1cb51d13254","width":108,"height":54},{"url":"https://external-preview.redd.it/S3ENLEeG2S1qRdSN-s_xJNZnxYIxW1L4lI691Agwkuo.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=5ab87b57baeb53d3191a3fd63fb1c0301e33ff3e","width":216,"height":108},{"url":"https://external-preview.redd.it/S3ENLEeG2S1qRdSN-s_xJNZnxYIxW1L4lI691Agwkuo.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=712cd45fb95dbea190d6fd432c2b799a7b72e1f3","width":320,"height":160},{"url":"https://external-preview.redd.it/S3ENLEeG2S1qRdSN-s_xJNZnxYIxW1L4lI691Agwkuo.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=7bcec270f5033ba2a559251096ffb9bdbd92f54c","width":640,"height":320},{"url":"https://external-preview.redd.it/S3ENLEeG2S1qRdSN-s_xJNZnxYIxW1L4lI691Agwkuo.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=d22a02484672b685e6e64e4d409e15ce93cd1015","width":960,"height":480},{"url":"https://external-preview.redd.it/S3ENLEeG2S1qRdSN-s_xJNZnxYIxW1L4lI691Agwkuo.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=deb89dfc1eca99991dee06eca9ce57ed03ce6705","width":1080,"height":540}],"variants":{},"id":"S3ENLEeG2S1qRdSN-s_xJNZnxYIxW1L4lI691Agwkuo"}],"enabled":false},"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"ced98442-f5d3-11ed-b657-66d3b15490c6","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":"llama.cpp","treatment_tags":[],"visited":false,"removed_by":null,"mod_note":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"num_reports":null,"removal_reason":null,"link_flair_background_color":"#ffb000","id":"1m31z4z","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"jacek2023","discussion_type":null,"num_comments":33,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":"light","permalink":"/r/LocalLLaMA/comments/1m31z4z/support_for_exaone_40_model_architecture_has_been/","stickied":false,"url":"https://github.com/ggml-org/llama.cpp/pull/14630","subreddit_subscribers":501753,"created_utc":1752844334,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3uywk4","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"FaceDeer","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3uhb7q","score":4,"author_fullname":"t2_4ljvm","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"&gt;  I would like to have Claude 3.5/3.7/4 Sonnet at home, even if I have to wait 2 minutes for a reply.\\n\\nThis is definitely a niche I'm interested in. I don't mind if the AI takes a long time to respond if its response is really high quality.\\n\\nI suspect in the long run I'll want a home AI \\"ecosystem\\" with fast, simple AIs that can call the heavyweight ones when they need a more betterer answer.","edited":false,"author_flair_css_class":null,"name":"t1_n3uywk4","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;I would like to have Claude 3.5/3.7/4 Sonnet at home, even if I have to wait 2 minutes for a reply.&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;This is definitely a niche I&amp;#39;m interested in. I don&amp;#39;t mind if the AI takes a long time to respond if its response is really high quality.&lt;/p&gt;\\n\\n&lt;p&gt;I suspect in the long run I&amp;#39;ll want a home AI &amp;quot;ecosystem&amp;quot; with fast, simple AIs that can call the heavyweight ones when they need a more betterer answer.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1m31z4z","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m31z4z/support_for_exaone_40_model_architecture_has_been/n3uywk4/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752862539,"author_flair_text":null,"collapsed":false,"created_utc":1752862539,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}}],"before":null}},"user_reports":[],"saved":false,"id":"n3uhb7q","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"FullOf_Bad_Ideas","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3u5qsk","score":3,"author_fullname":"t2_9s7pmakgx","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I think it's a golden goose for companies like Cerebras, Groq and Sambanova. If they can deliver quick inference on those kinds of models, they can provide superb user experience in those use cases. Cerebras offers inference at 1000-4000 t/s output for Qwen3 32B. \\n\\nBut we aren't on r slash CloudLLAMA - for us the benefit is still there - I would like to have Claude 3.5/3.7/4 Sonnet at home, even if I have to wait 2 minutes for a reply.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3uhb7q","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I think it&amp;#39;s a golden goose for companies like Cerebras, Groq and Sambanova. If they can deliver quick inference on those kinds of models, they can provide superb user experience in those use cases. Cerebras offers inference at 1000-4000 t/s output for Qwen3 32B. &lt;/p&gt;\\n\\n&lt;p&gt;But we aren&amp;#39;t on r slash CloudLLAMA - for us the benefit is still there - I would like to have Claude 3.5/3.7/4 Sonnet at home, even if I have to wait 2 minutes for a reply.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m31z4z","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m31z4z/support_for_exaone_40_model_architecture_has_been/n3uhb7q/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752857611,"author_flair_text":null,"treatment_tags":[],"created_utc":1752857611,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n3u5qsk","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"And-Bee","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3tmycr","score":5,"author_fullname":"t2_a81fjhk","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I really don’t like reasoning models for this problem","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3u5qsk","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I really don’t like reasoning models for this problem&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m31z4z","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m31z4z/support_for_exaone_40_model_architecture_has_been/n3u5qsk/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752854309,"author_flair_text":null,"treatment_tags":[],"created_utc":1752854309,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}}],"before":null}},"user_reports":[],"saved":false,"id":"n3tmycr","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"FullOf_Bad_Ideas","can_mod_post":false,"created_utc":1752849038,"send_replies":true,"parent_id":"t1_n3t99ir","score":5,"author_fullname":"t2_9s7pmakgx","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"If it performs great, but needs 32k tokens to reason out a single query, would that be a win in your eyes?\\n\\nTech report has those stats - it has great performance on AIME and LiveCodeBench only with 32K and 64K reasoning budgets, plus 8k response budget.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3tmycr","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;If it performs great, but needs 32k tokens to reason out a single query, would that be a win in your eyes?&lt;/p&gt;\\n\\n&lt;p&gt;Tech report has those stats - it has great performance on AIME and LiveCodeBench only with 32K and 64K reasoning budgets, plus 8k response budget.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m31z4z","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m31z4z/support_for_exaone_40_model_architecture_has_been/n3tmycr/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752849038,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3yit0s","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"FullOf_Bad_Ideas","can_mod_post":false,"created_utc":1752910326,"send_replies":true,"parent_id":"t1_n3t99ir","score":1,"author_fullname":"t2_9s7pmakgx","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I tried it out (q4_k_m, temp 1) and i like it's vibe with general QA - it seems lively, but it did worse than Qwen3 32b on 2 reasoning coding tasks I gave it.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3yit0s","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I tried it out (q4_k_m, temp 1) and i like it&amp;#39;s vibe with general QA - it seems lively, but it did worse than Qwen3 32b on 2 reasoning coding tasks I gave it.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m31z4z","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m31z4z/support_for_exaone_40_model_architecture_has_been/n3yit0s/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752910326,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3uhh0s","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"FullOf_Bad_Ideas","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3uf34a","score":2,"author_fullname":"t2_9s7pmakgx","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yes, there's a tech report.\\n\\nhttps://arxiv.org/abs/2507.11407","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3uhh0s","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yes, there&amp;#39;s a tech report.&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://arxiv.org/abs/2507.11407\\"&gt;https://arxiv.org/abs/2507.11407&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m31z4z","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m31z4z/support_for_exaone_40_model_architecture_has_been/n3uhh0s/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752857654,"author_flair_text":null,"treatment_tags":[],"created_utc":1752857654,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n3uf34a","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"jinnyjuice","can_mod_post":false,"created_utc":1752856993,"send_replies":true,"parent_id":"t1_n3t99ir","score":0,"author_fullname":"t2_4hrx8","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Just saw their benchmarks and wow the 1,2B model is absolutely insane. Are there any papers for this?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3uf34a","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Just saw their benchmarks and wow the 1,2B model is absolutely insane. Are there any papers for this?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m31z4z","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m31z4z/support_for_exaone_40_model_architecture_has_been/n3uf34a/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752856993,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}},"user_reports":[],"saved":false,"id":"n3t99ir","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Secure_Reflection409","can_mod_post":false,"created_utc":1752844890,"send_replies":true,"parent_id":"t3_1m31z4z","score":13,"author_fullname":"t2_by77ogdhr","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Will it live up to the hype? You decide!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3t99ir","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Will it live up to the hype? You decide!&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m31z4z/support_for_exaone_40_model_architecture_has_been/n3t99ir/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752844890,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m31z4z","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":13}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3w9v7m","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"--Tintin","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3w2rz4","score":2,"author_fullname":"t2_3rsks8xy","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Much appreciated. Thank you @Chromix_","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3w9v7m","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Much appreciated. Thank you @Chromix_&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m31z4z","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m31z4z/support_for_exaone_40_model_architecture_has_been/n3w9v7m/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752876710,"author_flair_text":null,"treatment_tags":[],"created_utc":1752876710,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n3w2rz4","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Chromix_","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3vxdg5","score":3,"author_fullname":"t2_k7w2h","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Imatrix: [Higher quality](https://www.reddit.com/r/LocalLLaMA/comments/1993iro/ggufs_quants_can_punch_above_their_weights_now/) while the size does not change. For example a Q3\\\\_K\\\\_M made with imatrix can be better than a larger Q3\\\\_K\\\\_L without imatrix.  \\nUD: [Unsloth](https://huggingface.co/unsloth) makes so called dynamic quants, where they adapt the quantization individually per model, which should lead to higher result quality.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3w2rz4","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Imatrix: &lt;a href=\\"https://www.reddit.com/r/LocalLLaMA/comments/1993iro/ggufs_quants_can_punch_above_their_weights_now/\\"&gt;Higher quality&lt;/a&gt; while the size does not change. For example a Q3_K_M made with imatrix can be better than a larger Q3_K_L without imatrix.&lt;br/&gt;\\nUD: &lt;a href=\\"https://huggingface.co/unsloth\\"&gt;Unsloth&lt;/a&gt; makes so called dynamic quants, where they adapt the quantization individually per model, which should lead to higher result quality.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m31z4z","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m31z4z/support_for_exaone_40_model_architecture_has_been/n3w2rz4/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752874422,"author_flair_text":null,"treatment_tags":[],"created_utc":1752874422,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n3vxdg5","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"--Tintin","can_mod_post":false,"created_utc":1752872728,"send_replies":true,"parent_id":"t1_n3tg0p2","score":1,"author_fullname":"t2_3rsks8xy","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"What’s the benefit of imatrix and UD? Asking for a friend …","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3vxdg5","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;What’s the benefit of imatrix and UD? Asking for a friend …&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m31z4z","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m31z4z/support_for_exaone_40_model_architecture_has_been/n3vxdg5/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752872728,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3ue1bk","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"jacek2023","can_mod_post":false,"created_utc":1752856692,"send_replies":true,"parent_id":"t1_n3tt17f","score":2,"author_fullname":"t2_vqgbql9w","approved_by":null,"mod_note":null,"all_awardings":[],"body":"and here it is  \\n[https://huggingface.co/mradermacher/EXAONE-4.0-32B-i1-GGUF](https://huggingface.co/mradermacher/EXAONE-4.0-32B-i1-GGUF)","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n3ue1bk","is_submitter":true,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;and here it is&lt;br/&gt;\\n&lt;a href=\\"https://huggingface.co/mradermacher/EXAONE-4.0-32B-i1-GGUF\\"&gt;https://huggingface.co/mradermacher/EXAONE-4.0-32B-i1-GGUF&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1m31z4z","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m31z4z/support_for_exaone_40_model_architecture_has_been/n3ue1bk/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752856692,"author_flair_text":"llama.cpp","treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":7,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n3tt17f","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"LicensedTerrapin","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3tsx74","score":2,"author_fullname":"t2_97zi8wea","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Yeah it's usually just a matter of time ☺️","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n3tt17f","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yeah it&amp;#39;s usually just a matter of time ☺️&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1m31z4z","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m31z4z/support_for_exaone_40_model_architecture_has_been/n3tt17f/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752850749,"author_flair_text":null,"treatment_tags":[],"created_utc":1752850749,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":6,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n3tsx74","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"jacek2023","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3tspwg","score":1,"author_fullname":"t2_vqgbql9w","approved_by":null,"mod_note":null,"all_awardings":[],"body":"But mradermacher already created EXAONE ggufs just not  imatrix yet ;)","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n3tsx74","is_submitter":true,"collapsed":false,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;But mradermacher already created EXAONE ggufs just not  imatrix yet ;)&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1m31z4z","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m31z4z/support_for_exaone_40_model_architecture_has_been/n3tsx74/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752850718,"author_flair_text":"llama.cpp","treatment_tags":[],"created_utc":1752850718,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3tspwg","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"LicensedTerrapin","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3tsmsv","score":7,"author_fullname":"t2_97zi8wea","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Or bartowski or unsloth.","edited":false,"author_flair_css_class":null,"name":"t1_n3tspwg","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Or bartowski or unsloth.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1m31z4z","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m31z4z/support_for_exaone_40_model_architecture_has_been/n3tspwg/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752850663,"author_flair_text":null,"collapsed":false,"created_utc":1752850663,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":7}}],"before":null}},"user_reports":[],"saved":false,"id":"n3tsmsv","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"jacek2023","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3trp41","score":2,"author_fullname":"t2_vqgbql9w","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Probably you can expect them here\\nhttps://huggingface.co/mradermacher/models","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3tsmsv","is_submitter":true,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Probably you can expect them here\\n&lt;a href=\\"https://huggingface.co/mradermacher/models\\"&gt;https://huggingface.co/mradermacher/models&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m31z4z","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m31z4z/support_for_exaone_40_model_architecture_has_been/n3tsmsv/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752850639,"author_flair_text":"llama.cpp","treatment_tags":[],"created_utc":1752850639,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n3trp41","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"LicensedTerrapin","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3tp7d3","score":7,"author_fullname":"t2_97zi8wea","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Neither do I. That's why people want IQ or UD.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3trp41","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Neither do I. That&amp;#39;s why people want IQ or UD.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m31z4z","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m31z4z/support_for_exaone_40_model_architecture_has_been/n3trp41/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752850376,"author_flair_text":null,"treatment_tags":[],"created_utc":1752850376,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":7}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3unp7u","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"GL-AI","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3tp7d3","score":2,"author_fullname":"t2_1sr5yw3yg0","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"quants like Q8_0 don't require an imatrix, but they still benefit from having one.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3unp7u","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;quants like Q8_0 don&amp;#39;t require an imatrix, but they still benefit from having one.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m31z4z","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m31z4z/support_for_exaone_40_model_architecture_has_been/n3unp7u/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752859376,"author_flair_text":null,"treatment_tags":[],"created_utc":1752859376,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n3tp7d3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"jacek2023","can_mod_post":false,"created_utc":1752849673,"send_replies":true,"parent_id":"t1_n3tg0p2","score":1,"author_fullname":"t2_vqgbql9w","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"But it means that you don't use Q8?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3tp7d3","is_submitter":true,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;But it means that you don&amp;#39;t use Q8?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m31z4z","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m31z4z/support_for_exaone_40_model_architecture_has_been/n3tp7d3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752849673,"author_flair_text":"llama.cpp","treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3tg0p2","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Chromix_","can_mod_post":false,"created_utc":1752846994,"send_replies":true,"parent_id":"t3_1m31z4z","score":4,"author_fullname":"t2_k7w2h","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"These quants were created without imatrix. I'd skip these and wait a bit for imatrix (and UD) quants from the usual sources.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3tg0p2","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;These quants were created without imatrix. I&amp;#39;d skip these and wait a bit for imatrix (and UD) quants from the usual sources.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m31z4z/support_for_exaone_40_model_architecture_has_been/n3tg0p2/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752846994,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m31z4z","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3ui7jy","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"FullOf_Bad_Ideas","can_mod_post":false,"created_utc":1752857857,"send_replies":true,"parent_id":"t1_n3ty3jf","score":2,"author_fullname":"t2_9s7pmakgx","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yes, 102-150k is the norm for the vocabulary of Asian LLMs nowadays.\\n\\n32B benchmarks higher, but it needs 32k+ reasoning budget to reach those scores. It seems to be like the DeepCoder 14B model where they let context length run loose to the point of being hard to use.\\n\\nI would hope for it to be better than Qwen3 32B - let me know if you test it out as I'll be holding out until it's supported in vLLM/SGLang probably, but it's quite likely that you'll need to wait a lot for it to finish a response, which sometimes is an issue.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3ui7jy","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yes, 102-150k is the norm for the vocabulary of Asian LLMs nowadays.&lt;/p&gt;\\n\\n&lt;p&gt;32B benchmarks higher, but it needs 32k+ reasoning budget to reach those scores. It seems to be like the DeepCoder 14B model where they let context length run loose to the point of being hard to use.&lt;/p&gt;\\n\\n&lt;p&gt;I would hope for it to be better than Qwen3 32B - let me know if you test it out as I&amp;#39;ll be holding out until it&amp;#39;s supported in vLLM/SGLang probably, but it&amp;#39;s quite likely that you&amp;#39;ll need to wait a lot for it to finish a response, which sometimes is an issue.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m31z4z","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m31z4z/support_for_exaone_40_model_architecture_has_been/n3ui7jy/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752857857,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3tyahl","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Lazy-Pattern-5171","can_mod_post":false,"created_utc":1752852229,"send_replies":true,"parent_id":"t1_n3ty3jf","score":1,"author_fullname":"t2_1lyjk8is25","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Maybe because of multilingual setup","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3tyahl","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Maybe because of multilingual setup&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m31z4z","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m31z4z/support_for_exaone_40_model_architecture_has_been/n3tyahl/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752852229,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3ufwm0","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"jinnyjuice","can_mod_post":false,"created_utc":1752857223,"send_replies":true,"parent_id":"t1_n3ty3jf","score":1,"author_fullname":"t2_4hrx8","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Older languages like Korean tend to have richer vocabulary, particularly with adjectives, onomatopoeia, and emotional words -- could be explained by that. Average American adult knows ~20k, where PhD may know ~40k words. Koreans and the Japanese can be 80k to 140k(?) or so.","edited":1752857441,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3ufwm0","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Older languages like Korean tend to have richer vocabulary, particularly with adjectives, onomatopoeia, and emotional words -- could be explained by that. Average American adult knows ~20k, where PhD may know ~40k words. Koreans and the Japanese can be 80k to 140k(?) or so.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m31z4z","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m31z4z/support_for_exaone_40_model_architecture_has_been/n3ufwm0/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752857223,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3ty3jf","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Lazy-Pattern-5171","can_mod_post":false,"created_utc":1752852174,"send_replies":true,"parent_id":"t3_1m31z4z","score":3,"author_fullname":"t2_1lyjk8is25","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"The 32B benchmarks seem to be much much better than the Qwen3 32B. The vocab size seems high though. Do models regularly have vocabs in the 100K region?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3ty3jf","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The 32B benchmarks seem to be much much better than the Qwen3 32B. The vocab size seems high though. Do models regularly have vocabs in the 100K region?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m31z4z/support_for_exaone_40_model_architecture_has_been/n3ty3jf/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752852174,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m31z4z","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3wqtw7","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Ok_Cow1976","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3w92ny","score":2,"author_fullname":"t2_3pwbsmdr","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"That's nice. Thanks!","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3wqtw7","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;That&amp;#39;s nice. Thanks!&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m31z4z","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m31z4z/support_for_exaone_40_model_architecture_has_been/n3wqtw7/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752882510,"author_flair_text":null,"treatment_tags":[],"created_utc":1752882510,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n3w92ny","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Foreign-Beginning-49","can_mod_post":false,"created_utc":1752876445,"send_replies":true,"parent_id":"t1_n3tvabm","score":1,"author_fullname":"t2_83u2l6o4","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yes it is a according to their HF repo","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3w92ny","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yes it is a according to their HF repo&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m31z4z","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m31z4z/support_for_exaone_40_model_architecture_has_been/n3w92ny/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752876445,"author_flair_text":"llama.cpp","treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3tvabm","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Ok_Cow1976","can_mod_post":false,"created_utc":1752851388,"send_replies":true,"parent_id":"t3_1m31z4z","score":2,"author_fullname":"t2_3pwbsmdr","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Is it possible to toggle think model?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3tvabm","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Is it possible to toggle think model?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m31z4z/support_for_exaone_40_model_architecture_has_been/n3tvabm/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752851388,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m31z4z","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3ufoau","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Acrobatic-Increase69","can_mod_post":false,"created_utc":1752857158,"send_replies":true,"parent_id":"t3_1m31z4z","score":3,"author_fullname":"t2_16q4k1t935","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I love Exaone, It's the only model that will talk about just anything if I ask it to with no issue","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3ufoau","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I love Exaone, It&amp;#39;s the only model that will talk about just anything if I ask it to with no issue&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m31z4z/support_for_exaone_40_model_architecture_has_been/n3ufoau/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752857158,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m31z4z","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"7d1f04e6-4920-11ef-b2e1-2e580594e1a1","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3wpo9o","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"xugik1","can_mod_post":false,"created_utc":1752882110,"send_replies":true,"parent_id":"t1_n3vgqco","score":1,"author_fullname":"t2_g28qjkj4","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yes, both models have 102.4k vocabs.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3wpo9o","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"Llama 3.1"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yes, both models have 102.4k vocabs.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m31z4z","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m31z4z/support_for_exaone_40_model_architecture_has_been/n3wpo9o/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752882110,"author_flair_text":"Llama 3.1","treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":"#93b1ba","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3vgqco","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"nivvis","can_mod_post":false,"created_utc":1752867808,"send_replies":true,"parent_id":"t3_1m31z4z","score":1,"author_fullname":"t2_39blx","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Anyone know if the small has the same vocab? Ie may make a decent draft model?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3vgqco","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Anyone know if the small has the same vocab? Ie may make a decent draft model?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m31z4z/support_for_exaone_40_model_architecture_has_been/n3vgqco/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752867808,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m31z4z","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3w2fwl","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ravage382","can_mod_post":false,"created_utc":1752874314,"send_replies":true,"parent_id":"t3_1m31z4z","score":1,"author_fullname":"t2_9sf41","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Is anyone else having problems with their posted chat template? I'm getting quite a few errors.\\n\\n...\\n\\n\`{%- endif %}\`\\n\\n\`at row 42, column 9:\`\\n\\n\`{%- if role not in role_indicators %}\`\\n\\n\`{{- raise_exception('Unknown role: ' ~ role) }}\`\\n\\n\`^\`\\n\\n\`...\`","edited":1752874534,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3w2fwl","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Is anyone else having problems with their posted chat template? I&amp;#39;m getting quite a few errors.&lt;/p&gt;\\n\\n&lt;p&gt;...&lt;/p&gt;\\n\\n&lt;p&gt;&lt;code&gt;{%- endif %}&lt;/code&gt;&lt;/p&gt;\\n\\n&lt;p&gt;&lt;code&gt;at row 42, column 9:&lt;/code&gt;&lt;/p&gt;\\n\\n&lt;p&gt;&lt;code&gt;{%- if role not in role_indicators %}&lt;/code&gt;&lt;/p&gt;\\n\\n&lt;p&gt;&lt;code&gt;{{- raise_exception(&amp;#39;Unknown role: &amp;#39; ~ role) }}&lt;/code&gt;&lt;/p&gt;\\n\\n&lt;p&gt;&lt;code&gt;^&lt;/code&gt;&lt;/p&gt;\\n\\n&lt;p&gt;&lt;code&gt;...&lt;/code&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m31z4z/support_for_exaone_40_model_architecture_has_been/n3w2fwl/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752874314,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m31z4z","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3wf7et","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"davidpfarrell","can_mod_post":false,"created_utc":1752878526,"send_replies":true,"parent_id":"t1_n3we1e6","score":1,"author_fullname":"t2_24bfwy6e","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"And it worked!\\n\\n\\n\\n*Prompt*:\\n\\n&gt;sudo make me a sandwich\\n\\n\\n\\n*exaone-4.0-32b*:\\n\\n&gt;Here’s your **perfect sandwich**—no actual command line required! 😄\\n\\n# 🥪 Command-Line Sandwich Recipe\\n\\n    sudo apt-get install bread cheese lettuce tomato mayo mustard --assume-yes &amp;&amp; \\\\\\n    mkdir -p /tmp/sandwich &amp;&amp; \\\\right/*.ingredients | sort &gt;&gt; /tmp/sandwich &amp; \\\\\\n    systemctl start sandwich_maker --no-ping || echo \\"Manual assembly required\\"\\n\\n*(Output: A delicious, slightly toasted sandwich with your favorite toppings.)*\\n\\n# 🔥 Bonus Hot Sauce Method (for the adventurous):\\n\\n    echo \\"🌶️ Spicy Mode Activated!\\" &amp;&amp; \\\\\\n    pip install sriracha &amp;&amp; \\\\\\n    apply --with-fire = yes\\n\\nLet me know if you'd like a *vegan*, *gluten-free*, or *overly complex* version. 🚀\\n\\n  \\n\\\\---\\n\\n9.84 tok/sec • 207 tokens • 1.07s to first token\\n\\nStop reason: EOS Token Found","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3wf7et","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;And it worked!&lt;/p&gt;\\n\\n&lt;p&gt;&lt;em&gt;Prompt&lt;/em&gt;:&lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;sudo make me a sandwich&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;&lt;em&gt;exaone-4.0-32b&lt;/em&gt;:&lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;Here’s your &lt;strong&gt;perfect sandwich&lt;/strong&gt;—no actual command line required! 😄&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;h1&gt;🥪 Command-Line Sandwich Recipe&lt;/h1&gt;\\n\\n&lt;pre&gt;&lt;code&gt;sudo apt-get install bread cheese lettuce tomato mayo mustard --assume-yes &amp;amp;&amp;amp; \\\\\\nmkdir -p /tmp/sandwich &amp;amp;&amp;amp; \\\\right/*.ingredients | sort &amp;gt;&amp;gt; /tmp/sandwich &amp;amp; \\\\\\nsystemctl start sandwich_maker --no-ping || echo &amp;quot;Manual assembly required&amp;quot;\\n&lt;/code&gt;&lt;/pre&gt;\\n\\n&lt;p&gt;&lt;em&gt;(Output: A delicious, slightly toasted sandwich with your favorite toppings.)&lt;/em&gt;&lt;/p&gt;\\n\\n&lt;h1&gt;🔥 Bonus Hot Sauce Method (for the adventurous):&lt;/h1&gt;\\n\\n&lt;pre&gt;&lt;code&gt;echo &amp;quot;🌶️ Spicy Mode Activated!&amp;quot; &amp;amp;&amp;amp; \\\\\\npip install sriracha &amp;amp;&amp;amp; \\\\\\napply --with-fire = yes\\n&lt;/code&gt;&lt;/pre&gt;\\n\\n&lt;p&gt;Let me know if you&amp;#39;d like a &lt;em&gt;vegan&lt;/em&gt;, &lt;em&gt;gluten-free&lt;/em&gt;, or &lt;em&gt;overly complex&lt;/em&gt; version. 🚀&lt;/p&gt;\\n\\n&lt;p&gt;---&lt;/p&gt;\\n\\n&lt;p&gt;9.84 tok/sec • 207 tokens • 1.07s to first token&lt;/p&gt;\\n\\n&lt;p&gt;Stop reason: EOS Token Found&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m31z4z","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m31z4z/support_for_exaone_40_model_architecture_has_been/n3wf7et/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752878526,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3we1e6","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"davidpfarrell","can_mod_post":false,"created_utc":1752878125,"send_replies":true,"parent_id":"t3_1m31z4z","score":-1,"author_fullname":"t2_24bfwy6e","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Okay LM Studio, ball is in your court for - I've had it downloaded for a few days now just for it to be supported so I can load it ... \\n\\n  \\nOH SNAP! I decided to look real quick before clicking submit to see if the LMS Ollama runtime was updated, and it is!\\n\\n  \\nI'm off to try to load it ...\\n\\n  \\nNote: There's an error in the original chat template - I can post an update if its still broked for people","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3we1e6","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Okay LM Studio, ball is in your court for - I&amp;#39;ve had it downloaded for a few days now just for it to be supported so I can load it ... &lt;/p&gt;\\n\\n&lt;p&gt;OH SNAP! I decided to look real quick before clicking submit to see if the LMS Ollama runtime was updated, and it is!&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;m off to try to load it ...&lt;/p&gt;\\n\\n&lt;p&gt;Note: There&amp;#39;s an error in the original chat template - I can post an update if its still broked for people&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m31z4z/support_for_exaone_40_model_architecture_has_been/n3we1e6/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752878125,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m31z4z","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-1}}],"before":null}}]`),n=()=>e.jsx(l,{data:t});export{n as default};
