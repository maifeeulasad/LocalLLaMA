import{j as e}from"./index-F0NXdzZX.js";import{R as l}from"./RedditPostRenderer-CoEZB1dt.js";import"./index-DrtravXm.js";const t=JSON.parse('[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"H-Net: a hierarchical network that replaces tokenization with a dynamic chunking process directly inside the model, automatically discovering and operating over meaningful units of data","link_flair_richtext":[{"e":"text","t":"News"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lxd7nh","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.98,"author_flair_background_color":null,"subreddit_type":"public","ups":49,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_gi7a36v6","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"News","can_mod_post":false,"score":49,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"default","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":false,"mod_note":null,"created":1752255519,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"arxiv.org","allow_live_comments":false,"selftext_html":null,"likes":null,"suggested_sort":null,"banned_at_utc":null,"url_overridden_by_dest":"https://arxiv.org/pdf/2507.07955","view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#cc3600","id":"1lxd7nh","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"HOLUPREDICTIONS","discussion_type":null,"num_comments":6,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lxd7nh/hnet_a_hierarchical_network_that_replaces/","stickied":false,"url":"https://arxiv.org/pdf/2507.07955","subreddit_subscribers":498115,"created_utc":1752255519,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"7d1f04e6-4920-11ef-b2e1-2e580594e1a1","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2mp5uz","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"LagOps91","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2lmp6j","score":2,"author_fullname":"t2_3wi6j7vwh","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"at first glance, yes. but look at it this way. the \\"ground truth\\" is just the individual characters. from this you typically go to tokens as a more coarse abstraction that bundles semantics. in large concept models, you go further, going on (sub)sentence level.\\n\\nyou could do the same with H-Net. from characters you go to token-like patches and from token-like patches you go to sub-sentence level patches. based on that you can run a transformer on sub-sentence level in/outputs. pretty much how the large concept model architecture works.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2mp5uz","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;at first glance, yes. but look at it this way. the &amp;quot;ground truth&amp;quot; is just the individual characters. from this you typically go to tokens as a more coarse abstraction that bundles semantics. in large concept models, you go further, going on (sub)sentence level.&lt;/p&gt;\\n\\n&lt;p&gt;you could do the same with H-Net. from characters you go to token-like patches and from token-like patches you go to sub-sentence level patches. based on that you can run a transformer on sub-sentence level in/outputs. pretty much how the large concept model architecture works.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lxd7nh","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxd7nh/hnet_a_hierarchical_network_that_replaces/n2mp5uz/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752272304,"author_flair_text":null,"treatment_tags":[],"created_utc":1752272304,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n2lmp6j","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ninjasaid13","can_mod_post":false,"created_utc":1752260601,"send_replies":true,"parent_id":"t1_n2le692","score":1,"author_fullname":"t2_qjpsv","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"&gt;kind of like the \\"large concept model\\", only that it naturally emerges from the architecture itself and is trained all in one go.\\n\\nreally? they look like complete different things.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2lmp6j","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"Llama 3.1"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;kind of like the &amp;quot;large concept model&amp;quot;, only that it naturally emerges from the architecture itself and is trained all in one go.&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;really? they look like complete different things.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lxd7nh","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxd7nh/hnet_a_hierarchical_network_that_replaces/n2lmp6j/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752260601,"author_flair_text":"Llama 3.1","treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":"#93b1ba","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2le692","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"LagOps91","can_mod_post":false,"created_utc":1752258126,"send_replies":true,"parent_id":"t3_1lxd7nh","score":10,"author_fullname":"t2_3wi6j7vwh","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"thanks for sharing the paper! self-learned chunking and a natural extension to hieararchical chunking? that could seriously elevate models to think more abstractly about concepts, even at the pre-training stage. this could seriously boost the performance of base models by building more abstract, rich representations from the get go. kind of like the \\"large concept model\\", only that it naturally emerges from the architecture itself and is trained all in one go.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2le692","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;thanks for sharing the paper! self-learned chunking and a natural extension to hieararchical chunking? that could seriously elevate models to think more abstractly about concepts, even at the pre-training stage. this could seriously boost the performance of base models by building more abstract, rich representations from the get go. kind of like the &amp;quot;large concept model&amp;quot;, only that it naturally emerges from the architecture itself and is trained all in one go.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxd7nh/hnet_a_hierarchical_network_that_replaces/n2le692/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752258126,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lxd7nh","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":10}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2lpq25","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ResidentPositive4122","can_mod_post":false,"created_utc":1752261496,"send_replies":true,"parent_id":"t3_1lxd7nh","score":2,"author_fullname":"t2_10nxrjjgay","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"bitter tokens is all you need :)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2lpq25","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;bitter tokens is all you need :)&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxd7nh/hnet_a_hierarchical_network_that_replaces/n2lpq25/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752261496,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lxd7nh","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2mu485","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Accomplished_Ad9530","can_mod_post":false,"created_utc":1752273992,"send_replies":true,"parent_id":"t3_1lxd7nh","score":6,"author_fullname":"t2_88fma001","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Nice one from my favorite lab (well, tied with Hazy Research). Anyway, I just checked their blog and they’ve got a few new posts about H-Nets for those interested. They’re a really good companion to their paper and I wish more labs would do blog deep dives.\\n\\nhttps://goombalab.github.io/blog/","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2mu485","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Nice one from my favorite lab (well, tied with Hazy Research). Anyway, I just checked their blog and they’ve got a few new posts about H-Nets for those interested. They’re a really good companion to their paper and I wish more labs would do blog deep dives.&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://goombalab.github.io/blog/\\"&gt;https://goombalab.github.io/blog/&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxd7nh/hnet_a_hierarchical_network_that_replaces/n2mu485/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752273992,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lxd7nh","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2mx8ud","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Accomplished_Mode170","can_mod_post":false,"created_utc":1752275062,"send_replies":true,"parent_id":"t3_1lxd7nh","score":2,"author_fullname":"t2_4hfmiefj","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Love this ❤️ VAEs all the way down 🐢","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2mx8ud","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Love this ❤️ VAEs all the way down 🐢&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxd7nh/hnet_a_hierarchical_network_that_replaces/n2mx8ud/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752275062,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lxd7nh","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}}]'),n=()=>e.jsx(l,{data:t});export{n as default};
