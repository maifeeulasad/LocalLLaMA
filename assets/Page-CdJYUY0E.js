import{j as e}from"./index-DQXiEb7D.js";import{R as l}from"./RedditPostRenderer-BjndLgq8.js";import"./index-B-ILyjT1.js";const t=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"So I have had FOMO on claudecode, but I refuse to give them my prompts or pay $100-$200 a month.   So 2 days ago, I saw that moonshot provides an anthropic API to kimi k2 so folks could use it with claude code.  Well, many folks are already doing that with local.   So if you don't know, now you know.  This is how I did it in Linux, should be easy to replicate in OSX or Windows with WSL.\\n\\nStart your local LLM API  \\n  \\nInstall claude code\\n\\ninstall a proxy - [https://github.com/1rgs/claude-code-proxy](https://github.com/1rgs/claude-code-proxy)\\n\\nEdit the [server.py](http://server.py) proxy and point it to your OpenAI endpoint,  could be llama.cpp, ollama, vllm, whatever you are running.  \\n\\n\\nAdd the line above load\\\\_dotenv  \\n\\\\+litellm.api\\\\_base = \\"http://yokujin:8083/v1\\"  # use your localhost name/IP/ports\\n\\nStart the proxy according to the docs which will run it in localhost:8082\\n\\n  \\nexport ANTHROPIC\\\\_BASE\\\\_URL=http://localhost:8082\\n\\nexport ANTHROPIC\\\\_AUTH\\\\_TOKEN=\\"sk-localkey\\"\\n\\nrun claude code\\n\\nI just created my first code then decided to post this.  I'm running the latest mistral-small-24b on that host.  I'm going to be driving it with various models, gemma3-27b, qwen3-32b/235b, deepseekv3 etc   \\n\\n","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Use claudecode with local models","link_flair_richtext":[{"e":"text","t":"Resources"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1m118is","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.95,"author_flair_background_color":"#bbbdbf","subreddit_type":"public","ups":59,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","is_original_content":false,"author_fullname":"t2_ah13x","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Resources","can_mod_post":false,"score":59,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"gildings":{},"post_hint":"self","content_categories":null,"is_self":true,"mod_note":null,"created":1752633482,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"richtext","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;So I have had FOMO on claudecode, but I refuse to give them my prompts or pay $100-$200 a month.   So 2 days ago, I saw that moonshot provides an anthropic API to kimi k2 so folks could use it with claude code.  Well, many folks are already doing that with local.   So if you don&amp;#39;t know, now you know.  This is how I did it in Linux, should be easy to replicate in OSX or Windows with WSL.&lt;/p&gt;\\n\\n&lt;p&gt;Start your local LLM API  &lt;/p&gt;\\n\\n&lt;p&gt;Install claude code&lt;/p&gt;\\n\\n&lt;p&gt;install a proxy - &lt;a href=\\"https://github.com/1rgs/claude-code-proxy\\"&gt;https://github.com/1rgs/claude-code-proxy&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;Edit the &lt;a href=\\"http://server.py\\"&gt;server.py&lt;/a&gt; proxy and point it to your OpenAI endpoint,  could be llama.cpp, ollama, vllm, whatever you are running.  &lt;/p&gt;\\n\\n&lt;p&gt;Add the line above load_dotenv&lt;br/&gt;\\n+litellm.api_base = &amp;quot;http://yokujin:8083/v1&amp;quot;  # use your localhost name/IP/ports&lt;/p&gt;\\n\\n&lt;p&gt;Start the proxy according to the docs which will run it in localhost:8082&lt;/p&gt;\\n\\n&lt;p&gt;export ANTHROPIC_BASE_URL=http://localhost:8082&lt;/p&gt;\\n\\n&lt;p&gt;export ANTHROPIC_AUTH_TOKEN=&amp;quot;sk-localkey&amp;quot;&lt;/p&gt;\\n\\n&lt;p&gt;run claude code&lt;/p&gt;\\n\\n&lt;p&gt;I just created my first code then decided to post this.  I&amp;#39;m running the latest mistral-small-24b on that host.  I&amp;#39;m going to be driving it with various models, gemma3-27b, qwen3-32b/235b, deepseekv3 etc   &lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"preview":{"images":[{"source":{"url":"https://external-preview.redd.it/QeKKT8LaH96at-4kJT3oqlCjE7lEbnowQ_YcqEz3vg8.png?auto=webp&amp;s=68bc6e3d6ec5115efa359cd639a136a139249cd6","width":1200,"height":600},"resolutions":[{"url":"https://external-preview.redd.it/QeKKT8LaH96at-4kJT3oqlCjE7lEbnowQ_YcqEz3vg8.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=64c4232e878310d5cd2b2010b00def92a85e8dfe","width":108,"height":54},{"url":"https://external-preview.redd.it/QeKKT8LaH96at-4kJT3oqlCjE7lEbnowQ_YcqEz3vg8.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=547875d21701770fe9eb0d034e9f235dc32a9745","width":216,"height":108},{"url":"https://external-preview.redd.it/QeKKT8LaH96at-4kJT3oqlCjE7lEbnowQ_YcqEz3vg8.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=bf40ec3077ffa1075c9f9f7bb08a5b709369e1a3","width":320,"height":160},{"url":"https://external-preview.redd.it/QeKKT8LaH96at-4kJT3oqlCjE7lEbnowQ_YcqEz3vg8.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=2fe791c6eb3a178aa4f0478e4b188da714f5ad8d","width":640,"height":320},{"url":"https://external-preview.redd.it/QeKKT8LaH96at-4kJT3oqlCjE7lEbnowQ_YcqEz3vg8.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=8c1db43071e871e0cdebaaefa0c8935a0144749f","width":960,"height":480},{"url":"https://external-preview.redd.it/QeKKT8LaH96at-4kJT3oqlCjE7lEbnowQ_YcqEz3vg8.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=8ad9d773b3465d4a4f8363755c862da032bedbe2","width":1080,"height":540}],"variants":{},"id":"QeKKT8LaH96at-4kJT3oqlCjE7lEbnowQ_YcqEz3vg8"}],"enabled":false},"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":"llama.cpp","treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#ccac2b","id":"1m118is","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"segmond","discussion_type":null,"num_comments":14,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":"light","permalink":"/r/LocalLLaMA/comments/1m118is/use_claudecode_with_local_models/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1m118is/use_claudecode_with_local_models/","subreddit_subscribers":499774,"created_utc":1752633482,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3eep5s","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ResidentPositive4122","can_mod_post":false,"created_utc":1752645491,"send_replies":true,"parent_id":"t1_n3e1nkx","score":4,"author_fullname":"t2_10nxrjjgay","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Did it actually work? \\n\\nWhen you have the chance, could you test devstral as well?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3eep5s","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Did it actually work? &lt;/p&gt;\\n\\n&lt;p&gt;When you have the chance, could you test devstral as well?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m118is","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m118is/use_claudecode_with_local_models/n3eep5s/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752645491,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}}],"before":null}},"user_reports":[],"saved":false,"id":"n3e1nkx","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"segmond","can_mod_post":false,"created_utc":1752638965,"send_replies":true,"parent_id":"t3_1m118is","score":8,"author_fullname":"t2_ah13x","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Sample output with mistral-small-24b on llama.cpp code base\\n\\nhttps://preview.redd.it/snx67md8v5df1.png?width=1404&amp;format=png&amp;auto=webp&amp;s=016bdc1469084069d5e52a8cbf15b3b98cd27cf7","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3e1nkx","is_submitter":true,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Sample output with mistral-small-24b on llama.cpp code base&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://preview.redd.it/snx67md8v5df1.png?width=1404&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=016bdc1469084069d5e52a8cbf15b3b98cd27cf7\\"&gt;https://preview.redd.it/snx67md8v5df1.png?width=1404&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=016bdc1469084069d5e52a8cbf15b3b98cd27cf7&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m118is/use_claudecode_with_local_models/n3e1nkx/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752638965,"media_metadata":{"snx67md8v5df1":{"status":"valid","e":"Image","m":"image/png","p":[{"y":95,"x":108,"u":"https://preview.redd.it/snx67md8v5df1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=0e176429f96f56b9207cea035d974683deeb66d6"},{"y":191,"x":216,"u":"https://preview.redd.it/snx67md8v5df1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=55b45d2213d67d7e99a808fc710c37286c5beccb"},{"y":283,"x":320,"u":"https://preview.redd.it/snx67md8v5df1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=53c5daa57fba3987f6fea6629172a9f1a33d18ac"},{"y":567,"x":640,"u":"https://preview.redd.it/snx67md8v5df1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=94d9a1e1fc294e27ff1bf88b6d5a07476a5708c9"},{"y":851,"x":960,"u":"https://preview.redd.it/snx67md8v5df1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=9f20cabc2e3925b949ba9e5a1a3ca1a9f1e0b65e"},{"y":958,"x":1080,"u":"https://preview.redd.it/snx67md8v5df1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=dfbd51733fd97cb25cdd1a87e532b4f1a545b040"}],"s":{"y":1246,"x":1404,"u":"https://preview.redd.it/snx67md8v5df1.png?width=1404&amp;format=png&amp;auto=webp&amp;s=016bdc1469084069d5e52a8cbf15b3b98cd27cf7"},"id":"snx67md8v5df1"}},"author_flair_text":"llama.cpp","treatment_tags":[],"link_id":"t3_1m118is","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":8}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3ek31h","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"madsheep","can_mod_post":false,"created_utc":1752648382,"send_replies":true,"parent_id":"t3_1m118is","score":10,"author_fullname":"t2_3hlhf","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Its a bit easier with https://github.com/musistudio/claude-code-router","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3ek31h","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Its a bit easier with &lt;a href=\\"https://github.com/musistudio/claude-code-router\\"&gt;https://github.com/musistudio/claude-code-router&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m118is/use_claudecode_with_local_models/n3ek31h/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752648382,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m118is","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":10}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3dra2q","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"segmond","can_mod_post":false,"created_utc":1752634834,"send_replies":true,"parent_id":"t1_n3dqj73","score":6,"author_fullname":"t2_ah13x","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I don't spend money on Anthropic or OpenAI, they are anti open AI and want it regulated so I won't support them at all.  No idea how sonnet performs.   Speed is a matter of money and GPU.  I'm running Mistral on a 3090.  If you want faster speed get 4090 or 5090.   Speed is also a matter of size of model, something like Deepseek I currently run at 5tk/s  I'll probably do 2tk/s with Kimi, but if I move my current system to epyc I can probably get 10tk/s.    So slow, however won't run into rate limiting like a lot of folks are doing or getting downgraded to lower quality models or quants.   But with this approach, you can point it Openrouter or even groq","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3dra2q","is_submitter":true,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I don&amp;#39;t spend money on Anthropic or OpenAI, they are anti open AI and want it regulated so I won&amp;#39;t support them at all.  No idea how sonnet performs.   Speed is a matter of money and GPU.  I&amp;#39;m running Mistral on a 3090.  If you want faster speed get 4090 or 5090.   Speed is also a matter of size of model, something like Deepseek I currently run at 5tk/s  I&amp;#39;ll probably do 2tk/s with Kimi, but if I move my current system to epyc I can probably get 10tk/s.    So slow, however won&amp;#39;t run into rate limiting like a lot of folks are doing or getting downgraded to lower quality models or quants.   But with this approach, you can point it Openrouter or even groq&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m118is","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m118is/use_claudecode_with_local_models/n3dra2q/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752634834,"author_flair_text":"llama.cpp","treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}}],"before":null}},"user_reports":[],"saved":false,"id":"n3dqj73","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"1doge-1usd","can_mod_post":false,"created_utc":1752634549,"send_replies":true,"parent_id":"t3_1m118is","score":3,"author_fullname":"t2_72zarncb","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"This is super cool. Would love to hear your thoughts comparing Sonnet vs Kimi vs local \\\\~20-30b models in terms of speed and \\"coding intelligence\\"!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3dqj73","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;This is super cool. Would love to hear your thoughts comparing Sonnet vs Kimi vs local ~20-30b models in terms of speed and &amp;quot;coding intelligence&amp;quot;!&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m118is/use_claudecode_with_local_models/n3dqj73/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752634549,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m118is","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3fj7kk","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Illustrious-Lake2603","can_mod_post":false,"created_utc":1752666930,"send_replies":true,"parent_id":"t1_n3es2a0","score":1,"author_fullname":"t2_8v00ut7b","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Its a pain to setup with LM Studio. Nothing I do works!! Always get some strange error when trying to run!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3fj7kk","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Its a pain to setup with LM Studio. Nothing I do works!! Always get some strange error when trying to run!&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m118is","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m118is/use_claudecode_with_local_models/n3fj7kk/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752666930,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3es2a0","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"The_Wismut","can_mod_post":false,"created_utc":1752652833,"send_replies":true,"parent_id":"t3_1m118is","score":3,"author_fullname":"t2_6l9d58w4","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Use opencode instead, it's at least as good and it supports many providers including local models out of the box: https://github.com/sst/opencode","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3es2a0","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Use opencode instead, it&amp;#39;s at least as good and it supports many providers including local models out of the box: &lt;a href=\\"https://github.com/sst/opencode\\"&gt;https://github.com/sst/opencode&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m118is/use_claudecode_with_local_models/n3es2a0/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752652833,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m118is","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3dqis1","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"segmond","can_mod_post":false,"created_utc":1752634544,"send_replies":true,"parent_id":"t1_n3dpq3e","score":1,"author_fullname":"t2_ah13x","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I don't know, I just installed it.  I haven't used roocode and haven't used Aider in a few months, with Aider you are the driver, you steer and do a good chunk of the work.  With claude code, you leave it and hope it figures it out.  If you are lucky, you can leave and come back 4 hours to working code.   My plan is to see how it goes, see if I can get Kimi K2 to run locally then put it to work.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3dqis1","is_submitter":true,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I don&amp;#39;t know, I just installed it.  I haven&amp;#39;t used roocode and haven&amp;#39;t used Aider in a few months, with Aider you are the driver, you steer and do a good chunk of the work.  With claude code, you leave it and hope it figures it out.  If you are lucky, you can leave and come back 4 hours to working code.   My plan is to see how it goes, see if I can get Kimi K2 to run locally then put it to work.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m118is","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m118is/use_claudecode_with_local_models/n3dqis1/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752634544,"author_flair_text":"llama.cpp","treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3dpq3e","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ForsookComparison","can_mod_post":false,"created_utc":1752634246,"send_replies":true,"parent_id":"t3_1m118is","score":1,"author_fullname":"t2_on5es7pe3","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"How does this work with straight-shot tasks (is it better than local Aider?)?\\n\\nHow does this work with agentic coding tasks (is it better than local Roo Code)?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3dpq3e","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;How does this work with straight-shot tasks (is it better than local Aider?)?&lt;/p&gt;\\n\\n&lt;p&gt;How does this work with agentic coding tasks (is it better than local Roo Code)?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m118is/use_claudecode_with_local_models/n3dpq3e/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752634246,"author_flair_text":"llama.cpp","treatment_tags":[],"link_id":"t3_1m118is","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3eqzt9","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"No-Dot-6573","can_mod_post":false,"created_utc":1752652216,"send_replies":true,"parent_id":"t3_1m118is","score":1,"author_fullname":"t2_ntj19d0dp","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Nice, thank you! \\nShouldn't devstral be a more viable option than mistral small for this usecase?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3eqzt9","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Nice, thank you! \\nShouldn&amp;#39;t devstral be a more viable option than mistral small for this usecase?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m118is/use_claudecode_with_local_models/n3eqzt9/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752652216,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m118is","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3erw34","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Busy-Chemistry7747","can_mod_post":false,"created_utc":1752652733,"send_replies":true,"parent_id":"t3_1m118is","score":1,"author_fullname":"t2_pu76ft4b","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Just use opencoder?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3erw34","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Just use opencoder?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m118is/use_claudecode_with_local_models/n3erw34/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752652733,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m118is","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3fdfft","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Danmoreng","can_mod_post":false,"created_utc":1752664485,"send_replies":true,"parent_id":"t3_1m118is","score":1,"author_fullname":"t2_7z26p","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"How does Claude code compare to Gemini CLI? Only used the later one by now because it has large free limits and had pretty good results with it.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3fdfft","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;How does Claude code compare to Gemini CLI? Only used the later one by now because it has large free limits and had pretty good results with it.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m118is/use_claudecode_with_local_models/n3fdfft/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752664485,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m118is","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3fi39a","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Putrid-Wafer6725","can_mod_post":false,"created_utc":1752666484,"send_replies":true,"parent_id":"t3_1m118is","score":1,"author_fullname":"t2_11j6m8mngt","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"nice to know \\n\\nI would use stt/opencode instead. I've also seen the  people that use kimi api and it works kinda ok but things like context window being different and whatever small prompt optimizations inside of cc black box has is going to make it less ideal for real use","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3fi39a","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;nice to know &lt;/p&gt;\\n\\n&lt;p&gt;I would use stt/opencode instead. I&amp;#39;ve also seen the  people that use kimi api and it works kinda ok but things like context window being different and whatever small prompt optimizations inside of cc black box has is going to make it less ideal for real use&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m118is/use_claudecode_with_local_models/n3fi39a/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752666484,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m118is","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3e2ff4","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":"LOW_SCORE","no_follow":true,"author":"Ok_Needleworker_5247","can_mod_post":false,"created_utc":1752639302,"send_replies":true,"parent_id":"t3_1m118is","score":-10,"author_fullname":"t2_1gmprv51a1","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":true,"body":"If you're interested in comparing different model setups, you might find [this article](https://deep-learning-performance-guide.com) useful. It explores performance and efficiency across various GPUs and models, which could give insight into optimizing your hardware for better speed in your local setup. Might help with decisions on configuring your system for running larger models efficiently.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3e2ff4","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;If you&amp;#39;re interested in comparing different model setups, you might find &lt;a href=\\"https://deep-learning-performance-guide.com\\"&gt;this article&lt;/a&gt; useful. It explores performance and efficiency across various GPUs and models, which could give insight into optimizing your hardware for better speed in your local setup. Might help with decisions on configuring your system for running larger models efficiently.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":"comment score below threshold","distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m118is/use_claudecode_with_local_models/n3e2ff4/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752639302,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m118is","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-10}}],"before":null}}]`),s=()=>e.jsx(l,{data:t});export{s as default};
