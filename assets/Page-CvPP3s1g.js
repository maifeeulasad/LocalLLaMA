import{j as e}from"./index-C_3vMATY.js";import{R as l}from"./RedditPostRenderer-BASV2HeL.js";import"./index-Cyi6qGnC.js";const t=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"I've gotten UD 2 bit quants to work with llama.cpp. I've merged the split ggufs and tried to load that into vllm (v0.9.1) and it says qwen3moe architecture isn't supported for gguf. So I guess my real question here is done anyone repackage unsloth quants in a format that vllm can load? Or is it possible for me to do that?","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Qwen3 tiny/unsloth quants with vllm?","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lmggiz","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":1,"author_flair_background_color":null,"subreddit_type":"public","ups":1,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_mcvyi","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":1,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1751093817,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;ve gotten UD 2 bit quants to work with llama.cpp. I&amp;#39;ve merged the split ggufs and tried to load that into vllm (v0.9.1) and it says qwen3moe architecture isn&amp;#39;t supported for gguf. So I guess my real question here is done anyone repackage unsloth quants in a format that vllm can load? Or is it possible for me to do that?&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":true,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1lmggiz","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"MengerianMango","discussion_type":null,"num_comments":10,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lmggiz/qwen3_tinyunsloth_quants_with_vllm/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lmggiz/qwen3_tinyunsloth_quants_with_vllm/","subreddit_subscribers":492232,"created_utc":1751093817,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"total_awards_received":0,"approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"ups":1,"removal_reason":null,"link_id":"t3_1lmggiz","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n07qasi","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"thirteen-bit","can_mod_post":false,"created_utc":1751102550,"send_replies":true,"parent_id":"t1_n07q382","score":1,"author_fullname":"t2_9l12dgc5","approved_by":null,"mod_note":null,"all_awardings":[],"body":"With your VRAM you may play with speculative decoding too. Try Qwen3 dense and 30B MoE models at lower quants. With 24Gb I've got no improvements, --draft-model actually made it slower","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n07qasi","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;With your VRAM you may play with speculative decoding too. Try Qwen3 dense and 30B MoE models at lower quants. With 24Gb I&amp;#39;ve got no improvements, --draft-model actually made it slower&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lmggiz","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmggiz/qwen3_tinyunsloth_quants_with_vllm/n07qasi/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751102550,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":7,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n07q382","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":"DELETED","no_follow":true,"author":"[deleted]","can_mod_post":false,"send_replies":true,"parent_id":"t1_n07p9eb","score":1,"approved_by":null,"report_reasons":null,"all_awardings":[],"subreddit_id":"t5_81eyvm","body":"[removed]","edited":false,"downs":0,"author_flair_css_class":null,"collapsed":true,"is_submitter":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;[removed]&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"associated_award":null,"stickied":false,"subreddit_type":"public","can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":"dark","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmggiz/qwen3_tinyunsloth_quants_with_vllm/n07q382/","num_reports":null,"locked":false,"name":"t1_n07q382","created":1751102421,"subreddit":"LocalLLaMA","author_flair_text":null,"treatment_tags":[],"created_utc":1751102421,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":6,"author_flair_background_color":"","collapsed_because_crowd_control":null,"mod_reports":[],"mod_note":null,"distinguished":null}}],"before":null}},"user_reports":[],"saved":false,"id":"n07p9eb","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MengerianMango","can_mod_post":false,"send_replies":true,"parent_id":"t1_n07ox23","score":1,"author_fullname":"t2_mcvyi","approved_by":null,"mod_note":null,"all_awardings":[],"body":"Mind showing me your exact llama.cpp command? I'm always wondering if there are flags I'm missing/unaware of.","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n07p9eb","is_submitter":true,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Mind showing me your exact llama.cpp command? I&amp;#39;m always wondering if there are flags I&amp;#39;m missing/unaware of.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lmggiz","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmggiz/qwen3_tinyunsloth_quants_with_vllm/n07p9eb/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751101919,"author_flair_text":null,"treatment_tags":[],"created_utc":1751101919,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n07ox23","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"thirteen-bit","can_mod_post":false,"send_replies":true,"parent_id":"t1_n07micn","score":1,"author_fullname":"t2_9l12dgc5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Ok, I'd not look at vLLM at all until the speed is critical - it may be faster but you'll have to dig through its documentation, github issues and source code for days to optimize it.\\n\\nRegarding llama.cpp: I'd start with Q3 or even Q4 of 235B for RTX 6000 Pro - I'm getting 3.6 tps on small prompts with unsloth's Qwen3-235B-A22B-UD-Q3\\\\_K\\\\_XL on 250W power limited RTX 3090 + i5-12400 w/ 96 Gb of slow DDR4 (unmatched RAM so running at 2133 MHz) and adjust the layers offloaded to CPU.","edited":false,"author_flair_css_class":null,"name":"t1_n07ox23","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Ok, I&amp;#39;d not look at vLLM at all until the speed is critical - it may be faster but you&amp;#39;ll have to dig through its documentation, github issues and source code for days to optimize it.&lt;/p&gt;\\n\\n&lt;p&gt;Regarding llama.cpp: I&amp;#39;d start with Q3 or even Q4 of 235B for RTX 6000 Pro - I&amp;#39;m getting 3.6 tps on small prompts with unsloth&amp;#39;s Qwen3-235B-A22B-UD-Q3_K_XL on 250W power limited RTX 3090 + i5-12400 w/ 96 Gb of slow DDR4 (unmatched RAM so running at 2133 MHz) and adjust the layers offloaded to CPU.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lmggiz","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmggiz/qwen3_tinyunsloth_quants_with_vllm/n07ox23/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751101711,"author_flair_text":null,"collapsed":false,"created_utc":1751101711,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n07micn","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MengerianMango","can_mod_post":false,"send_replies":true,"parent_id":"t1_n07lynw","score":1,"author_fullname":"t2_mcvyi","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Single user. I have an RTX Pro 6000 Blackwell and I'm just trying to get the most speed out of it I can so I can use it for agentic coding. It's already fast enough for chat under llama, but speed matters a lot more when you're having the llm actually do the work, yk.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n07micn","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Single user. I have an RTX Pro 6000 Blackwell and I&amp;#39;m just trying to get the most speed out of it I can so I can use it for agentic coding. It&amp;#39;s already fast enough for chat under llama, but speed matters a lot more when you&amp;#39;re having the llm actually do the work, yk.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lmggiz","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmggiz/qwen3_tinyunsloth_quants_with_vllm/n07micn/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751100276,"author_flair_text":null,"treatment_tags":[],"created_utc":1751100276,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n07lynw","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"thirteen-bit","can_mod_post":false,"send_replies":true,"parent_id":"t1_n07kjn8","score":1,"author_fullname":"t2_9l12dgc5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Ah, 235b is a large one.\\n\\nLooking at [https://github.com/vllm-project/vllm/issues/17327](https://github.com/vllm-project/vllm/issues/17327) it does not seem to work with GGUF.\\n\\nWhat is your target? Do you plan to serve multiple users or do you want to improve single user performance?\\n\\nIf multiple users is a target or vLLM is required for some other reason then you'll probably have to look for increased VRAM to fit at least 4-bit quantization and some context.\\n\\nIf you're targeting (somewhat) improved performance with your existing hardware look at ik\\\\_llama and this quantization: [https://huggingface.co/ubergarm/Qwen3-235B-A22B-GGUF](https://huggingface.co/ubergarm/Qwen3-235B-A22B-GGUF)","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n07lynw","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Ah, 235b is a large one.&lt;/p&gt;\\n\\n&lt;p&gt;Looking at &lt;a href=\\"https://github.com/vllm-project/vllm/issues/17327\\"&gt;https://github.com/vllm-project/vllm/issues/17327&lt;/a&gt; it does not seem to work with GGUF.&lt;/p&gt;\\n\\n&lt;p&gt;What is your target? Do you plan to serve multiple users or do you want to improve single user performance?&lt;/p&gt;\\n\\n&lt;p&gt;If multiple users is a target or vLLM is required for some other reason then you&amp;#39;ll probably have to look for increased VRAM to fit at least 4-bit quantization and some context.&lt;/p&gt;\\n\\n&lt;p&gt;If you&amp;#39;re targeting (somewhat) improved performance with your existing hardware look at ik_llama and this quantization: &lt;a href=\\"https://huggingface.co/ubergarm/Qwen3-235B-A22B-GGUF\\"&gt;https://huggingface.co/ubergarm/Qwen3-235B-A22B-GGUF&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lmggiz","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmggiz/qwen3_tinyunsloth_quants_with_vllm/n07lynw/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751099953,"author_flair_text":null,"treatment_tags":[],"created_utc":1751099953,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n07kjn8","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MengerianMango","can_mod_post":false,"created_utc":1751099108,"send_replies":true,"parent_id":"t1_n07js68","score":1,"author_fullname":"t2_mcvyi","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"&gt;Why are you looking at GGUF at all if you're using vLLM?\\n\\nI don't really know what I'm doing. I just want to run Qwen3 235b with a 2 bit quant, under vllm if possible since ofc I'd prefer to get the most performance I can.\\n\\n&gt;Wasn't AWQ best for vLLM?\\n\\nYou might be right. I hadn't heard of AWQ before now. Seems like it is strictly 4 bit. I don't have enough vram for that.","edited":1751099325,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n07kjn8","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;Why are you looking at GGUF at all if you&amp;#39;re using vLLM?&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;I don&amp;#39;t really know what I&amp;#39;m doing. I just want to run Qwen3 235b with a 2 bit quant, under vllm if possible since ofc I&amp;#39;d prefer to get the most performance I can.&lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;Wasn&amp;#39;t AWQ best for vLLM?&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;You might be right. I hadn&amp;#39;t heard of AWQ before now. Seems like it is strictly 4 bit. I don&amp;#39;t have enough vram for that.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lmggiz","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmggiz/qwen3_tinyunsloth_quants_with_vllm/n07kjn8/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751099108,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n07js68","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"thirteen-bit","can_mod_post":false,"created_utc":1751098656,"send_replies":true,"parent_id":"t3_1lmggiz","score":2,"author_fullname":"t2_9l12dgc5","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Why are you looking at GGUF at all if you're using vLLM?\\n\\nWasn't AWQ best for vLLM?\\n\\n[https://docs.vllm.ai/en/latest/features/quantization/index.html](https://docs.vllm.ai/en/latest/features/quantization/index.html)\\n\\n[https://www.reddit.com/r/LocalLLaMA/comments/1ieoxk0/vllm\\\\_quantization\\\\_performance\\\\_which\\\\_kinds\\\\_work/](https://www.reddit.com/r/LocalLLaMA/comments/1ieoxk0/vllm_quantization_performance_which_kinds_work/)\\n\\n  \\nOtherwise if you want some more meaningful answers here please at least specify the model? There are quite a few Qwen 3 models. [https://huggingface.co/models?search=Qwen/Qwen3](https://huggingface.co/models?search=Qwen/Qwen3)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n07js68","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Why are you looking at GGUF at all if you&amp;#39;re using vLLM?&lt;/p&gt;\\n\\n&lt;p&gt;Wasn&amp;#39;t AWQ best for vLLM?&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://docs.vllm.ai/en/latest/features/quantization/index.html\\"&gt;https://docs.vllm.ai/en/latest/features/quantization/index.html&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://www.reddit.com/r/LocalLLaMA/comments/1ieoxk0/vllm_quantization_performance_which_kinds_work/\\"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1ieoxk0/vllm_quantization_performance_which_kinds_work/&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;Otherwise if you want some more meaningful answers here please at least specify the model? There are quite a few Qwen 3 models. &lt;a href=\\"https://huggingface.co/models?search=Qwen/Qwen3\\"&gt;https://huggingface.co/models?search=Qwen/Qwen3&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmggiz/qwen3_tinyunsloth_quants_with_vllm/n07js68/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751098656,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lmggiz","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n07wasq","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ahmetegesel","can_mod_post":false,"created_utc":1751106069,"send_replies":true,"parent_id":"t3_1lmggiz","score":1,"author_fullname":"t2_69skhb61","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Welcome to the club. I have been trying to run 30B A3B UD 8bit on A6000 Ada with no luck. It looks like the support is missing on transformers side. I saw a PR for bringing qwen3 support but nobody is trying to bring qwen3moe support. I tried to fork transformers myself and tried a few things but couldn’t manage. \\n\\nFP8 is not working on A6000 apparently, it is a new architecture that old gpus do not support. INT4 was stupid, so was AWQ. I tried gguf but no luck. \\n\\nNow I am back to llamacpp but not sure how it would its concurrency performance be compared to vLLM.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n07wasq","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Welcome to the club. I have been trying to run 30B A3B UD 8bit on A6000 Ada with no luck. It looks like the support is missing on transformers side. I saw a PR for bringing qwen3 support but nobody is trying to bring qwen3moe support. I tried to fork transformers myself and tried a few things but couldn’t manage. &lt;/p&gt;\\n\\n&lt;p&gt;FP8 is not working on A6000 apparently, it is a new architecture that old gpus do not support. INT4 was stupid, so was AWQ. I tried gguf but no luck. &lt;/p&gt;\\n\\n&lt;p&gt;Now I am back to llamacpp but not sure how it would its concurrency performance be compared to vLLM.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmggiz/qwen3_tinyunsloth_quants_with_vllm/n07wasq/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751106069,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lmggiz","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n07bk98","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MengerianMango","can_mod_post":false,"created_utc":1751093916,"send_replies":true,"parent_id":"t3_1lmggiz","score":-4,"author_fullname":"t2_mcvyi","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Pinging u/danielhanchen","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n07bk98","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Pinging &lt;a href=\\"/u/danielhanchen\\"&gt;u/danielhanchen&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmggiz/qwen3_tinyunsloth_quants_with_vllm/n07bk98/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751093916,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lmggiz","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-4}}],"before":null}}]`),r=()=>e.jsx(l,{data:t});export{r as default};
