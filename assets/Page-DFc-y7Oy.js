import{j as e}from"./index-BOnf-UhU.js";import{R as l}from"./RedditPostRenderer-Ce39qICS.js";import"./index-CDase6VD.js";const a=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Less than two weeks Kimi K2's release, Alibaba Qwen's new Qwen3-Coder surpasses it with half the size and double the context window. Despite a significant initial lead, open source models are catching up to closed source and seem to be reaching escape velocity.","link_flair_richtext":[{"e":"text","t":"Discussion"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":95,"top_awarded_type":null,"hide_score":false,"name":"t3_1m7kkyn","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.93,"author_flair_background_color":null,"ups":102,"total_awards_received":0,"media_embed":{},"thumbnail_width":140,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_58qturpl","secure_media":null,"is_reddit_media_domain":true,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Discussion","can_mod_post":false,"score":102,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"https://a.thumbs.redditmedia.com/4F_QBog-_2mH7QRiv8VyzkdamiGlY40D_u3V_zWrFe8.jpg","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"post_hint":"image","content_categories":null,"is_self":false,"subreddit_type":"public","created":1753303228,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"i.redd.it","allow_live_comments":false,"selftext_html":null,"likes":null,"suggested_sort":null,"banned_at_utc":null,"url_overridden_by_dest":"https://i.redd.it/krjfba3oqoef1.jpeg","view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"preview":{"images":[{"source":{"url":"https://preview.redd.it/krjfba3oqoef1.jpeg?auto=webp&amp;s=b6cbfb5587cef2fa66062ecc89fb256764949473","width":1512,"height":1032},"resolutions":[{"url":"https://preview.redd.it/krjfba3oqoef1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=8cf6d39fa1fa4f5683732a0b8993daf74e849afa","width":108,"height":73},{"url":"https://preview.redd.it/krjfba3oqoef1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=75db258338b42adc68e3bb0413ff39fa017bc706","width":216,"height":147},{"url":"https://preview.redd.it/krjfba3oqoef1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=24d9a0eca278a1c3b8db5b848d01205a811bb68d","width":320,"height":218},{"url":"https://preview.redd.it/krjfba3oqoef1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=c50574d0e0fc9f8e0044c2d18d3618b1d155e4e7","width":640,"height":436},{"url":"https://preview.redd.it/krjfba3oqoef1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=a748b89f2ff4d8a9a5d9acb8b0eb069410e02c88","width":960,"height":655},{"url":"https://preview.redd.it/krjfba3oqoef1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e9a2fa720b512765ba2dc5a092f21cccc7eac5f8","width":1080,"height":737}],"variants":{},"id":"SyAH9oAX8vUViOHksDj2yqNlqn4fwNnt93W4G27ThZw"}],"enabled":true},"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"5f921ea4-c7bc-11ed-9c23-3a00622979b4","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"mod_note":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"num_reports":null,"removal_reason":null,"link_flair_background_color":"#646d73","id":"1m7kkyn","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"abdouhlili","discussion_type":null,"num_comments":25,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1m7kkyn/less_than_two_weeks_kimi_k2s_release_alibaba/","stickied":false,"url":"https://i.redd.it/krjfba3oqoef1.jpeg","subreddit_subscribers":503517,"created_utc":1753303228,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4sb7t8","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"No_Efficiency_1144","can_mod_post":false,"created_utc":1753305126,"send_replies":true,"parent_id":"t1_n4s6aur","score":9,"author_fullname":"t2_1nkj9l14b0","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yeah look at Dalle 3\\n\\n\\nIt’s literally an old school diffusion model (not flow matching) with the original GPT 4 as the text encoder.\\n\\n\\nYet their dataset was so good that to this day it has a very wide range of subjects and strong prompt following.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4sb7t8","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yeah look at Dalle 3&lt;/p&gt;\\n\\n&lt;p&gt;It’s literally an old school diffusion model (not flow matching) with the original GPT 4 as the text encoder.&lt;/p&gt;\\n\\n&lt;p&gt;Yet their dataset was so good that to this day it has a very wide range of subjects and strong prompt following.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m7kkyn","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7kkyn/less_than_two_weeks_kimi_k2s_release_alibaba/n4sb7t8/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753305126,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":9}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4skude","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"-dysangel-","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4seqq9","score":3,"author_fullname":"t2_12ggykute6","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I think at some point this is not going to be about the intelligence of the model - it's simply going to be about how effectively we can communicate to the model. Just like real software development teams are limited by how well they can communicate and stay in sync on their goals. I think we're already getting towards this point. With Claude 4.0, I no longer feel like it just doesn't \\"get\\" some things in the same way that Claude 3.5 and Claude 3.7 struggled - I feel like it can do anything that I can explain to it.","edited":false,"author_flair_css_class":null,"name":"t1_n4skude","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I think at some point this is not going to be about the intelligence of the model - it&amp;#39;s simply going to be about how effectively we can communicate to the model. Just like real software development teams are limited by how well they can communicate and stay in sync on their goals. I think we&amp;#39;re already getting towards this point. With Claude 4.0, I no longer feel like it just doesn&amp;#39;t &amp;quot;get&amp;quot; some things in the same way that Claude 3.5 and Claude 3.7 struggled - I feel like it can do anything that I can explain to it.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1m7kkyn","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":"light","treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7kkyn/less_than_two_weeks_kimi_k2s_release_alibaba/n4skude/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753308007,"author_flair_text":"llama.cpp","collapsed":false,"created_utc":1753308007,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n4seqq9","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"randombsname1","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4sda9s","score":-1,"author_fullname":"t2_8t0zww56","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Yeah im not saying these have no utility, and im sure they are good for a lot of tasks, but since I am using them for coding--typically new stacks with limited implementation examples. Then I like to squeeze every lazy drop i can get out of a model. \\n\\nEven Claude Opus I never take the initial code produced. I always iterate over it with documentation, and thus i need the best model available so im not just spinning my wheels longer than needed. \\n\\nWhich means essentially I'll always be looking for SOTA/cutting edge performance. \\n\\nWhich isnt going to come from any Chinese models as long as the entirety of their work is based on U.S. models. Its just not possible to lead when you copy what is actually in the lead, inherently, lol. \\n\\nAgain, I can see great uses for open source models like this. It's just not as exciting for me as new OpenAI, Google, or Anthropic models where everytime they release something it could be a complete game changer as to how workflows are enhanced moving forward.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4seqq9","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yeah im not saying these have no utility, and im sure they are good for a lot of tasks, but since I am using them for coding--typically new stacks with limited implementation examples. Then I like to squeeze every lazy drop i can get out of a model. &lt;/p&gt;\\n\\n&lt;p&gt;Even Claude Opus I never take the initial code produced. I always iterate over it with documentation, and thus i need the best model available so im not just spinning my wheels longer than needed. &lt;/p&gt;\\n\\n&lt;p&gt;Which means essentially I&amp;#39;ll always be looking for SOTA/cutting edge performance. &lt;/p&gt;\\n\\n&lt;p&gt;Which isnt going to come from any Chinese models as long as the entirety of their work is based on U.S. models. Its just not possible to lead when you copy what is actually in the lead, inherently, lol. &lt;/p&gt;\\n\\n&lt;p&gt;Again, I can see great uses for open source models like this. It&amp;#39;s just not as exciting for me as new OpenAI, Google, or Anthropic models where everytime they release something it could be a complete game changer as to how workflows are enhanced moving forward.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m7kkyn","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7kkyn/less_than_two_weeks_kimi_k2s_release_alibaba/n4seqq9/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753306161,"author_flair_text":null,"treatment_tags":[],"created_utc":1753306161,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":1,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4sda9s","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"-dysangel-","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4sbm4x","score":10,"author_fullname":"t2_12ggykute6","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"The frontier is rapidly approaching \\"good enough\\" for me. In the same way that I don't care about new generations of phones coming out, if Qwen 3 Coder is as good as Claude 4.0 - I am going to get a LOT of utility out of it for the rest of my life. And I still believe we can get Claude 4 or higher coding ability out of a model that only has 32B params. If we really focus on high quality reasoning and software engineering practices, and leave the more general knowledge to RAG.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n4sda9s","is_submitter":false,"collapsed":false,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The frontier is rapidly approaching &amp;quot;good enough&amp;quot; for me. In the same way that I don&amp;#39;t care about new generations of phones coming out, if Qwen 3 Coder is as good as Claude 4.0 - I am going to get a LOT of utility out of it for the rest of my life. And I still believe we can get Claude 4 or higher coding ability out of a model that only has 32B params. If we really focus on high quality reasoning and software engineering practices, and leave the more general knowledge to RAG.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m7kkyn","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7kkyn/less_than_two_weeks_kimi_k2s_release_alibaba/n4sda9s/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753305732,"author_flair_text":"llama.cpp","treatment_tags":[],"created_utc":1753305732,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":10}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4t4cfy","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"randombsname1","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4t32kz","score":0,"author_fullname":"t2_8t0zww56","approved_by":null,"mod_note":null,"all_awardings":[],"body":"Sure, it's fine. It's just doing it based on frontier U.S. LLMs. That's just a fact given jail breaks and the responses we have seen from pretty much all Chinese models.\\n\\nThere isnt any chance that Deepseek was originally trained with 1/10th the resources of U.S. models WITHOUT this being the case by the way. That was a deepseek claim. Not mine. \\n\\nThere isn't any indication that Chinese models are doing anything at the forefront of AI. That's my point. \\n\\nIts cool what they are doing. Which is bringing open source, high-quality models down to a cheap price.\\n\\nI just think it's different than being at the forefront of AI. Seeing as I dont think they have actually achieved anything new or exciting that U.S. frontier models didnt do 6 months prior.","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n4t4cfy","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Sure, it&amp;#39;s fine. It&amp;#39;s just doing it based on frontier U.S. LLMs. That&amp;#39;s just a fact given jail breaks and the responses we have seen from pretty much all Chinese models.&lt;/p&gt;\\n\\n&lt;p&gt;There isnt any chance that Deepseek was originally trained with 1/10th the resources of U.S. models WITHOUT this being the case by the way. That was a deepseek claim. Not mine. &lt;/p&gt;\\n\\n&lt;p&gt;There isn&amp;#39;t any indication that Chinese models are doing anything at the forefront of AI. That&amp;#39;s my point. &lt;/p&gt;\\n\\n&lt;p&gt;Its cool what they are doing. Which is bringing open source, high-quality models down to a cheap price.&lt;/p&gt;\\n\\n&lt;p&gt;I just think it&amp;#39;s different than being at the forefront of AI. Seeing as I dont think they have actually achieved anything new or exciting that U.S. frontier models didnt do 6 months prior.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1m7kkyn","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7kkyn/less_than_two_weeks_kimi_k2s_release_alibaba/n4t4cfy/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753314289,"author_flair_text":null,"treatment_tags":[],"created_utc":1753314289,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}},"user_reports":[],"saved":false,"id":"n4t32kz","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"YouDontSeemRight","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4sue8f","score":3,"author_fullname":"t2_1b7gjxtue9","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"If OpenAI uses an LLM to generate synthetic datasets is it not okay for them to do the same? It's about curating quality datasets. For sure OpenAI was needed to get going but once the fire is lit its only necessary for gain of function.","edited":false,"author_flair_css_class":null,"name":"t1_n4t32kz","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;If OpenAI uses an LLM to generate synthetic datasets is it not okay for them to do the same? It&amp;#39;s about curating quality datasets. For sure OpenAI was needed to get going but once the fire is lit its only necessary for gain of function.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1m7kkyn","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7kkyn/less_than_two_weeks_kimi_k2s_release_alibaba/n4t32kz/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753313869,"author_flair_text":null,"collapsed":false,"created_utc":1753313869,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n4sue8f","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"randombsname1","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4stnnb","score":-1,"author_fullname":"t2_8t0zww56","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"They distilled from U.S. models. That's the key detail, lol. \\n\\nThat's been the case since at least the first deepseek. \\n\\nThey also got slightly worse performance with a smaller dataset. Which is exactly what U.S. models show as well.\\n\\nSonnet and Opus don't show huge intelligence differences, but Opus keeps context far better/longer--which is the real differentiator. \\n\\nOtherwise Opus isn't much more intelligent even though it uses a far bigger dataset.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4sue8f","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;They distilled from U.S. models. That&amp;#39;s the key detail, lol. &lt;/p&gt;\\n\\n&lt;p&gt;That&amp;#39;s been the case since at least the first deepseek. &lt;/p&gt;\\n\\n&lt;p&gt;They also got slightly worse performance with a smaller dataset. Which is exactly what U.S. models show as well.&lt;/p&gt;\\n\\n&lt;p&gt;Sonnet and Opus don&amp;#39;t show huge intelligence differences, but Opus keeps context far better/longer--which is the real differentiator. &lt;/p&gt;\\n\\n&lt;p&gt;Otherwise Opus isn&amp;#39;t much more intelligent even though it uses a far bigger dataset.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m7kkyn","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7kkyn/less_than_two_weeks_kimi_k2s_release_alibaba/n4sue8f/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753311055,"author_flair_text":null,"treatment_tags":[],"created_utc":1753311055,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4stnnb","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"YouDontSeemRight","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4sbm4x","score":2,"author_fullname":"t2_1b7gjxtue9","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"They did it better for smaller, therefore, it is frontier and SOTA for the model size. I also highly doubt they rely on US models to product good datasets. They understand what makes a good dataset which is the key detail.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n4stnnb","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;They did it better for smaller, therefore, it is frontier and SOTA for the model size. I also highly doubt they rely on US models to product good datasets. They understand what makes a good dataset which is the key detail.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m7kkyn","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7kkyn/less_than_two_weeks_kimi_k2s_release_alibaba/n4stnnb/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753310815,"author_flair_text":null,"treatment_tags":[],"created_utc":1753310815,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n4sbm4x","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":"LOW_SCORE","no_follow":true,"author":"randombsname1","can_mod_post":false,"created_utc":1753305242,"send_replies":true,"parent_id":"t1_n4s6aur","score":-11,"author_fullname":"t2_8t0zww56","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yep, if you want to see where Chinese models are headed. Just watch American models do it 3 to 6 months earlier. \\n\\nDon't get me wrong, its great that they offer very good performance for a fraction of the cost--but none of this is really at the frontier. Which at present seems to be around 4-6 months windows. \\n\\nThis is why these new Chinese models releases are always just kind of \\"meh\\" -- for me.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4sbm4x","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yep, if you want to see where Chinese models are headed. Just watch American models do it 3 to 6 months earlier. &lt;/p&gt;\\n\\n&lt;p&gt;Don&amp;#39;t get me wrong, its great that they offer very good performance for a fraction of the cost--but none of this is really at the frontier. Which at present seems to be around 4-6 months windows. &lt;/p&gt;\\n\\n&lt;p&gt;This is why these new Chinese models releases are always just kind of &amp;quot;meh&amp;quot; -- for me.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":"comment score below threshold","distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m7kkyn","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7kkyn/less_than_two_weeks_kimi_k2s_release_alibaba/n4sbm4x/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753305242,"author_flair_text":null,"treatment_tags":[],"collapsed":true,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-11}}],"before":null}},"user_reports":[],"saved":false,"id":"n4s6aur","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"nrkishere","can_mod_post":false,"created_utc":1753303735,"send_replies":true,"parent_id":"t3_1m7kkyn","score":27,"author_fullname":"t2_o66k4w0to","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"there's not much magic in the model's architecture. It is all in the dataset. Initially claude and gpt used their custom datasets, which is now being used to create synthetic datasets","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4s6aur","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;there&amp;#39;s not much magic in the model&amp;#39;s architecture. It is all in the dataset. Initially claude and gpt used their custom datasets, which is now being used to create synthetic datasets&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7kkyn/less_than_two_weeks_kimi_k2s_release_alibaba/n4s6aur/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753303735,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m7kkyn","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":27}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4shc08","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Baldur-Norddahl","can_mod_post":false,"created_utc":1753306931,"send_replies":true,"parent_id":"t1_n4s7m7n","score":8,"author_fullname":"t2_bvqb8ng0","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Training at fp16 because that is better for training. Does not mean it is needed for inference. The fp16 is need for backpropagation due to the need to calculate fine grained gradients. It is just wasting resources to insist on using fp16 for inference at this point.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4shc08","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Training at fp16 because that is better for training. Does not mean it is needed for inference. The fp16 is need for backpropagation due to the need to calculate fine grained gradients. It is just wasting resources to insist on using fp16 for inference at this point.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m7kkyn","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7kkyn/less_than_two_weeks_kimi_k2s_release_alibaba/n4shc08/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753306931,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":8}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4sxm7f","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"No_Efficiency_1144","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4sbu3p","score":2,"author_fullname":"t2_1nkj9l14b0","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I see that logic, I used to think of model size that way as well. They are going to perform like their parameter counts though, once both are at FP8.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4sxm7f","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I see that logic, I used to think of model size that way as well. They are going to perform like their parameter counts though, once both are at FP8.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m7kkyn","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7kkyn/less_than_two_weeks_kimi_k2s_release_alibaba/n4sxm7f/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753312076,"author_flair_text":null,"treatment_tags":[],"created_utc":1753312076,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4thy29","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"HiddenoO","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4sbu3p","score":2,"author_fullname":"t2_8127x","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"That's a meaningless comparison because there's generally practically no performance degradation when running an FP16 trained model with FP8 during inference.\\n\\nHeck, this whole \\"same/better performance at half the size\\" is extremely misleading because performance never even remotely scales linear with size when quantizing models, and the degradation depends on the actual model. It'd make much more sense to compare performance at specific VRAM footprints and use appropriate quants for each model.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4thy29","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;That&amp;#39;s a meaningless comparison because there&amp;#39;s generally practically no performance degradation when running an FP16 trained model with FP8 during inference.&lt;/p&gt;\\n\\n&lt;p&gt;Heck, this whole &amp;quot;same/better performance at half the size&amp;quot; is extremely misleading because performance never even remotely scales linear with size when quantizing models, and the degradation depends on the actual model. It&amp;#39;d make much more sense to compare performance at specific VRAM footprints and use appropriate quants for each model.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m7kkyn","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7kkyn/less_than_two_weeks_kimi_k2s_release_alibaba/n4thy29/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753318978,"author_flair_text":null,"treatment_tags":[],"created_utc":1753318978,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n4sbu3p","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"fallingdowndizzyvr","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4sa7hb","score":10,"author_fullname":"t2_o65i6kx","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"It's not if the model was trained at FP8 and another at FP16. Since that is the full unquantized precision for both.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n4sbu3p","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It&amp;#39;s not if the model was trained at FP8 and another at FP16. Since that is the full unquantized precision for both.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m7kkyn","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7kkyn/less_than_two_weeks_kimi_k2s_release_alibaba/n4sbu3p/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753305306,"author_flair_text":null,"treatment_tags":[],"created_utc":1753305306,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":10}}],"before":null}},"user_reports":[],"saved":false,"id":"n4sa7hb","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"No_Efficiency_1144","can_mod_post":false,"created_utc":1753304833,"send_replies":true,"parent_id":"t1_n4s7m7n","score":10,"author_fullname":"t2_1nkj9l14b0","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Surely it is more misleading to compare FP8 to FP16","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4sa7hb","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Surely it is more misleading to compare FP8 to FP16&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m7kkyn","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7kkyn/less_than_two_weeks_kimi_k2s_release_alibaba/n4sa7hb/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753304833,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":10}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4saa18","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"GreenTreeAndBlueSky","can_mod_post":false,"created_utc":1753304853,"send_replies":true,"parent_id":"t1_n4s7m7n","score":5,"author_fullname":"t2_1p50pl73j2","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"It's very rare to see any degradation from fp16 to fp8 though, you would never know in a blind test which is which. Most models trained at fp16 are inferred at fp8 as new gpus support it (or less if quantized for vram space)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4saa18","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It&amp;#39;s very rare to see any degradation from fp16 to fp8 though, you would never know in a blind test which is which. Most models trained at fp16 are inferred at fp8 as new gpus support it (or less if quantized for vram space)&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m7kkyn","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7kkyn/less_than_two_weeks_kimi_k2s_release_alibaba/n4saa18/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753304853,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}}],"before":null}},"user_reports":[],"saved":false,"id":"n4s7m7n","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Few_Painter_5588","can_mod_post":false,"created_utc":1753304101,"send_replies":true,"parent_id":"t3_1m7kkyn","score":5,"author_fullname":"t2_uvgafqnfy","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Half it's size is misleading, at full precision they're nearly using the same amount of VRAM.\\n\\nQwen3 coder = 480B parameters at FP16 = 960GB of memory needed\\n\\nKimi M2 = 1T parameters at FP8 = 1000GB of memory used.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4s7m7n","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Half it&amp;#39;s size is misleading, at full precision they&amp;#39;re nearly using the same amount of VRAM.&lt;/p&gt;\\n\\n&lt;p&gt;Qwen3 coder = 480B parameters at FP16 = 960GB of memory needed&lt;/p&gt;\\n\\n&lt;p&gt;Kimi M2 = 1T parameters at FP8 = 1000GB of memory used.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7kkyn/less_than_two_weeks_kimi_k2s_release_alibaba/n4s7m7n/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753304101,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m7kkyn","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":1,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4sa0m7","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"No_Efficiency_1144","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4s791s","score":-4,"author_fullname":"t2_1nkj9l14b0","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Sadly I have a different interpretation.\\n\\n\\nThe trend was that open source would have overtaken closed source by now.\\n\\n\\nHowever O1 came out in September 2024 and since then closed source has been improving twice as fast as before.\\n\\n\\nOn the other side open source has seen less growth rate gains from the reasoning boom.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n4sa0m7","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Sadly I have a different interpretation.&lt;/p&gt;\\n\\n&lt;p&gt;The trend was that open source would have overtaken closed source by now.&lt;/p&gt;\\n\\n&lt;p&gt;However O1 came out in September 2024 and since then closed source has been improving twice as fast as before.&lt;/p&gt;\\n\\n&lt;p&gt;On the other side open source has seen less growth rate gains from the reasoning boom.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m7kkyn","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7kkyn/less_than_two_weeks_kimi_k2s_release_alibaba/n4sa0m7/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753304779,"author_flair_text":null,"treatment_tags":[],"created_utc":1753304779,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-4}}],"before":null}},"user_reports":[],"saved":false,"id":"n4s791s","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"BZ852","can_mod_post":false,"created_utc":1753304000,"send_replies":true,"parent_id":"t1_n4s5wl5","score":13,"author_fullname":"t2_6h5i414j","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"While true in the absolute metrics, look at it by time.\\n\\nOpen source started a year or more behind, now it's less than a few months.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4s791s","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;While true in the absolute metrics, look at it by time.&lt;/p&gt;\\n\\n&lt;p&gt;Open source started a year or more behind, now it&amp;#39;s less than a few months.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m7kkyn","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7kkyn/less_than_two_weeks_kimi_k2s_release_alibaba/n4s791s/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753304000,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":13}}],"before":null}},"user_reports":[],"saved":false,"id":"n4s5wl5","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"No_Efficiency_1144","can_mod_post":false,"created_utc":1753303623,"send_replies":true,"parent_id":"t3_1m7kkyn","score":2,"author_fullname":"t2_1nkj9l14b0","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"It’s a nice chart but this chart does show closed source moving further away over the course of 2025.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4s5wl5","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It’s a nice chart but this chart does show closed source moving further away over the course of 2025.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7kkyn/less_than_two_weeks_kimi_k2s_release_alibaba/n4s5wl5/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753303623,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m7kkyn","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":1,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4teto5","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"segmond","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4t0gh8","score":1,"author_fullname":"t2_ah13x","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"weird, I would imagine it faster since the active parameter is small than kimi.   perhaps the architecture?  i haven't read and contrasted on them.  my download just finished, granted it's for Q4\\\\_K\\\\_XL, will be giving it a drive tonight.  I hope you're wrong.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4teto5","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;weird, I would imagine it faster since the active parameter is small than kimi.   perhaps the architecture?  i haven&amp;#39;t read and contrasted on them.  my download just finished, granted it&amp;#39;s for Q4_K_XL, will be giving it a drive tonight.  I hope you&amp;#39;re wrong.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m7kkyn","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7kkyn/less_than_two_weeks_kimi_k2s_release_alibaba/n4teto5/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753317891,"author_flair_text":"llama.cpp","treatment_tags":[],"created_utc":1753317891,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4t0gh8","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"createthiscom","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4sh5ct","score":1,"author_fullname":"t2_ozxxf","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I'm running Kimi-K2-Instruct-GGUF Q4\\\\_K\\\\_XL locally. I switched to Qwen3-Coder-480B-A35B-Instruct-GGUF Q8\\\\_0. It's a smaller file size, but it infers slower on my system for some reason. 14 tok/s instead of kimi's 22 tok/s.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n4t0gh8","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;m running Kimi-K2-Instruct-GGUF Q4_K_XL locally. I switched to Qwen3-Coder-480B-A35B-Instruct-GGUF Q8_0. It&amp;#39;s a smaller file size, but it infers slower on my system for some reason. 14 tok/s instead of kimi&amp;#39;s 22 tok/s.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m7kkyn","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7kkyn/less_than_two_weeks_kimi_k2s_release_alibaba/n4t0gh8/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753313004,"author_flair_text":null,"treatment_tags":[],"created_utc":1753313004,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4sh5ct","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"segmond","can_mod_post":false,"created_utc":1753306876,"send_replies":true,"parent_id":"t1_n4sd9ax","score":2,"author_fullname":"t2_ah13x","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"which quant are you running? are you using suggested parameters?  full KV or quantized?   I hope you are wrong, I'm downloading file5 of 6 for my q4.gguf","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4sh5ct","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;which quant are you running? are you using suggested parameters?  full KV or quantized?   I hope you are wrong, I&amp;#39;m downloading file5 of 6 for my q4.gguf&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m7kkyn","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7kkyn/less_than_two_weeks_kimi_k2s_release_alibaba/n4sh5ct/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753306876,"author_flair_text":"llama.cpp","treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n4sd9ax","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"createthiscom","can_mod_post":false,"created_utc":1753305724,"send_replies":true,"parent_id":"t3_1m7kkyn","score":1,"author_fullname":"t2_ozxxf","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"It's slower on my system despite having a smaller size and it doesn't seem as capable. I'm sticking with Kimi for now.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4sd9ax","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It&amp;#39;s slower on my system despite having a smaller size and it doesn&amp;#39;t seem as capable. I&amp;#39;m sticking with Kimi for now.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7kkyn/less_than_two_weeks_kimi_k2s_release_alibaba/n4sd9ax/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753305724,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m7kkyn","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4shhc5","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Fantastic-Emu-3819","can_mod_post":false,"created_utc":1753306976,"send_replies":true,"parent_id":"t3_1m7kkyn","score":1,"author_fullname":"t2_1n5r32wumb","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Deepseek R1 0528 score is 68.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4shhc5","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Deepseek R1 0528 score is 68.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7kkyn/less_than_two_weeks_kimi_k2s_release_alibaba/n4shhc5/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753306976,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m7kkyn","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),r=()=>e.jsx(l,{data:a});export{r as default};
