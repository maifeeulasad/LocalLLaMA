import{j as e}from"./index-BpC9hjVs.js";import{R as l}from"./RedditPostRenderer-BEo6AnSR.js";import"./index-DwkJHX1_.js";const t=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Perplexity hasn't had too much for me - I'm assuming you know better\\n\\nI have never quantized / converted a full weights model to anything, but since I'm getting a GB10 DGX I want to have options if the model I want isn't already available in FP4. I know TensorRT model optimizer can do it, but it looks like it only supports NV-FP4 and I guess I'd prefer something non proprietary in the spirit of open source. \\n\\nSo what options are there. Which one is the best. \\n\\nDon't tell me FP4 isn't worth it, not the question, thanks in advance.","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Current best options to convert to FP4","link_flair_richtext":[{"e":"text","t":"Discussion"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lpd3y7","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.8,"author_flair_background_color":null,"subreddit_type":"public","ups":6,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_9yxfq","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Discussion","can_mod_post":false,"score":6,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1751403136,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Perplexity hasn&amp;#39;t had too much for me - I&amp;#39;m assuming you know better&lt;/p&gt;\\n\\n&lt;p&gt;I have never quantized / converted a full weights model to anything, but since I&amp;#39;m getting a GB10 DGX I want to have options if the model I want isn&amp;#39;t already available in FP4. I know TensorRT model optimizer can do it, but it looks like it only supports NV-FP4 and I guess I&amp;#39;d prefer something non proprietary in the spirit of open source. &lt;/p&gt;\\n\\n&lt;p&gt;So what options are there. Which one is the best. &lt;/p&gt;\\n\\n&lt;p&gt;Don&amp;#39;t tell me FP4 isn&amp;#39;t worth it, not the question, thanks in advance.&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"5f921ea4-c7bc-11ed-9c23-3a00622979b4","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#646d73","id":"1lpd3y7","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"zelkovamoon","discussion_type":null,"num_comments":8,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lpd3y7/current_best_options_to_convert_to_fp4/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lpd3y7/current_best_options_to_convert_to_fp4/","subreddit_subscribers":493457,"created_utc":1751403136,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0tv7ir","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"MoltenFace","can_mod_post":false,"created_utc":1751403850,"send_replies":true,"parent_id":"t3_1lpd3y7","score":4,"author_fullname":"t2_6l6rr","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"from what I gather (other than nvfp4)\\nllm-compressor has experimental support for fp4 https://github.com/vllm-project/llm-compressor","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0tv7ir","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;from what I gather (other than nvfp4)\\nllm-compressor has experimental support for fp4 &lt;a href=\\"https://github.com/vllm-project/llm-compressor\\"&gt;https://github.com/vllm-project/llm-compressor&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpd3y7/current_best_options_to_convert_to_fp4/n0tv7ir/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751403850,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lpd3y7","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0u81fg","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"zelkovamoon","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0u067c","score":2,"author_fullname":"t2_9yxfq","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"People on reddit are insufferable I really don't get it. I dread asking questions no matter how legitimate these days, and I gotta be honest, as soon as there is a better platform I'm jumping ship.\\n\\n*Edit - that was in response to people downvoting you","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0u81fg","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;People on reddit are insufferable I really don&amp;#39;t get it. I dread asking questions no matter how legitimate these days, and I gotta be honest, as soon as there is a better platform I&amp;#39;m jumping ship.&lt;/p&gt;\\n\\n&lt;p&gt;*Edit - that was in response to people downvoting you&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lpd3y7","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpd3y7/current_best_options_to_convert_to_fp4/n0u81fg/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751407730,"author_flair_text":null,"treatment_tags":[],"created_utc":1751407730,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n0u067c","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Kooshi_Govno","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0tz4m1","score":2,"author_fullname":"t2_7kg5p","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yeah, /u/MoltenFace's comment is the first I'm hearing of it, but it looks promising.\\n\\nedit: lol who downvotes this? I'm sorry I didn't know about llm-compressor before reading this thread?","edited":1751407474,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0u067c","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yeah, &lt;a href=\\"/u/MoltenFace\\"&gt;/u/MoltenFace&lt;/a&gt;&amp;#39;s comment is the first I&amp;#39;m hearing of it, but it looks promising.&lt;/p&gt;\\n\\n&lt;p&gt;edit: lol who downvotes this? I&amp;#39;m sorry I didn&amp;#39;t know about llm-compressor before reading this thread?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lpd3y7","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpd3y7/current_best_options_to_convert_to_fp4/n0u067c/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751405325,"author_flair_text":null,"treatment_tags":[],"created_utc":1751405325,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n0tz4m1","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"zelkovamoon","can_mod_post":false,"created_utc":1751405012,"send_replies":true,"parent_id":"t1_n0tx9sb","score":1,"author_fullname":"t2_9yxfq","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Very interesting, thanks for the info. From what I can tell vllm should?? Be able to run FP4?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0tz4m1","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Very interesting, thanks for the info. From what I can tell vllm should?? Be able to run FP4?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lpd3y7","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpd3y7/current_best_options_to_convert_to_fp4/n0tz4m1/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751405012,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0u2lge","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"smahs9","can_mod_post":false,"created_utc":1751406056,"send_replies":true,"parent_id":"t1_n0tx9sb","score":1,"author_fullname":"t2_neyagc1uz","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"TensorRT-llm does, since a couple of months iirc. Last I tested though, there were no fused mha kernels, so the utility is rather limited.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0u2lge","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;TensorRT-llm does, since a couple of months iirc. Last I tested though, there were no fused mha kernels, so the utility is rather limited.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lpd3y7","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpd3y7/current_best_options_to_convert_to_fp4/n0u2lge/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751406056,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0tx9sb","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Kooshi_Govno","can_mod_post":false,"created_utc":1751404454,"send_replies":true,"parent_id":"t3_1lpd3y7","score":4,"author_fullname":"t2_7kg5p","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Blackwell FP4 is bleeding edge, and slowly gaining support. I haven't come across any inference engines that use it yet, but on a related note, I have been keeping a close eye on this pull request, which will allow training in FP4 once they make their repos public: https://github.com/huggingface/transformers/pull/38696","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0tx9sb","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Blackwell FP4 is bleeding edge, and slowly gaining support. I haven&amp;#39;t come across any inference engines that use it yet, but on a related note, I have been keeping a close eye on this pull request, which will allow training in FP4 once they make their repos public: &lt;a href=\\"https://github.com/huggingface/transformers/pull/38696\\"&gt;https://github.com/huggingface/transformers/pull/38696&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpd3y7/current_best_options_to_convert_to_fp4/n0tx9sb/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751404454,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lpd3y7","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"total_awards_received":0,"approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"ups":1,"removal_reason":null,"link_id":"t3_1lpd3y7","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0tw3pr","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Kooshi_Govno","can_mod_post":false,"created_utc":1751404111,"send_replies":true,"parent_id":"t1_n0tthzd","score":1,"author_fullname":"t2_7kg5p","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Do these have actual Blackwell FP4 kernels already though? This post isn't about standard 4 bit quantization, but hardware level FP4 support.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0tw3pr","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Do these have actual Blackwell FP4 kernels already though? This post isn&amp;#39;t about standard 4 bit quantization, but hardware level FP4 support.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lpd3y7","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpd3y7/current_best_options_to_convert_to_fp4/n0tw3pr/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751404111,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0tthzd","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":"DELETED","no_follow":true,"author":"[deleted]","can_mod_post":false,"send_replies":true,"parent_id":"t3_1lpd3y7","score":1,"approved_by":null,"report_reasons":null,"all_awardings":[],"subreddit_id":"t5_81eyvm","body":"[deleted]","edited":false,"downs":0,"author_flair_css_class":null,"collapsed":true,"is_submitter":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;[deleted]&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"associated_award":null,"stickied":false,"subreddit_type":"public","can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":"dark","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpd3y7/current_best_options_to_convert_to_fp4/n0tthzd/","num_reports":null,"locked":false,"name":"t1_n0tthzd","created":1751403358,"subreddit":"LocalLLaMA","author_flair_text":null,"treatment_tags":[],"created_utc":1751403358,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"","collapsed_because_crowd_control":null,"mod_reports":[],"mod_note":null,"distinguished":null}}],"before":null}}]`),n=()=>e.jsx(l,{data:t});export{n as default};
