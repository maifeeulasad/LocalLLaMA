import{j as e}from"./index-MXAMZFAj.js";import{R as l}from"./RedditPostRenderer-AiHEQBDd.js";import"./index-B-YL2gc7.js";const a=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Am I missing something? The llama3.2:3B is giving me 29 t/s, but Gemma3n:2B is only doing 22 t/s.\\n\\nIs it still not fully supported? The VRAM footprint is indeed of a 2B, but the performance sucks.","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Gemma3n:2B and Gemma3n:4B models are ~40% slower than equivalent models in size running on Llama.cpp","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lmranc","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.89,"author_flair_background_color":null,"subreddit_type":"public","ups":35,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_vbzgnic","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":35,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1751128970,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Am I missing something? The llama3.2:3B is giving me 29 t/s, but Gemma3n:2B is only doing 22 t/s.&lt;/p&gt;\\n\\n&lt;p&gt;Is it still not fully supported? The VRAM footprint is indeed of a 2B, but the performance sucks.&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1lmranc","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"simracerman","discussion_type":null,"num_comments":17,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lmranc/gemma3n2b_and_gemma3n4b_models_are_40_slower_than/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lmranc/gemma3n2b_and_gemma3n4b_models_are_40_slower_than/","subreddit_subscribers":492626,"created_utc":1751128970,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0be4py","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Eden1506","can_mod_post":false,"send_replies":true,"parent_id":"t1_n09prnr","score":5,"author_fullname":"t2_2ezqqypt","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Not quite it is specifically designed for edge devices to work in RAM on a very small footprint with layers not currently utilised being saved to internal storage and the active layers being loaded in RAM dynamically. \\n\\nSpecifically via:\\nPer-Layer Embedding (PLE) Caching: PLE parameters are used to enhance the performance of each model layer, can be generated and cached to fast, local storage outside the model's main operating memory and are dynamically loaded when needed. \\n\\nMatFormer Architecture: This \\"Matryoshka Transformer\\" architecture allows for selective activation of model parameters per request or in other words vision parameters as an example are only loaded when you actually need them and can otherwise stay in internal storage until necessary unlike the normal 4b model where everything is always loaded. \\n\\nThis significantly reduces the live memory footprint during inference. \\n\\n\\nWhere exactly have you read that it runs offloaded interference on gpu and cpu? As far as I am aware it dynamically loads everything into the fastest available storage and only runs one interference instance.","edited":1751152512,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0be4py","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Not quite it is specifically designed for edge devices to work in RAM on a very small footprint with layers not currently utilised being saved to internal storage and the active layers being loaded in RAM dynamically. &lt;/p&gt;\\n\\n&lt;p&gt;Specifically via:\\nPer-Layer Embedding (PLE) Caching: PLE parameters are used to enhance the performance of each model layer, can be generated and cached to fast, local storage outside the model&amp;#39;s main operating memory and are dynamically loaded when needed. &lt;/p&gt;\\n\\n&lt;p&gt;MatFormer Architecture: This &amp;quot;Matryoshka Transformer&amp;quot; architecture allows for selective activation of model parameters per request or in other words vision parameters as an example are only loaded when you actually need them and can otherwise stay in internal storage until necessary unlike the normal 4b model where everything is always loaded. &lt;/p&gt;\\n\\n&lt;p&gt;This significantly reduces the live memory footprint during inference. &lt;/p&gt;\\n\\n&lt;p&gt;Where exactly have you read that it runs offloaded interference on gpu and cpu? As far as I am aware it dynamically loads everything into the fastest available storage and only runs one interference instance.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lmranc","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmranc/gemma3n2b_and_gemma3n4b_models_are_40_slower_than/n0be4py/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751150168,"author_flair_text":null,"treatment_tags":[],"created_utc":1751150168,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}}],"before":null}},"user_reports":[],"saved":false,"id":"n09prnr","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Fireflykid1","can_mod_post":false,"send_replies":true,"parent_id":"t1_n09ofc1","score":8,"author_fullname":"t2_15sr10","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"As far as I understand, (for 3n:2b for example) it’s running an internal 2b quickly (ideally stored in vram) and a slower 3b (designed to be computed on cpu in ram) around it. It should be faster than a typical 5b, but slower than a 3b. It’s not a moe like qwen3-30b-a3b where only 3b parameters are active at a given time. \\n\\nThat being said, I may be wrong about that.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n09prnr","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;As far as I understand, (for 3n:2b for example) it’s running an internal 2b quickly (ideally stored in vram) and a slower 3b (designed to be computed on cpu in ram) around it. It should be faster than a typical 5b, but slower than a 3b. It’s not a moe like qwen3-30b-a3b where only 3b parameters are active at a given time. &lt;/p&gt;\\n\\n&lt;p&gt;That being said, I may be wrong about that.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lmranc","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmranc/gemma3n2b_and_gemma3n4b_models_are_40_slower_than/n09prnr/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751130562,"author_flair_text":null,"treatment_tags":[],"created_utc":1751130562,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":8}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0cx1qz","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Euphoric_Ad9500","can_mod_post":false,"send_replies":true,"parent_id":"t1_n09ofc1","score":2,"author_fullname":"t2_8kbjrt7z","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"All parameters in an MoE are typically loaded in vram BC you can’t predetermine what experts to activate.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0cx1qz","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;All parameters in an MoE are typically loaded in vram BC you can’t predetermine what experts to activate.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lmranc","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmranc/gemma3n2b_and_gemma3n4b_models_are_40_slower_than/n0cx1qz/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751172119,"author_flair_text":null,"treatment_tags":[],"created_utc":1751172119,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0akzx2","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Expensive-Apricot-25","can_mod_post":false,"send_replies":true,"parent_id":"t1_n09ofc1","score":1,"author_fullname":"t2_idqkwio0","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"It takes up the same amount of vram for 5b and 8b models for me.\\n\\nAnd they have worse speed and performance.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0akzx2","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It takes up the same amount of vram for 5b and 8b models for me.&lt;/p&gt;\\n\\n&lt;p&gt;And they have worse speed and performance.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lmranc","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmranc/gemma3n2b_and_gemma3n4b_models_are_40_slower_than/n0akzx2/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751140382,"author_flair_text":null,"treatment_tags":[],"created_utc":1751140382,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0azo0p","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DinoAmino","can_mod_post":false,"send_replies":true,"parent_id":"t1_n09ofc1","score":1,"author_fullname":"t2_j1v7f","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"You're not missing anything. You're gaining capabilities other models don't have. In order to pull off things like this there is usually a price to pay. Like, adding vision capabilities on top of an LLM means more parameters and larger size.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0azo0p","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You&amp;#39;re not missing anything. You&amp;#39;re gaining capabilities other models don&amp;#39;t have. In order to pull off things like this there is usually a price to pay. Like, adding vision capabilities on top of an LLM means more parameters and larger size.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lmranc","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmranc/gemma3n2b_and_gemma3n4b_models_are_40_slower_than/n0azo0p/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751145134,"author_flair_text":null,"treatment_tags":[],"created_utc":1751145134,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n09ofc1","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"simracerman","can_mod_post":false,"created_utc":1751130150,"send_replies":true,"parent_id":"t1_n09mbhs","score":6,"author_fullname":"t2_vbzgnic","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I’m aware, but I thought the smaller VRAM footprint is what dictates output speed. Reference all the MoE models like Qwen3-30B-A3B for example. If only 2B are loaded actively on VRAM, shouldn’t that t/s be much higher..?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n09ofc1","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I’m aware, but I thought the smaller VRAM footprint is what dictates output speed. Reference all the MoE models like Qwen3-30B-A3B for example. If only 2B are loaded actively on VRAM, shouldn’t that t/s be much higher..?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lmranc","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmranc/gemma3n2b_and_gemma3n4b_models_are_40_slower_than/n09ofc1/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751130150,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0bj6n2","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Eden1506","can_mod_post":false,"created_utc":1751152011,"send_replies":true,"parent_id":"t1_n09mbhs","score":4,"author_fullname":"t2_2ezqqypt","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"It is also hard to define the model as actually 5b (or 8b in case of E4B) in density because the PLE layers are closer to a kind of lookup table to guide the model layers towards better answers basically context-specific \\"adjustments\\"\\n\\nInstead of performing a complex matrix multiplication on a continuous input vector like with other layers,when utilising PLE layers it takes a specific token ID and layer ID, and \\"looks up\\" a corresponding embedding vector from this large lookup table and adjust its values.\\n\\nAs a result those PLE layers can be stored on slower memory and loaded dynamically saving on the needed memory footprint.\\n\\nFor situation included in the \\"lookup table\\" it will perform better but it is not the same as an actual 8b dense model from what I understand.\\n\\nBasically for all situations included it will reach 8b quality or potentially slightly better while for all not included situations it will be somewhere in-between 4 to 8b.  Depending on how many layers benefit from the lookup table adjustments.\\n\\nYou can see it in GPQA Diamond (Scientific Reasoning) benchmarks or humanities last exam where it performs no different from the gemma 3 4b model or even slightly worse because it likely does not have \\"adjustments\\" saved for those situations but instead for more common use cases.","edited":1751153920,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0bj6n2","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It is also hard to define the model as actually 5b (or 8b in case of E4B) in density because the PLE layers are closer to a kind of lookup table to guide the model layers towards better answers basically context-specific &amp;quot;adjustments&amp;quot;&lt;/p&gt;\\n\\n&lt;p&gt;Instead of performing a complex matrix multiplication on a continuous input vector like with other layers,when utilising PLE layers it takes a specific token ID and layer ID, and &amp;quot;looks up&amp;quot; a corresponding embedding vector from this large lookup table and adjust its values.&lt;/p&gt;\\n\\n&lt;p&gt;As a result those PLE layers can be stored on slower memory and loaded dynamically saving on the needed memory footprint.&lt;/p&gt;\\n\\n&lt;p&gt;For situation included in the &amp;quot;lookup table&amp;quot; it will perform better but it is not the same as an actual 8b dense model from what I understand.&lt;/p&gt;\\n\\n&lt;p&gt;Basically for all situations included it will reach 8b quality or potentially slightly better while for all not included situations it will be somewhere in-between 4 to 8b.  Depending on how many layers benefit from the lookup table adjustments.&lt;/p&gt;\\n\\n&lt;p&gt;You can see it in GPQA Diamond (Scientific Reasoning) benchmarks or humanities last exam where it performs no different from the gemma 3 4b model or even slightly worse because it likely does not have &amp;quot;adjustments&amp;quot; saved for those situations but instead for more common use cases.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lmranc","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmranc/gemma3n2b_and_gemma3n4b_models_are_40_slower_than/n0bj6n2/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751152011,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}}],"before":null}},"user_reports":[],"saved":false,"id":"n09mbhs","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Fireflykid1","can_mod_post":false,"created_utc":1751129499,"send_replies":true,"parent_id":"t3_1lmranc","score":31,"author_fullname":"t2_15sr10","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"3n:2b is 5b parameters.\\n\\n3n:4b is 8b parameters.\\n\\n[Here’s some more info on them.](https://developers.googleblog.com/en/introducing-gemma-3n-developer-guide/)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n09mbhs","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;3n:2b is 5b parameters.&lt;/p&gt;\\n\\n&lt;p&gt;3n:4b is 8b parameters.&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://developers.googleblog.com/en/introducing-gemma-3n-developer-guide/\\"&gt;Here’s some more info on them.&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmranc/gemma3n2b_and_gemma3n4b_models_are_40_slower_than/n09mbhs/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751129499,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lmranc","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":31}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0ae49w","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"rerri","can_mod_post":false,"created_utc":1751138121,"send_replies":true,"parent_id":"t3_1lmranc","score":7,"author_fullname":"t2_12aeph","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Gemma3n E4B UD-Q6\\\\_K\\\\_XL is only slightly faster than Gemma 3 27B UD-Q4\\\\_K\\\\_XL for me on a 4090 with the latest version of llama.cpp.\\n\\nCPU usage is heavier with E4B.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0ae49w","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Gemma3n E4B UD-Q6_K_XL is only slightly faster than Gemma 3 27B UD-Q4_K_XL for me on a 4090 with the latest version of llama.cpp.&lt;/p&gt;\\n\\n&lt;p&gt;CPU usage is heavier with E4B.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmranc/gemma3n2b_and_gemma3n4b_models_are_40_slower_than/n0ae49w/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751138121,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lmranc","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":7}},{"kind":"t1","data":{"total_awards_received":0,"approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"ups":2,"removal_reason":null,"link_id":"t3_1lmranc","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n09nyrp","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"simracerman","can_mod_post":false,"created_utc":1751130009,"send_replies":true,"parent_id":"t1_n09nemp","score":1,"author_fullname":"t2_vbzgnic","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I’ve been following the same recommendations from Unsloth.\\n\\nhttps://huggingface.co/unsloth/gemma-3n-E2B-it-GGUF","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n09nyrp","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I’ve been following the same recommendations from Unsloth.&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://huggingface.co/unsloth/gemma-3n-E2B-it-GGUF\\"&gt;https://huggingface.co/unsloth/gemma-3n-E2B-it-GGUF&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lmranc","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmranc/gemma3n2b_and_gemma3n4b_models_are_40_slower_than/n09nyrp/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751130009,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n09nemp","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":"DELETED","no_follow":true,"author":"[deleted]","can_mod_post":false,"send_replies":true,"parent_id":"t3_1lmranc","score":2,"approved_by":null,"report_reasons":null,"all_awardings":[],"subreddit_id":"t5_81eyvm","body":"[deleted]","edited":false,"downs":0,"author_flair_css_class":null,"collapsed":true,"is_submitter":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;[deleted]&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"associated_award":null,"stickied":false,"subreddit_type":"public","can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":"dark","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmranc/gemma3n2b_and_gemma3n4b_models_are_40_slower_than/n09nemp/","num_reports":null,"locked":false,"name":"t1_n09nemp","created":1751129836,"subreddit":"LocalLLaMA","author_flair_text":null,"treatment_tags":[],"created_utc":1751129836,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"","collapsed_because_crowd_control":null,"mod_reports":[],"mod_note":null,"distinguished":null}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0byya4","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ThinkExtension2328","can_mod_post":false,"created_utc":1751157811,"send_replies":true,"parent_id":"t1_n0beyin","score":2,"author_fullname":"t2_8eneodlk","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Something  is wrong with your setup / model . I just tested full q8 on my 28gb a2000+4060 setup and it get 30tp/s","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0byya4","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Something  is wrong with your setup / model . I just tested full q8 on my 28gb a2000+4060 setup and it get 30tp/s&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lmranc","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmranc/gemma3n2b_and_gemma3n4b_models_are_40_slower_than/n0byya4/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751157811,"author_flair_text":"llama.cpp","treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0dy53b","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Porespellar","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0dr030","score":1,"author_fullname":"t2_y35oj","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I used both Unsloth and Ollama’s FP16 and had the same slow results with both. What quant did you use when you got your 30 tk/s?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0dy53b","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I used both Unsloth and Ollama’s FP16 and had the same slow results with both. What quant did you use when you got your 30 tk/s?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lmranc","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmranc/gemma3n2b_and_gemma3n4b_models_are_40_slower_than/n0dy53b/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751193365,"author_flair_text":null,"treatment_tags":[],"created_utc":1751193365,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0dr030","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Uncle___Marty","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0ckxz4","score":1,"author_fullname":"t2_75p7pgz3","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"This seemed low for me so I just grabbed the 4B and tested it on LM studio using cuda12 on a 3060ti(8 gig) and im getting 30 tk/s (I actually just wrote 30 FPS and just had to correct it to tk/s lol).\\n\\nI used the Bartowski quants if it matters. Hope you guys get this fixed and get decent speeds soon!","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0dr030","is_submitter":false,"collapsed":false,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;This seemed low for me so I just grabbed the 4B and tested it on LM studio using cuda12 on a 3060ti(8 gig) and im getting 30 tk/s (I actually just wrote 30 FPS and just had to correct it to tk/s lol).&lt;/p&gt;\\n\\n&lt;p&gt;I used the Bartowski quants if it matters. Hope you guys get this fixed and get decent speeds soon!&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lmranc","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmranc/gemma3n2b_and_gemma3n4b_models_are_40_slower_than/n0dr030/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751189060,"author_flair_text":"llama.cpp","treatment_tags":[],"created_utc":1751189060,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0ckxz4","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Porespellar","can_mod_post":false,"created_utc":1751166676,"send_replies":true,"parent_id":"t1_n0beyin","score":1,"author_fullname":"t2_y35oj","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Same here. Like 2-3 tk/s on an otherwise empty H100. No idea why it’s so slow","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0ckxz4","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Same here. Like 2-3 tk/s on an otherwise empty H100. No idea why it’s so slow&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lmranc","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmranc/gemma3n2b_and_gemma3n4b_models_are_40_slower_than/n0ckxz4/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751166676,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0beyin","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Turbulent_Jump_2000","can_mod_post":false,"created_utc":1751150473,"send_replies":true,"parent_id":"t3_1lmranc","score":2,"author_fullname":"t2_15l7mwafch","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"They’re running very very slowly like 3 t/s on my dual 3090 setup in lmstudio… I assume there’s some llama.cpp issue.  ","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0beyin","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;They’re running very very slowly like 3 t/s on my dual 3090 setup in lmstudio… I assume there’s some llama.cpp issue.  &lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmranc/gemma3n2b_and_gemma3n4b_models_are_40_slower_than/n0beyin/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751150473,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lmranc","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0cz53v","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ObjectiveOctopus2","can_mod_post":false,"created_utc":1751173122,"send_replies":true,"parent_id":"t3_1lmranc","score":1,"author_fullname":"t2_1c8oy97ay3","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Maybe llama.cpp isn’t set for it yet?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0cz53v","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Maybe llama.cpp isn’t set for it yet?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmranc/gemma3n2b_and_gemma3n4b_models_are_40_slower_than/n0cz53v/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751173122,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lmranc","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),o=()=>e.jsx(l,{data:a});export{o as default};
