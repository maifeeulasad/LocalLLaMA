import{j as e}from"./index-M4edQi1P.js";import{R as t}from"./RedditPostRenderer-CESBGIIy.js";import"./index-DFpL1mt4.js";const l=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"With 64GB RAM I could run [dots](https://huggingface.co/unsloth/dots.llm1.inst-GGUF) with \`mmap\` at Q4 with some hiccups (offloading a small part of the model to the SSD). I had [mixed feelings](https://www.reddit.com/r/LocalLLaMA/comments/1lqh55j/comment/n13cnzx/?utm_source=share&amp;utm_medium=web3x&amp;utm_name=web3xcss&amp;utm_term=1&amp;utm_content=share_button) about the model:\\n\\n&gt;I've been playing around with Dots at Q4\\\\_K\\\\_XL a bit, and it's one of those models that gives me mixed feelings. It's super-impressive at times, one of the best performing models I've ever used locally, but unimpressive other times, worse than much smaller models at 20b-30b.\\n\\nI upgraded to 128GB RAM and tried dots again at Q5\\\\_K\\\\_XL, and (unless I did something wrong before) it was noticeable better. I got curious and also tried Q6\\\\_K\\\\_XL (highest quant I can fit now) and it was even more noticeable better. \\n\\nI have no mixed feelings anymore. Compared to especially Q4, Q6 feels almost like a new model. It almost always impress me now, it feels very solid and overall powerful. I think this is now my new favorite overall model.\\n\\nI'm a little surprised that the difference between Q4, Q5 and Q6 is this large. I thought I would only see this sort of quality gap below Q4, starting at Q3. Has anyone else experienced this too with this model, or any other model for that matter?\\n\\nI can only fit the even larger model Qwen3-235b at Q4, I wonder if the quality difference is also this big at Q5/Q6 here?","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"dots.llm1 appears to be very sensitive to quantization?","link_flair_richtext":[{"e":"text","t":"Discussion"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lyy0yi","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.85,"author_flair_background_color":null,"subreddit_type":"public","ups":21,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_qhlcbiy3k","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Discussion","can_mod_post":false,"score":21,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":1752426930,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"post_hint":"self","content_categories":null,"is_self":true,"mod_note":null,"created":1752426515,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;With 64GB RAM I could run &lt;a href=\\"https://huggingface.co/unsloth/dots.llm1.inst-GGUF\\"&gt;dots&lt;/a&gt; with &lt;code&gt;mmap&lt;/code&gt; at Q4 with some hiccups (offloading a small part of the model to the SSD). I had &lt;a href=\\"https://www.reddit.com/r/LocalLLaMA/comments/1lqh55j/comment/n13cnzx/?utm_source=share&amp;amp;utm_medium=web3x&amp;amp;utm_name=web3xcss&amp;amp;utm_term=1&amp;amp;utm_content=share_button\\"&gt;mixed feelings&lt;/a&gt; about the model:&lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;I&amp;#39;ve been playing around with Dots at Q4_K_XL a bit, and it&amp;#39;s one of those models that gives me mixed feelings. It&amp;#39;s super-impressive at times, one of the best performing models I&amp;#39;ve ever used locally, but unimpressive other times, worse than much smaller models at 20b-30b.&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;I upgraded to 128GB RAM and tried dots again at Q5_K_XL, and (unless I did something wrong before) it was noticeable better. I got curious and also tried Q6_K_XL (highest quant I can fit now) and it was even more noticeable better. &lt;/p&gt;\\n\\n&lt;p&gt;I have no mixed feelings anymore. Compared to especially Q4, Q6 feels almost like a new model. It almost always impress me now, it feels very solid and overall powerful. I think this is now my new favorite overall model.&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;m a little surprised that the difference between Q4, Q5 and Q6 is this large. I thought I would only see this sort of quality gap below Q4, starting at Q3. Has anyone else experienced this too with this model, or any other model for that matter?&lt;/p&gt;\\n\\n&lt;p&gt;I can only fit the even larger model Qwen3-235b at Q4, I wonder if the quality difference is also this big at Q5/Q6 here?&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"preview":{"images":[{"source":{"url":"https://external-preview.redd.it/GsxrVt2s0faAa6ahT3QXDdQ0OgidqVHBIx2G_Y7sxrA.png?auto=webp&amp;s=f0d8a1e0eca0ded72123d7800bbfdc68ebd71f4a","width":1200,"height":648},"resolutions":[{"url":"https://external-preview.redd.it/GsxrVt2s0faAa6ahT3QXDdQ0OgidqVHBIx2G_Y7sxrA.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=ba3d79ad811c3a5dd1850b68eecf457670859c35","width":108,"height":58},{"url":"https://external-preview.redd.it/GsxrVt2s0faAa6ahT3QXDdQ0OgidqVHBIx2G_Y7sxrA.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=f6e12ba77bdf5d9d77ee38b4a963c471b423fa3f","width":216,"height":116},{"url":"https://external-preview.redd.it/GsxrVt2s0faAa6ahT3QXDdQ0OgidqVHBIx2G_Y7sxrA.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=a1f1774424e25aa6e286c88fa430ff3f6d1f4ab1","width":320,"height":172},{"url":"https://external-preview.redd.it/GsxrVt2s0faAa6ahT3QXDdQ0OgidqVHBIx2G_Y7sxrA.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=39a200fa54cb5b74c34bfbef3203d2dda0951238","width":640,"height":345},{"url":"https://external-preview.redd.it/GsxrVt2s0faAa6ahT3QXDdQ0OgidqVHBIx2G_Y7sxrA.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=8836a8bbf032eca97e032f8e95ecba04afbd729d","width":960,"height":518},{"url":"https://external-preview.redd.it/GsxrVt2s0faAa6ahT3QXDdQ0OgidqVHBIx2G_Y7sxrA.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=6136cce018d4c1fdd79c7c6109223f99e6ccc25a","width":1080,"height":583}],"variants":{},"id":"GsxrVt2s0faAa6ahT3QXDdQ0OgidqVHBIx2G_Y7sxrA"}],"enabled":false},"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"5f921ea4-c7bc-11ed-9c23-3a00622979b4","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#646d73","id":"1lyy0yi","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"Admirable-Star7088","discussion_type":null,"num_comments":11,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lyy0yi/dotsllm1_appears_to_be_very_sensitive_to/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lyy0yi/dotsllm1_appears_to_be_very_sensitive_to/","subreddit_subscribers":498850,"created_utc":1752426515,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ef488598-491f-11ef-a847-9a3dd315819c","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ef488598-491f-11ef-a847-9a3dd315819c","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2ylsi8","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Admirable-Star7088","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2yejjz","score":2,"author_fullname":"t2_qhlcbiy3k","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Nice, will check it out. Dots quant benchmarks would be very interesting also, to see if they match my experience or not :P","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2ylsi8","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Nice, will check it out. Dots quant benchmarks would be very interesting also, to see if they match my experience or not :P&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lyy0yi","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyy0yi/dotsllm1_appears_to_be_very_sensitive_to/n2ylsi8/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752439327,"author_flair_text":null,"treatment_tags":[],"created_utc":1752439327,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n2yejjz","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"panchovix","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2xl2h7","score":5,"author_fullname":"t2_j1kqr","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I added some info about deepseek quants here, if it helps [https://www.reddit.com/r/LocalLLaMA/comments/1lz1s8x/some\\\\_small\\\\_ppl\\\\_benchmarks\\\\_on\\\\_deepseek\\\\_r1\\\\_0528/](https://www.reddit.com/r/LocalLLaMA/comments/1lz1s8x/some_small_ppl_benchmarks_on_deepseek_r1_0528/)\\n\\nStill have to test Dots someday haha","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2yejjz","is_submitter":false,"collapsed":false,"author_flair_richtext":[{"e":"text","t":"Llama 405B"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I added some info about deepseek quants here, if it helps &lt;a href=\\"https://www.reddit.com/r/LocalLLaMA/comments/1lz1s8x/some_small_ppl_benchmarks_on_deepseek_r1_0528/\\"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1lz1s8x/some_small_ppl_benchmarks_on_deepseek_r1_0528/&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;Still have to test Dots someday haha&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lyy0yi","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyy0yi/dotsllm1_appears_to_be_very_sensitive_to/n2yejjz/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752437194,"author_flair_text":"Llama 405B","treatment_tags":[],"created_utc":1752437194,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}}],"before":null}},"user_reports":[],"saved":false,"id":"n2xl2h7","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Admirable-Star7088","can_mod_post":false,"created_utc":1752428422,"send_replies":true,"parent_id":"t1_n2xg3kq","score":2,"author_fullname":"t2_qhlcbiy3k","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I wonder if there is something going on specifically with very large models and quantization? I have compared the much smaller Qwen3 30B A3B Q4\\\\_K\\\\_XL with Q8\\\\_K\\\\_XL, and while I saw a slight difference, it was not significant at all.\\n\\nSadly, even 128GB RAM is way too low to run the behemoth DeepSeek at 2.8bpw haha, so that's not an option for me. Thanks for your interesting insights, though! Maybe it's time to upgrade to 256GB RAM :P","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2xl2h7","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I wonder if there is something going on specifically with very large models and quantization? I have compared the much smaller Qwen3 30B A3B Q4_K_XL with Q8_K_XL, and while I saw a slight difference, it was not significant at all.&lt;/p&gt;\\n\\n&lt;p&gt;Sadly, even 128GB RAM is way too low to run the behemoth DeepSeek at 2.8bpw haha, so that&amp;#39;s not an option for me. Thanks for your interesting insights, though! Maybe it&amp;#39;s time to upgrade to 256GB RAM :P&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lyy0yi","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyy0yi/dotsllm1_appears_to_be_very_sensitive_to/n2xl2h7/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752428422,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2z4d17","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Lissanro","can_mod_post":false,"created_utc":1752445107,"send_replies":true,"parent_id":"t1_n2xg3kq","score":2,"author_fullname":"t2_fpfao9g","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"DeepSeek 671B was trained at FP8, but Qwen3 at BF16, so for example 4bpw compresses DeepSeek model by a factor of two, but Qwen3 by a factor of 4. Architecture differences could be another contributing factor, but my impression that FP8 MoE models better handle being quantized.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2z4d17","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;DeepSeek 671B was trained at FP8, but Qwen3 at BF16, so for example 4bpw compresses DeepSeek model by a factor of two, but Qwen3 by a factor of 4. Architecture differences could be another contributing factor, but my impression that FP8 MoE models better handle being quantized.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lyy0yi","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyy0yi/dotsllm1_appears_to_be_very_sensitive_to/n2z4d17/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752445107,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n2xg3kq","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"panchovix","can_mod_post":false,"created_utc":1752427000,"send_replies":true,"parent_id":"t3_1lyy0yi","score":9,"author_fullname":"t2_j1kqr","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I haven't tried Dots, but on Qwen 235B is very sensible to quantization. I found Q4\\\\_K\\\\_XL noticeably better than Q3\\\\_K\\\\_XL, same with Q5\\\\_K\\\\_XL vs Q4\\\\_X\\\\_KL, and Q8 vs Q5\\\\_K\\\\_XL, each one was better than the other.\\n\\nOn the other hand for example, on DeepSeek V3 0324-R1 0528 as long as you have 3.5bpw or more, quality is really close. It is at 2.5-3.4bpw range you can notice a difference but even then that is better than other local models at higher quantization (I.E. I prefer deepseek r1 0528 at 2.8bpw vs Qwen 235B at Q8\\\\_0 lol)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2xg3kq","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"Llama 405B"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I haven&amp;#39;t tried Dots, but on Qwen 235B is very sensible to quantization. I found Q4_K_XL noticeably better than Q3_K_XL, same with Q5_K_XL vs Q4_X_KL, and Q8 vs Q5_K_XL, each one was better than the other.&lt;/p&gt;\\n\\n&lt;p&gt;On the other hand for example, on DeepSeek V3 0324-R1 0528 as long as you have 3.5bpw or more, quality is really close. It is at 2.5-3.4bpw range you can notice a difference but even then that is better than other local models at higher quantization (I.E. I prefer deepseek r1 0528 at 2.8bpw vs Qwen 235B at Q8_0 lol)&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyy0yi/dotsllm1_appears_to_be_very_sensitive_to/n2xg3kq/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752427000,"author_flair_text":"Llama 405B","treatment_tags":[],"link_id":"t3_1lyy0yi","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":9}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2xr5dt","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"a_beautiful_rhind","can_mod_post":false,"created_utc":1752430191,"send_replies":true,"parent_id":"t3_1lyy0yi","score":8,"author_fullname":"t2_h5utwre7","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Having used 235b on openrouter vs quants.. difference wasn't that huge. Have both IQ4_XS and exl 3.0bpw. This was testing general conversational logic and not something like code so maybe it's more pronounced there?\\n\\nThing is, these aren't \\"large\\" models by the active parameters. Another \\"gift\\" from the MoE arch.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2xr5dt","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Having used 235b on openrouter vs quants.. difference wasn&amp;#39;t that huge. Have both IQ4_XS and exl 3.0bpw. This was testing general conversational logic and not something like code so maybe it&amp;#39;s more pronounced there?&lt;/p&gt;\\n\\n&lt;p&gt;Thing is, these aren&amp;#39;t &amp;quot;large&amp;quot; models by the active parameters. Another &amp;quot;gift&amp;quot; from the MoE arch.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyy0yi/dotsllm1_appears_to_be_very_sensitive_to/n2xr5dt/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752430191,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lyy0yi","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":8}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2y6eii","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Chromix_","can_mod_post":false,"created_utc":1752434766,"send_replies":true,"parent_id":"t1_n2xg5yn","score":3,"author_fullname":"t2_k7w2h","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"The difference between the same quants made with a different imatrix is usually not noticeable in practice, and very, [very noisy to measure](https://www.reddit.com/r/LocalLLaMA/comments/1ah3w8d/comment/kouw5aj/?context=3).\\n\\nHowever, the Unsloth UD quants quantize layers differently than the regular quants. There could be a relevant difference in output quality to the regular quants of comparable size - for the better or worse.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2y6eii","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The difference between the same quants made with a different imatrix is usually not noticeable in practice, and very, &lt;a href=\\"https://www.reddit.com/r/LocalLLaMA/comments/1ah3w8d/comment/kouw5aj/?context=3\\"&gt;very noisy to measure&lt;/a&gt;.&lt;/p&gt;\\n\\n&lt;p&gt;However, the Unsloth UD quants quantize layers differently than the regular quants. There could be a relevant difference in output quality to the regular quants of comparable size - for the better or worse.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lyy0yi","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyy0yi/dotsllm1_appears_to_be_very_sensitive_to/n2y6eii/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752434766,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2xlgoh","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Admirable-Star7088","can_mod_post":false,"created_utc":1752428535,"send_replies":true,"parent_id":"t1_n2xg5yn","score":1,"author_fullname":"t2_qhlcbiy3k","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yea, it could be interesting to compare with some other quants. Back to downloading 100s of gigabytes again I guess, lol.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2xlgoh","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yea, it could be interesting to compare with some other quants. Back to downloading 100s of gigabytes again I guess, lol.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lyy0yi","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyy0yi/dotsllm1_appears_to_be_very_sensitive_to/n2xlgoh/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752428535,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2xg5yn","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Awwtifishal","can_mod_post":false,"created_utc":1752427019,"send_replies":true,"parent_id":"t3_1lyy0yi","score":3,"author_fullname":"t2_1d96a8k10t","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Maybe there's something going on with unsloth's quants. Maybe try mradermacher's weighted imatrix quants to compare. Or bartowski's. They all may be using different importance matrices.\\n\\n  \\nIn any case I wonder how difficult is to do QAT on dots.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2xg5yn","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Maybe there&amp;#39;s something going on with unsloth&amp;#39;s quants. Maybe try mradermacher&amp;#39;s weighted imatrix quants to compare. Or bartowski&amp;#39;s. They all may be using different importance matrices.&lt;/p&gt;\\n\\n&lt;p&gt;In any case I wonder how difficult is to do QAT on dots.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyy0yi/dotsllm1_appears_to_be_very_sensitive_to/n2xg5yn/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752427019,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lyy0yi","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n314zcq","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"onil_gova","can_mod_post":false,"created_utc":1752473430,"send_replies":true,"parent_id":"t3_1lyy0yi","score":1,"author_fullname":"t2_vcawomd","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Such an underated model, I been running [dots.llm1.inst-mixed-4-6bit](https://huggingface.co/mlx-community/dots.llm1.inst-mixed-4-6bit)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n314zcq","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Such an underated model, I been running &lt;a href=\\"https://huggingface.co/mlx-community/dots.llm1.inst-mixed-4-6bit\\"&gt;dots.llm1.inst-mixed-4-6bit&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyy0yi/dotsllm1_appears_to_be_very_sensitive_to/n314zcq/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752473430,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lyy0yi","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n31c4nd","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"georgejrjrjr","can_mod_post":false,"created_utc":1752477394,"send_replies":true,"parent_id":"t3_1lyy0yi","score":1,"author_fullname":"t2_ei8i09kjx","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Have you tried quantizing only the experts? They've each seen far fewer tokens in training, and they're used less often, so they should be a lot less sensitive than the common parameters.  \\n  \\n(EDIT: Just searched ArXiv, found this paper which backs up my intuition with some data: [https://arxiv.org/html/2406.08155v1](https://arxiv.org/html/2406.08155v1) )","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n31c4nd","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Have you tried quantizing only the experts? They&amp;#39;ve each seen far fewer tokens in training, and they&amp;#39;re used less often, so they should be a lot less sensitive than the common parameters.  &lt;/p&gt;\\n\\n&lt;p&gt;(EDIT: Just searched ArXiv, found this paper which backs up my intuition with some data: &lt;a href=\\"https://arxiv.org/html/2406.08155v1\\"&gt;https://arxiv.org/html/2406.08155v1&lt;/a&gt; )&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyy0yi/dotsllm1_appears_to_be_very_sensitive_to/n31c4nd/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752477394,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lyy0yi","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),s=()=>e.jsx(t,{data:l});export{s as default};
