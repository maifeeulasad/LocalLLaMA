import{j as e}from"./index-BpC9hjVs.js";import{R as l}from"./RedditPostRenderer-BEo6AnSR.js";import"./index-DwkJHX1_.js";const t=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"What kind of fine tuning or LoRA project can be done with $1000 in second hand GPUs or cloud compute? ","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Fine-tuning with $1000?","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1loxf1b","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.4,"author_flair_background_color":null,"subreddit_type":"public","ups":0,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_5iznl","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":0,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1751362893,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;What kind of fine tuning or LoRA project can be done with $1000 in second hand GPUs or cloud compute? &lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":true,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1loxf1b","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"sumguysr","discussion_type":null,"num_comments":20,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1loxf1b/finetuning_with_1000/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1loxf1b/finetuning_with_1000/","subreddit_subscribers":493458,"created_utc":1751362893,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0qcxey","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"_bachrc","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0qcpmb","score":5,"author_fullname":"t2_2q9geryu","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I doubt that you'd be able to buy sufficent hardware with 1000$, so if you already have something in mind, [Runpod GPUs](https://www.runpod.io/product/cloud-gpus) would be your best bet","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0qcxey","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I doubt that you&amp;#39;d be able to buy sufficent hardware with 1000$, so if you already have something in mind, &lt;a href=\\"https://www.runpod.io/product/cloud-gpus\\"&gt;Runpod GPUs&lt;/a&gt; would be your best bet&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1loxf1b","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1loxf1b/finetuning_with_1000/n0qcxey/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751363567,"author_flair_text":null,"treatment_tags":[],"created_utc":1751363567,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0qd41i","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Plenty_Extent_9047","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0qcpmb","score":3,"author_fullname":"t2_eau3hzdik","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Depending if you want to finetune base or instruct modell, indtruct needs less data entries then base. Creating synthetic data is max 100$ FOR A BIG dataset around 100-150k entries with Gemini 2.5 api for example. Fine-tuning itself depends if u doing qlora, Lora or full fine-tuning. What batch size you doing as seq length.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0qd41i","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Depending if you want to finetune base or instruct modell, indtruct needs less data entries then base. Creating synthetic data is max 100$ FOR A BIG dataset around 100-150k entries with Gemini 2.5 api for example. Fine-tuning itself depends if u doing qlora, Lora or full fine-tuning. What batch size you doing as seq length.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1loxf1b","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1loxf1b/finetuning_with_1000/n0qd41i/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751363669,"author_flair_text":null,"treatment_tags":[],"created_utc":1751363669,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"50c36eba-fdca-11ee-9735-92a88d7e3b87","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0rkk0d","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"iKy1e","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0qcpmb","score":3,"author_fullname":"t2_aqewd","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"$1000 is about enough to rent a 3090/4090 class GPU for 1 month non-stop.\\n\\nOr you could buy a 3090 system for that much.\\n\\nBut with renting GPU time you only pay for what you use (spin up do a test, spin down). So leaving it running for a month is wasteful anyway. And you can rent bigger GPUs for more complex and capable models.\\n\\nHowever, if you are going to have it running 24/7 buying a GPU or 2 is cheaper.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0rkk0d","is_submitter":false,"collapsed":false,"author_flair_richtext":[{"e":"text","t":"Ollama"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;$1000 is about enough to rent a 3090/4090 class GPU for 1 month non-stop.&lt;/p&gt;\\n\\n&lt;p&gt;Or you could buy a 3090 system for that much.&lt;/p&gt;\\n\\n&lt;p&gt;But with renting GPU time you only pay for what you use (spin up do a test, spin down). So leaving it running for a month is wasteful anyway. And you can rent bigger GPUs for more complex and capable models.&lt;/p&gt;\\n\\n&lt;p&gt;However, if you are going to have it running 24/7 buying a GPU or 2 is cheaper.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1loxf1b","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1loxf1b/finetuning_with_1000/n0rkk0d/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751380648,"author_flair_text":"Ollama","treatment_tags":[],"created_utc":1751380648,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0qeobd","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"sumguysr","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0qd0a9","score":1,"author_fullname":"t2_5iznl","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Right. That's why I asked about cloud compute.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0qeobd","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Right. That&amp;#39;s why I asked about cloud compute.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1loxf1b","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1loxf1b/finetuning_with_1000/n0qeobd/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751364543,"author_flair_text":null,"treatment_tags":[],"created_utc":1751364543,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0qd0a9","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"narca_hakan","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0qcpmb","score":2,"author_fullname":"t2_d9gk5hdlt","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Instead of spending on GPUs you can rent them online much cheaper for fine-tuning","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0qd0a9","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Instead of spending on GPUs you can rent them online much cheaper for fine-tuning&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1loxf1b","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1loxf1b/finetuning_with_1000/n0qd0a9/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751363610,"author_flair_text":null,"treatment_tags":[],"created_utc":1751363610,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n0qcpmb","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"sumguysr","can_mod_post":false,"created_utc":1751363444,"send_replies":true,"parent_id":"t1_n0qcfeh","score":1,"author_fullname":"t2_5iznl","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I have plenty of ideas for fine tuning. The question is really what size model and how much you can accomplish on that kind of budget.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0qcpmb","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I have plenty of ideas for fine tuning. The question is really what size model and how much you can accomplish on that kind of budget.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1loxf1b","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1loxf1b/finetuning_with_1000/n0qcpmb/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751363444,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0qcfeh","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"_bachrc","can_mod_post":false,"created_utc":1751363284,"send_replies":true,"parent_id":"t3_1loxf1b","score":13,"author_fullname":"t2_2q9geryu","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"If you have spare 1000$, I suggest you take a day off you think about a project that you may like. Why would you fine tune a model? What do you want to achieve?\\n\\nWhen you know, runpod is a good start","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0qcfeh","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;If you have spare 1000$, I suggest you take a day off you think about a project that you may like. Why would you fine tune a model? What do you want to achieve?&lt;/p&gt;\\n\\n&lt;p&gt;When you know, runpod is a good start&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1loxf1b/finetuning_with_1000/n0qcfeh/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751363284,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1loxf1b","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":13}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0qcx31","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"numsu","can_mod_post":false,"created_utc":1751363562,"send_replies":true,"parent_id":"t3_1loxf1b","score":6,"author_fullname":"t2_ejgiq","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"That buys you over 500 H100 80gb pcie hours. You can do several LoRA finetunes with that on anything that can fit on the card if you use open datasets.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0qcx31","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;That buys you over 500 H100 80gb pcie hours. You can do several LoRA finetunes with that on anything that can fit on the card if you use open datasets.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1loxf1b/finetuning_with_1000/n0qcx31/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751363562,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1loxf1b","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0qdckd","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Theio666","can_mod_post":false,"created_utc":1751363801,"send_replies":true,"parent_id":"t3_1loxf1b","score":5,"author_fullname":"t2_ikhuo","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Just for numbers, ML cards like h100 require 24/7 usage for more than a year to get more value that you'd get by simply renting them, so buying hardware is not an economically good decision if you want just a few personal projects.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0qdckd","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Just for numbers, ML cards like h100 require 24/7 usage for more than a year to get more value that you&amp;#39;d get by simply renting them, so buying hardware is not an economically good decision if you want just a few personal projects.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1loxf1b/finetuning_with_1000/n0qdckd/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751363801,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1loxf1b","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0rczco","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ILoveMy2Balls","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0rb1o3","score":3,"author_fullname":"t2_1nisx8ggay","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I made some initial mistakes of choosing the wrong difficulty levels of the math questions but yes there was improvement of nearly 40% in accuracy and the style of answering questions was much better. My problem couldn't have been solved by RAG as RAG doesn't improve the capabilities of the model it just adds data to it and it is on the model+ instructions of how to handle the queries. If you have a budget of 1000 and you want to only fine tune for a few time and not long term I would suggest renting GPUs online. But if you have to make a pipeline of continuous fine-tune then you can consider buying a gpu","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0rczco","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I made some initial mistakes of choosing the wrong difficulty levels of the math questions but yes there was improvement of nearly 40% in accuracy and the style of answering questions was much better. My problem couldn&amp;#39;t have been solved by RAG as RAG doesn&amp;#39;t improve the capabilities of the model it just adds data to it and it is on the model+ instructions of how to handle the queries. If you have a budget of 1000 and you want to only fine tune for a few time and not long term I would suggest renting GPUs online. But if you have to make a pipeline of continuous fine-tune then you can consider buying a gpu&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1loxf1b","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1loxf1b/finetuning_with_1000/n0rczco/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751378371,"author_flair_text":null,"treatment_tags":[],"created_utc":1751378371,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n0rb1o3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"sumguysr","can_mod_post":false,"created_utc":1751377753,"send_replies":true,"parent_id":"t1_n0rah3v","score":1,"author_fullname":"t2_5iznl","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"What kind of results did you get? Was it better at answering questions in a particular category?\\n\\nDo you think your fine tuning did better than putting your data in a RAG?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0rb1o3","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;What kind of results did you get? Was it better at answering questions in a particular category?&lt;/p&gt;\\n\\n&lt;p&gt;Do you think your fine tuning did better than putting your data in a RAG?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1loxf1b","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1loxf1b/finetuning_with_1000/n0rb1o3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751377753,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0rah3v","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ILoveMy2Balls","can_mod_post":false,"created_utc":1751377567,"send_replies":true,"parent_id":"t3_1loxf1b","score":3,"author_fullname":"t2_1nisx8ggay","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I am not an expert but I fine tuned a 7b model with 200,000 questions and answers with a cost of 7 dollars of GPU rental ðŸ˜­, but I couldn't do experimentation and very little testing. It was fine tuning just for the sake of it","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0rah3v","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I am not an expert but I fine tuned a 7b model with 200,000 questions and answers with a cost of 7 dollars of GPU rental ðŸ˜­, but I couldn&amp;#39;t do experimentation and very little testing. It was fine tuning just for the sake of it&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1loxf1b/finetuning_with_1000/n0rah3v/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751377567,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1loxf1b","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0qfbsi","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Ok_Needleworker_5247","can_mod_post":false,"created_utc":1751364902,"send_replies":true,"parent_id":"t3_1loxf1b","score":2,"author_fullname":"t2_1gmprv51a1","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"If you're looking to optimize your budget, consider using cloud services like Google Cloud or AWS which offer free credits for new users. This could extend your compute time beyond what $1000 would typically cover. Also, explore using open-source models and datasets to cut costs.","edited":1751392507,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0qfbsi","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;If you&amp;#39;re looking to optimize your budget, consider using cloud services like Google Cloud or AWS which offer free credits for new users. This could extend your compute time beyond what $1000 would typically cover. Also, explore using open-source models and datasets to cut costs.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1loxf1b/finetuning_with_1000/n0qfbsi/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751364902,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1loxf1b","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0robj8","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"de4dee","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0r11q1","score":1,"author_fullname":"t2_jc4t2","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"yes using Unsloth.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0robj8","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;yes using Unsloth.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1loxf1b","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1loxf1b/finetuning_with_1000/n0robj8/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751381757,"author_flair_text":null,"treatment_tags":[],"created_utc":1751381757,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0r11q1","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"sumguysr","can_mod_post":false,"created_utc":1751374395,"send_replies":true,"parent_id":"t1_n0qvosw","score":1,"author_fullname":"t2_5iznl","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Have you done fine tuning on qwen3?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0r11q1","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Have you done fine tuning on qwen3?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1loxf1b","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1loxf1b/finetuning_with_1000/n0r11q1/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751374395,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0qvosw","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"de4dee","can_mod_post":false,"created_utc":1751372397,"send_replies":true,"parent_id":"t3_1loxf1b","score":2,"author_fullname":"t2_jc4t2","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I would do qwen3 14b. \\n\\nmaybe start with Maxime Labonne's abliterated model to do something uncensored?\\n\\nit fits in RTX 3090.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0qvosw","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I would do qwen3 14b. &lt;/p&gt;\\n\\n&lt;p&gt;maybe start with Maxime Labonne&amp;#39;s abliterated model to do something uncensored?&lt;/p&gt;\\n\\n&lt;p&gt;it fits in RTX 3090.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1loxf1b/finetuning_with_1000/n0qvosw/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751372397,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1loxf1b","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0s6z4y","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ahstanin","can_mod_post":false,"created_utc":1751386950,"send_replies":true,"parent_id":"t3_1loxf1b","score":2,"author_fullname":"t2_im30t","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I used [vast.ai](http://vast.ai) for a while, great price for H200 GPU.  \\nSpent more than $8000 there, and last week bought a Jetson AGX Orin for 2100 USD.  \\nYesterday installed everything needed and am running training on this device now. Taking 6x more time, but I am not in a rush.\\n\\nYou can see my post here : [https://www.reddit.com/r/LocalLLaMA/comments/1lp37v0/lora\\\\_training\\\\_on\\\\_nvidia\\\\_jetson\\\\_agx\\\\_orin\\\\_64gb/](https://www.reddit.com/r/LocalLLaMA/comments/1lp37v0/lora_training_on_nvidia_jetson_agx_orin_64gb/)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0s6z4y","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I used &lt;a href=\\"http://vast.ai\\"&gt;vast.ai&lt;/a&gt; for a while, great price for H200 GPU.&lt;br/&gt;\\nSpent more than $8000 there, and last week bought a Jetson AGX Orin for 2100 USD.&lt;br/&gt;\\nYesterday installed everything needed and am running training on this device now. Taking 6x more time, but I am not in a rush.&lt;/p&gt;\\n\\n&lt;p&gt;You can see my post here : &lt;a href=\\"https://www.reddit.com/r/LocalLLaMA/comments/1lp37v0/lora_training_on_nvidia_jetson_agx_orin_64gb/\\"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1lp37v0/lora_training_on_nvidia_jetson_agx_orin_64gb/&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1loxf1b/finetuning_with_1000/n0s6z4y/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751386950,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1loxf1b","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0tp1ub","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Double_Cause4609","can_mod_post":false,"created_utc":1751402103,"send_replies":true,"parent_id":"t3_1loxf1b","score":2,"author_fullname":"t2_1kubzxt2ww","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"First of all:\\n\\nWhatever you do, don't go nuts and spend it all in a single go or all at once if you haven't done fine tuning before.\\n\\nConsider making use of Colab or Kaggle GPUs to get a workflow going on a smaller model prior to training your target model.\\n\\nThat aside, for about $1,000 in cloud compute, you could:\\n\\n* Produce a high quality literary fine tune of a moderately sized MoE model (Scout, perhaps the new Hunyuan, etc)\\n* Train QLoRA on a 70B model for presumably some kind of reasoning operation, and then distill it down onto a small 6-10B model for deployment (bonus points if you use something like a QAT recipe on the student)\\n* You could train probably dozens of LoRAs on a small model (8-14B size), on a variety of topics.\\n\\nWith $1,000 in local comppute, you could:    \\n\\nGet possibly three or four P40s (or MI60s if you're feeling lucky), which would be enough to  \\n\\n* Do QLoRA fine tuning of a 70B if you squint really hard and are super careful with memory management (it'll be slow though. I don't think you'd ever actually do it)\\n* Train and iterate on LoRAs on small models at a pretty rapid pace (I think you could knock out reasonable LoRAs on smaller models at possibly two a day if you were really going crazy)\\n* Also run inference. You could run up to around 70B models with such a setup.\\n\\nYou could also just about be in the price range to get a used server CPU, which is less useful for training (though it can be done by the sufficiently mentally deranged), but is super useful if the prior of larger models with solid prompt engineering is more valuable for your purposes than fine tuning. In particular large sparse MoE models are fairly liveable on CPU.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0tp1ub","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;First of all:&lt;/p&gt;\\n\\n&lt;p&gt;Whatever you do, don&amp;#39;t go nuts and spend it all in a single go or all at once if you haven&amp;#39;t done fine tuning before.&lt;/p&gt;\\n\\n&lt;p&gt;Consider making use of Colab or Kaggle GPUs to get a workflow going on a smaller model prior to training your target model.&lt;/p&gt;\\n\\n&lt;p&gt;That aside, for about $1,000 in cloud compute, you could:&lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;Produce a high quality literary fine tune of a moderately sized MoE model (Scout, perhaps the new Hunyuan, etc)&lt;/li&gt;\\n&lt;li&gt;Train QLoRA on a 70B model for presumably some kind of reasoning operation, and then distill it down onto a small 6-10B model for deployment (bonus points if you use something like a QAT recipe on the student)&lt;/li&gt;\\n&lt;li&gt;You could train probably dozens of LoRAs on a small model (8-14B size), on a variety of topics.&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;p&gt;With $1,000 in local comppute, you could:    &lt;/p&gt;\\n\\n&lt;p&gt;Get possibly three or four P40s (or MI60s if you&amp;#39;re feeling lucky), which would be enough to  &lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;Do QLoRA fine tuning of a 70B if you squint really hard and are super careful with memory management (it&amp;#39;ll be slow though. I don&amp;#39;t think you&amp;#39;d ever actually do it)&lt;/li&gt;\\n&lt;li&gt;Train and iterate on LoRAs on small models at a pretty rapid pace (I think you could knock out reasonable LoRAs on smaller models at possibly two a day if you were really going crazy)&lt;/li&gt;\\n&lt;li&gt;Also run inference. You could run up to around 70B models with such a setup.&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;p&gt;You could also just about be in the price range to get a used server CPU, which is less useful for training (though it can be done by the sufficiently mentally deranged), but is super useful if the prior of larger models with solid prompt engineering is more valuable for your purposes than fine tuning. In particular large sparse MoE models are fairly liveable on CPU.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1loxf1b/finetuning_with_1000/n0tp1ub/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751402103,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1loxf1b","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}}]`),r=()=>e.jsx(l,{data:t});export{r as default};
