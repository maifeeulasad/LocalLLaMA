import{j as e}from"./index-BgwOAK4-.js";import{R as l}from"./RedditPostRenderer-BOBjDTFu.js";import"./index-BL22wVg5.js";const t=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":" Currently i have workstation. Which is powered by AMD EPYC 7452 32 core cpu with 256GB  RAM . The worksration has 5 x 4Gen pcie slots  and has A100 40Gb currently running with it. So i planned to upgrade it .I wanna load all the other 4 slots with either RTX 6000 ADA for with L40S . which can i go for????, i know there is gonna be a release of RTX blackwell series i cant use it since it needs  5TH gen pcie slots.PSU for the workstation is 2400w.\\n\\nMy questions are;  \\n1. Which gpu should i choose and why?  \\n2. Does the nvlink works on them. cuz some internet resources say it can be used or some say it doesn't.\\n\\nMY use cases are for   \\nFine-tuning,Model distillation, Local inference ,Unity and omniverse .","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"GPU UPGRADE!!!!NEED Suggestion!!!!.Upgrading current workstation either with 4x RTX 6000 ada or 4x L40s. Can i use NVlink bridge the pair them up.??","link_flair_richtext":[{"e":"text","t":"Discussion"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1ly4xvb","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.27,"author_flair_background_color":null,"subreddit_type":"public","ups":0,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_adm2b7kq","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Discussion","can_mod_post":false,"score":0,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1752338570,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Currently i have workstation. Which is powered by AMD EPYC 7452 32 core cpu with 256GB  RAM . The worksration has 5 x 4Gen pcie slots  and has A100 40Gb currently running with it. So i planned to upgrade it .I wanna load all the other 4 slots with either RTX 6000 ADA for with L40S . which can i go for????, i know there is gonna be a release of RTX blackwell series i cant use it since it needs  5TH gen pcie slots.PSU for the workstation is 2400w.&lt;/p&gt;\\n\\n&lt;p&gt;My questions are;&lt;br/&gt;\\n1. Which gpu should i choose and why?&lt;br/&gt;\\n2. Does the nvlink works on them. cuz some internet resources say it can be used or some say it doesn&amp;#39;t.&lt;/p&gt;\\n\\n&lt;p&gt;MY use cases are for&lt;br/&gt;\\nFine-tuning,Model distillation, Local inference ,Unity and omniverse .&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":true,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"5f921ea4-c7bc-11ed-9c23-3a00622979b4","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#646d73","id":"1ly4xvb","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"logii33","discussion_type":null,"num_comments":15,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1ly4xvb/gpu_upgradeneed_suggestionupgrading_current/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1ly4xvb/gpu_upgradeneed_suggestionupgrading_current/","subreddit_subscribers":498345,"created_utc":1752338570,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2s08am","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"logii33","can_mod_post":false,"created_utc":1752348670,"send_replies":true,"parent_id":"t1_n2rplnh","score":0,"author_fullname":"t2_adm2b7kq","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Understood!! So it should be a optimal one . Not like a top one!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2s08am","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Understood!! So it should be a optimal one . Not like a top one!&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ly4xvb","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ly4xvb/gpu_upgradeneed_suggestionupgrading_current/n2s08am/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752348670,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2tnscc","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"segmond","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2t0vtu","score":1,"author_fullname":"t2_ah13x","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"i'm too poor to know the specs, i just know \\n\\nit's expensive, not as fast as a 4090, 48gb of ram, decent power usage and I'll like one or more but can't afford it. :D","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2tnscc","is_submitter":false,"collapsed":false,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;i&amp;#39;m too poor to know the specs, i just know &lt;/p&gt;\\n\\n&lt;p&gt;it&amp;#39;s expensive, not as fast as a 4090, 48gb of ram, decent power usage and I&amp;#39;ll like one or more but can&amp;#39;t afford it. :D&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ly4xvb","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ly4xvb/gpu_upgradeneed_suggestionupgrading_current/n2tnscc/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752369158,"author_flair_text":"llama.cpp","treatment_tags":[],"created_utc":1752369158,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2t0vtu","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"CKtalon","can_mod_post":false,"created_utc":1752360829,"send_replies":true,"parent_id":"t1_n2rplnh","score":2,"author_fullname":"t2_4xml7","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"RTX 6000 Ada and up donâ€™t even have NVlink","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2t0vtu","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;RTX 6000 Ada and up donâ€™t even have NVlink&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ly4xvb","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ly4xvb/gpu_upgradeneed_suggestionupgrading_current/n2t0vtu/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752360829,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n2rplnh","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"segmond","can_mod_post":false,"created_utc":1752345261,"send_replies":true,"parent_id":"t3_1ly4xvb","score":7,"author_fullname":"t2_ah13x","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"you need as much sense as you have money, so spend some time and do some reading up.   most folks on here are going to speculate, and will speculate right, but most speculating because very few of us have such a system.  i do have plenty of multi GPU systems, but should I be advising you because I got a bunch of P40s and 3060s?   \\n\\n  \\nwith that said, I'll go with rtx 6000 adas, I will not worry about nvlink, very few people have shown any benefits despite the money they spend and the trouble they go through.   go without it first and if you find yourself still desperate for performance then look into it.  will only matter if training and parallel inference.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2rplnh","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;you need as much sense as you have money, so spend some time and do some reading up.   most folks on here are going to speculate, and will speculate right, but most speculating because very few of us have such a system.  i do have plenty of multi GPU systems, but should I be advising you because I got a bunch of P40s and 3060s?   &lt;/p&gt;\\n\\n&lt;p&gt;with that said, I&amp;#39;ll go with rtx 6000 adas, I will not worry about nvlink, very few people have shown any benefits despite the money they spend and the trouble they go through.   go without it first and if you find yourself still desperate for performance then look into it.  will only matter if training and parallel inference.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ly4xvb/gpu_upgradeneed_suggestionupgrading_current/n2rplnh/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752345261,"author_flair_text":"llama.cpp","treatment_tags":[],"link_id":"t3_1ly4xvb","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":7}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2s0igz","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"MelodicRecognition7","can_mod_post":false,"created_utc":1752348764,"send_replies":true,"parent_id":"t3_1ly4xvb","score":7,"author_fullname":"t2_1eex9ug5","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"lol are you on coke?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2s0igz","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;lol are you on coke?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ly4xvb/gpu_upgradeneed_suggestionupgrading_current/n2s0igz/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752348764,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1ly4xvb","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":7}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2rk7it","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DinoAmino","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2r6wfh","score":3,"author_fullname":"t2_j1v7f","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"A100 has NVLINK. RTX 6000 ADA does not. NVLINK will only help with stuff like training and batch loads, so your multi user concurrency should benefit from it if you choose A100.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2rk7it","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;A100 has NVLINK. RTX 6000 ADA does not. NVLINK will only help with stuff like training and batch loads, so your multi user concurrency should benefit from it if you choose A100.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ly4xvb","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ly4xvb/gpu_upgradeneed_suggestionupgrading_current/n2rk7it/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752343553,"author_flair_text":null,"treatment_tags":[],"created_utc":1752343553,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n2r6wfh","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"logii33","can_mod_post":false,"created_utc":1752339533,"send_replies":true,"parent_id":"t1_n2r5f5j","score":0,"author_fullname":"t2_adm2b7kq","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"What about the nvlink , any any idea i can pair them up. Man i got only budget for l40s or rtx 6000 ada . The ampere arch i costly ðŸ˜… both a100 40gb is costly than both of them. And also the workstation works as VM , \\nim not the only person using it there are other 3 to 4 persons operating it simultaneously.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2r6wfh","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;What about the nvlink , any any idea i can pair them up. Man i got only budget for l40s or rtx 6000 ada . The ampere arch i costly ðŸ˜… both a100 40gb is costly than both of them. And also the workstation works as VM , \\nim not the only person using it there are other 3 to 4 persons operating it simultaneously.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ly4xvb","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ly4xvb/gpu_upgradeneed_suggestionupgrading_current/n2r6wfh/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752339533,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}},"user_reports":[],"saved":false,"id":"n2r5f5j","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"GeekyBit","can_mod_post":false,"created_utc":1752339087,"send_replies":true,"parent_id":"t3_1ly4xvb","score":1,"author_fullname":"t2_zq180","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"TBH other than fine-tuning and model distillation it is a little overkill already if you got 5x a100 40gb ...\\n\\nIf you only have 1 A100 40gb get more a100 they have 1.56TB/s bandwidth where the RTX 6000 ADA has only 960MB/s\\n\\nSo yea you are going to get and extra 8 gb per slot which would be 40gb, but that is going from 200 to 240gb so likely still not enough to run a large model purely GPU if the other can't run it... I mean highly distilled models sure.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2r5f5j","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;TBH other than fine-tuning and model distillation it is a little overkill already if you got 5x a100 40gb ...&lt;/p&gt;\\n\\n&lt;p&gt;If you only have 1 A100 40gb get more a100 they have 1.56TB/s bandwidth where the RTX 6000 ADA has only 960MB/s&lt;/p&gt;\\n\\n&lt;p&gt;So yea you are going to get and extra 8 gb per slot which would be 40gb, but that is going from 200 to 240gb so likely still not enough to run a large model purely GPU if the other can&amp;#39;t run it... I mean highly distilled models sure.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ly4xvb/gpu_upgradeneed_suggestionupgrading_current/n2r5f5j/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752339087,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1ly4xvb","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2rtu51","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DinoAmino","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2rro9o","score":2,"author_fullname":"t2_j1v7f","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"A6000 does support NVLINK. vLLM loads the Marlin kernels to support FP8, so you can use those models - at least you can reliably use the ones from RedHat.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2rtu51","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;A6000 does support NVLINK. vLLM loads the Marlin kernels to support FP8, so you can use those models - at least you can reliably use the ones from RedHat.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ly4xvb","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ly4xvb/gpu_upgradeneed_suggestionupgrading_current/n2rtu51/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752346597,"author_flair_text":null,"treatment_tags":[],"created_utc":1752346597,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2sbzcm","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Freonr2","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2rss7x","score":1,"author_fullname":"t2_8xi6x","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Also the 600W Blackwell is physically large. It takes up 3 slots and is very long.  Physically fitting it is a challenge, especially if you want more than 1 or 2 down the road.  Plan carefully...  Max-Q would definitely be smarter if you ever intent to run more than 1.","edited":false,"author_flair_css_class":null,"name":"t1_n2sbzcm","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Also the 600W Blackwell is physically large. It takes up 3 slots and is very long.  Physically fitting it is a challenge, especially if you want more than 1 or 2 down the road.  Plan carefully...  Max-Q would definitely be smarter if you ever intent to run more than 1.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1ly4xvb","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ly4xvb/gpu_upgradeneed_suggestionupgrading_current/n2sbzcm/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752352454,"author_flair_text":null,"collapsed":false,"created_utc":1752352454,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2rss7x","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Tyme4Trouble","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2rsk46","score":2,"author_fullname":"t2_973amyap","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Iâ€™ll add if you do go with the RTX Pro 6000 BW get the Max-Q version. It runs at 300W versus 600W for the full sized workstation card.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2rss7x","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Iâ€™ll add if you do go with the RTX Pro 6000 BW get the Max-Q version. It runs at 300W versus 600W for the full sized workstation card.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ly4xvb","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ly4xvb/gpu_upgradeneed_suggestionupgrading_current/n2rss7x/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752346264,"author_flair_text":null,"treatment_tags":[],"created_utc":1752346264,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n2rsk46","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Tyme4Trouble","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2rro9o","score":1,"author_fullname":"t2_973amyap","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"The only drawback would be PCIe bandwidth. The memory bandwidth of the RTX Pro 6000 Blackwell is nearly double the 6000 Ada. 1.6-1.7TB/s vs 960GB/s\\n\\nIf you want NVLink on a modern PCIe card youâ€™d need to get an H100NVL or H200NVL.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2rsk46","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The only drawback would be PCIe bandwidth. The memory bandwidth of the RTX Pro 6000 Blackwell is nearly double the 6000 Ada. 1.6-1.7TB/s vs 960GB/s&lt;/p&gt;\\n\\n&lt;p&gt;If you want NVLink on a modern PCIe card youâ€™d need to get an H100NVL or H200NVL.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ly4xvb","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ly4xvb/gpu_upgradeneed_suggestionupgrading_current/n2rsk46/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752346194,"author_flair_text":null,"treatment_tags":[],"created_utc":1752346194,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2rro9o","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"logii33","can_mod_post":false,"created_utc":1752345916,"send_replies":true,"parent_id":"t1_n2rkvzy","score":1,"author_fullname":"t2_adm2b7kq","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"So you are telling me to go with the blackwell series, the only drawback i have is memory bandwidth of the PCIe 4.0 right . I saw that even the RTX PRo series doesnt support nvlink. What about the RTX A6000 it supports nvlink , is it too old for Ai . It doesnt support fp8 , bf16 like the lovelace architecture . Should i go for RTX pro series then??????","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2rro9o","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;So you are telling me to go with the blackwell series, the only drawback i have is memory bandwidth of the PCIe 4.0 right . I saw that even the RTX PRo series doesnt support nvlink. What about the RTX A6000 it supports nvlink , is it too old for Ai . It doesnt support fp8 , bf16 like the lovelace architecture . Should i go for RTX pro series then??????&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ly4xvb","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ly4xvb/gpu_upgradeneed_suggestionupgrading_current/n2rro9o/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752345916,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2rkvzy","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Tyme4Trouble","can_mod_post":false,"created_utc":1752343766,"send_replies":true,"parent_id":"t3_1ly4xvb","score":1,"author_fullname":"t2_973amyap","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Neither the RTX 6000 Ada or L40S support NVLink. Those two cards are basically the same so unless you can get a deal on L40Ses and have adequate airflow stick with the RTX 6000 Ada. \\n\\nRTX Pro 6000 Blackwell supports PCIe 5.0. PCIe is backwards compatible so it should work fine in a PCIe 4.0 slot. The max theoretical bandwidth for the slot will just be 64GB/s rather than 128GB/s.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2rkvzy","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Neither the RTX 6000 Ada or L40S support NVLink. Those two cards are basically the same so unless you can get a deal on L40Ses and have adequate airflow stick with the RTX 6000 Ada. &lt;/p&gt;\\n\\n&lt;p&gt;RTX Pro 6000 Blackwell supports PCIe 5.0. PCIe is backwards compatible so it should work fine in a PCIe 4.0 slot. The max theoretical bandwidth for the slot will just be 64GB/s rather than 128GB/s.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ly4xvb/gpu_upgradeneed_suggestionupgrading_current/n2rkvzy/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752343766,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1ly4xvb","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2sa0t4","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Freonr2","can_mod_post":false,"created_utc":1752351826,"send_replies":true,"parent_id":"t3_1ly4xvb","score":1,"author_fullname":"t2_8xi6x","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"You don't need 5th gen slots to use Blackwell.  It will run in older gen slots just fine.  The PCIe bandwidth may not be a bottleneck anyway, like for LLM inference I don't think it will matter.\\n\\nAda generation workstation cards do not have NVlink.  Ampere A6000 still has it, no more in 6000 Ada.  I assume the L40S is doesn't have it either, and don't see any mention of NVlink on nvidia's product page, but I could be wrong.  The RTX 6000 Ada I'm sure of, I own one, no NVlink.\\n\\nI'm not certain NVlink is super important for anything but fine tuning and even then your training software needs to utilize it properly and it might take a lot of tweaking and tuning to use optimally.  The software/training scripts that use it are likely tuned for DGX/HGX systems (8x GPU SXM interface), not workstations (PCIe+NVlink connectors). Tuning as in actually modifying the software, not just adjusting settings.\\n\\nBlackwell is out.  The Pro 6000 96GB might be a bit hard to source at the moment but I'd say just watch for stock. It's a great card if you have the money for it.  Consider Max-Q edition if you plan on running more than 1 for the smaller form factor and lower power.  Unless you are really itching, I might wait.  $8500 for a 6000 Pro 96GB is actually not that bad considering the 6000 Ada 48GB was $7k and has substantially less bandwidth and half the memory.  Even the Blackwell 5000 48GB at $4500 isn't a bad look, though it trails the 6000 Ada 48GB in FP16 TFLOPs slightly.\\n\\nAnd no, Blackwell RTX Pro cards don't have NVlink either, and NVLink is not coming back for workstation cards.  I think it is virtually dead outside the x100/x200 datacenter parts.  So, basically spend $350k on an entire DGX/HGX server or just don't worry about NVLink","edited":1752352175,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2sa0t4","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You don&amp;#39;t need 5th gen slots to use Blackwell.  It will run in older gen slots just fine.  The PCIe bandwidth may not be a bottleneck anyway, like for LLM inference I don&amp;#39;t think it will matter.&lt;/p&gt;\\n\\n&lt;p&gt;Ada generation workstation cards do not have NVlink.  Ampere A6000 still has it, no more in 6000 Ada.  I assume the L40S is doesn&amp;#39;t have it either, and don&amp;#39;t see any mention of NVlink on nvidia&amp;#39;s product page, but I could be wrong.  The RTX 6000 Ada I&amp;#39;m sure of, I own one, no NVlink.&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;m not certain NVlink is super important for anything but fine tuning and even then your training software needs to utilize it properly and it might take a lot of tweaking and tuning to use optimally.  The software/training scripts that use it are likely tuned for DGX/HGX systems (8x GPU SXM interface), not workstations (PCIe+NVlink connectors). Tuning as in actually modifying the software, not just adjusting settings.&lt;/p&gt;\\n\\n&lt;p&gt;Blackwell is out.  The Pro 6000 96GB might be a bit hard to source at the moment but I&amp;#39;d say just watch for stock. It&amp;#39;s a great card if you have the money for it.  Consider Max-Q edition if you plan on running more than 1 for the smaller form factor and lower power.  Unless you are really itching, I might wait.  $8500 for a 6000 Pro 96GB is actually not that bad considering the 6000 Ada 48GB was $7k and has substantially less bandwidth and half the memory.  Even the Blackwell 5000 48GB at $4500 isn&amp;#39;t a bad look, though it trails the 6000 Ada 48GB in FP16 TFLOPs slightly.&lt;/p&gt;\\n\\n&lt;p&gt;And no, Blackwell RTX Pro cards don&amp;#39;t have NVlink either, and NVLink is not coming back for workstation cards.  I think it is virtually dead outside the x100/x200 datacenter parts.  So, basically spend $350k on an entire DGX/HGX server or just don&amp;#39;t worry about NVLink&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ly4xvb/gpu_upgradeneed_suggestionupgrading_current/n2sa0t4/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752351826,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1ly4xvb","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),n=()=>e.jsx(l,{data:t});export{n as default};
