import{j as l}from"./index-Cd3v0jxz.js";import{R as e}from"./RedditPostRenderer-cI96YLhy.js";import"./index-DGBoKyQm.js";const t=JSON.parse('[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Hi there, I\'m Elie from the smollm team at huggingface, sharing this new model we built for local/on device use!   \\n  \\nblog: [https://huggingface.co/blog/smollm3](https://huggingface.co/blog/smollm3)  \\nGGUF/ONIX ckpt are being uploaded here: [https://huggingface.co/collections/HuggingFaceTB/smollm3-686d33c1fdffe8e635317e23](https://huggingface.co/collections/HuggingFaceTB/smollm3-686d33c1fdffe8e635317e23) \\n\\nLet us know what you think!!","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"SmolLM3: reasoning, long context and multilinguality for 3B parameter only","link_flair_richtext":[{"e":"text","t":"Resources"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":84,"top_awarded_type":null,"hide_score":false,"name":"t3_1lusr7l","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.99,"author_flair_background_color":null,"ups":362,"total_awards_received":0,"media_embed":{},"thumbnail_width":140,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_169jzqdxe5","secure_media":null,"is_reddit_media_domain":true,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Resources","can_mod_post":false,"score":362,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"https://b.thumbs.redditmedia.com/Pgo7DLY-b9pkx5rEmpEdJXAScPnm7YQcCBljCmubJHo.jpg","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"post_hint":"image","content_categories":null,"is_self":false,"subreddit_type":"public","created":1751991256,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"i.redd.it","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Hi there, I&amp;#39;m Elie from the smollm team at huggingface, sharing this new model we built for local/on device use!   &lt;/p&gt;\\n\\n&lt;p&gt;blog: &lt;a href=\\"https://huggingface.co/blog/smollm3\\"&gt;https://huggingface.co/blog/smollm3&lt;/a&gt;&lt;br/&gt;\\nGGUF/ONIX ckpt are being uploaded here: &lt;a href=\\"https://huggingface.co/collections/HuggingFaceTB/smollm3-686d33c1fdffe8e635317e23\\"&gt;https://huggingface.co/collections/HuggingFaceTB/smollm3-686d33c1fdffe8e635317e23&lt;/a&gt; &lt;/p&gt;\\n\\n&lt;p&gt;Let us know what you think!!&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"url_overridden_by_dest":"https://i.redd.it/njam3shfcobf1.png","view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"preview":{"images":[{"source":{"url":"https://preview.redd.it/njam3shfcobf1.png?auto=webp&amp;s=878940560256d58bede1ec736ad4c2822215c7c1","width":2048,"height":1229},"resolutions":[{"url":"https://preview.redd.it/njam3shfcobf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=02512340691c024aa56fdfadf2cf00ed3eaa8f6c","width":108,"height":64},{"url":"https://preview.redd.it/njam3shfcobf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=25ba7f9ae299557841788002dd85f2bfb310dcf0","width":216,"height":129},{"url":"https://preview.redd.it/njam3shfcobf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=e95743ece71a6073e3f1f54779ed3a9ed19335f5","width":320,"height":192},{"url":"https://preview.redd.it/njam3shfcobf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=ac0783544f10bf513ae61c3adb68fd4ef3c75281","width":640,"height":384},{"url":"https://preview.redd.it/njam3shfcobf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=7bc6aa4dcf1326bad4d74b5ab8d700075183cc35","width":960,"height":576},{"url":"https://preview.redd.it/njam3shfcobf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=d6eafa575097ea3c5327a9d03316297602f59fdd","width":1080,"height":648}],"variants":{},"id":"arz-YdPLxSkV6C1oumi84U9PyaRfc_uq0EOj0ruohzc"}],"enabled":true},"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"mod_note":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"num_reports":null,"removal_reason":null,"link_flair_background_color":"#ccac2b","id":"1lusr7l","is_robot_indexable":true,"num_duplicates":1,"report_reasons":null,"author":"eliebakk","discussion_type":null,"num_comments":46,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lusr7l/smollm3_reasoning_long_context_and/","stickied":false,"url":"https://i.redd.it/njam3shfcobf1.png","subreddit_subscribers":497025,"created_utc":1751991256,"num_crossposts":1,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"5e607f92-4428-11ee-be78-faa6ca8ae2cf","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n22pye4","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"lewtun","can_mod_post":false,"send_replies":true,"parent_id":"t1_n21nmcd","score":6,"author_fullname":"t2_cttou014","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"You can disable thinking by appending /no_think to the system message ","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n22pye4","is_submitter":false,"collapsed":false,"author_flair_richtext":[{"e":"text","t":"Hugging Face Staff"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You can disable thinking by appending /no_think to the system message &lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lusr7l","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lusr7l/smollm3_reasoning_long_context_and/n22pye4/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752015581,"author_flair_text":"Hugging Face Staff","treatment_tags":[],"created_utc":1752015581,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":"#5a74cc","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n23nsrx","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"GoodbyeThings","can_mod_post":false,"send_replies":true,"parent_id":"t1_n22nhsu","score":1,"author_fullname":"t2_m28kd7l","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"MacBook M2Max ","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n23nsrx","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;MacBook M2Max &lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lusr7l","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lusr7l/smollm3_reasoning_long_context_and/n23nsrx/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752026827,"author_flair_text":null,"treatment_tags":[],"created_utc":1752026827,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n22nhsu","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"simracerman","can_mod_post":false,"send_replies":true,"parent_id":"t1_n21nmcd","score":1,"author_fullname":"t2_vbzgnic","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"What’s your setup?","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n22nhsu","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;What’s your setup?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lusr7l","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lusr7l/smollm3_reasoning_long_context_and/n22nhsu/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752014799,"author_flair_text":null,"treatment_tags":[],"created_utc":1752014799,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n21nmcd","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"GoodbyeThings","can_mod_post":false,"created_utc":1752004482,"send_replies":true,"parent_id":"t1_n20f1e5","score":9,"author_fullname":"t2_m28kd7l","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Just built it first try and ran it. Super happy. Just not sure if or how I disable thinking locally. \\n\\n\\n\\n\\n    Prompt\\n    - Tokens: 229\\n    - Time: 270.599 ms\\n    - Speed: 846.3 t/s\\n    Generation\\n    - Tokens: 199\\n    - Time: 2332.691 ms\\n    - Speed: 85.3 t/s","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n21nmcd","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Just built it first try and ran it. Super happy. Just not sure if or how I disable thinking locally. &lt;/p&gt;\\n\\n&lt;pre&gt;&lt;code&gt;Prompt\\n- Tokens: 229\\n- Time: 270.599 ms\\n- Speed: 846.3 t/s\\nGeneration\\n- Tokens: 199\\n- Time: 2332.691 ms\\n- Speed: 85.3 t/s\\n&lt;/code&gt;&lt;/pre&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lusr7l","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lusr7l/smollm3_reasoning_long_context_and/n21nmcd/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752004482,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":9}}],"before":null}},"user_reports":[],"saved":false,"id":"n20f1e5","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"newsletternew","can_mod_post":false,"created_utc":1751992296,"send_replies":true,"parent_id":"t3_1lusr7l","score":55,"author_fullname":"t2_ueopbg0h","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Oh, support for SmolLM3 has just been merged in LLaMa.cpp. Great timing!  \\n[https://github.com/ggml-org/llama.cpp/pull/14581](https://github.com/ggml-org/llama.cpp/pull/14581)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n20f1e5","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Oh, support for SmolLM3 has just been merged in LLaMa.cpp. Great timing!&lt;br/&gt;\\n&lt;a href=\\"https://github.com/ggml-org/llama.cpp/pull/14581\\"&gt;https://github.com/ggml-org/llama.cpp/pull/14581&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lusr7l/smollm3_reasoning_long_context_and/n20f1e5/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751992296,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lusr7l","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":55}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2591yw","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AffectionateSnow8803","can_mod_post":false,"created_utc":1752053812,"send_replies":true,"parent_id":"t1_n20jjy4","score":3,"author_fullname":"t2_1ex2of00ee","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I am getting this error  \\nError: unable to load model: /Users/name/.ollama/models/blobs/sha256-8334b850b7bd46238c16b0c550df2138f0889bf433809008cc17a8b05761863e","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2591yw","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I am getting this error&lt;br/&gt;\\nError: unable to load model: /Users/name/.ollama/models/blobs/sha256-8334b850b7bd46238c16b0c550df2138f0889bf433809008cc17a8b05761863e&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lusr7l","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lusr7l/smollm3_reasoning_long_context_and/n2591yw/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752053812,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n20jjy4","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"akukuta","can_mod_post":false,"created_utc":1751993572,"send_replies":true,"parent_id":"t3_1lusr7l","score":20,"author_fullname":"t2_6l78zbgo","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"[ggml-org/SmolLM3-3B-GGUF · Hugging Face](https://huggingface.co/ggml-org/SmolLM3-3B-GGUF)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n20jjy4","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;&lt;a href=\\"https://huggingface.co/ggml-org/SmolLM3-3B-GGUF\\"&gt;ggml-org/SmolLM3-3B-GGUF · Hugging Face&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lusr7l/smollm3_reasoning_long_context_and/n20jjy4/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751993572,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lusr7l","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":20}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n22k65t","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"eliebakk","can_mod_post":false,"created_utc":1752013744,"send_replies":true,"parent_id":"t1_n20mah5","score":7,"author_fullname":"t2_169jzqdxe5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"There is for next model (or at least to do ablation to see how it behave)!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n22k65t","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;There is for next model (or at least to do ablation to see how it behave)!&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lusr7l","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lusr7l/smollm3_reasoning_long_context_and/n22k65t/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752013744,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":7}}],"before":null}},"user_reports":[],"saved":false,"id":"n20mah5","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"BlueSwordM","can_mod_post":false,"created_utc":1751994324,"send_replies":true,"parent_id":"t3_1lusr7l","score":12,"author_fullname":"t2_qhqon","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Thanks for the new release.\\n\\nI\'m curious, but were there any plans to use MLA instead of GQA for better performance and much lower memory usage?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n20mah5","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thanks for the new release.&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;m curious, but were there any plans to use MLA instead of GQA for better performance and much lower memory usage?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lusr7l/smollm3_reasoning_long_context_and/n20mah5/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751994324,"author_flair_text":"llama.cpp","treatment_tags":[],"link_id":"t3_1lusr7l","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":12}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n26vvkv","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"eliebakk","can_mod_post":false,"send_replies":true,"parent_id":"t1_n24xvpf","score":2,"author_fullname":"t2_169jzqdxe5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"i think they are already converted! thanks","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n26vvkv","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;i think they are already converted! thanks&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lusr7l","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lusr7l/smollm3_reasoning_long_context_and/n26vvkv/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752074741,"author_flair_text":null,"treatment_tags":[],"created_utc":1752074741,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n24xvpf","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Chromix_","can_mod_post":false,"send_replies":true,"parent_id":"t1_n22f8y2","score":2,"author_fullname":"t2_k7w2h","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"The chat template issue was [just fixed](https://github.com/ggml-org/llama.cpp/pull/14586). The GGUFs need to be re-converted.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n24xvpf","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The chat template issue was &lt;a href=\\"https://github.com/ggml-org/llama.cpp/pull/14586\\"&gt;just fixed&lt;/a&gt;. The GGUFs need to be re-converted.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lusr7l","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lusr7l/smollm3_reasoning_long_context_and/n24xvpf/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752047427,"author_flair_text":null,"treatment_tags":[],"created_utc":1752047427,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n22f8y2","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"eliebakk","can_mod_post":false,"created_utc":1752012225,"send_replies":true,"parent_id":"t1_n20q7d5","score":4,"author_fullname":"t2_169jzqdxe5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"for llama.cpp i don\'t know i\'ll try to look at this (if it\'s not fix yet?)  \\nFor the context we claim to have a 128k context length, 256k was our first target but it falls a bit short with only 30% on ruler (better than qwen3, worst than llama3). If you want to use it for 64k+ you need to change the rope\\\\_scaling to yarn, just updated the model card to explain how to do this, thanks a lot for the feedback!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n22f8y2","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;for llama.cpp i don&amp;#39;t know i&amp;#39;ll try to look at this (if it&amp;#39;s not fix yet?)&lt;br/&gt;\\nFor the context we claim to have a 128k context length, 256k was our first target but it falls a bit short with only 30% on ruler (better than qwen3, worst than llama3). If you want to use it for 64k+ you need to change the rope_scaling to yarn, just updated the model card to explain how to do this, thanks a lot for the feedback!&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lusr7l","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lusr7l/smollm3_reasoning_long_context_and/n22f8y2/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752012225,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n22czux","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"eliebakk","can_mod_post":false,"send_replies":true,"parent_id":"t1_n21khzw","score":2,"author_fullname":"t2_169jzqdxe5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yeah we use ruler! and have eval for 32/64/128k (eval for 256k were around 30% which is not great but better than qwen3)   \\nWe also have ideas on how to improve it! :)","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n22czux","is_submitter":true,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yeah we use ruler! and have eval for 32/64/128k (eval for 256k were around 30% which is not great but better than qwen3)&lt;br/&gt;\\nWe also have ideas on how to improve it! :)&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lusr7l","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lusr7l/smollm3_reasoning_long_context_and/n22czux/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752011555,"author_flair_text":null,"treatment_tags":[],"created_utc":1752011555,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n21khzw","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"hak8or","can_mod_post":false,"created_utc":1752003629,"send_replies":true,"parent_id":"t1_n20q7d5","score":2,"author_fullname":"t2_95p4l","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I really hope we get a proper context size benchmark number for this, like the ruler test or the fiction test.\\n\\nEdit: an they actually included a ruler benchmark nice! Though would love to see how it deteriorates by context window size.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n21khzw","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I really hope we get a proper context size benchmark number for this, like the ruler test or the fiction test.&lt;/p&gt;\\n\\n&lt;p&gt;Edit: an they actually included a ruler benchmark nice! Though would love to see how it deteriorates by context window size.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lusr7l","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lusr7l/smollm3_reasoning_long_context_and/n21khzw/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752003629,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n20q7d5","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Chromix_","can_mod_post":false,"created_utc":1751995390,"send_replies":true,"parent_id":"t3_1lusr7l","score":10,"author_fullname":"t2_k7w2h","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Context size clarification: The blog mentions \\"extend the context to 256k tokens\\". Yet also \\"handle up to 128k context (2x extension beyond the 64k training length)\\". The model config itself is set to 64k. This is probably for getting higher-quality results up to 64k, with the possibility to use YaRN manually to extend to 128k and 256k when needed?\\n\\nWhen running with the latest llama.cpp I get this template error when loading the provided GGUF model. Apparently it doesn\'t like being loaded without tools:\\n\\n&gt;common\\\\_chat\\\\_templates\\\\_init: failed to parse chat template (defaulting to chatml): Empty index in subscript at row 49, column 34\\n\\n&gt;{%- set ns = namespace(xml\\\\_tool\\\\_string=\\"You may call one or more functions to assist with the user query.\\\\\\\\nYou are provided with function signatures within &lt;tools&gt;&lt;/tools&gt; XML tags:\\\\\\\\n\\\\\\\\n&lt;tools&gt;\\\\\\\\n\\") -%}  \\n{%- for tool in xml\\\\_tools\\\\[:\\\\] -%} {# The slicing makes sure that xml\\\\_tools is a list #}  \\n\\\\^\\n\\nIt then switches to the default template which is probably not optimal for getting good results.","edited":1751996656,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n20q7d5","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Context size clarification: The blog mentions &amp;quot;extend the context to 256k tokens&amp;quot;. Yet also &amp;quot;handle up to 128k context (2x extension beyond the 64k training length)&amp;quot;. The model config itself is set to 64k. This is probably for getting higher-quality results up to 64k, with the possibility to use YaRN manually to extend to 128k and 256k when needed?&lt;/p&gt;\\n\\n&lt;p&gt;When running with the latest llama.cpp I get this template error when loading the provided GGUF model. Apparently it doesn&amp;#39;t like being loaded without tools:&lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;common_chat_templates_init: failed to parse chat template (defaulting to chatml): Empty index in subscript at row 49, column 34&lt;/p&gt;\\n\\n&lt;p&gt;{%- set ns = namespace(xml_tool_string=&amp;quot;You may call one or more functions to assist with the user query.\\\\nYou are provided with function signatures within &amp;lt;tools&amp;gt;&amp;lt;/tools&amp;gt; XML tags:\\\\n\\\\n&amp;lt;tools&amp;gt;\\\\n&amp;quot;) -%}&lt;br/&gt;\\n{%- for tool in xml_tools[:] -%} {# The slicing makes sure that xml_tools is a list #}&lt;br/&gt;\\n^&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;It then switches to the default template which is probably not optimal for getting good results.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lusr7l/smollm3_reasoning_long_context_and/n20q7d5/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751995390,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lusr7l","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":10}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n26wusu","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"eliebakk","can_mod_post":false,"send_replies":true,"parent_id":"t1_n264t3z","score":2,"author_fullname":"t2_169jzqdxe5","approved_by":null,"mod_note":null,"all_awardings":[],"body":"yes, we\'re looking at it the non thinking mode is broken right now, i\'ve been tell you can switch chat template with --chat-template-file, so one solution i see is to copy paste the current chat template and set set enable\\\\_thinking from true to false  \\n  \\n\\\\`\\\\`\\\\`  \\n\\\\# ───── defaults ───── #}\\n\\n{%- if enable\\\\_thinking is not defined -%}\\n\\n{%- set enable\\\\_thinking = true -%}\\n\\n{%- endif -%}  \\n\\\\`\\\\`\\\\`","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n26wusu","is_submitter":true,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;yes, we&amp;#39;re looking at it the non thinking mode is broken right now, i&amp;#39;ve been tell you can switch chat template with --chat-template-file, so one solution i see is to copy paste the current chat template and set set enable_thinking from true to false  &lt;/p&gt;\\n\\n&lt;p&gt;```&lt;br/&gt;\\n# ───── defaults ───── #}&lt;/p&gt;\\n\\n&lt;p&gt;{%- if enable_thinking is not defined -%}&lt;/p&gt;\\n\\n&lt;p&gt;{%- set enable_thinking = true -%}&lt;/p&gt;\\n\\n&lt;p&gt;{%- endif -%}&lt;br/&gt;\\n```&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lusr7l","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lusr7l/smollm3_reasoning_long_context_and/n26wusu/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752075011,"author_flair_text":null,"treatment_tags":[],"created_utc":1752075011,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n264t3z","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Chromix_","can_mod_post":false,"send_replies":true,"parent_id":"t1_n25bl9k","score":3,"author_fullname":"t2_k7w2h","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"By the way, the model apparently only does thinking, well or handle thinking properly, when passing --jinja as documented. Without it even putting /think into the system prompt doesn\'t have any effect. Manually reproducing what the prompt template would do, and adding that lengthy text to the system prompt works though.","edited":false,"author_flair_css_class":null,"name":"t1_n264t3z","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;By the way, the model apparently only does thinking, well or handle thinking properly, when passing --jinja as documented. Without it even putting /think into the system prompt doesn&amp;#39;t have any effect. Manually reproducing what the prompt template would do, and adding that lengthy text to the system prompt works though.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lusr7l","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lusr7l/smollm3_reasoning_long_context_and/n264t3z/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752066838,"author_flair_text":null,"collapsed":false,"created_utc":1752066838,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n28zfze","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ArcaneThoughts","can_mod_post":false,"send_replies":true,"parent_id":"t1_n27uord","score":2,"author_fullname":"t2_hgivzvub","approved_by":null,"mod_note":null,"all_awardings":[],"body":"Assigning the correct answer to a given question, having a QnA with many questions and answers to pick from.","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n28zfze","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Assigning the correct answer to a given question, having a QnA with many questions and answers to pick from.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lusr7l","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lusr7l/smollm3_reasoning_long_context_and/n28zfze/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752095429,"author_flair_text":null,"treatment_tags":[],"created_utc":1752095429,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n28zjsg","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ArcaneThoughts","can_mod_post":false,"send_replies":true,"parent_id":"t1_n27uord","score":2,"author_fullname":"t2_hgivzvub","approved_by":null,"mod_note":null,"all_awardings":[],"body":"It got better but still not as good as qwen3 1.7b","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n28zjsg","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It got better but still not as good as qwen3 1.7b&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lusr7l","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lusr7l/smollm3_reasoning_long_context_and/n28zjsg/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752095459,"author_flair_text":null,"treatment_tags":[],"created_utc":1752095459,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n27uord","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Sadmanray","can_mod_post":false,"send_replies":true,"parent_id":"t1_n25bl9k","score":3,"author_fullname":"t2_u1xfp","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Let us know if it got better! Just curious if you could describe the use case in generic terms.","edited":false,"author_flair_css_class":null,"name":"t1_n27uord","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Let us know if it got better! Just curious if you could describe the use case in generic terms.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lusr7l","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lusr7l/smollm3_reasoning_long_context_and/n27uord/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752084168,"author_flair_text":null,"collapsed":false,"created_utc":1752084168,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n25bl9k","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"ArcaneThoughts","can_mod_post":false,"send_replies":true,"parent_id":"t1_n24y2vn","score":4,"author_fullname":"t2_hgivzvub","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"That\'s great to know, will try it again, thank you!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n25bl9k","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;That&amp;#39;s great to know, will try it again, thank you!&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lusr7l","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lusr7l/smollm3_reasoning_long_context_and/n25bl9k/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752055156,"author_flair_text":null,"treatment_tags":[],"created_utc":1752055156,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}}],"before":null}},"user_reports":[],"saved":false,"id":"n24y2vn","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Chromix_","can_mod_post":false,"send_replies":true,"parent_id":"t1_n20wbfm","score":7,"author_fullname":"t2_k7w2h","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Your results were probably impacted by the [broken chat template](https://www.reddit.com/r/LocalLLaMA/comments/1lusr7l/comment/n24xvpf/). You\'ll need updated GGUFs, or apply a tiny binary edit to the one you already downloaded.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n24y2vn","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Your results were probably impacted by the &lt;a href=\\"https://www.reddit.com/r/LocalLLaMA/comments/1lusr7l/comment/n24xvpf/\\"&gt;broken chat template&lt;/a&gt;. You&amp;#39;ll need updated GGUFs, or apply a tiny binary edit to the one you already downloaded.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lusr7l","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lusr7l/smollm3_reasoning_long_context_and/n24y2vn/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752047539,"author_flair_text":null,"treatment_tags":[],"created_utc":1752047539,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":7}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2a8g4k","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"eliebakk","can_mod_post":false,"created_utc":1752109940,"send_replies":true,"parent_id":"t1_n28z0ke","score":1,"author_fullname":"t2_169jzqdxe5","approved_by":null,"mod_note":null,"all_awardings":[],"body":"Are you using llama.cpp? If so i recommend this fix that should work [https://www.reddit.com/r/LocalLLaMA/comments/1lusr7l/comment/n26wusu/?utm\\\\_source=share&amp;utm\\\\_medium=web3x&amp;utm\\\\_name=web3xcss&amp;utm\\\\_term=1&amp;utm\\\\_content=share\\\\_button](https://www.reddit.com/r/LocalLLaMA/comments/1lusr7l/comment/n26wusu/?utm_source=share&amp;utm_medium=web3x&amp;utm_name=web3xcss&amp;utm_term=1&amp;utm_content=share_button) (in the one you copy paste the enable\\\\_thinking is still true so it will default to the thinking mode). Also make sure to run with the \\\\`--jinja\\\\` flag.  \\nSorry for the inconvenience :(","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n2a8g4k","is_submitter":true,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Are you using llama.cpp? If so i recommend this fix that should work &lt;a href=\\"https://www.reddit.com/r/LocalLLaMA/comments/1lusr7l/comment/n26wusu/?utm_source=share&amp;amp;utm_medium=web3x&amp;amp;utm_name=web3xcss&amp;amp;utm_term=1&amp;amp;utm_content=share_button\\"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1lusr7l/comment/n26wusu/?utm_source=share&amp;amp;utm_medium=web3x&amp;amp;utm_name=web3xcss&amp;amp;utm_term=1&amp;amp;utm_content=share_button&lt;/a&gt; (in the one you copy paste the enable_thinking is still true so it will default to the thinking mode). Also make sure to run with the `--jinja` flag.&lt;br/&gt;\\nSorry for the inconvenience :(&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lusr7l/smollm3_reasoning_long_context_and/n2a8g4k/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752109940,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lusr7l","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":8,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n28z0ke","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ArcaneThoughts","can_mod_post":false,"created_utc":1752095310,"send_replies":true,"parent_id":"t1_n26wan5","score":2,"author_fullname":"t2_hgivzvub","approved_by":null,"mod_note":null,"all_awardings":[],"body":"My evaluation doesn\'t always correlate with benchmark results, but I am somewhat surprised by the bad results. I did try the new model, got quite better results but still not better that Qwen3 1.7b (it gets 60% now).\\n\\nCan you easily tell if this is the correct template? I don\'t use thinking mode by the way.\\n\\n`{# ───── defaults ───── #}`\\n\\n`{%- if enable_thinking is not defined -%}`\\n\\n`{%- set enable_thinking = true -%}`\\n\\n`{%- endif -%}`\\n\\n\\n\\n`{# ───── reasoning mode ───── #}`\\n\\n`{%- if enable_thinking -%}`\\n\\n  `{%- set reasoning_mode = \\"/think\\" -%}`\\n\\n`{%- else -%}`\\n\\n  `{%- set reasoning_mode = \\"/no_think\\" -%}`\\n\\n`{%- endif -%}`...","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n28z0ke","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;My evaluation doesn&amp;#39;t always correlate with benchmark results, but I am somewhat surprised by the bad results. I did try the new model, got quite better results but still not better that Qwen3 1.7b (it gets 60% now).&lt;/p&gt;\\n\\n&lt;p&gt;Can you easily tell if this is the correct template? I don&amp;#39;t use thinking mode by the way.&lt;/p&gt;\\n\\n&lt;p&gt;&lt;code&gt;{# ───── defaults ───── #}&lt;/code&gt;&lt;/p&gt;\\n\\n&lt;p&gt;&lt;code&gt;{%- if enable_thinking is not defined -%}&lt;/code&gt;&lt;/p&gt;\\n\\n&lt;p&gt;&lt;code&gt;{%- set enable_thinking = true -%}&lt;/code&gt;&lt;/p&gt;\\n\\n&lt;p&gt;&lt;code&gt;{%- endif -%}&lt;/code&gt;&lt;/p&gt;\\n\\n&lt;p&gt;&lt;code&gt;{# ───── reasoning mode ───── #}&lt;/code&gt;&lt;/p&gt;\\n\\n&lt;p&gt;&lt;code&gt;{%- if enable_thinking -%}&lt;/code&gt;&lt;/p&gt;\\n\\n&lt;p&gt;&lt;code&gt;{%- set reasoning_mode = &amp;quot;/think&amp;quot; -%}&lt;/code&gt;&lt;/p&gt;\\n\\n&lt;p&gt;&lt;code&gt;{%- else -%}&lt;/code&gt;&lt;/p&gt;\\n\\n&lt;p&gt;&lt;code&gt;{%- set reasoning_mode = &amp;quot;/no_think&amp;quot; -%}&lt;/code&gt;&lt;/p&gt;\\n\\n&lt;p&gt;&lt;code&gt;{%- endif -%}&lt;/code&gt;...&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lusr7l","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lusr7l/smollm3_reasoning_long_context_and/n28z0ke/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752095310,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":7,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n26wan5","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"eliebakk","can_mod_post":false,"send_replies":true,"parent_id":"t1_n22zeuf","score":2,"author_fullname":"t2_169jzqdxe5","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Was curious because the model is performing better than the model ou mention (except qwen3) overall. As mention by u/Chromix_ they was a bug in the chat template on the gguf so should be better, lmk when you rerun it 🙏","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n26wan5","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Was curious because the model is performing better than the model ou mention (except qwen3) overall. As mention by &lt;a href=\\"/u/Chromix_\\"&gt;u/Chromix_&lt;/a&gt; they was a bug in the chat template on the gguf so should be better, lmk when you rerun it 🙏&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lusr7l","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lusr7l/smollm3_reasoning_long_context_and/n26wan5/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752074857,"author_flair_text":null,"treatment_tags":[],"created_utc":1752074857,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":6,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n22zeuf","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"ArcaneThoughts","can_mod_post":false,"send_replies":true,"parent_id":"t1_n22xkxo","score":10,"author_fullname":"t2_hgivzvub","approved_by":null,"mod_note":null,"all_awardings":[],"body":"Of course, smollm3 gets 60% (results updated with latest ggufs as of 7/9/25), qwen3-1.7b 85%, qwen3-4b 96%, gemma3-4b 81%, granite 3.2-2b 79%\\n\\nI used the 8 bit quantization for smollm3 (I used similar quantization for the others, usually q5 or q4).\\n\\nDo you suspect there may be an issue with the quantization? Have you received other reports?","edited":1752057175,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n22zeuf","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Of course, smollm3 gets 60% (results updated with latest ggufs as of 7/9/25), qwen3-1.7b 85%, qwen3-4b 96%, gemma3-4b 81%, granite 3.2-2b 79%&lt;/p&gt;\\n\\n&lt;p&gt;I used the 8 bit quantization for smollm3 (I used similar quantization for the others, usually q5 or q4).&lt;/p&gt;\\n\\n&lt;p&gt;Do you suspect there may be an issue with the quantization? Have you received other reports?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lusr7l","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lusr7l/smollm3_reasoning_long_context_and/n22zeuf/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752018604,"author_flair_text":null,"treatment_tags":[],"created_utc":1752018604,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":10}}],"before":null}},"user_reports":[],"saved":false,"id":"n22xkxo","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"eliebakk","can_mod_post":false,"send_replies":true,"parent_id":"t1_n22slep","score":5,"author_fullname":"t2_169jzqdxe5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"mind sharing smollm3 number compare to qwen3-1.7b (and other small models if you have)? i\'m surprise it\'s better","edited":false,"author_flair_css_class":null,"name":"t1_n22xkxo","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;mind sharing smollm3 number compare to qwen3-1.7b (and other small models if you have)? i&amp;#39;m surprise it&amp;#39;s better&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lusr7l","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lusr7l/smollm3_reasoning_long_context_and/n22xkxo/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752018012,"author_flair_text":null,"collapsed":false,"created_utc":1752018012,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}}],"before":null}},"user_reports":[],"saved":false,"id":"n22slep","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"ArcaneThoughts","can_mod_post":false,"send_replies":true,"parent_id":"t1_n22cqkc","score":6,"author_fullname":"t2_hgivzvub","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I have a dataset of text classification tasks that I use to test models. It\'s relatively easy, gemma2 9b aces it 100%","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n22slep","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I have a dataset of text classification tasks that I use to test models. It&amp;#39;s relatively easy, gemma2 9b aces it 100%&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lusr7l","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lusr7l/smollm3_reasoning_long_context_and/n22slep/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752016422,"author_flair_text":null,"treatment_tags":[],"created_utc":1752016422,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}}],"before":null}},"user_reports":[],"saved":false,"id":"n22cqkc","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"eliebakk","can_mod_post":false,"send_replies":true,"parent_id":"t1_n20wbfm","score":12,"author_fullname":"t2_169jzqdxe5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"i\'m curious what is the use case?","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n22cqkc","is_submitter":true,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;i&amp;#39;m curious what is the use case?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lusr7l","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lusr7l/smollm3_reasoning_long_context_and/n22cqkc/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752011479,"author_flair_text":null,"treatment_tags":[],"created_utc":1752011479,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":12}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n217k4e","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"IrisColt","can_mod_post":false,"send_replies":true,"parent_id":"t1_n20wbfm","score":5,"author_fullname":"t2_c2f558x","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Thanks!","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n217k4e","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thanks!&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lusr7l","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lusr7l/smollm3_reasoning_long_context_and/n217k4e/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752000075,"author_flair_text":null,"treatment_tags":[],"created_utc":1752000075,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}}],"before":null}},"user_reports":[],"saved":false,"id":"n20wbfm","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"ArcaneThoughts","can_mod_post":false,"created_utc":1751997064,"send_replies":true,"parent_id":"t1_n20cl6m","score":24,"author_fullname":"t2_hgivzvub","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Loses to Qwen3 1.7b for my use case if anyone was wondering.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n20wbfm","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Loses to Qwen3 1.7b for my use case if anyone was wondering.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lusr7l","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lusr7l/smollm3_reasoning_long_context_and/n20wbfm/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751997064,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":24}}],"before":null}},"user_reports":[],"saved":false,"id":"n20cl6m","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"ArcaneThoughts","can_mod_post":false,"created_utc":1751991606,"send_replies":true,"parent_id":"t3_1lusr7l","score":17,"author_fullname":"t2_hgivzvub","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Nice size! Will test it for my use cases once the ggufs are out.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n20cl6m","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Nice size! Will test it for my use cases once the ggufs are out.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lusr7l/smollm3_reasoning_long_context_and/n20cl6m/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751991606,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lusr7l","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":17}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n21bwbd","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"jamaalwakamaal","can_mod_post":false,"created_utc":1752001246,"send_replies":true,"parent_id":"t3_1lusr7l","score":6,"author_fullname":"t2_alyeos2m","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"W SmolLM","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n21bwbd","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;W SmolLM&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lusr7l/smollm3_reasoning_long_context_and/n21bwbd/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752001246,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lusr7l","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n22xnsn","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"eliebakk","can_mod_post":false,"created_utc":1752018038,"send_replies":true,"parent_id":"t1_n22pct1","score":7,"author_fullname":"t2_169jzqdxe5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"here: [https://huggingface.co/datasets/HuggingFaceTB/smollm3-blueprint](https://huggingface.co/datasets/HuggingFaceTB/smollm3-blueprint)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n22xnsn","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;here: &lt;a href=\\"https://huggingface.co/datasets/HuggingFaceTB/smollm3-blueprint\\"&gt;https://huggingface.co/datasets/HuggingFaceTB/smollm3-blueprint&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lusr7l","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lusr7l/smollm3_reasoning_long_context_and/n22xnsn/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752018038,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":7}}],"before":null}},"user_reports":[],"saved":false,"id":"n22pct1","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"celsowm","can_mod_post":false,"created_utc":1752015393,"send_replies":true,"parent_id":"t3_1lusr7l","score":4,"author_fullname":"t2_dyvrh","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"High res of this image?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n22pct1","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;High res of this image?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lusr7l/smollm3_reasoning_long_context_and/n22pct1/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752015393,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lusr7l","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n218ezt","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"GabryIta","can_mod_post":false,"created_utc":1752000306,"send_replies":true,"parent_id":"t3_1lusr7l","score":7,"author_fullname":"t2_pv1nb9469","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"The benchmarks don\'t seem very exciting... :(","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n218ezt","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The benchmarks don&amp;#39;t seem very exciting... :(&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lusr7l/smollm3_reasoning_long_context_and/n218ezt/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752000306,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lusr7l","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":7}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n21k13p","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"CalypsoTheKitty","can_mod_post":false,"created_utc":1752003503,"send_replies":true,"parent_id":"t3_1lusr7l","score":3,"author_fullname":"t2_qbkt3","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"That\'s a beautiful Blueprint!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n21k13p","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;That&amp;#39;s a beautiful Blueprint!&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lusr7l/smollm3_reasoning_long_context_and/n21k13p/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752003503,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lusr7l","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n22stah","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"thebadslime","can_mod_post":false,"created_utc":1752016492,"send_replies":true,"parent_id":"t3_1lusr7l","score":3,"author_fullname":"t2_i5os0v0","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Super interesting!\\n\\nI am also making a 3B with a phased curriculum, but I\'m sorting my data by grade levels and progressively ramping up. I am also redducing language over time to add code, but a little more planned.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n22stah","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Super interesting!&lt;/p&gt;\\n\\n&lt;p&gt;I am also making a 3B with a phased curriculum, but I&amp;#39;m sorting my data by grade levels and progressively ramping up. I am also redducing language over time to add code, but a little more planned.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lusr7l/smollm3_reasoning_long_context_and/n22stah/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752016492,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lusr7l","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n22ou27","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"simracerman","can_mod_post":false,"created_utc":1752015230,"send_replies":true,"parent_id":"t3_1lusr7l","score":1,"author_fullname":"t2_vbzgnic","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"How does it compare to Cogito 3B. Curious if you did that comparison, since it’s based on Llama3.2-3B and supports reasoning too.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n22ou27","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;How does it compare to Cogito 3B. Curious if you did that comparison, since it’s based on Llama3.2-3B and supports reasoning too.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lusr7l/smollm3_reasoning_long_context_and/n22ou27/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752015230,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lusr7l","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n23fvu4","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Daemontatox","can_mod_post":false,"created_utc":1752024157,"send_replies":true,"parent_id":"t3_1lusr7l","score":1,"author_fullname":"t2_1rm9syq1nb","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Time to get Fine-Tuning.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n23fvu4","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Time to get Fine-Tuning.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lusr7l/smollm3_reasoning_long_context_and/n23fvu4/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752024157,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lusr7l","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n28ce9l","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"lavilao","can_mod_post":false,"created_utc":1752089090,"send_replies":true,"parent_id":"t3_1lusr7l","score":1,"author_fullname":"t2_8r6zinl9","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Are there plans for models under 1 billion parameters, similar to SmollLM2?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n28ce9l","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Are there plans for models under 1 billion parameters, similar to SmollLM2?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lusr7l/smollm3_reasoning_long_context_and/n28ce9l/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752089090,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lusr7l","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2awudd","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Quagmirable","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2ahqkg","score":1,"author_fullname":"t2_17i8f5bprh","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Ah yes, that could be the case.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2awudd","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Ah yes, that could be the case.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lusr7l","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lusr7l/smollm3_reasoning_long_context_and/n2awudd/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752118904,"author_flair_text":null,"treatment_tags":[],"created_utc":1752118904,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2ahqkg","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"redditrasberry","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2689uq","score":1,"author_fullname":"t2_2nzkn","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"unfortunately it ends with \\n\\n    Error: unable to load model: ....\\n\\nAssuming we need to wait for ollama to update it\'s llama.cpp implementation","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2ahqkg","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;unfortunately it ends with &lt;/p&gt;\\n\\n&lt;pre&gt;&lt;code&gt;Error: unable to load model: ....\\n&lt;/code&gt;&lt;/pre&gt;\\n\\n&lt;p&gt;Assuming we need to wait for ollama to update it&amp;#39;s llama.cpp implementation&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lusr7l","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lusr7l/smollm3_reasoning_long_context_and/n2ahqkg/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752113186,"author_flair_text":null,"treatment_tags":[],"created_utc":1752113186,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2689uq","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Quagmirable","can_mod_post":false,"created_utc":1752067950,"send_replies":true,"parent_id":"t1_n24t8af","score":2,"author_fullname":"t2_17i8f5bprh","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"You can download models directly from HuggingFace with Ollama:\\n\\nhttps://huggingface.co/docs/hub/en/ollama","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2689uq","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You can download models directly from HuggingFace with Ollama:&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://huggingface.co/docs/hub/en/ollama\\"&gt;https://huggingface.co/docs/hub/en/ollama&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lusr7l","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lusr7l/smollm3_reasoning_long_context_and/n2689uq/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752067950,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n27l7di","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Maleficent_Day682","can_mod_post":false,"created_utc":1752081633,"send_replies":true,"parent_id":"t1_n24t8af","score":1,"author_fullname":"t2_1ovg7v0fpz","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"how far behind is ollama implementaion of llama.cpp ? I think we are better waiting for a merge","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n27l7di","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;how far behind is ollama implementaion of llama.cpp ? I think we are better waiting for a merge&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lusr7l","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lusr7l/smollm3_reasoning_long_context_and/n27l7di/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752081633,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n24t8af","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"outofbandii","can_mod_post":false,"created_utc":1752044804,"send_replies":true,"parent_id":"t3_1lusr7l","score":0,"author_fullname":"t2_39mp2","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Can you put this on ollama? Looking forward to testing it out!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n24t8af","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Can you put this on ollama? Looking forward to testing it out!&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lusr7l/smollm3_reasoning_long_context_and/n24t8af/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752044804,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lusr7l","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}}]'),o=()=>l.jsx(e,{data:t});export{o as default};
