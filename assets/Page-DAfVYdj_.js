import{j as e}from"./index-DLSqWzaI.js";import{R as t}from"./RedditPostRenderer-CysRo2D_.js";import"./index-COXiL3Lo.js";const l=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"I completed my local LLM rig in May, just after Qwen3's release (thanks to r/LocalLLaMA 's folks for the invaluable guidance!). Now that I've settled into the setup, I'm excited to share my build and how it's performing with local LLMs.\\n\\nhttps://preview.redd.it/wiim1ouai7ef1.jpg?width=1280&amp;format=pjpg&amp;auto=webp&amp;s=f933a9957fdbd5dae4472662c2381d7d68d39bbf\\n\\n  \\n\\n\\nThis is a consumer-grade rig optimized for running Qwen3-30B-A3B and similar models via llama.cpp. Let's dive in!\\n\\n# Key Specs\\n\\n|Component|Specs|\\n|:-|:-|\\n|**CPU**|AMD Ryzen 7 7700 (8C/16T)|\\n|**GPU**|2 x NVIDIA RTX 3090 (48GB VRAM total)|\\n|**RAM**|64GB DDR5 @ 6400 MHz|\\n|**Storage**|2TB NVMe + 3 x 8TB WD Purple (ZFS mirror)|\\n|**Motherboard**|ASUS TUF B650-PLUS|\\n|**PSU**|850W ADATA XPG CORE REACTOR II (undervolted to 200W per GPU)|\\n|**Case**|Lian Li LANCOOL 216|\\n|**Cooling**|a lot of fans 💨|\\n\\nTried to run the following:\\n\\n* **30B-A3B Q4\\\\_K\\\\_XL**, **32B Q4\\\\_K\\\\_XL** – fit into one GPU with ample context window\\n* **32B Q8\\\\_K\\\\_XL** – runs well on 2 GPUs, not significantly smarter than A3B for my tasks, but slower in inference\\n* **30B-A3B Q8\\\\_K\\\\_XL** – now runs on dual GPUs. The same model also runs on CPU only, mostly for background tasks (to preserve the main model's context. However, this approach is slightly inefficient, as it requires storing model weights in both VRAM and system RAM. I haven’t found an optimal way to store weights once and manage contexts separately, so this remains a WiP).\\n\\nPrimary use: Running Qwen3-30B-A3B models with \`llama.cpp\`. The performance for this model is \\\\~ 1000 pp512 / 100 tg128\\n\\nWhat's next? I think I will play with this one for a while. But... I'm already eyeing an EPYC-based system with 4x 4090s (48GB each). 😎","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"My (practical) dual 3090 setup for local inference","link_flair_richtext":[{"e":"text","t":"Discussion"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":105,"top_awarded_type":null,"hide_score":false,"media_metadata":{"wiim1ouai7ef1":{"status":"valid","e":"Image","m":"image/jpg","p":[{"y":81,"x":108,"u":"https://preview.redd.it/wiim1ouai7ef1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b4174de20838a1b6b769288eb5b1b5f70f0f9602"},{"y":162,"x":216,"u":"https://preview.redd.it/wiim1ouai7ef1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c538aa32a3f813976d5ce771e34c0d48b0440eaa"},{"y":240,"x":320,"u":"https://preview.redd.it/wiim1ouai7ef1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=122f1edd42a0419bd780aa64f40f29f91c0ed509"},{"y":480,"x":640,"u":"https://preview.redd.it/wiim1ouai7ef1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=41b8c09a47c3c8926c73db4b4303c394b1b45f6f"},{"y":720,"x":960,"u":"https://preview.redd.it/wiim1ouai7ef1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=1eb4a1cb38cbbf6d104e970f4b00623ccd4033db"},{"y":810,"x":1080,"u":"https://preview.redd.it/wiim1ouai7ef1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=720ffaef64226dedbcf3e6ecd6efd90763b1edcf"}],"s":{"y":961,"x":1280,"u":"https://preview.redd.it/wiim1ouai7ef1.jpg?width=1280&amp;format=pjpg&amp;auto=webp&amp;s=f933a9957fdbd5dae4472662c2381d7d68d39bbf"},"id":"wiim1ouai7ef1"}},"name":"t3_1m5fkts","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.74,"author_flair_background_color":null,"subreddit_type":"public","ups":7,"total_awards_received":0,"media_embed":{},"thumbnail_width":140,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_1nkpqiujlm","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Discussion","can_mod_post":false,"score":7,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"https://a.thumbs.redditmedia.com/vvE88N6ubX7Gsux9A2O-Hn5WBZJNPCIpKoUwojUDik4.jpg","edited":1753095094,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1753094600,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;I completed my local LLM rig in May, just after Qwen3&amp;#39;s release (thanks to &lt;a href=\\"/r/LocalLLaMA\\"&gt;r/LocalLLaMA&lt;/a&gt; &amp;#39;s folks for the invaluable guidance!). Now that I&amp;#39;ve settled into the setup, I&amp;#39;m excited to share my build and how it&amp;#39;s performing with local LLMs.&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://preview.redd.it/wiim1ouai7ef1.jpg?width=1280&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=f933a9957fdbd5dae4472662c2381d7d68d39bbf\\"&gt;https://preview.redd.it/wiim1ouai7ef1.jpg?width=1280&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=f933a9957fdbd5dae4472662c2381d7d68d39bbf&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;This is a consumer-grade rig optimized for running Qwen3-30B-A3B and similar models via llama.cpp. Let&amp;#39;s dive in!&lt;/p&gt;\\n\\n&lt;h1&gt;Key Specs&lt;/h1&gt;\\n\\n&lt;table&gt;&lt;thead&gt;\\n&lt;tr&gt;\\n&lt;th align=\\"left\\"&gt;Component&lt;/th&gt;\\n&lt;th align=\\"left\\"&gt;Specs&lt;/th&gt;\\n&lt;/tr&gt;\\n&lt;/thead&gt;&lt;tbody&gt;\\n&lt;tr&gt;\\n&lt;td align=\\"left\\"&gt;&lt;strong&gt;CPU&lt;/strong&gt;&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;AMD Ryzen 7 7700 (8C/16T)&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;tr&gt;\\n&lt;td align=\\"left\\"&gt;&lt;strong&gt;GPU&lt;/strong&gt;&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;2 x NVIDIA RTX 3090 (48GB VRAM total)&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;tr&gt;\\n&lt;td align=\\"left\\"&gt;&lt;strong&gt;RAM&lt;/strong&gt;&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;64GB DDR5 @ 6400 MHz&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;tr&gt;\\n&lt;td align=\\"left\\"&gt;&lt;strong&gt;Storage&lt;/strong&gt;&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;2TB NVMe + 3 x 8TB WD Purple (ZFS mirror)&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;tr&gt;\\n&lt;td align=\\"left\\"&gt;&lt;strong&gt;Motherboard&lt;/strong&gt;&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;ASUS TUF B650-PLUS&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;tr&gt;\\n&lt;td align=\\"left\\"&gt;&lt;strong&gt;PSU&lt;/strong&gt;&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;850W ADATA XPG CORE REACTOR II (undervolted to 200W per GPU)&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;tr&gt;\\n&lt;td align=\\"left\\"&gt;&lt;strong&gt;Case&lt;/strong&gt;&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;Lian Li LANCOOL 216&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;tr&gt;\\n&lt;td align=\\"left\\"&gt;&lt;strong&gt;Cooling&lt;/strong&gt;&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;a lot of fans 💨&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;/tbody&gt;&lt;/table&gt;\\n\\n&lt;p&gt;Tried to run the following:&lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;&lt;strong&gt;30B-A3B Q4_K_XL&lt;/strong&gt;, &lt;strong&gt;32B Q4_K_XL&lt;/strong&gt; – fit into one GPU with ample context window&lt;/li&gt;\\n&lt;li&gt;&lt;strong&gt;32B Q8_K_XL&lt;/strong&gt; – runs well on 2 GPUs, not significantly smarter than A3B for my tasks, but slower in inference&lt;/li&gt;\\n&lt;li&gt;&lt;strong&gt;30B-A3B Q8_K_XL&lt;/strong&gt; – now runs on dual GPUs. The same model also runs on CPU only, mostly for background tasks (to preserve the main model&amp;#39;s context. However, this approach is slightly inefficient, as it requires storing model weights in both VRAM and system RAM. I haven’t found an optimal way to store weights once and manage contexts separately, so this remains a WiP).&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;p&gt;Primary use: Running Qwen3-30B-A3B models with &lt;code&gt;llama.cpp&lt;/code&gt;. The performance for this model is ~ 1000 pp512 / 100 tg128&lt;/p&gt;\\n\\n&lt;p&gt;What&amp;#39;s next? I think I will play with this one for a while. But... I&amp;#39;m already eyeing an EPYC-based system with 4x 4090s (48GB each). 😎&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":true,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"5f921ea4-c7bc-11ed-9c23-3a00622979b4","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#646d73","id":"1m5fkts","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"ColdImplement1319","discussion_type":null,"num_comments":13,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1m5fkts/my_practical_dual_3090_setup_for_local_inference/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1m5fkts/my_practical_dual_3090_setup_for_local_inference/","subreddit_subscribers":502721,"created_utc":1753094600,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4ez9la","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"dinerburgeryum","can_mod_post":false,"created_utc":1753133596,"send_replies":true,"parent_id":"t1_n4esdfe","score":1,"author_fullname":"t2_1j53p3yv3e","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"The PR seems to indicate it’s more of a kludge than a feature. https://github.com/ggml-org/llama.cpp/pull/14425#issuecomment-3016149085","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4ez9la","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The PR seems to indicate it’s more of a kludge than a feature. &lt;a href=\\"https://github.com/ggml-org/llama.cpp/pull/14425#issuecomment-3016149085\\"&gt;https://github.com/ggml-org/llama.cpp/pull/14425#issuecomment-3016149085&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5fkts/my_practical_dual_3090_setup_for_local_inference/n4ez9la/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753133596,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m5fkts","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":9,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4esdfe","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"jacek2023","can_mod_post":false,"created_utc":1753131528,"send_replies":true,"parent_id":"t1_n4es6xj","score":1,"author_fullname":"t2_vqgbql9w","approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yes","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n4esdfe","is_submitter":false,"collapsed":false,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yes&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5fkts/my_practical_dual_3090_setup_for_local_inference/n4esdfe/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753131528,"author_flair_text":"llama.cpp","treatment_tags":[],"link_id":"t3_1m5fkts","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":8,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4es6xj","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"dinerburgeryum","can_mod_post":false,"created_utc":1753131477,"send_replies":true,"parent_id":"t1_n4ehyfe","score":1,"author_fullname":"t2_1j53p3yv3e","approved_by":null,"mod_note":null,"all_awardings":[],"body":"You’re referring to the custom expert router implementation?","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n4es6xj","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You’re referring to the custom expert router implementation?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1m5fkts","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5fkts/my_practical_dual_3090_setup_for_local_inference/n4es6xj/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753131477,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":7,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4ehyfe","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"jacek2023","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4d8nkl","score":2,"author_fullname":"t2_vqgbql9w","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Hunyuan implementation in llama.cpp is not \\"complete\\", so the output may be not best","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n4ehyfe","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Hunyuan implementation in llama.cpp is not &amp;quot;complete&amp;quot;, so the output may be not best&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1m5fkts","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5fkts/my_practical_dual_3090_setup_for_local_inference/n4ehyfe/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753128560,"author_flair_text":"llama.cpp","treatment_tags":[],"created_utc":1753128560,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":6,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n4d8nkl","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"dinerburgeryum","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4d04kh","score":2,"author_fullname":"t2_1j53p3yv3e","approved_by":null,"mod_note":null,"all_awardings":[],"body":"In my experience Hunyuan isn’t particularly useful for anything. Jamba is excellent for context handling and instruction following but so-so for tool calling. Still looking for a really killer multi-turn tool calling model to be honest. Dots seems to have good “smarts” but it’s a little heavy for local. I’m not a huge fan of test time scaling so I generally disable “thinking” on Qwen. ","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n4d8nkl","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;In my experience Hunyuan isn’t particularly useful for anything. Jamba is excellent for context handling and instruction following but so-so for tool calling. Still looking for a really killer multi-turn tool calling model to be honest. Dots seems to have good “smarts” but it’s a little heavy for local. I’m not a huge fan of test time scaling so I generally disable “thinking” on Qwen. &lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1m5fkts","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5fkts/my_practical_dual_3090_setup_for_local_inference/n4d8nkl/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753115909,"author_flair_text":null,"treatment_tags":[],"created_utc":1753115909,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n4d04kh","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Zc5Gwu","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4c9gpr","score":1,"author_fullname":"t2_67qrvlir","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Would love to hear more thoughts on these models. I messed with Hunyuan a bit but found qwen3 32b to still be better overall (speed vs smartness vs accuracy). The bigger models may have better world knowledge though…\\n\\nDo you have an idea how they fare for “knowledge”, “agentic”, “smartness”?","edited":false,"author_flair_css_class":null,"name":"t1_n4d04kh","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Would love to hear more thoughts on these models. I messed with Hunyuan a bit but found qwen3 32b to still be better overall (speed vs smartness vs accuracy). The bigger models may have better world knowledge though…&lt;/p&gt;\\n\\n&lt;p&gt;Do you have an idea how they fare for “knowledge”, “agentic”, “smartness”?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1m5fkts","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5fkts/my_practical_dual_3090_setup_for_local_inference/n4d04kh/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753113488,"author_flair_text":null,"collapsed":false,"created_utc":1753113488,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4c9gpr","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"dinerburgeryum","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4bk6mg","score":4,"author_fullname":"t2_1j53p3yv3e","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Seconding Jamba. Hunyuan is a real hit-or-miss, but Dots has been reliable for me. Jamba lacks in-built “knowledge” in my experience but is a context handling champ. Give it what it needs and it spits back great results at high speed. ","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4c9gpr","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Seconding Jamba. Hunyuan is a real hit-or-miss, but Dots has been reliable for me. Jamba lacks in-built “knowledge” in my experience but is a context handling champ. Give it what it needs and it spits back great results at high speed. &lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m5fkts","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5fkts/my_practical_dual_3090_setup_for_local_inference/n4c9gpr/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753105597,"author_flair_text":null,"treatment_tags":[],"created_utc":1753105597,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}}],"before":null}},"user_reports":[],"saved":false,"id":"n4bk6mg","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"jacek2023","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4bk0zy","score":3,"author_fullname":"t2_vqgbql9w","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Jamba, Dots, Hunyuan, Llama Scout","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n4bk6mg","is_submitter":false,"collapsed":false,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Jamba, Dots, Hunyuan, Llama Scout&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m5fkts","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5fkts/my_practical_dual_3090_setup_for_local_inference/n4bk6mg/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753096022,"author_flair_text":"llama.cpp","treatment_tags":[],"created_utc":1753096022,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n4bk0zy","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ColdImplement1319","can_mod_post":false,"created_utc":1753095950,"send_replies":true,"parent_id":"t1_n4bjcw3","score":1,"author_fullname":"t2_1nkpqiujlm","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Do you have any recommendations? I'm currently running Qwen3 30B-A3B, which is an MoE model and quite up-to-date.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4bk0zy","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Do you have any recommendations? I&amp;#39;m currently running Qwen3 30B-A3B, which is an MoE model and quite up-to-date.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m5fkts","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5fkts/my_practical_dual_3090_setup_for_local_inference/n4bk0zy/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753095950,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4bjcw3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"jacek2023","can_mod_post":false,"created_utc":1753095632,"send_replies":true,"parent_id":"t3_1m5fkts","score":3,"author_fullname":"t2_vqgbql9w","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Try some modern MoE models","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4bjcw3","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Try some modern MoE models&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5fkts/my_practical_dual_3090_setup_for_local_inference/n4bjcw3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753095632,"author_flair_text":"llama.cpp","treatment_tags":[],"link_id":"t3_1m5fkts","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4bm04e","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"fizzy1242","can_mod_post":false,"created_utc":1753096860,"send_replies":true,"parent_id":"t3_1m5fkts","score":4,"author_fullname":"t2_16zcsx","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"try some 70b models in exl2 format. they're very fast, even with 200W powerlimit.\\n\\n3rd one lets you run 4.0bpw mistral large, wink.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4bm04e","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;try some 70b models in exl2 format. they&amp;#39;re very fast, even with 200W powerlimit.&lt;/p&gt;\\n\\n&lt;p&gt;3rd one lets you run 4.0bpw mistral large, wink.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5fkts/my_practical_dual_3090_setup_for_local_inference/n4bm04e/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753096860,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m5fkts","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4e7703","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ColdImplement1319","can_mod_post":false,"created_utc":1753125479,"send_replies":true,"parent_id":"t1_n4dwase","score":1,"author_fullname":"t2_1nkpqiujlm","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I do it like that (maybe it's not the best solution, but it works) :\\n\\n    setup_nvidia_undervolt() {\\n      sudo tee /usr/local/bin/undervolt-nvidia.sh &gt; /dev/null &lt;&lt;'EOF'\\n    #!/usr/bin/env bash\\n    \\n    nvidia-smi --persistence-mode ENABLED\\n    nvidia-smi --power-limit 200\\n    EOF\\n      sudo chmod +x /usr/local/bin/undervolt-nvidia.sh\\n    \\n      sudo tee /etc/systemd/system/nvidia-undervolt.service &gt; /dev/null &lt;&lt;'EOF'\\n    [Unit]\\n    Description=Apply NVIDIA GPU power limit (undervolt)\\n    Wants=nvidia-persistenced.service\\n    After=nvidia-persistenced.service\\n    \\n    [Service]\\n    Type=oneshot\\n    ExecStart=/usr/local/bin/undervolt-nvidia.sh\\n    RemainAfterExit=yes\\n    \\n    [Install]\\n    WantedBy=multi-user.target\\n    EOF\\n    \\n      sudo systemctl daemon-reload\\n      sudo systemctl enable --now nvidia-undervolt.service\\n    }\\n\\nI know there are other parameters to set - throttling/etc, but I kinda settled on it.\\n\\n    ubuntu@homelab:~$ nvidia-smi \\n    Mon Jul 21 22:20:32 2025       \\n    +-----------------------------------------------------------------------------------------+\\n    | NVIDIA-SMI 570.169                Driver Version: 570.169        CUDA Version: 12.8     |\\n    |-----------------------------------------+------------------------+----------------------+\\n    | GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\\n    | Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\\n    |                                         |                        |               MIG M. |\\n    |=========================================+========================+======================|\\n    |   0  NVIDIA GeForce RTX 3090        On  |   00000000:01:00.0 Off |                  N/A |\\n    |  0%   46C    P8             32W /  200W |   23623MiB /  24576MiB |      0%      Default |\\n    |                                         |                        |                  N/A |\\n    +-----------------------------------------+------------------------+----------------------+\\n    |   1  NVIDIA GeForce RTX 3090        On  |   00000000:05:00.0 Off |                  N/A |\\n    |  0%   38C    P8             21W /  200W |   23291MiB /  24576MiB |      0%      Default |\\n    |                                         |                        |                  N/A |\\n    +-----------------------------------------+------------------------+----------------------+\\n                                                                                             \\n    +-----------------------------------------------------------------------------------------+\\n    | Processes:                                                                              |\\n    |  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\\n    |        ID   ID                                                               Usage      |\\n    |=========================================================================================|\\n    |    0   N/A  N/A            2504      G   /usr/lib/xorg/Xorg                        4MiB |\\n    |    0   N/A  N/A           43614      C   ...ma.cpp/build/bin/llama-server      23600MiB |\\n    |    1   N/A  N/A            2504      G   /usr/lib/xorg/Xorg                        4MiB |\\n    |    1   N/A  N/A           43614      C   ...ma.cpp/build/bin/llama-server      23268MiB |\\n    +-----------------------------------------------------------------------------------------+","edited":1753125682,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4e7703","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I do it like that (maybe it&amp;#39;s not the best solution, but it works) :&lt;/p&gt;\\n\\n&lt;pre&gt;&lt;code&gt;setup_nvidia_undervolt() {\\n  sudo tee /usr/local/bin/undervolt-nvidia.sh &amp;gt; /dev/null &amp;lt;&amp;lt;&amp;#39;EOF&amp;#39;\\n#!/usr/bin/env bash\\n\\nnvidia-smi --persistence-mode ENABLED\\nnvidia-smi --power-limit 200\\nEOF\\n  sudo chmod +x /usr/local/bin/undervolt-nvidia.sh\\n\\n  sudo tee /etc/systemd/system/nvidia-undervolt.service &amp;gt; /dev/null &amp;lt;&amp;lt;&amp;#39;EOF&amp;#39;\\n[Unit]\\nDescription=Apply NVIDIA GPU power limit (undervolt)\\nWants=nvidia-persistenced.service\\nAfter=nvidia-persistenced.service\\n\\n[Service]\\nType=oneshot\\nExecStart=/usr/local/bin/undervolt-nvidia.sh\\nRemainAfterExit=yes\\n\\n[Install]\\nWantedBy=multi-user.target\\nEOF\\n\\n  sudo systemctl daemon-reload\\n  sudo systemctl enable --now nvidia-undervolt.service\\n}\\n&lt;/code&gt;&lt;/pre&gt;\\n\\n&lt;p&gt;I know there are other parameters to set - throttling/etc, but I kinda settled on it.&lt;/p&gt;\\n\\n&lt;pre&gt;&lt;code&gt;ubuntu@homelab:~$ nvidia-smi \\nMon Jul 21 22:20:32 2025       \\n+-----------------------------------------------------------------------------------------+\\n| NVIDIA-SMI 570.169                Driver Version: 570.169        CUDA Version: 12.8     |\\n|-----------------------------------------+------------------------+----------------------+\\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\\n|                                         |                        |               MIG M. |\\n|=========================================+========================+======================|\\n|   0  NVIDIA GeForce RTX 3090        On  |   00000000:01:00.0 Off |                  N/A |\\n|  0%   46C    P8             32W /  200W |   23623MiB /  24576MiB |      0%      Default |\\n|                                         |                        |                  N/A |\\n+-----------------------------------------+------------------------+----------------------+\\n|   1  NVIDIA GeForce RTX 3090        On  |   00000000:05:00.0 Off |                  N/A |\\n|  0%   38C    P8             21W /  200W |   23291MiB /  24576MiB |      0%      Default |\\n|                                         |                        |                  N/A |\\n+-----------------------------------------+------------------------+----------------------+\\n\\n+-----------------------------------------------------------------------------------------+\\n| Processes:                                                                              |\\n|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\\n|        ID   ID                                                               Usage      |\\n|=========================================================================================|\\n|    0   N/A  N/A            2504      G   /usr/lib/xorg/Xorg                        4MiB |\\n|    0   N/A  N/A           43614      C   ...ma.cpp/build/bin/llama-server      23600MiB |\\n|    1   N/A  N/A            2504      G   /usr/lib/xorg/Xorg                        4MiB |\\n|    1   N/A  N/A           43614      C   ...ma.cpp/build/bin/llama-server      23268MiB |\\n+-----------------------------------------------------------------------------------------+\\n&lt;/code&gt;&lt;/pre&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m5fkts","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5fkts/my_practical_dual_3090_setup_for_local_inference/n4e7703/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753125479,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4dwase","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"_hephaestus","can_mod_post":false,"created_utc":1753122403,"send_replies":true,"parent_id":"t3_1m5fkts","score":1,"author_fullname":"t2_158x8q","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"How do you do the undervolting? I’ve looked into it in the past and got a few conflicting reports about how spikes are handled/powerlimits were reset on boot (that may just be me failing to read it probably requires a startup script)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4dwase","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;How do you do the undervolting? I’ve looked into it in the past and got a few conflicting reports about how spikes are handled/powerlimits were reset on boot (that may just be me failing to read it probably requires a startup script)&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5fkts/my_practical_dual_3090_setup_for_local_inference/n4dwase/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753122403,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m5fkts","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),o=()=>e.jsx(t,{data:l});export{o as default};
