import{j as t}from"./index-BlGsFJYy.js";import{R as e}from"./RedditPostRenderer-B6uvq_Zl.js";import"./index-DDvVtNwD.js";const a=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Note: The following was generated via Gemini, simply because I am lazy and don't wanna summarize things personally. You can view the code [Here](https://pastebin.com/82Cn7022), and the text output comparisons [Here](https://pastebin.com/73Zn2bP4)\\n\\nI used the Puffin dataset for the Proof of concept, all in all it at least seems promising. Sadly its purely simulated, its my understanding that we would need custom cuda code in order to on the fly quantize (if its even currently possible with current hardware).\\n\\nGiven that this was a quick vibecoded proof of concept attempt to see how qwen3 0.6b would handle on the fly dynamic quantization in different sized chunks, I am rather impressed. But I don't know if the results were genuine. I would love to hear from other people about the topic.\\n\\nFinally the End goal for this would be:  \\nKeep entire Model Loaded in system Memory. Quantize on the fly based off the current prompt.  \\nUpdate the gpu based on the new quantized values.  \\nThink Dynamic Mixture of Experts but using quantization over an entire model based on current tasks.\\n\\n\\\\[Edit: I should mention that the accuracy is based off the Full models output (Using Puffin dataset for the prompts/context) and compared with the quantized output. At no point did the accuracy compare with the datasets expected output\\\\]\\n\\nOk what follows was an AI generated summary from Gemini of my results.  \\n\\\\------\\n\\nI've been experimenting with **dynamic quantization** for Large Language Models, and I wanted to share what I've found and get some community input.\\n\\n**The Idea:** My goal is to make LLMs more efficient by having them adjust the precision (bit-width) of their weights *as they process input*. Think of it as a model deciding, \\"Okay, this simple query can use 4-bit, but that complex reasoning part needs 16-bit,\\" all to save VRAM and potentially speed things up.\\n\\n**My Setup:** I'm using the **Qwen3-0.6B** model (which is typically BF16) and a smaller, separate neural network I'm calling the \\"**Quantization Controller**.\\" This controller's job is to predict the best bit-width (from 0-bit pruning to 32-bit full precision) for small \\"chunks\\" of the LLM's weights for each specific input.\\n\\nI'm training this controller to balance two things:\\n\\n1. **Output Similarity:** Keep the quantized model's output logits as close as possible to the full-precision model's.\\n2. **VRAM Use:** Add a penalty for using higher bit-widths to encourage memory savings. The VRAM penalty changes dynamically based on how well the quantized model is doing on accuracy – if it's too accurate, the penalty for VRAM goes up, pushing it to compress more; if accuracy drops, the penalty goes down, letting it use more bits.\\n\\n**What I've Seen So Far:**\\n\\n* **VRAM Savings:** I've managed to get the simulated VRAM footprint down from around 2.2GB (full BF16) to about 1.1GB, which is a pretty good reduction.\\n* **Token-Level Accuracy:** On my small dataset, the quantized model often matches the full-precision model almost perfectly in terms of predicting the next token.\\n* **\\"Settling\\" Bit-widths:** Even with the dynamic penalty, the controller seems to mostly stick to a couple of main bit-widths (like 9-bit and 11-bit) for most chunks. Only a small fraction of chunks (e.g., 8-30 out of \\\\~4500) actually change their quantization level per step. This makes it feel more like it's found a good static setup for these specific prompts.\\n* **Quality vs. Accuracy Gap:** The interesting part is, even with high token accuracy, the *generated text* from the quantized model can sometimes be incoherent or factually wrong (e.g., saying something is \\"not feasible\\" when it clearly is). This suggests that while it gets the next token right, some of the deeper semantic quality is lost with aggressive quantization.\\n\\n**Questions for Discussion:**\\n\\n1. **More Dynamic Behavior:** How can I get the controller to truly adapt more dynamically, meaning more fluctuation in bit-widths per chunk per prompt? Should I increase the \\"entropy penalty\\" in the controller's loss function to encourage it to explore more?\\n2. **Improving Output Quality:** To fix the coherence issues, I'm thinking about adding **trainable adapters (like LoRA)** to the quantized LLM. The idea is these small adapters would learn to correct the errors caused by quantization. Does this sound like a good next step, or are there other efficient ways to tackle this?\\n3. **Generating LoRA Weights?** A more out-there idea: could a *tiny, separate model* be trained to *generate* those LoRA weights dynamically for each input? (I know this is complex, but curious if anyone's explored this \\"hypernetwork\\" approach for quantization).\\n4. **Real-World Quantization:** My current setup \\"fakes\\" quantization (values are re-mapped in BF16, but the actual memory footprint doesn't change). How do people typically test and implement *true* dynamic quantization with actual low-bit integer types (like 4-bit or 8-bit) in PyTorch, especially since libraries like \`bitsandbytes\` don't seem to expose easy dynamic per-chunk switching?\\n\\nI'm pretty excited about the potential of adaptive quantization to make LLMs more accessible and efficient. Any thoughts, relevant papers, or advice would be super helpful!\\n\\nThanks for reading!","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Vibecoding: Exploring Dynamic Quantization for LLMs: My PoC with Qwen-0.6B","link_flair_richtext":[{"e":"text","t":"Discussion"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lsses1","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.45,"author_flair_background_color":null,"subreddit_type":"public","ups":0,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_mf1dz","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Discussion","can_mod_post":false,"score":0,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":1751775891,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1751775457,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Note: The following was generated via Gemini, simply because I am lazy and don&amp;#39;t wanna summarize things personally. You can view the code &lt;a href=\\"https://pastebin.com/82Cn7022\\"&gt;Here&lt;/a&gt;, and the text output comparisons &lt;a href=\\"https://pastebin.com/73Zn2bP4\\"&gt;Here&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;I used the Puffin dataset for the Proof of concept, all in all it at least seems promising. Sadly its purely simulated, its my understanding that we would need custom cuda code in order to on the fly quantize (if its even currently possible with current hardware).&lt;/p&gt;\\n\\n&lt;p&gt;Given that this was a quick vibecoded proof of concept attempt to see how qwen3 0.6b would handle on the fly dynamic quantization in different sized chunks, I am rather impressed. But I don&amp;#39;t know if the results were genuine. I would love to hear from other people about the topic.&lt;/p&gt;\\n\\n&lt;p&gt;Finally the End goal for this would be:&lt;br/&gt;\\nKeep entire Model Loaded in system Memory. Quantize on the fly based off the current prompt.&lt;br/&gt;\\nUpdate the gpu based on the new quantized values.&lt;br/&gt;\\nThink Dynamic Mixture of Experts but using quantization over an entire model based on current tasks.&lt;/p&gt;\\n\\n&lt;p&gt;[Edit: I should mention that the accuracy is based off the Full models output (Using Puffin dataset for the prompts/context) and compared with the quantized output. At no point did the accuracy compare with the datasets expected output]&lt;/p&gt;\\n\\n&lt;p&gt;Ok what follows was an AI generated summary from Gemini of my results.&lt;br/&gt;\\n------&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;ve been experimenting with &lt;strong&gt;dynamic quantization&lt;/strong&gt; for Large Language Models, and I wanted to share what I&amp;#39;ve found and get some community input.&lt;/p&gt;\\n\\n&lt;p&gt;&lt;strong&gt;The Idea:&lt;/strong&gt; My goal is to make LLMs more efficient by having them adjust the precision (bit-width) of their weights &lt;em&gt;as they process input&lt;/em&gt;. Think of it as a model deciding, &amp;quot;Okay, this simple query can use 4-bit, but that complex reasoning part needs 16-bit,&amp;quot; all to save VRAM and potentially speed things up.&lt;/p&gt;\\n\\n&lt;p&gt;&lt;strong&gt;My Setup:&lt;/strong&gt; I&amp;#39;m using the &lt;strong&gt;Qwen3-0.6B&lt;/strong&gt; model (which is typically BF16) and a smaller, separate neural network I&amp;#39;m calling the &amp;quot;&lt;strong&gt;Quantization Controller&lt;/strong&gt;.&amp;quot; This controller&amp;#39;s job is to predict the best bit-width (from 0-bit pruning to 32-bit full precision) for small &amp;quot;chunks&amp;quot; of the LLM&amp;#39;s weights for each specific input.&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;m training this controller to balance two things:&lt;/p&gt;\\n\\n&lt;ol&gt;\\n&lt;li&gt;&lt;strong&gt;Output Similarity:&lt;/strong&gt; Keep the quantized model&amp;#39;s output logits as close as possible to the full-precision model&amp;#39;s.&lt;/li&gt;\\n&lt;li&gt;&lt;strong&gt;VRAM Use:&lt;/strong&gt; Add a penalty for using higher bit-widths to encourage memory savings. The VRAM penalty changes dynamically based on how well the quantized model is doing on accuracy – if it&amp;#39;s too accurate, the penalty for VRAM goes up, pushing it to compress more; if accuracy drops, the penalty goes down, letting it use more bits.&lt;/li&gt;\\n&lt;/ol&gt;\\n\\n&lt;p&gt;&lt;strong&gt;What I&amp;#39;ve Seen So Far:&lt;/strong&gt;&lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;&lt;strong&gt;VRAM Savings:&lt;/strong&gt; I&amp;#39;ve managed to get the simulated VRAM footprint down from around 2.2GB (full BF16) to about 1.1GB, which is a pretty good reduction.&lt;/li&gt;\\n&lt;li&gt;&lt;strong&gt;Token-Level Accuracy:&lt;/strong&gt; On my small dataset, the quantized model often matches the full-precision model almost perfectly in terms of predicting the next token.&lt;/li&gt;\\n&lt;li&gt;&lt;strong&gt;&amp;quot;Settling&amp;quot; Bit-widths:&lt;/strong&gt; Even with the dynamic penalty, the controller seems to mostly stick to a couple of main bit-widths (like 9-bit and 11-bit) for most chunks. Only a small fraction of chunks (e.g., 8-30 out of ~4500) actually change their quantization level per step. This makes it feel more like it&amp;#39;s found a good static setup for these specific prompts.&lt;/li&gt;\\n&lt;li&gt;&lt;strong&gt;Quality vs. Accuracy Gap:&lt;/strong&gt; The interesting part is, even with high token accuracy, the &lt;em&gt;generated text&lt;/em&gt; from the quantized model can sometimes be incoherent or factually wrong (e.g., saying something is &amp;quot;not feasible&amp;quot; when it clearly is). This suggests that while it gets the next token right, some of the deeper semantic quality is lost with aggressive quantization.&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;p&gt;&lt;strong&gt;Questions for Discussion:&lt;/strong&gt;&lt;/p&gt;\\n\\n&lt;ol&gt;\\n&lt;li&gt;&lt;strong&gt;More Dynamic Behavior:&lt;/strong&gt; How can I get the controller to truly adapt more dynamically, meaning more fluctuation in bit-widths per chunk per prompt? Should I increase the &amp;quot;entropy penalty&amp;quot; in the controller&amp;#39;s loss function to encourage it to explore more?&lt;/li&gt;\\n&lt;li&gt;&lt;strong&gt;Improving Output Quality:&lt;/strong&gt; To fix the coherence issues, I&amp;#39;m thinking about adding &lt;strong&gt;trainable adapters (like LoRA)&lt;/strong&gt; to the quantized LLM. The idea is these small adapters would learn to correct the errors caused by quantization. Does this sound like a good next step, or are there other efficient ways to tackle this?&lt;/li&gt;\\n&lt;li&gt;&lt;strong&gt;Generating LoRA Weights?&lt;/strong&gt; A more out-there idea: could a &lt;em&gt;tiny, separate model&lt;/em&gt; be trained to &lt;em&gt;generate&lt;/em&gt; those LoRA weights dynamically for each input? (I know this is complex, but curious if anyone&amp;#39;s explored this &amp;quot;hypernetwork&amp;quot; approach for quantization).&lt;/li&gt;\\n&lt;li&gt;&lt;strong&gt;Real-World Quantization:&lt;/strong&gt; My current setup &amp;quot;fakes&amp;quot; quantization (values are re-mapped in BF16, but the actual memory footprint doesn&amp;#39;t change). How do people typically test and implement &lt;em&gt;true&lt;/em&gt; dynamic quantization with actual low-bit integer types (like 4-bit or 8-bit) in PyTorch, especially since libraries like &lt;code&gt;bitsandbytes&lt;/code&gt; don&amp;#39;t seem to expose easy dynamic per-chunk switching?&lt;/li&gt;\\n&lt;/ol&gt;\\n\\n&lt;p&gt;I&amp;#39;m pretty excited about the potential of adaptive quantization to make LLMs more accessible and efficient. Any thoughts, relevant papers, or advice would be super helpful!&lt;/p&gt;\\n\\n&lt;p&gt;Thanks for reading!&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":true,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"5f921ea4-c7bc-11ed-9c23-3a00622979b4","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#646d73","id":"1lsses1","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"jasonmbrown","discussion_type":null,"num_comments":4,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lsses1/vibecoding_exploring_dynamic_quantization_for/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lsses1/vibecoding_exploring_dynamic_quantization_for/","subreddit_subscribers":495396,"created_utc":1751775457,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1muxtd","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Chromix_","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1lt0uw","score":2,"author_fullname":"t2_k7w2h","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Even with the same seed the generation is always different, even for the same quant when using a slightly different imatrix. That'd be interesting to find a few cases where \\"different\\" also consistently means \\"better\\". So far I've only found an extreme edge-case where Q8 K (not V, not model) quantization consistently led to the wrong results - not always, just consistently in around 5% of the cases.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n1muxtd","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Even with the same seed the generation is always different, even for the same quant when using a slightly different imatrix. That&amp;#39;d be interesting to find a few cases where &amp;quot;different&amp;quot; also consistently means &amp;quot;better&amp;quot;. So far I&amp;#39;ve only found an extreme edge-case where Q8 K (not V, not model) quantization consistently led to the wrong results - not always, just consistently in around 5% of the cases.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lsses1","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lsses1/vibecoding_exploring_dynamic_quantization_for/n1muxtd/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751810019,"author_flair_text":null,"treatment_tags":[],"created_utc":1751810019,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n1lt0uw","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AppearanceHeavy6724","can_mod_post":false,"created_utc":1751791480,"send_replies":true,"parent_id":"t1_n1lkep8","score":2,"author_fullname":"t2_uz37qfx5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"It might be a placebo, but I still could see difference in output between bf16 and q8 in some models. Q8 imatrix could be very useful for some purists, I would certainly use one to squeeze the last bit of KLD out.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1lt0uw","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It might be a placebo, but I still could see difference in output between bf16 and q8 in some models. Q8 imatrix could be very useful for some purists, I would certainly use one to squeeze the last bit of KLD out.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lsses1","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lsses1/vibecoding_exploring_dynamic_quantization_for/n1lt0uw/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751791480,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1pwxxa","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"jasonmbrown","can_mod_post":false,"created_utc":1751844729,"send_replies":true,"parent_id":"t1_n1lkep8","score":2,"author_fullname":"t2_mf1dz","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yeah, after I did the quick proof of concept, I started looking into dynamic quantization... but it would require very low level code. Since It essentially requires a nn.linear layer that uses weights at different bit widths. (I cannot find any way to perform this)\\n\\nMy intentions behind this was:  \\nKeep full model loaded on system ram (Which is cheaper then Vram)  \\nPredict which weights to quantize in order to decrease the Required VRAM size, while not losing performance.  \\nUser inputs data-&gt;QuantizeManager Predicts the required bit width for the weights-&gt; Model is quantized and loaded into Vram -&gt; Model Processes output until it hits its EOS token -&gt; Process Repeats\\n\\nMy thoughts were that certain tasks don't need as much 'detail' in regards with what they are handling. I do not have a large enough dataset to full test this theory though.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1pwxxa","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yeah, after I did the quick proof of concept, I started looking into dynamic quantization... but it would require very low level code. Since It essentially requires a nn.linear layer that uses weights at different bit widths. (I cannot find any way to perform this)&lt;/p&gt;\\n\\n&lt;p&gt;My intentions behind this was:&lt;br/&gt;\\nKeep full model loaded on system ram (Which is cheaper then Vram)&lt;br/&gt;\\nPredict which weights to quantize in order to decrease the Required VRAM size, while not losing performance.&lt;br/&gt;\\nUser inputs data-&amp;gt;QuantizeManager Predicts the required bit width for the weights-&amp;gt; Model is quantized and loaded into Vram -&amp;gt; Model Processes output until it hits its EOS token -&amp;gt; Process Repeats&lt;/p&gt;\\n\\n&lt;p&gt;My thoughts were that certain tasks don&amp;#39;t need as much &amp;#39;detail&amp;#39; in regards with what they are handling. I do not have a large enough dataset to full test this theory though.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lsses1","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lsses1/vibecoding_exploring_dynamic_quantization_for/n1pwxxa/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751844729,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n1lkep8","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Chromix_","can_mod_post":false,"created_utc":1751786345,"send_replies":true,"parent_id":"t3_1lsses1","score":6,"author_fullname":"t2_k7w2h","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"&gt;...VRAM footprint down from around 2.2GB (full BF16) to about 1.1GB...\\n\\nThus, essentially created a Q8 model. In terms of quality it's so close to the original BF16 that using an imatrix for quantizing via llama.cpp doesn't change the result quality in practical tests. Your dynamic quantization has a few higher and lower bits layers, which might improve the quality, yet the difference between \\"can't tell the difference\\" and \\"a bit better than that\\" isn't that large.\\n\\nIn practice you might get better results faster when using imatrix and potentially [QAT](https://www.reddit.com/r/LocalLLaMA/comments/1jqnnfp/comment/mlfqgei/?context=3). It'd take some serious work to build something that performs optimal dynamic quantization (relative to what dataset?).","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1lkep8","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;...VRAM footprint down from around 2.2GB (full BF16) to about 1.1GB...&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;Thus, essentially created a Q8 model. In terms of quality it&amp;#39;s so close to the original BF16 that using an imatrix for quantizing via llama.cpp doesn&amp;#39;t change the result quality in practical tests. Your dynamic quantization has a few higher and lower bits layers, which might improve the quality, yet the difference between &amp;quot;can&amp;#39;t tell the difference&amp;quot; and &amp;quot;a bit better than that&amp;quot; isn&amp;#39;t that large.&lt;/p&gt;\\n\\n&lt;p&gt;In practice you might get better results faster when using imatrix and potentially &lt;a href=\\"https://www.reddit.com/r/LocalLLaMA/comments/1jqnnfp/comment/mlfqgei/?context=3\\"&gt;QAT&lt;/a&gt;. It&amp;#39;d take some serious work to build something that performs optimal dynamic quantization (relative to what dataset?).&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lsses1/vibecoding_exploring_dynamic_quantization_for/n1lkep8/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751786345,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lsses1","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}}],"before":null}}]`),i=()=>t.jsx(e,{data:a});export{i as default};
