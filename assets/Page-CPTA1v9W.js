import{j as e}from"./index-BlGsFJYy.js";import{R as l}from"./RedditPostRenderer-B6uvq_Zl.js";import"./index-DDvVtNwD.js";const a=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"I am working at a listed OCR company and am in the on-premise OCR research department based on LLM. Since I am conducting research with large models such as Qwen2.5 VL 72B, I have a lot of personal time while the models are running. Are there any things I can do on my own related to LLM with two H100s? I would appreciate it if you could recommend them. After completing my Masters in Vision and moving to LLM, it is not easy to find things to study on my own.","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Can you recommend something I can personally do with two H100?","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1m2u9n3","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.73,"author_flair_background_color":null,"subreddit_type":"public","ups":7,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_1ld1b995hk","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":7,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1752817529,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;I am working at a listed OCR company and am in the on-premise OCR research department based on LLM. Since I am conducting research with large models such as Qwen2.5 VL 72B, I have a lot of personal time while the models are running. Are there any things I can do on my own related to LLM with two H100s? I would appreciate it if you could recommend them. After completing my Masters in Vision and moving to LLM, it is not easy to find things to study on my own.&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1m2u9n3","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"CantaloupeDismal1195","discussion_type":null,"num_comments":14,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1m2u9n3/can_you_recommend_something_i_can_personally_do/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1m2u9n3/can_you_recommend_something_i_can_personally_do/","subreddit_subscribers":501233,"created_utc":1752817529,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3t5afi","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"CertainlyBright","can_mod_post":false,"created_utc":1752843600,"send_replies":true,"parent_id":"t1_n3s6jk1","score":2,"author_fullname":"t2_5xx2c0t2","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Lol","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3t5afi","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Lol&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2u9n3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2u9n3/can_you_recommend_something_i_can_personally_do/n3t5afi/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752843600,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n3s6jk1","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"cantgetthistowork","can_mod_post":false,"created_utc":1752827627,"send_replies":true,"parent_id":"t3_1m2u9n3","score":30,"author_fullname":"t2_j1i0o","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Bro's getting paid 7 figures to ask reddit to do his work","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3s6jk1","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Bro&amp;#39;s getting paid 7 figures to ask reddit to do his work&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2u9n3/can_you_recommend_something_i_can_personally_do/n3s6jk1/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752827627,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m2u9n3","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":30}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3tpsz1","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"eidrag","can_mod_post":false,"created_utc":1752849844,"send_replies":true,"parent_id":"t3_1m2u9n3","score":3,"author_fullname":"t2_avrld","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"fix your ocr. especially east asian","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3tpsz1","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;fix your ocr. especially east asian&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2u9n3/can_you_recommend_something_i_can_personally_do/n3tpsz1/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752849844,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m2u9n3","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3rrifa","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ttkciar","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3rq0eq","score":2,"author_fullname":"t2_cpegz","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"It should be, if you lower the batch size sufficiently.\\n\\nIf it's not, or if you want to train it at a higher batch size, you can freeze the upper layers and just train the unfrozen lower layers.  Starling-LM-7B-alpha did that to very positive effect.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3rrifa","is_submitter":false,"collapsed":false,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It should be, if you lower the batch size sufficiently.&lt;/p&gt;\\n\\n&lt;p&gt;If it&amp;#39;s not, or if you want to train it at a higher batch size, you can freeze the upper layers and just train the unfrozen lower layers.  Starling-LM-7B-alpha did that to very positive effect.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2u9n3","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2u9n3/can_you_recommend_something_i_can_personally_do/n3rrifa/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752819335,"author_flair_text":"llama.cpp","treatment_tags":[],"created_utc":1752819335,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n3rq0eq","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"CantaloupeDismal1195","can_mod_post":false,"created_utc":1752818550,"send_replies":true,"parent_id":"t1_n3rp2qm","score":1,"author_fullname":"t2_1ld1b995hk","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Thanks. I guess I'll have to try applying it to Qwen3. Does the vram seem sufficient?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3rq0eq","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thanks. I guess I&amp;#39;ll have to try applying it to Qwen3. Does the vram seem sufficient?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2u9n3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2u9n3/can_you_recommend_something_i_can_personally_do/n3rq0eq/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752818550,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3rp2qm","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ttkciar","can_mod_post":false,"created_utc":1752818071,"send_replies":true,"parent_id":"t3_1m2u9n3","score":3,"author_fullname":"t2_cpegz","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Perhaps try applying the Tulu3 retraining recipe to Gemma3-12B or Qwen3-14B?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3rp2qm","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Perhaps try applying the Tulu3 retraining recipe to Gemma3-12B or Qwen3-14B?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2u9n3/can_you_recommend_something_i_can_personally_do/n3rp2qm/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752818071,"author_flair_text":"llama.cpp","treatment_tags":[],"link_id":"t3_1m2u9n3","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3skrh5","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"arthurwolf","can_mod_post":false,"created_utc":1752835442,"send_replies":true,"parent_id":"t3_1m2u9n3","score":6,"author_fullname":"t2_db9zf","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"* 1 Write a script that gathers a list of 1000 top politician and public figure people in the US using LLMs and web search/browser use\\n* 2 For each politician, figure out if they are a friend or a foe of each other politician by searching the name of both together and using LLMs to analyze the results, giving you 1 million connections\\n* 3 For each politician, figure out using browser use/search and LLMs if they are pro or against the release of the Epstein files\\n* 4 Use all this information together (shouldn't require LLMs for this part) to graph who the most people who are against the release of the Epstein files are **also** friend with, and also the most people who are **for** the release of the Epstein files are **not** friends with (essentially, find somebody whom all their friends are against the release/none of their friends are for the release and all their enemies are for the release) \\n\\nCongrats, you have graphed who is most likely to be in the Epstein files (by finding who most likely pressured all their friends to be against the release). Contact some newspaper to present your findings.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3skrh5","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;ul&gt;\\n&lt;li&gt;1 Write a script that gathers a list of 1000 top politician and public figure people in the US using LLMs and web search/browser use&lt;/li&gt;\\n&lt;li&gt;2 For each politician, figure out if they are a friend or a foe of each other politician by searching the name of both together and using LLMs to analyze the results, giving you 1 million connections&lt;/li&gt;\\n&lt;li&gt;3 For each politician, figure out using browser use/search and LLMs if they are pro or against the release of the Epstein files&lt;/li&gt;\\n&lt;li&gt;4 Use all this information together (shouldn&amp;#39;t require LLMs for this part) to graph who the most people who are against the release of the Epstein files are &lt;strong&gt;also&lt;/strong&gt; friend with, and also the most people who are &lt;strong&gt;for&lt;/strong&gt; the release of the Epstein files are &lt;strong&gt;not&lt;/strong&gt; friends with (essentially, find somebody whom all their friends are against the release/none of their friends are for the release and all their enemies are for the release) &lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;p&gt;Congrats, you have graphed who is most likely to be in the Epstein files (by finding who most likely pressured all their friends to be against the release). Contact some newspaper to present your findings.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2u9n3/can_you_recommend_something_i_can_personally_do/n3skrh5/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752835442,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m2u9n3","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"c07aa42e-51fe-11f0-afcc-462aad931709","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3t2w3a","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"entsnack","can_mod_post":false,"created_utc":1752842792,"send_replies":true,"parent_id":"t3_1m2u9n3","score":1,"author_fullname":"t2_1a48h7vf","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Reinforcement fine tuning","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3t2w3a","is_submitter":false,"downs":0,"author_flair_richtext":[{"a":":X:","u":"https://emoji.redditmedia.com/tbgegafk739f1_t5_81eyvm/X","e":"emoji"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Reinforcement fine tuning&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"dark","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2u9n3/can_you_recommend_something_i_can_personally_do/n3t2w3a/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752842792,"author_flair_text":":X:","treatment_tags":[],"link_id":"t3_1m2u9n3","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"transparent","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3uo3sr","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Hot-Entrepreneur2934","can_mod_post":false,"created_utc":1752859488,"send_replies":true,"parent_id":"t3_1m2u9n3","score":1,"author_fullname":"t2_1kmdyhhvg4","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"It's 2025. You should be asking an AI. We humans are no longer the source of ideas.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3uo3sr","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It&amp;#39;s 2025. You should be asking an AI. We humans are no longer the source of ideas.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2u9n3/can_you_recommend_something_i_can_personally_do/n3uo3sr/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752859488,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m2u9n3","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3s72uw","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"kaisurniwurer","can_mod_post":false,"created_utc":1752827935,"send_replies":true,"parent_id":"t3_1m2u9n3","score":0,"author_fullname":"t2_qafso","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I have been wanting to see if quants really impact the nuance compared to the full model in real case scenario rather than in calculated parameters.\\n\\nSame thing for context. I found information that KVcache quant really degrades performance on bigger context lengths but sadly just some bits and pieces, nothing concrete.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3s72uw","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I have been wanting to see if quants really impact the nuance compared to the full model in real case scenario rather than in calculated parameters.&lt;/p&gt;\\n\\n&lt;p&gt;Same thing for context. I found information that KVcache quant really degrades performance on bigger context lengths but sadly just some bits and pieces, nothing concrete.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2u9n3/can_you_recommend_something_i_can_personally_do/n3s72uw/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752827935,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m2u9n3","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3sqik7","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"kaisurniwurer","can_mod_post":false,"created_utc":1752838050,"send_replies":true,"parent_id":"t3_1m2u9n3","score":0,"author_fullname":"t2_qafso","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Run queries without any prompt and evaluate topics of completely randomly generated responses to estimate bias of a model.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3sqik7","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Run queries without any prompt and evaluate topics of completely randomly generated responses to estimate bias of a model.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2u9n3/can_you_recommend_something_i_can_personally_do/n3sqik7/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752838050,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m2u9n3","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3tvjwx","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"101m4n","can_mod_post":false,"created_utc":1752851462,"send_replies":true,"parent_id":"t1_n3rtwm8","score":3,"author_fullname":"t2_p7nc2","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yeah, basically just e-waste.\\n\\nGive them all to me and I will dispose of them safely.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3tvjwx","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yeah, basically just e-waste.&lt;/p&gt;\\n\\n&lt;p&gt;Give them all to me and I will dispose of them safely.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2u9n3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2u9n3/can_you_recommend_something_i_can_personally_do/n3tvjwx/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752851462,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n3rtwm8","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":"LOW_SCORE","no_follow":true,"author":"GPTrack_ai","can_mod_post":false,"created_utc":1752820602,"send_replies":true,"parent_id":"t3_1m2u9n3","score":-10,"author_fullname":"t2_1tpuoj72sa","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":true,"body":"H100 is EOL. Convince you company that they need to buy Blackwell.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3rtwm8","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;H100 is EOL. Convince you company that they need to buy Blackwell.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":"comment score below threshold","distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2u9n3/can_you_recommend_something_i_can_personally_do/n3rtwm8/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752820602,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m2u9n3","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-10}}],"before":null}}]`),o=()=>e.jsx(l,{data:a});export{o as default};
