import{j as l}from"./index-CNyNkRpk.js";import{R as e}from"./RedditPostRenderer-Dza0u9i2.js";import"./index-BUchu_-K.js";const a=JSON.parse('[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"I’m considering transitioning from Ollama llama.cpp. Does llama.cpp have an equivalent feature to Ollama’s modelfiles, whereby you can bake a system prompt into the model itself before calling it from a Python script (or wherever)?  ","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Ollama to llama.cpp: system prompt?","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lo3l7w","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.6,"author_flair_background_color":null,"subreddit_type":"public","ups":2,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_4sqhqdgr","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":2,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1751277592,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;I’m considering transitioning from Ollama llama.cpp. Does llama.cpp have an equivalent feature to Ollama’s modelfiles, whereby you can bake a system prompt into the model itself before calling it from a Python script (or wherever)?  &lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1lo3l7w","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"psychonomy","discussion_type":null,"num_comments":6,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lo3l7w/ollama_to_llamacpp_system_prompt/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lo3l7w/ollama_to_llamacpp_system_prompt/","subreddit_subscribers":493240,"created_utc":1751277592,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0k19za","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"i-eat-kittens","can_mod_post":false,"created_utc":1751280673,"send_replies":true,"parent_id":"t3_1lo3l7w","score":7,"author_fullname":"t2_z290n","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"llama-cli accepts a system prompt or filename on the command line, which is pretty convenient for some simple testing.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0k19za","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;llama-cli accepts a system prompt or filename on the command line, which is pretty convenient for some simple testing.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lo3l7w/ollama_to_llamacpp_system_prompt/n0k19za/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751280673,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lo3l7w","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":7}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0jy0cx","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"ZucchiniCalm4617","can_mod_post":false,"created_utc":1751278935,"send_replies":true,"parent_id":"t3_1lo3l7w","score":5,"author_fullname":"t2_fecz50nu","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"No equivalent of Modelfile. You have to pass system prompt in the messages param of chat completion calls.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0jy0cx","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;No equivalent of Modelfile. You have to pass system prompt in the messages param of chat completion calls.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lo3l7w/ollama_to_llamacpp_system_prompt/n0jy0cx/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751278935,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lo3l7w","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0mbrce","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"emprahsFury","can_mod_post":false,"created_utc":1751307434,"send_replies":false,"parent_id":"t3_1lo3l7w","score":5,"author_fullname":"t2_177r8n","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"The gguf itself is essentially a modelfile. All ggufs support a system message template and Bartowski at least does embed the prompt in the appropriate field. If you start llama-server with --jinja it will use the embedded system prompt.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0mbrce","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The gguf itself is essentially a modelfile. All ggufs support a system message template and Bartowski at least does embed the prompt in the appropriate field. If you start llama-server with --jinja it will use the embedded system prompt.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lo3l7w/ollama_to_llamacpp_system_prompt/n0mbrce/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751307434,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lo3l7w","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0jy8ns","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"JustImmunity","can_mod_post":false,"created_utc":1751279063,"send_replies":true,"parent_id":"t3_1lo3l7w","score":3,"author_fullname":"t2_c4pwgz16","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Yeah you’d probably need to make a layer for that since llama.cpp doesn’t do that natively, if you don’t want to define a system prompt in your calls\\n\\nIt usually leaves that functionality up to the user and the application they use","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0jy8ns","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yeah you’d probably need to make a layer for that since llama.cpp doesn’t do that natively, if you don’t want to define a system prompt in your calls&lt;/p&gt;\\n\\n&lt;p&gt;It usually leaves that functionality up to the user and the application they use&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lo3l7w/ollama_to_llamacpp_system_prompt/n0jy8ns/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751279063,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lo3l7w","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0k7oqd","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"psychonomy","can_mod_post":false,"created_utc":1751283708,"send_replies":true,"parent_id":"t3_1lo3l7w","score":2,"author_fullname":"t2_4sqhqdgr","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Thanks all.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0k7oqd","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thanks all.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lo3l7w/ollama_to_llamacpp_system_prompt/n0k7oqd/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751283708,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lo3l7w","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0o48h5","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"poita66","can_mod_post":false,"created_utc":1751327050,"send_replies":true,"parent_id":"t3_1lo3l7w","score":1,"author_fullname":"t2_hbp5l","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Ollama is to llama.cpp like Docker is to chroots. It’s just a layer on top to allow easy packaging of models.\\n\\nSo if you’re going to use llama.cpp directly, you’ll need to emulate what Ollama is doing where it unpacks the model file into arguments.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0o48h5","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Ollama is to llama.cpp like Docker is to chroots. It’s just a layer on top to allow easy packaging of models.&lt;/p&gt;\\n\\n&lt;p&gt;So if you’re going to use llama.cpp directly, you’ll need to emulate what Ollama is doing where it unpacks the model file into arguments.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lo3l7w/ollama_to_llamacpp_system_prompt/n0o48h5/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751327050,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lo3l7w","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]'),r=()=>l.jsx(e,{data:a});export{r as default};
