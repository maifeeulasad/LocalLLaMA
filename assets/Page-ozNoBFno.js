import{j as e}from"./index-CWmJdUH_.js";import{R as t}from"./RedditPostRenderer-D2iunoQ9.js";import"./index-BCg9RP6g.js";const a=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Which models offer the best quality-to-performance in terms of prompt adherence and context length for such a usecase? I am currently using NousResearch/Hermes-3-Llama-3.1-8B-GGUF for this task after having failed in trying to get Qwen2.5 7B to give questions from the actual theory text not sections of the book. I am using an RTX 4060 8GB with 16 GB RAM, which severely limits my options but I'd want to use the best I could for my hardware.","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Models for generating QA-pairs from text dataset","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lo1d8t","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.74,"author_flair_background_color":null,"subreddit_type":"public","ups":5,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_giwl2ppl","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":5,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1751268537,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Which models offer the best quality-to-performance in terms of prompt adherence and context length for such a usecase? I am currently using NousResearch/Hermes-3-Llama-3.1-8B-GGUF for this task after having failed in trying to get Qwen2.5 7B to give questions from the actual theory text not sections of the book. I am using an RTX 4060 8GB with 16 GB RAM, which severely limits my options but I&amp;#39;d want to use the best I could for my hardware.&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1lo1d8t","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"Sasikuttan2163","discussion_type":null,"num_comments":15,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lo1d8t/models_for_generating_qapairs_from_text_dataset/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lo1d8t/models_for_generating_qapairs_from_text_dataset/","subreddit_subscribers":493242,"created_utc":1751268537,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0k3zro","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Sasikuttan2163","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0jqbmu","score":2,"author_fullname":"t2_giwl2ppl","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Wow that's actually insane! I'll try to implement something like this, thank you!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0k3zro","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Wow that&amp;#39;s actually insane! I&amp;#39;ll try to implement something like this, thank you!&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lo1d8t","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lo1d8t/models_for_generating_qapairs_from_text_dataset/n0k3zro/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751282010,"author_flair_text":null,"treatment_tags":[],"created_utc":1751282010,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n0jqbmu","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"iamnotapuck","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0jo2lv","score":3,"author_fullname":"t2_dyna1211","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"The current gold standard, at least for me, is [Chonkie](https://github.com/chonkie-inc/chonkie). It pretty much does everything you would want automatically (to an extent) dealing with chunking, embedding, and formatting raw text documents effectively. In your use case, I would export the chunks as json, so it easier for the locallm to processes what it wants it to generate. But if you read up on the documentation for Chonkie, it can be very powerful to combine it with embedding and RAG implantation.\\n\\nMy original pipeline is a simple one, but you can get very advanced in the weeds if you want to make sure the questions and answers are higher quality. My more advanced q&amp;a generator is to use Chonkie to segment the raw historical text into a set amount, generally the same size you use or a little smaller, but more overlap, which will then be embedded in a vector DB. Then I have the locallm do a first pass by reading the original chunks to generate questions (usually three). These are stored in a csv or json file. I then have it do a second pass using RAG pipeline. Now that the full document/s are embedded, I can just perform a query for each original question from the chunks I did before. But now since the full document is embedded, I can get a more semenatic result back, taking the top 5 results. Then have the locallm ingest those 5 results, and provide a more detailed answer to the question (than just what the chunk might have had). This provides a higher quality answer with a lower tier LLM.\\n\\nIf you want to look more on the RAG side of this implementation, I know that IBM is doing an amazing job with the granite models that are relatively small and can be used for local hosting. This is my current experimental high advanced one that I am currently trying to figure out on my own :)) ... [IBM agent RAG](https://huggingface.co/ibm-granite/granite-3.3-8b-rag-agent-lib)\\n\\nHope this helps, and if you have any questions, again ask away.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0jqbmu","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The current gold standard, at least for me, is &lt;a href=\\"https://github.com/chonkie-inc/chonkie\\"&gt;Chonkie&lt;/a&gt;. It pretty much does everything you would want automatically (to an extent) dealing with chunking, embedding, and formatting raw text documents effectively. In your use case, I would export the chunks as json, so it easier for the locallm to processes what it wants it to generate. But if you read up on the documentation for Chonkie, it can be very powerful to combine it with embedding and RAG implantation.&lt;/p&gt;\\n\\n&lt;p&gt;My original pipeline is a simple one, but you can get very advanced in the weeds if you want to make sure the questions and answers are higher quality. My more advanced q&amp;amp;a generator is to use Chonkie to segment the raw historical text into a set amount, generally the same size you use or a little smaller, but more overlap, which will then be embedded in a vector DB. Then I have the locallm do a first pass by reading the original chunks to generate questions (usually three). These are stored in a csv or json file. I then have it do a second pass using RAG pipeline. Now that the full document/s are embedded, I can just perform a query for each original question from the chunks I did before. But now since the full document is embedded, I can get a more semenatic result back, taking the top 5 results. Then have the locallm ingest those 5 results, and provide a more detailed answer to the question (than just what the chunk might have had). This provides a higher quality answer with a lower tier LLM.&lt;/p&gt;\\n\\n&lt;p&gt;If you want to look more on the RAG side of this implementation, I know that IBM is doing an amazing job with the granite models that are relatively small and can be used for local hosting. This is my current experimental high advanced one that I am currently trying to figure out on my own :)) ... &lt;a href=\\"https://huggingface.co/ibm-granite/granite-3.3-8b-rag-agent-lib\\"&gt;IBM agent RAG&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;Hope this helps, and if you have any questions, again ask away.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lo1d8t","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lo1d8t/models_for_generating_qapairs_from_text_dataset/n0jqbmu/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751274397,"author_flair_text":null,"treatment_tags":[],"created_utc":1751274397,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n0jo2lv","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Sasikuttan2163","can_mod_post":false,"created_utc":1751273021,"send_replies":true,"parent_id":"t1_n0jndkh","score":1,"author_fullname":"t2_giwl2ppl","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Thanks for the descriptive answer! Could you tell me what approach you use for chunking? Right now I'm using a pretty basic langchain RecursiveCharacterTextSplitter with 2000 chunk size and 200 overlap. I realize that the results would vary greatly for different chunk sizes in different models but what have you had the best experience with?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0jo2lv","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thanks for the descriptive answer! Could you tell me what approach you use for chunking? Right now I&amp;#39;m using a pretty basic langchain RecursiveCharacterTextSplitter with 2000 chunk size and 200 overlap. I realize that the results would vary greatly for different chunk sizes in different models but what have you had the best experience with?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lo1d8t","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lo1d8t/models_for_generating_qapairs_from_text_dataset/n0jo2lv/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751273021,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0jndkh","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"iamnotapuck","can_mod_post":false,"created_utc":1751272587,"send_replies":true,"parent_id":"t3_1lo1d8t","score":3,"author_fullname":"t2_dyna1211","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"If just trying to create Q&amp;A pairs, I've found that the specific llm from 7-12B generally perform the same in question and answer generation. More verbose as you increase in parameters. What needs more specificity is the prompt engineering during api requests.\\n\\nMy general pipeline goes something like this:\\n\\nlarge textbook --&gt; chunk into paragraphs (token amounts might vary) --&gt; locallm summarizes chunk --&gt; prompt locallm to generate three questions based on summarization --&gt; prompt locallm to generate three answers based on questions, summarization, &amp; chunk.\\n\\ncsv output: [chunk text][summary][question][answer]\\n\\nThis is helpful to make sure the answers are grounded in the context and not just made up. For human fact checking.\\n\\nMost of my pipeline deals with history texts, so it might not be the same in your use case. I would say it might be less about the model you select, and more about how you construct the pipeline for q&amp;a generation.\\n\\nI've used a intel arc750 gpu with 8GB using LM Studio's api server to run these question and answers format. So your gpu and RAM should be fine, depending on the model quants. But I then would use a local instance of jupyter notebooks to run the python script for requests to LM Studio. \\n\\nHope that helps, and if you need any specific help, just drop me a line.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0jndkh","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;If just trying to create Q&amp;amp;A pairs, I&amp;#39;ve found that the specific llm from 7-12B generally perform the same in question and answer generation. More verbose as you increase in parameters. What needs more specificity is the prompt engineering during api requests.&lt;/p&gt;\\n\\n&lt;p&gt;My general pipeline goes something like this:&lt;/p&gt;\\n\\n&lt;p&gt;large textbook --&amp;gt; chunk into paragraphs (token amounts might vary) --&amp;gt; locallm summarizes chunk --&amp;gt; prompt locallm to generate three questions based on summarization --&amp;gt; prompt locallm to generate three answers based on questions, summarization, &amp;amp; chunk.&lt;/p&gt;\\n\\n&lt;p&gt;csv output: [chunk text][summary][question][answer]&lt;/p&gt;\\n\\n&lt;p&gt;This is helpful to make sure the answers are grounded in the context and not just made up. For human fact checking.&lt;/p&gt;\\n\\n&lt;p&gt;Most of my pipeline deals with history texts, so it might not be the same in your use case. I would say it might be less about the model you select, and more about how you construct the pipeline for q&amp;amp;a generation.&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;ve used a intel arc750 gpu with 8GB using LM Studio&amp;#39;s api server to run these question and answers format. So your gpu and RAM should be fine, depending on the model quants. But I then would use a local instance of jupyter notebooks to run the python script for requests to LM Studio. &lt;/p&gt;\\n\\n&lt;p&gt;Hope that helps, and if you need any specific help, just drop me a line.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lo1d8t/models_for_generating_qapairs_from_text_dataset/n0jndkh/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751272587,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lo1d8t","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0jgzt1","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Sasikuttan2163","can_mod_post":false,"created_utc":1751268639,"send_replies":true,"parent_id":"t3_1lo1d8t","score":1,"author_fullname":"t2_giwl2ppl","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"If you need more details please feel free to ask questions in the comments, I'll try to give the answers.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0jgzt1","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;If you need more details please feel free to ask questions in the comments, I&amp;#39;ll try to give the answers.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lo1d8t/models_for_generating_qapairs_from_text_dataset/n0jgzt1/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751268639,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lo1d8t","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0k48x4","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Sasikuttan2163","can_mod_post":false,"created_utc":1751282133,"send_replies":true,"parent_id":"t1_n0jx8p9","score":1,"author_fullname":"t2_giwl2ppl","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yeah I am using the 4 bit quantised version of Hermes 3 to avoid filling up my whole VRAM. Any resources where I can look into prompts proven to work for this purpose which I can adapt?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0k48x4","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yeah I am using the 4 bit quantised version of Hermes 3 to avoid filling up my whole VRAM. Any resources where I can look into prompts proven to work for this purpose which I can adapt?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lo1d8t","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lo1d8t/models_for_generating_qapairs_from_text_dataset/n0k48x4/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751282133,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0jx8p9","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Longjumpingfish0403","can_mod_post":false,"created_utc":1751278502,"send_replies":true,"parent_id":"t3_1lo1d8t","score":1,"author_fullname":"t2_jarttha4","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"If you're aiming for better performance on RTX 4060, you might want to explore quantized models or explore [GPTQ](https://huggingface.co/blog/4bit-transformers-better-faster-cheaper) for efficiency. Also, try using dynamic chunk sizes based on paragraph structure to maintain context. If your model struggles with prompt adherence, refining prompt templates or experimenting with length constraints in prompts can help. This might boost relevance without heavily taxing your hardware.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0jx8p9","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;If you&amp;#39;re aiming for better performance on RTX 4060, you might want to explore quantized models or explore &lt;a href=\\"https://huggingface.co/blog/4bit-transformers-better-faster-cheaper\\"&gt;GPTQ&lt;/a&gt; for efficiency. Also, try using dynamic chunk sizes based on paragraph structure to maintain context. If your model struggles with prompt adherence, refining prompt templates or experimenting with length constraints in prompts can help. This might boost relevance without heavily taxing your hardware.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lo1d8t/models_for_generating_qapairs_from_text_dataset/n0jx8p9/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751278502,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lo1d8t","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0p05zk","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DeProgrammer99","can_mod_post":false,"created_utc":1751338390,"send_replies":true,"parent_id":"t3_1lo1d8t","score":1,"author_fullname":"t2_w4j8t","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I gave rough evaluations of the models I tested for flash card generation on RTX 4060 Ti here: [https://github.com/dpmm99/Faxtract/blob/main/appsettings.json#L11](https://github.com/dpmm99/Faxtract/blob/main/appsettings.json#L11)\\n\\n&gt;phi-4-Q4\\\\_K\\\\_M.gguf is fairly good.  \\nQwen3-14B-UD-Q5\\\\_K\\\\_XL.gguf is very good.  \\nDeepSeek-R1-0528-Qwen3-8B-Q6\\\\_K.gguf is questionable.  \\nMistral-Small-3.2-24B-Instruct-2506-UD-Q5\\\\_K\\\\_XL is also good, but you shouldn't quantize the KV cache; its quality greatly suffers as context grows when KV is quantized.  \\nI tried Gemma 3 27B (the 4-bit QAT one), Qwen3 30B-A3B Q6\\\\_K, and Qwen3 4B Q6\\\\_K, but they're all far worse at following the instructions than Phi-4, and only Qwen3-4B is anywhere near as fast.  \\nMistral-Small-24B-Instruct-2501-Q5\\\\_K\\\\_M.gguf is also single-digit tokens/second with a big batch.  \\nAlso tried Qwen3-32B-UD-Q2\\\\_K\\\\_XL.gguf but it was super slow despite being quite small because it used shared sysmem; turning that off made it fast.\\n\\n(Except Mistral. I think I ran that on my RX 7900 XTX.)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0p05zk","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I gave rough evaluations of the models I tested for flash card generation on RTX 4060 Ti here: &lt;a href=\\"https://github.com/dpmm99/Faxtract/blob/main/appsettings.json#L11\\"&gt;https://github.com/dpmm99/Faxtract/blob/main/appsettings.json#L11&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;phi-4-Q4_K_M.gguf is fairly good.&lt;br/&gt;\\nQwen3-14B-UD-Q5_K_XL.gguf is very good.&lt;br/&gt;\\nDeepSeek-R1-0528-Qwen3-8B-Q6_K.gguf is questionable.&lt;br/&gt;\\nMistral-Small-3.2-24B-Instruct-2506-UD-Q5_K_XL is also good, but you shouldn&amp;#39;t quantize the KV cache; its quality greatly suffers as context grows when KV is quantized.&lt;br/&gt;\\nI tried Gemma 3 27B (the 4-bit QAT one), Qwen3 30B-A3B Q6_K, and Qwen3 4B Q6_K, but they&amp;#39;re all far worse at following the instructions than Phi-4, and only Qwen3-4B is anywhere near as fast.&lt;br/&gt;\\nMistral-Small-24B-Instruct-2501-Q5_K_M.gguf is also single-digit tokens/second with a big batch.&lt;br/&gt;\\nAlso tried Qwen3-32B-UD-Q2_K_XL.gguf but it was super slow despite being quite small because it used shared sysmem; turning that off made it fast.&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;(Except Mistral. I think I ran that on my RX 7900 XTX.)&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lo1d8t/models_for_generating_qapairs_from_text_dataset/n0p05zk/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751338390,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lo1d8t","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0l53t4","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"umtksa","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0ktb1f","score":3,"author_fullname":"t2_ut19jt","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"14B at f32  for the Turkish domain quantization really degrades the quality","edited":false,"author_flair_css_class":null,"name":"t1_n0l53t4","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;14B at f32  for the Turkish domain quantization really degrades the quality&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lo1d8t","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lo1d8t/models_for_generating_qapairs_from_text_dataset/n0l53t4/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751295340,"author_flair_text":null,"collapsed":false,"created_utc":1751295340,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0l8d88","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Sasikuttan2163","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0l5uyy","score":1,"author_fullname":"t2_giwl2ppl","approved_by":null,"mod_note":null,"all_awardings":[],"body":"Unfortunately my data is not online, thanks for the suggestions though!","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n0l8d88","is_submitter":true,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Unfortunately my data is not online, thanks for the suggestions though!&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lo1d8t","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lo1d8t/models_for_generating_qapairs_from_text_dataset/n0l8d88/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751296295,"author_flair_text":null,"treatment_tags":[],"created_utc":1751296295,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0l5uyy","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"umtksa","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0ktb1f","score":1,"author_fullname":"t2_ut19jt","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"let me give you a hint Brave browser have some integrated ai's if your data is online you can try this models for that task I just write my promt and add desired format  \\n{\\"input\\":\\"question\\", \\"output\\":\\"answer\\"}\\n\\nhttps://preview.redd.it/obsk1m1hw2af1.png?width=768&amp;format=png&amp;auto=webp&amp;s=99014c4302790ec69825e3cf30c1e74d54d0f7d5","edited":false,"author_flair_css_class":null,"name":"t1_n0l5uyy","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;let me give you a hint Brave browser have some integrated ai&amp;#39;s if your data is online you can try this models for that task I just write my promt and add desired format&lt;br/&gt;\\n{&amp;quot;input&amp;quot;:&amp;quot;question&amp;quot;, &amp;quot;output&amp;quot;:&amp;quot;answer&amp;quot;}&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://preview.redd.it/obsk1m1hw2af1.png?width=768&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=99014c4302790ec69825e3cf30c1e74d54d0f7d5\\"&gt;https://preview.redd.it/obsk1m1hw2af1.png?width=768&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=99014c4302790ec69825e3cf30c1e74d54d0f7d5&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lo1d8t","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lo1d8t/models_for_generating_qapairs_from_text_dataset/n0l5uyy/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751295560,"media_metadata":{"obsk1m1hw2af1":{"status":"valid","e":"Image","m":"image/png","p":[{"y":178,"x":108,"u":"https://preview.redd.it/obsk1m1hw2af1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=a027291e9447be216d66fb3b632a46beb6fcac3a"},{"y":357,"x":216,"u":"https://preview.redd.it/obsk1m1hw2af1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=d3385f0d321faf1ac8e201c5aae6f43527cf3b71"},{"y":529,"x":320,"u":"https://preview.redd.it/obsk1m1hw2af1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=b3ee14a6966c98563fa05ae1cb91db23a61bc4c0"},{"y":1058,"x":640,"u":"https://preview.redd.it/obsk1m1hw2af1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=efe62b124fbccfc61556bd3aadd5d76f58bb0199"}],"s":{"y":1270,"x":768,"u":"https://preview.redd.it/obsk1m1hw2af1.png?width=768&amp;format=png&amp;auto=webp&amp;s=99014c4302790ec69825e3cf30c1e74d54d0f7d5"},"id":"obsk1m1hw2af1"}},"author_flair_text":null,"collapsed":false,"created_utc":1751295560,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0ktb1f","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Sasikuttan2163","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0ksfki","score":1,"author_fullname":"t2_giwl2ppl","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"How many parameters and what quantization did you use?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0ktb1f","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;How many parameters and what quantization did you use?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lo1d8t","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lo1d8t/models_for_generating_qapairs_from_text_dataset/n0ktb1f/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751291744,"author_flair_text":null,"treatment_tags":[],"created_utc":1751291744,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0ksfki","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"umtksa","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0ko1x3","score":2,"author_fullname":"t2_ut19jt","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I use it to generate QnA pairs from wikipedia page but I tried in turkish and yes 3 is really good at turkish then 2.5","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0ksfki","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I use it to generate QnA pairs from wikipedia page but I tried in turkish and yes 3 is really good at turkish then 2.5&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lo1d8t","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lo1d8t/models_for_generating_qapairs_from_text_dataset/n0ksfki/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751291472,"author_flair_text":null,"treatment_tags":[],"created_utc":1751291472,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n0ko1x3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Sasikuttan2163","can_mod_post":false,"created_utc":1751290018,"send_replies":true,"parent_id":"t1_n0jya88","score":1,"author_fullname":"t2_giwl2ppl","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Is Qwen3 that big of an upgrade from 2.5? I was initially using Qwen 2.5 7B with 4 bit quant but it didn't give me good results for the same prompt.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0ko1x3","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Is Qwen3 that big of an upgrade from 2.5? I was initially using Qwen 2.5 7B with 4 bit quant but it didn&amp;#39;t give me good results for the same prompt.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lo1d8t","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lo1d8t/models_for_generating_qapairs_from_text_dataset/n0ko1x3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751290018,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0jya88","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"umtksa","can_mod_post":false,"created_utc":1751279087,"send_replies":true,"parent_id":"t3_1lo1d8t","score":1,"author_fullname":"t2_ut19jt","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Qwen3","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0jya88","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Qwen3&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lo1d8t/models_for_generating_qapairs_from_text_dataset/n0jya88/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751279087,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lo1d8t","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),n=()=>e.jsx(t,{data:a});export{n as default};
