import{j as e}from"./index-CeRg6Q3f.js";import{R as l}from"./RedditPostRenderer-D7n1g-D8.js";import"./index-DPToWe3n.js";const t=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"I've pre ordered 3 Spark units which will be connected via infiniband at 200 GB/s. While not cheap, all other options that are comperable seem to be much more expensive. AMD's max+ is cheaper, but also less capable, particularly with interconnect. Mac's equivalent has much better memory bandwidth, but that's about it. Tenstorrent's Blackhole is tempting, but lack of literature is too much of a risk for me. I just wanted to check to see if I was missing a better option.","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Local 405B Model on 3 DGX Spark units.","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lpi0mn","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.83,"author_flair_background_color":null,"subreddit_type":"public","ups":4,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_ly0a55dz","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":4,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1751415915,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;ve pre ordered 3 Spark units which will be connected via infiniband at 200 GB/s. While not cheap, all other options that are comperable seem to be much more expensive. AMD&amp;#39;s max+ is cheaper, but also less capable, particularly with interconnect. Mac&amp;#39;s equivalent has much better memory bandwidth, but that&amp;#39;s about it. Tenstorrent&amp;#39;s Blackhole is tempting, but lack of literature is too much of a risk for me. I just wanted to check to see if I was missing a better option.&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1lpi0mn","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"elephantgif","discussion_type":null,"num_comments":10,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lpi0mn/local_405b_model_on_3_dgx_spark_units/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lpi0mn/local_405b_model_on_3_dgx_spark_units/","subreddit_subscribers":493457,"created_utc":1751415915,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0w2d5g","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Conscious_Cut_6144","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0v0gjb","score":1,"author_fullname":"t2_9hl4ymvj","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Pro 6000’s are 96GB each, you just need 4 of them total.\\nGrab the max-q’s and you could do in a regular atx desktop / server.\\n\\n4 pro 6000’s is going to be an order of magnitude faster and better supported.\\n\\nIf you actually plan on running 405b for some reason, realize that model is not moe and will run quite slow on the m3 ultra and the dgx spark.\\n\\nThat said there are like 5 400b class moe models that would run fine on a Mac or dgx spark.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0w2d5g","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Pro 6000’s are 96GB each, you just need 4 of them total.\\nGrab the max-q’s and you could do in a regular atx desktop / server.&lt;/p&gt;\\n\\n&lt;p&gt;4 pro 6000’s is going to be an order of magnitude faster and better supported.&lt;/p&gt;\\n\\n&lt;p&gt;If you actually plan on running 405b for some reason, realize that model is not moe and will run quite slow on the m3 ultra and the dgx spark.&lt;/p&gt;\\n\\n&lt;p&gt;That said there are like 5 400b class moe models that would run fine on a Mac or dgx spark.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lpi0mn","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpi0mn/local_405b_model_on_3_dgx_spark_units/n0w2d5g/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751431838,"author_flair_text":null,"treatment_tags":[],"created_utc":1751431838,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0v0gjb","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"elephantgif","can_mod_post":false,"created_utc":1751417343,"send_replies":true,"parent_id":"t1_n0uxe8k","score":2,"author_fullname":"t2_ly0a55dz","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"It would take three times as many, and each 6000 is twice as much. Plus, I'd have to house them.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0v0gjb","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It would take three times as many, and each 6000 is twice as much. Plus, I&amp;#39;d have to house them.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lpi0mn","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpi0mn/local_405b_model_on_3_dgx_spark_units/n0v0gjb/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751417343,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0v0662","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"colin_colout","can_mod_post":false,"created_utc":1751417241,"send_replies":true,"parent_id":"t1_n0uxe8k","score":1,"author_fullname":"t2_14l4ya","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"but I'm here for it. I preordered the framework desktop even though m4 exists. \\n\\nWe could all just use openai or anthropic for cheaper AND better quality models than trying to run it ourselves.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0v0662","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;but I&amp;#39;m here for it. I preordered the framework desktop even though m4 exists. &lt;/p&gt;\\n\\n&lt;p&gt;We could all just use openai or anthropic for cheaper AND better quality models than trying to run it ourselves.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lpi0mn","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpi0mn/local_405b_model_on_3_dgx_spark_units/n0v0662/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751417241,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0uxe8k","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"FireWoIf","can_mod_post":false,"created_utc":1751416247,"send_replies":true,"parent_id":"t3_1lpi0mn","score":6,"author_fullname":"t2_2uk7f1v5","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Seems silly when the RTX PRO 6000 exists","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0uxe8k","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Seems silly when the RTX PRO 6000 exists&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpi0mn/local_405b_model_on_3_dgx_spark_units/n0uxe8k/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751416247,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lpi0mn","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0v3gbf","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"_xulion","can_mod_post":false,"created_utc":1751418405,"send_replies":true,"parent_id":"t3_1lpi0mn","score":2,"author_fullname":"t2_a1dvxm4d","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I think each can do 1P TOPs FP4? that's pretty impressive for a small box. You now have 3 boxes with 384G of ram and 3P TOPs. I'd love to know how well it runs. Pls share when you have some result!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0v3gbf","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I think each can do 1P TOPs FP4? that&amp;#39;s pretty impressive for a small box. You now have 3 boxes with 384G of ram and 3P TOPs. I&amp;#39;d love to know how well it runs. Pls share when you have some result!&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpi0mn/local_405b_model_on_3_dgx_spark_units/n0v3gbf/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751418405,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lpi0mn","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0vcinv","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"eloquentemu","can_mod_post":false,"created_utc":1751421596,"send_replies":true,"parent_id":"t3_1lpi0mn","score":1,"author_fullname":"t2_lpdsy","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I'm assuming that your 200 *GB/s* is *Gb/s*?  I'm curious where you've seen support for that... Yeah, it has a ConnectX-7 and that should, I've only seen the connectivity advertised as 10GbE though PHY says 100GbE but ConnectX-7 should support 400GbE so I'm not sure what to believe (why is this not clearly stated!?).\\n\\nAnyways, to sanity check it looks like $12k to run the model at q4 at ~3.5 t/s if tensor parallelism works perfectly?  (Is Llama 405B still worth that vs Deepseek or something?)\\n\\n- As I understand it, you need 2 or 4 GPUs to run tensor parallelism but I haven't used it extensively.  So if you want to actually get your 3.5t/s (which would be 4.6t/s).  You could always always run in layer/pipeline parallel mode but that wouldn't multiply your memory bandwidth / inference speed.  You'd be able to batch multiple inferences but would get a peak of ~1.2 t/s.\\n- What's your concern with the Mac M3 Ultra?  It's not a powerhouse (e.g. if you wanted to run diffusion or something) but has comparable memory bandwidth and is cheaper.\\n- The AMD AI Max is probably out as the connectivity isn't great.  It only has PCIe 4 (x4 in the the implementations I've seen) so you're limited to like 40GbE if you can make that work.\\n- If you're only planning on running layer/pipeline parallel, you could match the 273GBps you could meet that spec with an 8 channel DDR5-4800 server system $12k buys one hell of a server and even a 5090 or two.\\n\\nOf course, they have good compute and connectivity but if you want to run 405B you're basically going to need to plan around memory bandwidth.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0vcinv","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;m assuming that your 200 &lt;em&gt;GB/s&lt;/em&gt; is &lt;em&gt;Gb/s&lt;/em&gt;?  I&amp;#39;m curious where you&amp;#39;ve seen support for that... Yeah, it has a ConnectX-7 and that should, I&amp;#39;ve only seen the connectivity advertised as 10GbE though PHY says 100GbE but ConnectX-7 should support 400GbE so I&amp;#39;m not sure what to believe (why is this not clearly stated!?).&lt;/p&gt;\\n\\n&lt;p&gt;Anyways, to sanity check it looks like $12k to run the model at q4 at ~3.5 t/s if tensor parallelism works perfectly?  (Is Llama 405B still worth that vs Deepseek or something?)&lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;As I understand it, you need 2 or 4 GPUs to run tensor parallelism but I haven&amp;#39;t used it extensively.  So if you want to actually get your 3.5t/s (which would be 4.6t/s).  You could always always run in layer/pipeline parallel mode but that wouldn&amp;#39;t multiply your memory bandwidth / inference speed.  You&amp;#39;d be able to batch multiple inferences but would get a peak of ~1.2 t/s.&lt;/li&gt;\\n&lt;li&gt;What&amp;#39;s your concern with the Mac M3 Ultra?  It&amp;#39;s not a powerhouse (e.g. if you wanted to run diffusion or something) but has comparable memory bandwidth and is cheaper.&lt;/li&gt;\\n&lt;li&gt;The AMD AI Max is probably out as the connectivity isn&amp;#39;t great.  It only has PCIe 4 (x4 in the the implementations I&amp;#39;ve seen) so you&amp;#39;re limited to like 40GbE if you can make that work.&lt;/li&gt;\\n&lt;li&gt;If you&amp;#39;re only planning on running layer/pipeline parallel, you could match the 273GBps you could meet that spec with an 8 channel DDR5-4800 server system $12k buys one hell of a server and even a 5090 or two.&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;p&gt;Of course, they have good compute and connectivity but if you want to run 405B you&amp;#39;re basically going to need to plan around memory bandwidth.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpi0mn/local_405b_model_on_3_dgx_spark_units/n0vcinv/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751421596,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lpi0mn","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0vkuin","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Ok_Warning2146","can_mod_post":false,"created_utc":1751424536,"send_replies":true,"parent_id":"t3_1lpi0mn","score":1,"author_fullname":"t2_s6sfw4yy","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Isn't it m3 ultra 512gb abt the same price?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0vkuin","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Isn&amp;#39;t it m3 ultra 512gb abt the same price?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpi0mn/local_405b_model_on_3_dgx_spark_units/n0vkuin/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751424536,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lpi0mn","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0w2zy2","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"elephantgif","can_mod_post":false,"created_utc":1751432136,"send_replies":true,"parent_id":"t1_n0vnygg","score":1,"author_fullname":"t2_ly0a55dz","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"M3 hast the edge in bandwidth, but the Spark processors have way more raw compute power. Plus expandability.If I wanted to introduce models to run concurrently down the road, infiniband is far superior to Thunderbolt for integration.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0w2zy2","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;M3 hast the edge in bandwidth, but the Spark processors have way more raw compute power. Plus expandability.If I wanted to introduce models to run concurrently down the road, infiniband is far superior to Thunderbolt for integration.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lpi0mn","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpi0mn/local_405b_model_on_3_dgx_spark_units/n0w2zy2/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751432136,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0w38bx","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Conscious_Cut_6144","can_mod_post":false,"created_utc":1751432248,"send_replies":true,"parent_id":"t1_n0vnygg","score":1,"author_fullname":"t2_9hl4ymvj","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I have no idea what 3 dgx sparks will do irl,\\nBut 405b will bring a m3 ultra to its knees. Much harder to run than deepseek.\\n\\n(Theoretical max speed for 405b mlx 4bit with 800GB/s is under 3.5 T/s, that is with 0 context length)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0w38bx","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I have no idea what 3 dgx sparks will do irl,\\nBut 405b will bring a m3 ultra to its knees. Much harder to run than deepseek.&lt;/p&gt;\\n\\n&lt;p&gt;(Theoretical max speed for 405b mlx 4bit with 800GB/s is under 3.5 T/s, that is with 0 context length)&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lpi0mn","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpi0mn/local_405b_model_on_3_dgx_spark_units/n0w38bx/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751432248,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0vnygg","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"YRUTROLLINGURSELF","can_mod_post":false,"created_utc":1751425717,"send_replies":true,"parent_id":"t3_1lpi0mn","score":1,"author_fullname":"t2_jdbx7","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"3x DGX - 384@200 for $9k\\n\\nm3 ultra - 512@800 for $9.5k\\n\\nare my numbers wrong or else why would you choose this willingly","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0vnygg","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;3x DGX - 384@200 for $9k&lt;/p&gt;\\n\\n&lt;p&gt;m3 ultra - 512@800 for $9.5k&lt;/p&gt;\\n\\n&lt;p&gt;are my numbers wrong or else why would you choose this willingly&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpi0mn/local_405b_model_on_3_dgx_spark_units/n0vnygg/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751425717,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lpi0mn","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),n=()=>e.jsx(l,{data:t});export{n as default};
