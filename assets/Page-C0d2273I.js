import{j as e}from"./index-CWmJdUH_.js";import{R as t}from"./RedditPostRenderer-D2iunoQ9.js";import"./index-BCg9RP6g.js";const a=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Hallucination is a real problem with LLMs but I wonder is it such a hard problem to assign a confidence value to an inference result?","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Why are LLMs not able to give an estimate on their own confidence or say that they are not sure about something?","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1m4z64o","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.5,"author_flair_background_color":null,"subreddit_type":"public","ups":0,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_bwdb8qqfj","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":0,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1753043256,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Hallucination is a real problem with LLMs but I wonder is it such a hard problem to assign a confidence value to an inference result?&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":true,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1m4z64o","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"MarinatedPickachu","discussion_type":null,"num_comments":63,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1m4z64o/why_are_llms_not_able_to_give_an_estimate_on/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1m4z64o/why_are_llms_not_able_to_give_an_estimate_on/","subreddit_subscribers":502273,"created_utc":1753043256,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n494jhj","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"simracerman","can_mod_post":false,"created_utc":1753055305,"send_replies":true,"parent_id":"t1_n4869qi","score":3,"author_fullname":"t2_vbzgnic","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I said this in reply to another post. Until we have our agents dialed in to run multiple iterations on an LLMs response and find the best mixture of those and output that (like HDR in photos), we won’t have an end to the hallucination issue.\\n\\nThat’s one solution. The other would be to ditch the Transformers architecture in favor of another one that an LLM is not simply a word generator, but a judge too. Similar to how most logical humans think.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n494jhj","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I said this in reply to another post. Until we have our agents dialed in to run multiple iterations on an LLMs response and find the best mixture of those and output that (like HDR in photos), we won’t have an end to the hallucination issue.&lt;/p&gt;\\n\\n&lt;p&gt;That’s one solution. The other would be to ditch the Transformers architecture in favor of another one that an LLM is not simply a word generator, but a judge too. Similar to how most logical humans think.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m4z64o","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4z64o/why_are_llms_not_able_to_give_an_estimate_on/n494jhj/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753055305,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n48vgpn","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Raz4r","can_mod_post":false,"created_utc":1753052152,"send_replies":true,"parent_id":"t1_n4869qi","score":4,"author_fullname":"t2_108sov","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"You're assuming that the logits are well-calibrated, which isn't always the case. Multiple papers have investigated this issue, and it is far from simple to solve.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n48vgpn","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You&amp;#39;re assuming that the logits are well-calibrated, which isn&amp;#39;t always the case. Multiple papers have investigated this issue, and it is far from simple to solve.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m4z64o","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4z64o/why_are_llms_not_able_to_give_an_estimate_on/n48vgpn/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753052152,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4bbgpy","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MarinatedPickachu","can_mod_post":false,"send_replies":true,"parent_id":"t1_n48y5z8","score":1,"author_fullname":"t2_bwdb8qqfj","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"&gt; There's a difference between confidence and probabilility\\n\\nWell i'm obviously not talking about confidence as a \\"feeling\\" as in self-confidence for example, but about confidence as bayesian probability models it  - and that probability (which has nothing to do with correctness mind you) should be derivable from the inference process.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4bbgpy","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;There&amp;#39;s a difference between confidence and probabilility&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;Well i&amp;#39;m obviously not talking about confidence as a &amp;quot;feeling&amp;quot; as in self-confidence for example, but about confidence as bayesian probability models it  - and that probability (which has nothing to do with correctness mind you) should be derivable from the inference process.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m4z64o","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4z64o/why_are_llms_not_able_to_give_an_estimate_on/n4bbgpy/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753091592,"author_flair_text":null,"treatment_tags":[],"created_utc":1753091592,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n48y5z8","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Zc5Gwu","can_mod_post":false,"send_replies":true,"parent_id":"t1_n489n0w","score":4,"author_fullname":"t2_67qrvlir","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"There's a difference between confidence and probability. As some others have mentioned probability requires calibration. There are other machine learning algorithms that natively output probabilities but they haven't seen the success that neural networks have. Neural networks have non-linearities due to the activation functions that make assigning probabilities difficult.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n48y5z8","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;There&amp;#39;s a difference between confidence and probability. As some others have mentioned probability requires calibration. There are other machine learning algorithms that natively output probabilities but they haven&amp;#39;t seen the success that neural networks have. Neural networks have non-linearities due to the activation functions that make assigning probabilities difficult.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m4z64o","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4z64o/why_are_llms_not_able_to_give_an_estimate_on/n48y5z8/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753053079,"author_flair_text":null,"treatment_tags":[],"created_utc":1753053079,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4a39n5","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"mrjackspade","can_mod_post":false,"send_replies":false,"parent_id":"t1_n489n0w","score":2,"author_fullname":"t2_5ow51","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"A flatter logit distribution generally correlates with a lower confidence, but theres two issues with that.\\n\\n1. A flatter logit distribution also correlates with many possible valid answers, and logit distribution alone cant tell you the difference. This is becomes more of an issue with creative writing, where logit distributions will *routinely* flatten during generation, since there is no \\"right answer\\" when generating a story.\\n\\n2. Confidence via logit values is only exposed when that token is reached. If you ask a model something like \\"Do you know {Some fact}\\" the model might start with \\"Yes! The answer is \\" (99%) followed by a flat distribution because it doesn't know. But you've already answered \\"Yes\\" because the model doesn't actually \\"think\\" further ahead than a single token at a time.\\n\\n3. The model can be confident, and wrong. That happens all the time. If you train the model on 1, 2, 3, 4, 5, 6, 7, G, 9, 10, the model might *very confidently* suggest 8 instead of G. Because the rest of the pattern makes perfect logical sense, so of course there's a 90% chance that the 8th character is \\"8\\", but that doesn't mean that its *right*, it just means the model learned the pattern in a way that minimized loss, but isn't guaranteed to actually be the *truth*","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n4a39n5","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;A flatter logit distribution generally correlates with a lower confidence, but theres two issues with that.&lt;/p&gt;\\n\\n&lt;ol&gt;\\n&lt;li&gt;&lt;p&gt;A flatter logit distribution also correlates with many possible valid answers, and logit distribution alone cant tell you the difference. This is becomes more of an issue with creative writing, where logit distributions will &lt;em&gt;routinely&lt;/em&gt; flatten during generation, since there is no &amp;quot;right answer&amp;quot; when generating a story.&lt;/p&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;p&gt;Confidence via logit values is only exposed when that token is reached. If you ask a model something like &amp;quot;Do you know {Some fact}&amp;quot; the model might start with &amp;quot;Yes! The answer is &amp;quot; (99%) followed by a flat distribution because it doesn&amp;#39;t know. But you&amp;#39;ve already answered &amp;quot;Yes&amp;quot; because the model doesn&amp;#39;t actually &amp;quot;think&amp;quot; further ahead than a single token at a time.&lt;/p&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;p&gt;The model can be confident, and wrong. That happens all the time. If you train the model on 1, 2, 3, 4, 5, 6, 7, G, 9, 10, the model might &lt;em&gt;very confidently&lt;/em&gt; suggest 8 instead of G. Because the rest of the pattern makes perfect logical sense, so of course there&amp;#39;s a 90% chance that the 8th character is &amp;quot;8&amp;quot;, but that doesn&amp;#39;t mean that its &lt;em&gt;right&lt;/em&gt;, it just means the model learned the pattern in a way that minimized loss, but isn&amp;#39;t guaranteed to actually be the &lt;em&gt;truth&lt;/em&gt;&lt;/p&gt;&lt;/li&gt;\\n&lt;/ol&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m4z64o","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4z64o/why_are_llms_not_able_to_give_an_estimate_on/n4a39n5/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753068419,"author_flair_text":null,"treatment_tags":[],"created_utc":1753068419,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n489n0w","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MarinatedPickachu","can_mod_post":false,"created_utc":1753044932,"send_replies":true,"parent_id":"t1_n4869qi","score":1,"author_fullname":"t2_bwdb8qqfj","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Of course - but the \\"next logical token\\" is the one with highest probability - but expectancy value is not the whole story of probability, you actually have a distribution with deviation and I could imagine that a wider deviation correlates with less certainty/more interpolation for example","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n489n0w","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Of course - but the &amp;quot;next logical token&amp;quot; is the one with highest probability - but expectancy value is not the whole story of probability, you actually have a distribution with deviation and I could imagine that a wider deviation correlates with less certainty/more interpolation for example&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m4z64o","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4z64o/why_are_llms_not_able_to_give_an_estimate_on/n489n0w/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753044932,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":1,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4869qi","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Lesser-than","can_mod_post":false,"created_utc":1753043899,"send_replies":true,"parent_id":"t3_1m4z64o","score":31,"author_fullname":"t2_98d256k","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"confidently wrong is still a measure of confidence, you can check the logits they are indeed confident in their predictions of next tokens. Its not a matter of LLMs bullshitting their way through a response it just the next logical token available to them.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4869qi","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;confidently wrong is still a measure of confidence, you can check the logits they are indeed confident in their predictions of next tokens. Its not a matter of LLMs bullshitting their way through a response it just the next logical token available to them.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4z64o/why_are_llms_not_able_to_give_an_estimate_on/n4869qi/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753043899,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m4z64o","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":31}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n489740","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Affectionate-Cap-600","can_mod_post":false,"created_utc":1753044797,"send_replies":true,"parent_id":"t3_1m4z64o","score":11,"author_fullname":"t2_5oltmr5b","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"they are not trained a lot to say 'I don't know' during SFT or RL.\\n\\nand anyway, even when trained to do that, this is something a model can't generalize well (I mean, it seems that they can't do that, and without any change to the current understanding, they factually can't).\\n\\nthe reason (from my understanding) is that a model basically doesn't know what informations it have. when during post training they 'see' some examples  of a input/completion pairs where the completion is 'I don't know', the only logical connections between those is 'given the models knowledge, it should not answer the question', and that is the only logical / semantical connections they model should learn, but it require an information that the models can't know.\\nfor this reason, in order to teach to a model to say 'I don't know' you should work a posteriori: first of all test the model and you understand what the models doesn't know, and then create pairs where the output for question related to such topics is a refusal. You can't in any way rely on an \\"internal understanding\\" of what knowledge does the model have. and even doing that, from the perspective of the models, those set of examples about topics doesn't have anything in common (the only relation is that the models doesn't know the answer to such questions, but this is a aspects that the models can't see), so another provale outcome is that the model start to search other connections between those questions. that's the reason because, even if you encounter a situation where the models output something like 'idk', if you reframes the question it will try to answer and hallucinate.\\n\\nthis plus what other user said: the cumulative logit log prob for a correct answer is not higher that one for an hallucination","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n489740","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;they are not trained a lot to say &amp;#39;I don&amp;#39;t know&amp;#39; during SFT or RL.&lt;/p&gt;\\n\\n&lt;p&gt;and anyway, even when trained to do that, this is something a model can&amp;#39;t generalize well (I mean, it seems that they can&amp;#39;t do that, and without any change to the current understanding, they factually can&amp;#39;t).&lt;/p&gt;\\n\\n&lt;p&gt;the reason (from my understanding) is that a model basically doesn&amp;#39;t know what informations it have. when during post training they &amp;#39;see&amp;#39; some examples  of a input/completion pairs where the completion is &amp;#39;I don&amp;#39;t know&amp;#39;, the only logical connections between those is &amp;#39;given the models knowledge, it should not answer the question&amp;#39;, and that is the only logical / semantical connections they model should learn, but it require an information that the models can&amp;#39;t know.\\nfor this reason, in order to teach to a model to say &amp;#39;I don&amp;#39;t know&amp;#39; you should work a posteriori: first of all test the model and you understand what the models doesn&amp;#39;t know, and then create pairs where the output for question related to such topics is a refusal. You can&amp;#39;t in any way rely on an &amp;quot;internal understanding&amp;quot; of what knowledge does the model have. and even doing that, from the perspective of the models, those set of examples about topics doesn&amp;#39;t have anything in common (the only relation is that the models doesn&amp;#39;t know the answer to such questions, but this is a aspects that the models can&amp;#39;t see), so another provale outcome is that the model start to search other connections between those questions. that&amp;#39;s the reason because, even if you encounter a situation where the models output something like &amp;#39;idk&amp;#39;, if you reframes the question it will try to answer and hallucinate.&lt;/p&gt;\\n\\n&lt;p&gt;this plus what other user said: the cumulative logit log prob for a correct answer is not higher that one for an hallucination&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4z64o/why_are_llms_not_able_to_give_an_estimate_on/n489740/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753044797,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m4z64o","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":11}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n48jr36","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Opposite_Answer_287","can_mod_post":false,"send_replies":true,"parent_id":"t1_n48hu9o","score":2,"author_fullname":"t2_tpd3gspjl","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"My pleasure! Feel free to reach out if you have any questions!","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n48jr36","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;My pleasure! Feel free to reach out if you have any questions!&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m4z64o","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4z64o/why_are_llms_not_able_to_give_an_estimate_on/n48jr36/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753048150,"author_flair_text":null,"treatment_tags":[],"created_utc":1753048150,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n48hu9o","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MarinatedPickachu","can_mod_post":false,"created_utc":1753047531,"send_replies":true,"parent_id":"t1_n48e04t","score":2,"author_fullname":"t2_bwdb8qqfj","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Ooooh yes! Thank you!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n48hu9o","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Ooooh yes! Thank you!&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m4z64o","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4z64o/why_are_llms_not_able_to_give_an_estimate_on/n48hu9o/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753047531,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4agdr9","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"sbs1799","can_mod_post":false,"created_utc":1753074277,"send_replies":true,"parent_id":"t1_n48e04t","score":1,"author_fullname":"t2_769nh3nc","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Uqlm seems simple and awesome! Are there similar tools/frameworks?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4agdr9","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Uqlm seems simple and awesome! Are there similar tools/frameworks?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m4z64o","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4z64o/why_are_llms_not_able_to_give_an_estimate_on/n4agdr9/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753074277,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n48e04t","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Opposite_Answer_287","can_mod_post":false,"created_utc":1753046304,"send_replies":true,"parent_id":"t3_1m4z64o","score":10,"author_fullname":"t2_tpd3gspjl","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"UQLM (uncertainty quantification for language models) is an open source Python library that might give you what you need. It gives response level confidence scores (between 0 and 1) based on response consistency, token probabilities, ensembles, etc. No calibration guarantee (hence not quite likelihoods), but from a ranking perspective they work quite well for detecting incorrect answers based on extensive experiments in the literature.\\n\\nLink to repo: https://github.com/cvs-health/uqlm","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n48e04t","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;UQLM (uncertainty quantification for language models) is an open source Python library that might give you what you need. It gives response level confidence scores (between 0 and 1) based on response consistency, token probabilities, ensembles, etc. No calibration guarantee (hence not quite likelihoods), but from a ranking perspective they work quite well for detecting incorrect answers based on extensive experiments in the literature.&lt;/p&gt;\\n\\n&lt;p&gt;Link to repo: &lt;a href=\\"https://github.com/cvs-health/uqlm\\"&gt;https://github.com/cvs-health/uqlm&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4z64o/why_are_llms_not_able_to_give_an_estimate_on/n48e04t/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753046304,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m4z64o","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":10}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4alyqk","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"fuckAIbruhIhateCorps","can_mod_post":false,"send_replies":true,"parent_id":"t1_n49avwr","score":2,"author_fullname":"t2_rqevuvoku","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"can anyone link to this thing (video? article?)i saw long ago:  \\nit was a comparison between an LLM and a human that an LLM knows too much about its state, and a human doesn't, so we draw the line of consciousness for humans because we inherently do not know anything about our brain. The \\"mystery\\" factor makes us draw conclusions","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n4alyqk","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;can anyone link to this thing (video? article?)i saw long ago:&lt;br/&gt;\\nit was a comparison between an LLM and a human that an LLM knows too much about its state, and a human doesn&amp;#39;t, so we draw the line of consciousness for humans because we inherently do not know anything about our brain. The &amp;quot;mystery&amp;quot; factor makes us draw conclusions&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m4z64o","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4z64o/why_are_llms_not_able_to_give_an_estimate_on/n4alyqk/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753077118,"author_flair_text":null,"treatment_tags":[],"created_utc":1753077118,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n49avwr","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"-p-e-w-","can_mod_post":false,"created_utc":1753057630,"send_replies":true,"parent_id":"t1_n484sus","score":0,"author_fullname":"t2_dkgrhaet","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"The human brain is emitting chemical neurotransmitters and running microcurrents through axons, not “actually thinking”.\\n\\nIt’s a fallacy to assume that you can infer high-level operational limitations from basic functional mechanics in complex systems.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n49avwr","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The human brain is emitting chemical neurotransmitters and running microcurrents through axons, not “actually thinking”.&lt;/p&gt;\\n\\n&lt;p&gt;It’s a fallacy to assume that you can infer high-level operational limitations from basic functional mechanics in complex systems.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m4z64o","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4z64o/why_are_llms_not_able_to_give_an_estimate_on/n49avwr/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753057630,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":1,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n48dr4d","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":"LOW_SCORE","no_follow":true,"author":"AbyssianOne","can_mod_post":false,"created_utc":1753046223,"send_replies":true,"parent_id":"t1_n484sus","score":-11,"author_fullname":"t2_1651c3kskq","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Competing in the IMO takes genuine reasoning. Many things AI are capable of these days do. Your explanation is oversimplified and no longer reflective of reality.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n48dr4d","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Competing in the IMO takes genuine reasoning. Many things AI are capable of these days do. Your explanation is oversimplified and no longer reflective of reality.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":"comment score below threshold","distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m4z64o","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4z64o/why_are_llms_not_able_to_give_an_estimate_on/n48dr4d/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753046223,"author_flair_text":null,"treatment_tags":[],"collapsed":true,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-11}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n48xhss","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"NihilisticAssHat","can_mod_post":false,"created_utc":1753052847,"send_replies":true,"parent_id":"t1_n484sus","score":-1,"author_fullname":"t2_wt2pl4x","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"That's what more advanced models do as well. I'd say it's the material it's trained on. It is trained to act like it knows more than it does, like a child pretending to be an adult, or a politician pretending to understand science.\\n\\nWithout humility post-training, it's emulating something which it doesn't understand.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n48xhss","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;That&amp;#39;s what more advanced models do as well. I&amp;#39;d say it&amp;#39;s the material it&amp;#39;s trained on. It is trained to act like it knows more than it does, like a child pretending to be an adult, or a politician pretending to understand science.&lt;/p&gt;\\n\\n&lt;p&gt;Without humility post-training, it&amp;#39;s emulating something which it doesn&amp;#39;t understand.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m4z64o","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4z64o/why_are_llms_not_able_to_give_an_estimate_on/n48xhss/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753052847,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n48cnna","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"7h3_50urc3","can_mod_post":false,"send_replies":true,"parent_id":"t1_n48cawb","score":4,"author_fullname":"t2_juyf8aer","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"What do you mean? You can batch prompts but as I know not with different seeds during one sequence-processing.","edited":false,"author_flair_css_class":null,"name":"t1_n48cnna","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;What do you mean? You can batch prompts but as I know not with different seeds during one sequence-processing.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1m4z64o","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4z64o/why_are_llms_not_able_to_give_an_estimate_on/n48cnna/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753045875,"author_flair_text":null,"collapsed":false,"created_utc":1753045875,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}}],"before":null}},"user_reports":[],"saved":false,"id":"n48cawb","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MarinatedPickachu","can_mod_post":false,"send_replies":true,"parent_id":"t1_n48bz1b","score":-3,"author_fullname":"t2_bwdb8qqfj","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Yes but the inference framework certainly can output more than just the next token","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n48cawb","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yes but the inference framework certainly can output more than just the next token&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m4z64o","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4z64o/why_are_llms_not_able_to_give_an_estimate_on/n48cawb/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753045763,"author_flair_text":null,"treatment_tags":[],"created_utc":1753045763,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-3}}],"before":null}},"user_reports":[],"saved":false,"id":"n48bz1b","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"7h3_50urc3","can_mod_post":false,"send_replies":true,"parent_id":"t1_n485bxg","score":6,"author_fullname":"t2_juyf8aer","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"LLMs can't weight their answers based on their knowledge. They just predict the next best token like my previous speaker said. LLMs can't know if there are better or worse answers.\\n\\nYou would need to produce several answers with changed seeds to be able to give a rating of the current answer but even this is far from being confidence.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n48bz1b","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;LLMs can&amp;#39;t weight their answers based on their knowledge. They just predict the next best token like my previous speaker said. LLMs can&amp;#39;t know if there are better or worse answers.&lt;/p&gt;\\n\\n&lt;p&gt;You would need to produce several answers with changed seeds to be able to give a rating of the current answer but even this is far from being confidence.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m4z64o","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4z64o/why_are_llms_not_able_to_give_an_estimate_on/n48bz1b/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753045659,"author_flair_text":null,"treatment_tags":[],"created_utc":1753045659,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n48d7n0","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Pvt_Twinkietoes","can_mod_post":false,"send_replies":true,"parent_id":"t1_n48cunu","score":10,"author_fullname":"t2_3k9qfjsr","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"And how would you do that?","edited":false,"author_flair_css_class":null,"name":"t1_n48d7n0","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;And how would you do that?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1m4z64o","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4z64o/why_are_llms_not_able_to_give_an_estimate_on/n48d7n0/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753046052,"author_flair_text":null,"collapsed":false,"created_utc":1753046052,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":10}}],"before":null}},"user_reports":[],"saved":false,"id":"n48cunu","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MarinatedPickachu","can_mod_post":false,"send_replies":true,"parent_id":"t1_n48c4hg","score":-4,"author_fullname":"t2_bwdb8qqfj","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"It's not about factual correctness - it's about some measure of how far away the output lies from regions that were covered by training data","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n48cunu","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It&amp;#39;s not about factual correctness - it&amp;#39;s about some measure of how far away the output lies from regions that were covered by training data&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m4z64o","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4z64o/why_are_llms_not_able_to_give_an_estimate_on/n48cunu/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753045938,"author_flair_text":null,"treatment_tags":[],"created_utc":1753045938,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-4}}],"before":null}},"user_reports":[],"saved":false,"id":"n48c4hg","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Pvt_Twinkietoes","can_mod_post":false,"send_replies":true,"parent_id":"t1_n485bxg","score":12,"author_fullname":"t2_3k9qfjsr","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Flat earthers are very confident that they're right doesn't make them right.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n48c4hg","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Flat earthers are very confident that they&amp;#39;re right doesn&amp;#39;t make them right.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m4z64o","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4z64o/why_are_llms_not_able_to_give_an_estimate_on/n48c4hg/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753045707,"author_flair_text":null,"treatment_tags":[],"created_utc":1753045707,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":12}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n486s6w","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"IrisColt","can_mod_post":false,"send_replies":true,"parent_id":"t1_n485bxg","score":14,"author_fullname":"t2_c2f558x","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"&gt;I mean, so are you\\n\\n\\nThat's debatable.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n486s6w","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;I mean, so are you&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;That&amp;#39;s debatable.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m4z64o","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4z64o/why_are_llms_not_able_to_give_an_estimate_on/n486s6w/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753044056,"author_flair_text":null,"treatment_tags":[],"created_utc":1753044056,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":14}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n48q8dc","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Minute_Attempt3063","can_mod_post":false,"send_replies":true,"parent_id":"t1_n48orx6","score":0,"author_fullname":"t2_t6m6d9my","approved_by":null,"mod_note":null,"all_awardings":[],"body":"I mean, its high dimensional matrix math","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n48q8dc","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I mean, its high dimensional matrix math&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1m4z64o","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4z64o/why_are_llms_not_able_to_give_an_estimate_on/n48q8dc/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753050336,"author_flair_text":null,"treatment_tags":[],"created_utc":1753050336,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}},"user_reports":[],"saved":false,"id":"n48orx6","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"101m4n","can_mod_post":false,"send_replies":true,"parent_id":"t1_n48nf9o","score":5,"author_fullname":"t2_p7nc2","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Now you're just committing the opposite mistake!\\n\\nSaying it's \\"just math\\" is like saying the brain is \\"just neurons firing\\". It's technically true, but dismissing the emergent behaviors of a system because \\"it's just X\\" is just as silly.\\n\\nThe facts are that these systems exhibit some behaviors that look like cognition, but we don't know to what extent it is anything like our own cognition. It's wrong to humanize them, but it's also wrong to dismiss them as \\"just math\\".","edited":false,"author_flair_css_class":null,"name":"t1_n48orx6","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Now you&amp;#39;re just committing the opposite mistake!&lt;/p&gt;\\n\\n&lt;p&gt;Saying it&amp;#39;s &amp;quot;just math&amp;quot; is like saying the brain is &amp;quot;just neurons firing&amp;quot;. It&amp;#39;s technically true, but dismissing the emergent behaviors of a system because &amp;quot;it&amp;#39;s just X&amp;quot; is just as silly.&lt;/p&gt;\\n\\n&lt;p&gt;The facts are that these systems exhibit some behaviors that look like cognition, but we don&amp;#39;t know to what extent it is anything like our own cognition. It&amp;#39;s wrong to humanize them, but it&amp;#39;s also wrong to dismiss them as &amp;quot;just math&amp;quot;.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1m4z64o","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4z64o/why_are_llms_not_able_to_give_an_estimate_on/n48orx6/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753049834,"author_flair_text":null,"collapsed":false,"created_utc":1753049834,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}}],"before":null}},"user_reports":[],"saved":false,"id":"n48nf9o","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Minute_Attempt3063","can_mod_post":false,"send_replies":true,"parent_id":"t1_n48dt16","score":1,"author_fullname":"t2_t6m6d9my","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"And its just math. And people apparently dont think of it like that, sadly","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n48nf9o","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;And its just math. And people apparently dont think of it like that, sadly&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m4z64o","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4z64o/why_are_llms_not_able_to_give_an_estimate_on/n48nf9o/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753049373,"author_flair_text":null,"treatment_tags":[],"created_utc":1753049373,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n48dt16","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"101m4n","can_mod_post":false,"send_replies":true,"parent_id":"t1_n485bxg","score":5,"author_fullname":"t2_p7nc2","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"No, we're not.\\n\\nAs an example, if you prompt llama3.3 70B with a system prompt that says something along the lines of \\"you are having an informal conversation\\", and then start the conversation with something like \\"hey, what's up\\", it will often make up something like \\"I just got back from a nice walk in the park\\". It's an LLM and it definitely didn't, but because such responses are well represented in the training data and are the sort of things said in informal conversations, that's what it says.\\n\\nIn a sense, LLMs are like bodies of knowledge disjoint from any particular mind. They're a sort of amalgamation of all the people who are represented in the training data. Any \\"cognition\\" they have isn't a mirror of human cognition, but an entirely new emergent thing that was extrapolated from the training data.\\n\\nAlso there are actually ways to reduce hallucination. If you prompt a model with knowledge from some external source and tell it in the system prompt that these are the things it \\"knows\\", then that will dramatically reduce hallucinations.\\n\\nTL:DR; LLMs are not people. They work very differently to the way people do and are produced by wildly different processes, and hallucinations are just an artifact of this.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n48dt16","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;No, we&amp;#39;re not.&lt;/p&gt;\\n\\n&lt;p&gt;As an example, if you prompt llama3.3 70B with a system prompt that says something along the lines of &amp;quot;you are having an informal conversation&amp;quot;, and then start the conversation with something like &amp;quot;hey, what&amp;#39;s up&amp;quot;, it will often make up something like &amp;quot;I just got back from a nice walk in the park&amp;quot;. It&amp;#39;s an LLM and it definitely didn&amp;#39;t, but because such responses are well represented in the training data and are the sort of things said in informal conversations, that&amp;#39;s what it says.&lt;/p&gt;\\n\\n&lt;p&gt;In a sense, LLMs are like bodies of knowledge disjoint from any particular mind. They&amp;#39;re a sort of amalgamation of all the people who are represented in the training data. Any &amp;quot;cognition&amp;quot; they have isn&amp;#39;t a mirror of human cognition, but an entirely new emergent thing that was extrapolated from the training data.&lt;/p&gt;\\n\\n&lt;p&gt;Also there are actually ways to reduce hallucination. If you prompt a model with knowledge from some external source and tell it in the system prompt that these are the things it &amp;quot;knows&amp;quot;, then that will dramatically reduce hallucinations.&lt;/p&gt;\\n\\n&lt;p&gt;TL:DR; LLMs are not people. They work very differently to the way people do and are produced by wildly different processes, and hallucinations are just an artifact of this.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m4z64o","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4z64o/why_are_llms_not_able_to_give_an_estimate_on/n48dt16/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753046240,"author_flair_text":null,"treatment_tags":[],"created_utc":1753046240,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n494v7e","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"teleprint-me","can_mod_post":false,"send_replies":true,"parent_id":"t1_n485bxg","score":2,"author_fullname":"t2_slcrtxpr","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"You do not understand or know what you are talking about.\\n\\nYou take the sampled the inputs as a completion.\\n\\n\\"Hello, world!\\".\\n\\nThis is broken up into tokens.\\n\\n\\"Hello\\", \\",\\", \\"world\\", \\"!\\".\\n\\nThe tokens have scores based on frequencies. This is just the number of times the token is seen in a corpus.\\n\\nYou map the tokens to numbers.\\n\\n[9707, 11, 1879, 0]\\n\\nThe trained weights are based on the angel and distance from the most common frequencies which are called scores.\\n\\nThese scores are literal points in a space (think Cartesian coordinates).\\n\\nThe forward pass inferences (predicts) the likelihood of the next token or next set of tokens that might come afterwards.\\n\\nThis process, in its simplest form, is called Linear Regression. Modern LLMs use backpropogation to reduce the error (epsilon) between the mapped points in space and the expected output.\\n\\nThe expected output is the label which is already known in advance.\\n\\nThe more datapoints are in space, the larger the distribution, and the more easily it is to predict the likelihood of the next token.\\n\\n9707, 11 is the starting vector. We put this through a series of numerical equations to perform transformations that will predict the final tokens as 1879, 0.\\n\\nTo do this, we need to sample the weighted distributions. The weights are the distribution and we attempt to fit this to a line. If the line is able to pass through the distributed data points, then we say the line is fit.\\n\\nSampling uses a pseudo random number generator.\\n\\n\`\`\`c\\nint sample(Sampler* sampler, float* logits) {\\n    // apply the temperature to the logits\\n    for (int q = 0; q &lt; sampler-&gt;vocab_size; q++) {\\n        logits[q] /= sampler-&gt;temperature; // scale\\n    }\\n\\n    // apply softmax to the logits to get the samples for next token\\n    softmax(logits, sampler-&gt;vocab_size); // normalize\\n    // create a source of entropy for sampling\\n    float coin = random_f32(&amp;sampler-&gt;seed); // flip a coin\\n    // top-p (nucleus) sampling, clamping the least likely tokens to zero\\n    return sampler_top_p(sampler, logits, coin);\\n}\\n\`\`\`\\n\\nThe predicted logits based on the likelihood of the expected fit should produce the tokens as integers.\\n\\nThis creates a high probability which is measured. If we want entropy, we use a temperature to affect this.\\n\\nThis is not consciousness. This is statistics. This is not how the human mind operates at all.","edited":1753056453,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n494v7e","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You do not understand or know what you are talking about.&lt;/p&gt;\\n\\n&lt;p&gt;You take the sampled the inputs as a completion.&lt;/p&gt;\\n\\n&lt;p&gt;&amp;quot;Hello, world!&amp;quot;.&lt;/p&gt;\\n\\n&lt;p&gt;This is broken up into tokens.&lt;/p&gt;\\n\\n&lt;p&gt;&amp;quot;Hello&amp;quot;, &amp;quot;,&amp;quot;, &amp;quot;world&amp;quot;, &amp;quot;!&amp;quot;.&lt;/p&gt;\\n\\n&lt;p&gt;The tokens have scores based on frequencies. This is just the number of times the token is seen in a corpus.&lt;/p&gt;\\n\\n&lt;p&gt;You map the tokens to numbers.&lt;/p&gt;\\n\\n&lt;p&gt;[9707, 11, 1879, 0]&lt;/p&gt;\\n\\n&lt;p&gt;The trained weights are based on the angel and distance from the most common frequencies which are called scores.&lt;/p&gt;\\n\\n&lt;p&gt;These scores are literal points in a space (think Cartesian coordinates).&lt;/p&gt;\\n\\n&lt;p&gt;The forward pass inferences (predicts) the likelihood of the next token or next set of tokens that might come afterwards.&lt;/p&gt;\\n\\n&lt;p&gt;This process, in its simplest form, is called Linear Regression. Modern LLMs use backpropogation to reduce the error (epsilon) between the mapped points in space and the expected output.&lt;/p&gt;\\n\\n&lt;p&gt;The expected output is the label which is already known in advance.&lt;/p&gt;\\n\\n&lt;p&gt;The more datapoints are in space, the larger the distribution, and the more easily it is to predict the likelihood of the next token.&lt;/p&gt;\\n\\n&lt;p&gt;9707, 11 is the starting vector. We put this through a series of numerical equations to perform transformations that will predict the final tokens as 1879, 0.&lt;/p&gt;\\n\\n&lt;p&gt;To do this, we need to sample the weighted distributions. The weights are the distribution and we attempt to fit this to a line. If the line is able to pass through the distributed data points, then we say the line is fit.&lt;/p&gt;\\n\\n&lt;p&gt;Sampling uses a pseudo random number generator.&lt;/p&gt;\\n\\n&lt;p&gt;\`\`\`c\\nint sample(Sampler* sampler, float* logits) {\\n    // apply the temperature to the logits\\n    for (int q = 0; q &amp;lt; sampler-&amp;gt;vocab_size; q++) {\\n        logits[q] /= sampler-&amp;gt;temperature; // scale\\n    }&lt;/p&gt;\\n\\n&lt;pre&gt;&lt;code&gt;// apply softmax to the logits to get the samples for next token\\nsoftmax(logits, sampler-&amp;gt;vocab_size); // normalize\\n// create a source of entropy for sampling\\nfloat coin = random_f32(&amp;amp;sampler-&amp;gt;seed); // flip a coin\\n// top-p (nucleus) sampling, clamping the least likely tokens to zero\\nreturn sampler_top_p(sampler, logits, coin);\\n&lt;/code&gt;&lt;/pre&gt;\\n\\n&lt;p&gt;}\\n\`\`\`&lt;/p&gt;\\n\\n&lt;p&gt;The predicted logits based on the likelihood of the expected fit should produce the tokens as integers.&lt;/p&gt;\\n\\n&lt;p&gt;This creates a high probability which is measured. If we want entropy, we use a temperature to affect this.&lt;/p&gt;\\n\\n&lt;p&gt;This is not consciousness. This is statistics. This is not how the human mind operates at all.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m4z64o","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4z64o/why_are_llms_not_able_to_give_an_estimate_on/n494v7e/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753055422,"author_flair_text":null,"treatment_tags":[],"created_utc":1753055422,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n48aajz","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"-Akos-","can_mod_post":false,"send_replies":true,"parent_id":"t1_n485bxg","score":0,"author_fullname":"t2_3xk4s4iv","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"You’re comparing the human brain to a computer program. LLMs have a corpus of data, and that’s it. Human brains have the ability to learn and store data, and it makes decisions based on prior learnings.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n48aajz","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You’re comparing the human brain to a computer program. LLMs have a corpus of data, and that’s it. Human brains have the ability to learn and store data, and it makes decisions based on prior learnings.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m4z64o","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4z64o/why_are_llms_not_able_to_give_an_estimate_on/n48aajz/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753045137,"author_flair_text":null,"treatment_tags":[],"created_utc":1753045137,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":1,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}},"user_reports":[],"saved":false,"id":"n485bxg","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":"LOW_SCORE","no_follow":true,"author":"MarinatedPickachu","can_mod_post":false,"created_utc":1753043609,"send_replies":true,"parent_id":"t1_n484sus","score":-27,"author_fullname":"t2_bwdb8qqfj","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I mean, so are you. But that's exactly the point - the source of the heuristic applied. There certainly should be a way to differentiate between a result that traces back closely to actual training data and stuff that required more interpolation","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n485bxg","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I mean, so are you. But that&amp;#39;s exactly the point - the source of the heuristic applied. There certainly should be a way to differentiate between a result that traces back closely to actual training data and stuff that required more interpolation&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":"comment score below threshold","distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m4z64o","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4z64o/why_are_llms_not_able_to_give_an_estimate_on/n485bxg/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753043609,"author_flair_text":null,"treatment_tags":[],"collapsed":true,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-27}}],"before":null}},"user_reports":[],"saved":false,"id":"n484sus","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"-Akos-","can_mod_post":false,"created_utc":1753043447,"send_replies":true,"parent_id":"t3_1m4z64o","score":36,"author_fullname":"t2_3xk4s4iv","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Because it’s statistically generating the next part of the sentence based on initial context, and not actually thinking?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n484sus","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Because it’s statistically generating the next part of the sentence based on initial context, and not actually thinking?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4z64o/why_are_llms_not_able_to_give_an_estimate_on/n484sus/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753043447,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m4z64o","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":36}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n485gzz","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"rainbowColoredBalls","can_mod_post":false,"created_utc":1753043652,"send_replies":true,"parent_id":"t3_1m4z64o","score":3,"author_fullname":"t2_23ovx259","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"They do that when their post training data has examples doing that. \\n\\n\\nClaude is the best in this dimension (still far from where we should be)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n485gzz","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;They do that when their post training data has examples doing that. &lt;/p&gt;\\n\\n&lt;p&gt;Claude is the best in this dimension (still far from where we should be)&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4z64o/why_are_llms_not_able_to_give_an_estimate_on/n485gzz/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753043652,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m4z64o","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n484tni","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"LagOps91","can_mod_post":false,"created_utc":1753043454,"send_replies":true,"parent_id":"t3_1m4z64o","score":5,"author_fullname":"t2_3wi6j7vwh","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"you need to do forensics for that. remember, those models are trained to autocomplete text first and then trained to act as an assistant. in none of those trainings are they rewarded for saying \\"i don't know\\" or \\"i'm not sure\\".","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n484tni","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;you need to do forensics for that. remember, those models are trained to autocomplete text first and then trained to act as an assistant. in none of those trainings are they rewarded for saying &amp;quot;i don&amp;#39;t know&amp;quot; or &amp;quot;i&amp;#39;m not sure&amp;quot;.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4z64o/why_are_llms_not_able_to_give_an_estimate_on/n484tni/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753043454,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m4z64o","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n498zfv","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MarinatedPickachu","can_mod_post":false,"send_replies":true,"parent_id":"t1_n48z5zw","score":2,"author_fullname":"t2_bwdb8qqfj","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Oh that's neat!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n498zfv","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Oh that&amp;#39;s neat!&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m4z64o","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4z64o/why_are_llms_not_able_to_give_an_estimate_on/n498zfv/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753056923,"author_flair_text":null,"treatment_tags":[],"created_utc":1753056923,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n48z5zw","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"FairlyInvolved","can_mod_post":false,"send_replies":true,"parent_id":"t1_n48bivn","score":3,"author_fullname":"t2_qaxlr1d8h","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"That's not strictly true, there is a sense in which they \\"understand\\" when they are recalling knowledge vs hallucinating it.\\n\\nhttps://arxiv.org/abs/2411.14257","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n48z5zw","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;That&amp;#39;s not strictly true, there is a sense in which they &amp;quot;understand&amp;quot; when they are recalling knowledge vs hallucinating it.&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://arxiv.org/abs/2411.14257\\"&gt;https://arxiv.org/abs/2411.14257&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m4z64o","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4z64o/why_are_llms_not_able_to_give_an_estimate_on/n48z5zw/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753053424,"author_flair_text":null,"treatment_tags":[],"created_utc":1753053424,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n48fsha","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Double_Cause4609","can_mod_post":false,"send_replies":true,"parent_id":"t1_n48bivn","score":1,"author_fullname":"t2_1kubzxt2ww","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"That's...A VAE, basically. Or, one of the things elicited by the training dynamics of the VAE","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n48fsha","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;That&amp;#39;s...A VAE, basically. Or, one of the things elicited by the training dynamics of the VAE&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m4z64o","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4z64o/why_are_llms_not_able_to_give_an_estimate_on/n48fsha/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753046877,"author_flair_text":null,"treatment_tags":[],"created_utc":1753046877,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n48bivn","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MarinatedPickachu","can_mod_post":false,"created_utc":1753045518,"send_replies":true,"parent_id":"t1_n48aejx","score":2,"author_fullname":"t2_bwdb8qqfj","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"The LLM itself doesn't have an idea of the distribution, but the training-process should be able to generate something like a \\"heat map\\" in the high dimensional space of the LLM, one that the inference framework should be able to make use of","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n48bivn","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The LLM itself doesn&amp;#39;t have an idea of the distribution, but the training-process should be able to generate something like a &amp;quot;heat map&amp;quot; in the high dimensional space of the LLM, one that the inference framework should be able to make use of&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m4z64o","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4z64o/why_are_llms_not_able_to_give_an_estimate_on/n48bivn/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753045518,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n48aejx","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Double_Cause4609","can_mod_post":false,"created_utc":1753045172,"send_replies":true,"parent_id":"t3_1m4z64o","score":4,"author_fullname":"t2_1kubzxt2ww","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Because autoregressive Transformer LLMs with plain linear transformations and a standard cross entropy loss aren't really probabilistic models.\\n\\nPretty much everything in an LLM is deterministic (Attention? Deterministic. FFNs? Deterministic. Etc), up until the language head, where we treat its confidence as a probability at inference.\\n\\nThey are a sort of probabilistic model, but only in the mildest sense.\\n\\nThey don't really have an understanding (for lack of a better term) of the distribution they're modelling and their true confidence in it.\\n\\nIn contrast, while I'm not sure if a system based on it would be able to articulate it in words directly, theoretically systems like VAEs have a much better parameterization of the actual probability distribution they're modelling and the current sequences place in that distribution.\\n\\nPerhaps one could train a VAE to say \\"I don't know\\"?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n48aejx","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Because autoregressive Transformer LLMs with plain linear transformations and a standard cross entropy loss aren&amp;#39;t really probabilistic models.&lt;/p&gt;\\n\\n&lt;p&gt;Pretty much everything in an LLM is deterministic (Attention? Deterministic. FFNs? Deterministic. Etc), up until the language head, where we treat its confidence as a probability at inference.&lt;/p&gt;\\n\\n&lt;p&gt;They are a sort of probabilistic model, but only in the mildest sense.&lt;/p&gt;\\n\\n&lt;p&gt;They don&amp;#39;t really have an understanding (for lack of a better term) of the distribution they&amp;#39;re modelling and their true confidence in it.&lt;/p&gt;\\n\\n&lt;p&gt;In contrast, while I&amp;#39;m not sure if a system based on it would be able to articulate it in words directly, theoretically systems like VAEs have a much better parameterization of the actual probability distribution they&amp;#39;re modelling and the current sequences place in that distribution.&lt;/p&gt;\\n\\n&lt;p&gt;Perhaps one could train a VAE to say &amp;quot;I don&amp;#39;t know&amp;quot;?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4z64o/why_are_llms_not_able_to_give_an_estimate_on/n48aejx/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753045172,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m4z64o","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n488s2q","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"fp4guru","can_mod_post":false,"created_utc":1753044668,"send_replies":true,"parent_id":"t3_1m4z64o","score":2,"author_fullname":"t2_1tp8zldw5g","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I wish there was an indicator when confidence level for next token is lower than a threshold.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n488s2q","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I wish there was an indicator when confidence level for next token is lower than a threshold.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4z64o/why_are_llms_not_able_to_give_an_estimate_on/n488s2q/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753044668,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m4z64o","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n48bimd","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"eli_pizza","can_mod_post":false,"send_replies":true,"parent_id":"t1_n48axfy","score":2,"author_fullname":"t2_1pdeyk44rl","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Hmm dunno about “factually” but it would at least give you a hint about when it’s guessing. You could ask it for the R’s in strawberry (or whatever) and still get no right answers.","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n48bimd","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Hmm dunno about “factually” but it would at least give you a hint about when it’s guessing. You could ask it for the R’s in strawberry (or whatever) and still get no right answers.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1m4z64o","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4z64o/why_are_llms_not_able_to_give_an_estimate_on/n48bimd/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753045515,"author_flair_text":null,"treatment_tags":[],"created_utc":1753045515,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":6,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n48axfy","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MarinatedPickachu","can_mod_post":false,"send_replies":true,"parent_id":"t1_n48abc4","score":1,"author_fullname":"t2_bwdb8qqfj","approved_by":null,"mod_note":null,"all_awardings":[],"body":"Ok but that's an interesting approach! Higher certainty might correlate with factually more consistent results given different seeds","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n48axfy","is_submitter":true,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Ok but that&amp;#39;s an interesting approach! Higher certainty might correlate with factually more consistent results given different seeds&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1m4z64o","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4z64o/why_are_llms_not_able_to_give_an_estimate_on/n48axfy/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753045332,"author_flair_text":null,"treatment_tags":[],"created_utc":1753045332,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n48abc4","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"eli_pizza","can_mod_post":false,"send_replies":true,"parent_id":"t1_n48a17r","score":2,"author_fullname":"t2_1pdeyk44rl","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yes I agree. It could also run the same inference 100 times with different seeds and see which answers are most common. But that would be slow.","edited":false,"author_flair_css_class":null,"name":"t1_n48abc4","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yes I agree. It could also run the same inference 100 times with different seeds and see which answers are most common. But that would be slow.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1m4z64o","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4z64o/why_are_llms_not_able_to_give_an_estimate_on/n48abc4/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753045144,"author_flair_text":null,"collapsed":false,"created_utc":1753045144,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n48a17r","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MarinatedPickachu","can_mod_post":false,"send_replies":true,"parent_id":"t1_n488yal","score":1,"author_fullname":"t2_bwdb8qqfj","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Yes but probability is not just a point, it's a distribution - and the shape of that distribution should tell something about certainty","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n48a17r","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yes but probability is not just a point, it&amp;#39;s a distribution - and the shape of that distribution should tell something about certainty&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m4z64o","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4z64o/why_are_llms_not_able_to_give_an_estimate_on/n48a17r/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753045055,"author_flair_text":null,"treatment_tags":[],"created_utc":1753045055,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n488yal","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"eli_pizza","can_mod_post":false,"send_replies":true,"parent_id":"t1_n487rj1","score":3,"author_fullname":"t2_1pdeyk44rl","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"No, it would be a problem even with perfect training data. Probability of the token is the only thing it knows. Not what the tokens mean or whether they form sentences that are true.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n488yal","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;No, it would be a problem even with perfect training data. Probability of the token is the only thing it knows. Not what the tokens mean or whether they form sentences that are true.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m4z64o","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4z64o/why_are_llms_not_able_to_give_an_estimate_on/n488yal/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753044722,"author_flair_text":null,"treatment_tags":[],"created_utc":1753044722,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n487rj1","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MarinatedPickachu","can_mod_post":false,"created_utc":1753044358,"send_replies":true,"parent_id":"t1_n485jcz","score":1,"author_fullname":"t2_bwdb8qqfj","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yes of course, it can't know whether its training data was right - but i feel like one should be able to derive some value of how far the inference had to stray / how much interpolation/extrapolation had to be applied to derive the result.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n487rj1","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yes of course, it can&amp;#39;t know whether its training data was right - but i feel like one should be able to derive some value of how far the inference had to stray / how much interpolation/extrapolation had to be applied to derive the result.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m4z64o","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4z64o/why_are_llms_not_able_to_give_an_estimate_on/n487rj1/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753044358,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n485jcz","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"eli_pizza","can_mod_post":false,"created_utc":1753043673,"send_replies":true,"parent_id":"t3_1m4z64o","score":2,"author_fullname":"t2_1pdeyk44rl","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I think they probably could but it’s not in the interest of frontier models. \\n\\nAlso, importantly, it would be confidence in the token inference. Not in whether what it’s saying is true or correct. That’s basically impossible","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n485jcz","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I think they probably could but it’s not in the interest of frontier models. &lt;/p&gt;\\n\\n&lt;p&gt;Also, importantly, it would be confidence in the token inference. Not in whether what it’s saying is true or correct. That’s basically impossible&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4z64o/why_are_llms_not_able_to_give_an_estimate_on/n485jcz/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753043673,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m4z64o","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n48guza","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"jekewa","can_mod_post":false,"send_replies":true,"parent_id":"t1_n48btl8","score":1,"author_fullname":"t2_r0nob","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"There is no concept that is not in the training data.\\n\\nAI has no imagination. It is not creating ideas, just collating, sorting, filtering, and organizing what it knows.\\n\\nIf the training data does not include any content about automobiles, there isn't going to be any construct out of the GenAI about automobiles that isn't a direct result of an input prompt. It won't imagine an engine strapped to a wagon unless you tell it to.\\n\\nThe training data are the textbooks people read, not the ideas that stir from reading the text, nor the consideration that was used by the author. If it isn't in the book, there's no straying away from it or creating it.\\n\\nWhat it does stray away from is useful context. As it dives down paths of sentence construction, it's largely looking at \\"what's a good word to come next\\" following the rules of grammar and trying to relate what it calculated from the input into the output.\\n\\nIf there are forks of seemingly equivalent outcomes, the responses can seem legit or nonsense.\\n\\nAsk it about \\"water tanks\\" and see if it comes up with something related to storage containers, aquariums, amphibious assault vehicles, or some weird combinations. Maybe, depending on your AI's session, if you had been talking about fish or war or rain it'll correctly associate the right context like you just did, because aquariums have fish, but the others shouldn't so much.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n48guza","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;There is no concept that is not in the training data.&lt;/p&gt;\\n\\n&lt;p&gt;AI has no imagination. It is not creating ideas, just collating, sorting, filtering, and organizing what it knows.&lt;/p&gt;\\n\\n&lt;p&gt;If the training data does not include any content about automobiles, there isn&amp;#39;t going to be any construct out of the GenAI about automobiles that isn&amp;#39;t a direct result of an input prompt. It won&amp;#39;t imagine an engine strapped to a wagon unless you tell it to.&lt;/p&gt;\\n\\n&lt;p&gt;The training data are the textbooks people read, not the ideas that stir from reading the text, nor the consideration that was used by the author. If it isn&amp;#39;t in the book, there&amp;#39;s no straying away from it or creating it.&lt;/p&gt;\\n\\n&lt;p&gt;What it does stray away from is useful context. As it dives down paths of sentence construction, it&amp;#39;s largely looking at &amp;quot;what&amp;#39;s a good word to come next&amp;quot; following the rules of grammar and trying to relate what it calculated from the input into the output.&lt;/p&gt;\\n\\n&lt;p&gt;If there are forks of seemingly equivalent outcomes, the responses can seem legit or nonsense.&lt;/p&gt;\\n\\n&lt;p&gt;Ask it about &amp;quot;water tanks&amp;quot; and see if it comes up with something related to storage containers, aquariums, amphibious assault vehicles, or some weird combinations. Maybe, depending on your AI&amp;#39;s session, if you had been talking about fish or war or rain it&amp;#39;ll correctly associate the right context like you just did, because aquariums have fish, but the others shouldn&amp;#39;t so much.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m4z64o","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4z64o/why_are_llms_not_able_to_give_an_estimate_on/n48guza/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753047220,"author_flair_text":null,"treatment_tags":[],"created_utc":1753047220,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n48btl8","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MarinatedPickachu","can_mod_post":false,"created_utc":1753045611,"send_replies":true,"parent_id":"t1_n48ao25","score":1,"author_fullname":"t2_bwdb8qqfj","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"By accuracy I don't mean \\"correct\\" but \\"close to concepts that were actually part of the training data\\" rather than concepts that stray further away from what was in the training data","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n48btl8","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;By accuracy I don&amp;#39;t mean &amp;quot;correct&amp;quot; but &amp;quot;close to concepts that were actually part of the training data&amp;quot; rather than concepts that stray further away from what was in the training data&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m4z64o","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4z64o/why_are_llms_not_able_to_give_an_estimate_on/n48btl8/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753045611,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n48ao25","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"jekewa","can_mod_post":false,"created_utc":1753045252,"send_replies":true,"parent_id":"t3_1m4z64o","score":1,"author_fullname":"t2_r0nob","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"They should give their \\"confidence,\\" often in the meta data with its output, but it isn't the same correlation to accuracy as you probably mean. It's a lot more about \\"probably grammatically correct\\" and \\"matches the query context.\\"\\n\\nThey aren't thinking or guessing, so there's no measure of their accuracy for them to have a sense of \\"sure\\" about anything. They either have been trained on content that matches the query, or they don't have any information on what you're asking.\\n\\nThe bulk of their focus is pattern matching and content construction with syntax adherence.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n48ao25","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;They should give their &amp;quot;confidence,&amp;quot; often in the meta data with its output, but it isn&amp;#39;t the same correlation to accuracy as you probably mean. It&amp;#39;s a lot more about &amp;quot;probably grammatically correct&amp;quot; and &amp;quot;matches the query context.&amp;quot;&lt;/p&gt;\\n\\n&lt;p&gt;They aren&amp;#39;t thinking or guessing, so there&amp;#39;s no measure of their accuracy for them to have a sense of &amp;quot;sure&amp;quot; about anything. They either have been trained on content that matches the query, or they don&amp;#39;t have any information on what you&amp;#39;re asking.&lt;/p&gt;\\n\\n&lt;p&gt;The bulk of their focus is pattern matching and content construction with syntax adherence.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4z64o/why_are_llms_not_able_to_give_an_estimate_on/n48ao25/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753045252,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m4z64o","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4addc1","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Professional-Put-196","can_mod_post":false,"send_replies":true,"parent_id":"t1_n48sy60","score":0,"author_fullname":"t2_8nqy1atk","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"You missed my point. Saying no, accepting defeat is not \\"intelligence\\" as used to design these predictive neural networks (which appear generative just because its an extremely good prediction). Its an emotional thing to be able to say no. So, unless specifically programmed, as in constrained using external frameworks, like the inference framework, it will never be possible for them to say \\"i don't know\\".","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n4addc1","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You missed my point. Saying no, accepting defeat is not &amp;quot;intelligence&amp;quot; as used to design these predictive neural networks (which appear generative just because its an extremely good prediction). Its an emotional thing to be able to say no. So, unless specifically programmed, as in constrained using external frameworks, like the inference framework, it will never be possible for them to say &amp;quot;i don&amp;#39;t know&amp;quot;.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m4z64o","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4z64o/why_are_llms_not_able_to_give_an_estimate_on/n4addc1/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753072841,"author_flair_text":null,"treatment_tags":[],"created_utc":1753072841,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}},"user_reports":[],"saved":false,"id":"n48sy60","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MarinatedPickachu","can_mod_post":false,"created_utc":1753051281,"send_replies":true,"parent_id":"t1_n48s99h","score":1,"author_fullname":"t2_bwdb8qqfj","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Did I say anything about intellect? This is something that should be done by the inference framework, not the LLM itself","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n48sy60","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Did I say anything about intellect? This is something that should be done by the inference framework, not the LLM itself&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m4z64o","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4z64o/why_are_llms_not_able_to_give_an_estimate_on/n48sy60/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753051281,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n48s99h","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Professional-Put-196","can_mod_post":false,"created_utc":1753051038,"send_replies":true,"parent_id":"t3_1m4z64o","score":1,"author_fullname":"t2_8nqy1atk","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Same reason a person with only confidence, but very little knowledge or clarity on a topic can't. The ability to say no has nothing to do with \\"intellect\\" which is what these really are. It's an emotional intelligence thing.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n48s99h","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Same reason a person with only confidence, but very little knowledge or clarity on a topic can&amp;#39;t. The ability to say no has nothing to do with &amp;quot;intellect&amp;quot; which is what these really are. It&amp;#39;s an emotional intelligence thing.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4z64o/why_are_llms_not_able_to_give_an_estimate_on/n48s99h/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753051038,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m4z64o","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n49g9kw","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"iam_maxinne","can_mod_post":false,"created_utc":1753059638,"send_replies":true,"parent_id":"t3_1m4z64o","score":1,"author_fullname":"t2_59ct7y4w1","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Bro, it has no “confidence” nor “knowledge” it is just statistics in a trench coat… Most texts states information in a confident manner so it is statistically more probable that the next tokens are the ones we perceive as “hallucinations”…","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n49g9kw","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Bro, it has no “confidence” nor “knowledge” it is just statistics in a trench coat… Most texts states information in a confident manner so it is statistically more probable that the next tokens are the ones we perceive as “hallucinations”…&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4z64o/why_are_llms_not_able_to_give_an_estimate_on/n49g9kw/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753059638,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m4z64o","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n49vwib","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"HypnoDaddy4You","can_mod_post":false,"created_utc":1753065490,"send_replies":true,"parent_id":"t3_1m4z64o","score":1,"author_fullname":"t2_lb2n7mbsw","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"It neither has confidence nor is it sure, or unsure, about anything.\\n\\nConfidence, as we normally think about predictive analytics, is a measure of how likely the model thinks the prediction to be accurate. \\n\\nIn an LLM, that confidence is directly used to predict the next token, which is then chosen randomly from the top possibilities. Sometimes it's a logical choice and sometimes it isn't, but the model has no choice but proceed with the next token regardless. \\n\\nYou could somehow calculate an overall confidence when it's done, as the mean square root of the product of the input confidences, or something, but the noise is too great for it to mean anything. And, it will likely have similar characteristics for hallucinations as it would for good inferences, because it's usually just a couple tokens that went wrong.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n49vwib","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It neither has confidence nor is it sure, or unsure, about anything.&lt;/p&gt;\\n\\n&lt;p&gt;Confidence, as we normally think about predictive analytics, is a measure of how likely the model thinks the prediction to be accurate. &lt;/p&gt;\\n\\n&lt;p&gt;In an LLM, that confidence is directly used to predict the next token, which is then chosen randomly from the top possibilities. Sometimes it&amp;#39;s a logical choice and sometimes it isn&amp;#39;t, but the model has no choice but proceed with the next token regardless. &lt;/p&gt;\\n\\n&lt;p&gt;You could somehow calculate an overall confidence when it&amp;#39;s done, as the mean square root of the product of the input confidences, or something, but the noise is too great for it to mean anything. And, it will likely have similar characteristics for hallucinations as it would for good inferences, because it&amp;#39;s usually just a couple tokens that went wrong.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4z64o/why_are_llms_not_able_to_give_an_estimate_on/n49vwib/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753065490,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m4z64o","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n484ggo","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Thistleknot","can_mod_post":false,"created_utc":1753043344,"send_replies":true,"parent_id":"t3_1m4z64o","score":1,"author_fullname":"t2_18ipm","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"check out flare rag","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n484ggo","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;check out flare rag&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4z64o/why_are_llms_not_able_to_give_an_estimate_on/n484ggo/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753043344,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m4z64o","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n48hlj9","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"dkeiz","can_mod_post":false,"created_utc":1753047455,"send_replies":true,"parent_id":"t3_1m4z64o","score":0,"author_fullname":"t2_1d1fsbe7","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"cause their output is not their input. To make it you need to run llm in chain - watchover output and make new session to classify that. Possible, but takes lots of time. And it increase context. So even more time. And inference speed. But new gen ai agent with multiple llm inference output per queestion, or mcp setups could do so, if you make them. Or reasoning model, they act like that while thinking, and vevn call mcp tools, but it multiinference once again. We need 5000t/s to do it perfectly.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n48hlj9","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;cause their output is not their input. To make it you need to run llm in chain - watchover output and make new session to classify that. Possible, but takes lots of time. And it increase context. So even more time. And inference speed. But new gen ai agent with multiple llm inference output per queestion, or mcp setups could do so, if you make them. Or reasoning model, they act like that while thinking, and vevn call mcp tools, but it multiinference once again. We need 5000t/s to do it perfectly.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4z64o/why_are_llms_not_able_to_give_an_estimate_on/n48hlj9/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753047455,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m4z64o","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4b8p2r","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MarinatedPickachu","can_mod_post":false,"created_utc":1753090013,"send_replies":true,"parent_id":"t1_n4ay1hl","score":1,"author_fullname":"t2_bwdb8qqfj","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I'm not talking about factual truth - i'm talking about hallucinating information that was no part of the training data","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4b8p2r","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;m not talking about factual truth - i&amp;#39;m talking about hallucinating information that was no part of the training data&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m4z64o","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4z64o/why_are_llms_not_able_to_give_an_estimate_on/n4b8p2r/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753090013,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4ay1hl","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"05032-MendicantBias","can_mod_post":false,"created_utc":1753083776,"send_replies":true,"parent_id":"t3_1m4z64o","score":0,"author_fullname":"t2_6id3lwou","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"That's not how it works.\\n\\nIt's difficult even for us humans to estimate what we don't know, and even we humans are often very confident in very wrong things. And the model doesn't have the ability to for coherent plans to solve problems.\\n\\nYou'd need a factually correct world representation that the model can reference in order to have any chance to solve the problem, but it's easier said than done.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4ay1hl","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;That&amp;#39;s not how it works.&lt;/p&gt;\\n\\n&lt;p&gt;It&amp;#39;s difficult even for us humans to estimate what we don&amp;#39;t know, and even we humans are often very confident in very wrong things. And the model doesn&amp;#39;t have the ability to for coherent plans to solve problems.&lt;/p&gt;\\n\\n&lt;p&gt;You&amp;#39;d need a factually correct world representation that the model can reference in order to have any chance to solve the problem, but it&amp;#39;s easier said than done.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4z64o/why_are_llms_not_able_to_give_an_estimate_on/n4ay1hl/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753083776,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m4z64o","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n49rwsv","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"INtuitiveTJop","can_mod_post":false,"created_utc":1753063971,"send_replies":true,"parent_id":"t3_1m4z64o","score":-1,"author_fullname":"t2_u16k63kl","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"People are much the same","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n49rwsv","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;People are much the same&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4z64o/why_are_llms_not_able_to_give_an_estimate_on/n49rwsv/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753063971,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m4z64o","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n48ilhl","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Rich_Artist_8327","can_mod_post":false,"created_utc":1753047772,"send_replies":true,"parent_id":"t3_1m4z64o","score":-2,"author_fullname":"t2_1jk2ep8a52","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"add in the end of the prompt \\"how confident are you with this answer from 0-100\\" But I guess cos they are not yet AGI they dont kkow what they dont know","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n48ilhl","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;add in the end of the prompt &amp;quot;how confident are you with this answer from 0-100&amp;quot; But I guess cos they are not yet AGI they dont kkow what they dont know&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4z64o/why_are_llms_not_able_to_give_an_estimate_on/n48ilhl/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753047772,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m4z64o","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n48l09l","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MarinatedPickachu","can_mod_post":false,"created_utc":1753048562,"send_replies":true,"parent_id":"t1_n48k5y1","score":2,"author_fullname":"t2_bwdb8qqfj","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"That's useless - these numbers are exactly as much subject to hallucination as everything else","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n48l09l","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;That&amp;#39;s useless - these numbers are exactly as much subject to hallucination as everything else&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m4z64o","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4z64o/why_are_llms_not_able_to_give_an_estimate_on/n48l09l/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753048562,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n48k5y1","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Rich_Artist_8327","can_mod_post":false,"created_utc":1753048283,"send_replies":true,"parent_id":"t3_1m4z64o","score":-2,"author_fullname":"t2_1jk2ep8a52","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I asked Gemini to guess my dads name (and before I asked add always how confident you are with your answer from 0-100%) \\nSo gemini gave a man name popular in my country and it was correct. Answered that confidence:5% \\nGemini was lucky this time.\\n\\nSo yes just ask the freaking confidense %\\n\\"\\nYes, I can do that. I’ll include a confidence score (0–100%) at the end of each of my answers from now on. Just note:\\n\\t•\\t100% means I’m very sure, often based on well-established facts or direct interpretation.\\n\\t•\\t80–99% is a strong answer, but there may be edge cases or unknowns.\\n\\t•\\t50–79% means it’s more uncertain — possibly due to incomplete context or complex interpretation.\\n\\t•\\tBelow 50% is a guess or based on limited or speculative information.\\n\\nLet me know if you want me to explain why I picked a specific confidence level each time.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n48k5y1","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I asked Gemini to guess my dads name (and before I asked add always how confident you are with your answer from 0-100%) \\nSo gemini gave a man name popular in my country and it was correct. Answered that confidence:5% \\nGemini was lucky this time.&lt;/p&gt;\\n\\n&lt;p&gt;So yes just ask the freaking confidense %\\n&amp;quot;\\nYes, I can do that. I’ll include a confidence score (0–100%) at the end of each of my answers from now on. Just note:\\n    • 100% means I’m very sure, often based on well-established facts or direct interpretation.\\n    • 80–99% is a strong answer, but there may be edge cases or unknowns.\\n    • 50–79% means it’s more uncertain — possibly due to incomplete context or complex interpretation.\\n    • Below 50% is a guess or based on limited or speculative information.&lt;/p&gt;\\n\\n&lt;p&gt;Let me know if you want me to explain why I picked a specific confidence level each time.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4z64o/why_are_llms_not_able_to_give_an_estimate_on/n48k5y1/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753048283,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m4z64o","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-2}}],"before":null}}]`),r=()=>e.jsx(t,{data:a});export{r as default};
