import{j as e}from"./index-DACS7Nh6.js";import{R as l}from"./RedditPostRenderer-Dqa1NZuX.js";import"./index-DiMIVQx4.js";const t=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Hello everyone, \\n\\nHere is a question that has been in my head for some time. Would it be possible to lighten an LLM by removing content? \\n\\nI know it's a question that for someone really knowledgeable will be crazy and stupid. \\n\\nThe idea would be, if possible, to remove information that is not relevant to the user on a topic. \\n\\nLet's give an example: let's say we have a 3B model of parameters that needs 10 gigabytes of VRAM and only a graph of 8 gigabytes of VRAM. We could refine the model or distill it to remove information, for example, from sports and the final result would be 2.7 B of parameters. It is a theoretical question and not a real case, the numbers are invented. \\n\\n\\nBasically, see if there is a technique that allows you to reduce the size of a model (not quantize) by removing content not necessary for its use and thus improving its performance (less size, more layers in GPU) T\\n\\nhank you very much and a little patience for those of us who ask stupid questions. \\n\\nThanks a lot. \\n\\nGreetings.","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Madness, the ignorant's question. Would it be possible to lighten an LLM model?","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lz17w8","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.64,"author_flair_background_color":null,"subreddit_type":"public","ups":5,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_1mbsf8cel3","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":5,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1752434266,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Hello everyone, &lt;/p&gt;\\n\\n&lt;p&gt;Here is a question that has been in my head for some time. Would it be possible to lighten an LLM by removing content? &lt;/p&gt;\\n\\n&lt;p&gt;I know it&amp;#39;s a question that for someone really knowledgeable will be crazy and stupid. &lt;/p&gt;\\n\\n&lt;p&gt;The idea would be, if possible, to remove information that is not relevant to the user on a topic. &lt;/p&gt;\\n\\n&lt;p&gt;Let&amp;#39;s give an example: let&amp;#39;s say we have a 3B model of parameters that needs 10 gigabytes of VRAM and only a graph of 8 gigabytes of VRAM. We could refine the model or distill it to remove information, for example, from sports and the final result would be 2.7 B of parameters. It is a theoretical question and not a real case, the numbers are invented. &lt;/p&gt;\\n\\n&lt;p&gt;Basically, see if there is a technique that allows you to reduce the size of a model (not quantize) by removing content not necessary for its use and thus improving its performance (less size, more layers in GPU) T&lt;/p&gt;\\n\\n&lt;p&gt;hank you very much and a little patience for those of us who ask stupid questions. &lt;/p&gt;\\n\\n&lt;p&gt;Thanks a lot. &lt;/p&gt;\\n\\n&lt;p&gt;Greetings.&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1lz17w8","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"Macestudios32","discussion_type":null,"num_comments":26,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lz17w8/madness_the_ignorants_question_would_it_be/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lz17w8/madness_the_ignorants_question_would_it_be/","subreddit_subscribers":498850,"created_utc":1752434266,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2yn3fm","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"NandaVegg","can_mod_post":false,"created_utc":1752439711,"send_replies":true,"parent_id":"t1_n2y6d3y","score":3,"author_fullname":"t2_u97fj6y8","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Note that Nemotron models were continuously trained for 60B\\\\~ tokens after pruning and before post-training for reasoning.\\n\\nIt is not easy (likely impossible?) to simply prune weights without retraining, though pruning seems easier than upscaling which usually takes much more training time and data to \\"heal\\".\\n\\nWithout a sizeable continuous training, the pruned model would look fine in most benchmarks and maybe eval loss, but would be unusable in real use case, especially with non-deterministic sampling (i.e. non-zero temperature).","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2yn3fm","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Note that Nemotron models were continuously trained for 60B~ tokens after pruning and before post-training for reasoning.&lt;/p&gt;\\n\\n&lt;p&gt;It is not easy (likely impossible?) to simply prune weights without retraining, though pruning seems easier than upscaling which usually takes much more training time and data to &amp;quot;heal&amp;quot;.&lt;/p&gt;\\n\\n&lt;p&gt;Without a sizeable continuous training, the pruned model would look fine in most benchmarks and maybe eval loss, but would be unusable in real use case, especially with non-deterministic sampling (i.e. non-zero temperature).&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lz17w8","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lz17w8/madness_the_ignorants_question_would_it_be/n2yn3fm/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752439711,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2ygnby","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"stoppableDissolution","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2yd7pr","score":2,"author_fullname":"t2_1n0su21k4z","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Idk, I use the 49b as my daily driver and find it miles ahead of any 32b (and generally better than 70b too), especially in prompt adherence. But I use it in Q6, and it is indeed prone to shattering on lower quants.\\n\\nAlso probably depends on the task.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2ygnby","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Idk, I use the 49b as my daily driver and find it miles ahead of any 32b (and generally better than 70b too), especially in prompt adherence. But I use it in Q6, and it is indeed prone to shattering on lower quants.&lt;/p&gt;\\n\\n&lt;p&gt;Also probably depends on the task.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lz17w8","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lz17w8/madness_the_ignorants_question_would_it_be/n2ygnby/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752437812,"author_flair_text":null,"treatment_tags":[],"created_utc":1752437812,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n2yd7pr","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Lissanro","can_mod_post":false,"created_utc":1752436800,"send_replies":true,"parent_id":"t1_n2y6d3y","score":2,"author_fullname":"t2_fpfao9g","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I actually tried Nemotron 253B and it was glitchy, prone to mistakes LLMs normally unlikely to make, or only on rare occasions. Like messing up basic syntax or indentation, etc., especially in already given code. I never yet seen a successfully optimized model -Qwen3 235B for example will be much better and faster than Nemotron 253B, and model 32B models are generally better than Nemotron 49B (this is what I got based on reading feedback from others about the smaller version; I tried only 253B Nemotron myself).\\n\\nIt is interesting research and worth a try, but in practice quantization is a better way to reduce size, especially if imatrix or other methods are used that make it more efficient. This is another disadventage of Nemotron series, that it degrades more from quantization since its remaining weights have less redundancy, ultimately ending up less efficient in terms of size for the same quality.\\n\\nI also remember people trying to eliminate less used experts or merge them together, only to discover issues in some cases where they were needed and that expensive fine-tuning is needed to maintain quality, and even then, still likely to reduce it overall for little gain. Again, interesting research, but I did not see any attempts that resulted in something that I would use as my daily driver model.\\n\\nMost recent advancement in reducing model sizes was EXL3, where 4bpw version maintains practically the same quality as 5bpw EXL2 or a typical Q4 GGUF - however currently EXL3 is still considered in early development and does not support offload to RAM, only VRAM inference.","edited":1752438091,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2yd7pr","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I actually tried Nemotron 253B and it was glitchy, prone to mistakes LLMs normally unlikely to make, or only on rare occasions. Like messing up basic syntax or indentation, etc., especially in already given code. I never yet seen a successfully optimized model -Qwen3 235B for example will be much better and faster than Nemotron 253B, and model 32B models are generally better than Nemotron 49B (this is what I got based on reading feedback from others about the smaller version; I tried only 253B Nemotron myself).&lt;/p&gt;\\n\\n&lt;p&gt;It is interesting research and worth a try, but in practice quantization is a better way to reduce size, especially if imatrix or other methods are used that make it more efficient. This is another disadventage of Nemotron series, that it degrades more from quantization since its remaining weights have less redundancy, ultimately ending up less efficient in terms of size for the same quality.&lt;/p&gt;\\n\\n&lt;p&gt;I also remember people trying to eliminate less used experts or merge them together, only to discover issues in some cases where they were needed and that expensive fine-tuning is needed to maintain quality, and even then, still likely to reduce it overall for little gain. Again, interesting research, but I did not see any attempts that resulted in something that I would use as my daily driver model.&lt;/p&gt;\\n\\n&lt;p&gt;Most recent advancement in reducing model sizes was EXL3, where 4bpw version maintains practically the same quality as 5bpw EXL2 or a typical Q4 GGUF - however currently EXL3 is still considered in early development and does not support offload to RAM, only VRAM inference.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lz17w8","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lz17w8/madness_the_ignorants_question_would_it_be/n2yd7pr/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752436800,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n2y6d3y","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"stoppableDissolution","can_mod_post":false,"created_utc":1752434754,"send_replies":true,"parent_id":"t3_1lz17w8","score":13,"author_fullname":"t2_1n0su21k4z","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Kinda. Thats what nvidia does with the nemotron series, for example - they take a model and start chopping away some weights and see whether the model degrades on certain benchmarks or not. Got the 405B llama down to 253B, and 70B down to 49B, for example.  \\n  \\nOverall pruning is totally a thing, its just quite compute- and labor-intensive, so its not done very often.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2y6d3y","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Kinda. Thats what nvidia does with the nemotron series, for example - they take a model and start chopping away some weights and see whether the model degrades on certain benchmarks or not. Got the 405B llama down to 253B, and 70B down to 49B, for example.  &lt;/p&gt;\\n\\n&lt;p&gt;Overall pruning is totally a thing, its just quite compute- and labor-intensive, so its not done very often.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lz17w8/madness_the_ignorants_question_would_it_be/n2y6d3y/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752434754,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lz17w8","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":13}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2y60ib","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"RhubarbSimilar1683","can_mod_post":false,"created_utc":1752434647,"send_replies":true,"parent_id":"t3_1lz17w8","score":4,"author_fullname":"t2_1k4sjdwzk2","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I think it's called distillation for now but directly just removing parameters I don't think so. I believe you'd have to retrain the model afterwards if you remove parameters ","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2y60ib","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I think it&amp;#39;s called distillation for now but directly just removing parameters I don&amp;#39;t think so. I believe you&amp;#39;d have to retrain the model afterwards if you remove parameters &lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lz17w8/madness_the_ignorants_question_would_it_be/n2y60ib/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752434647,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lz17w8","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2zaqej","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Figai","can_mod_post":false,"created_utc":1752447255,"send_replies":true,"parent_id":"t1_n2y6kqc","score":1,"author_fullname":"t2_jq11updl","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Just one thing to think about, least used does not entail that an expert is unimportant to the overall output of the model. \\n\\nThe gating network for routing might just be very efficient and well tuned, and a single expert may contribute a lot to a final response, you’d also need to look at how much probability mass is given to that expert during the routing, if it’s extremely high it’s because fits the niche of that expert perfectly, even if it considered less important, because less overall tokens get routed to it.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2zaqej","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Just one thing to think about, least used does not entail that an expert is unimportant to the overall output of the model. &lt;/p&gt;\\n\\n&lt;p&gt;The gating network for routing might just be very efficient and well tuned, and a single expert may contribute a lot to a final response, you’d also need to look at how much probability mass is given to that expert during the routing, if it’s extremely high it’s because fits the niche of that expert perfectly, even if it considered less important, because less overall tokens get routed to it.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lz17w8","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lz17w8/madness_the_ignorants_question_would_it_be/n2zaqej/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752447255,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2yqxmh","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Macestudios32","can_mod_post":false,"created_utc":1752440851,"send_replies":true,"parent_id":"t1_n2y6kqc","score":0,"author_fullname":"t2_1mbsf8cel3","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Something like this was going through my mind, there will be experts that would not make sense of use in my case. And removing them could win by being less heavy. Hence my question, although I am a novice and it is still far from me. \\n\\n\\nBut knowledge does not take up space and I learn a lot from those who do.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2yqxmh","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Something like this was going through my mind, there will be experts that would not make sense of use in my case. And removing them could win by being less heavy. Hence my question, although I am a novice and it is still far from me. &lt;/p&gt;\\n\\n&lt;p&gt;But knowledge does not take up space and I learn a lot from those who do.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lz17w8","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lz17w8/madness_the_ignorants_question_would_it_be/n2yqxmh/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752440851,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}},"user_reports":[],"saved":false,"id":"n2y6kqc","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Limp_Manufacturer_65","can_mod_post":false,"created_utc":1752434818,"send_replies":true,"parent_id":"t3_1lz17w8","score":1,"author_fullname":"t2_kwq6f5zi","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I've been considering attempting this. Plan would be to prune the least used experts from an MoE model after running inference on relevant question set.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2y6kqc","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;ve been considering attempting this. Plan would be to prune the least used experts from an MoE model after running inference on relevant question set.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lz17w8/madness_the_ignorants_question_would_it_be/n2y6kqc/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752434818,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lz17w8","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2y8wbd","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MDT-49","can_mod_post":false,"created_utc":1752435520,"send_replies":true,"parent_id":"t3_1lz17w8","score":1,"author_fullname":"t2_h8yrica5","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I think there is some experimentation with (the combination of) pruning and distillation methods. However, it's not trivial, and LLMs don't work in a way that neatly organizes everything into categories from which you can opt out of what you don't need.\\n\\nI guess that's why the opt-in approach is the way to go (for now). Start with a smaller model and fine-tune (or train) it for the knowledge and use cases you actually want.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2y8wbd","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I think there is some experimentation with (the combination of) pruning and distillation methods. However, it&amp;#39;s not trivial, and LLMs don&amp;#39;t work in a way that neatly organizes everything into categories from which you can opt out of what you don&amp;#39;t need.&lt;/p&gt;\\n\\n&lt;p&gt;I guess that&amp;#39;s why the opt-in approach is the way to go (for now). Start with a smaller model and fine-tune (or train) it for the knowledge and use cases you actually want.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lz17w8/madness_the_ignorants_question_would_it_be/n2y8wbd/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752435520,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lz17w8","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2yimzj","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"SeymourBits","can_mod_post":false,"created_utc":1752438397,"send_replies":true,"parent_id":"t3_1lz17w8","score":3,"author_fullname":"t2_hb7wj","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"It's called \\"pruning\\" and the current implementations are like using garden shears for brain surgery. Nvidia's Nemotron models are pruned.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2yimzj","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It&amp;#39;s called &amp;quot;pruning&amp;quot; and the current implementations are like using garden shears for brain surgery. Nvidia&amp;#39;s Nemotron models are pruned.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lz17w8/madness_the_ignorants_question_would_it_be/n2yimzj/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752438397,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lz17w8","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n30xora","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"VentureSatchel","can_mod_post":false,"created_utc":1752469695,"send_replies":true,"parent_id":"t1_n2z8kao","score":1,"author_fullname":"t2_ag4wbc6o","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I suspect circuit tracing will enable this.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n30xora","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I suspect circuit tracing will enable this.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lz17w8","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lz17w8/madness_the_ignorants_question_would_it_be/n30xora/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752469695,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2z8kao","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Figai","can_mod_post":false,"created_utc":1752446532,"send_replies":true,"parent_id":"t3_1lz17w8","score":1,"author_fullname":"t2_jq11updl","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Oooh, cool idea, if we could that would be a big breakthrough or a field called mechanistic interpretability. You would need to look to see if certain neurons or circuits (clusters of specific families of neurons) correspond to a feature, a feature is roughly equivalent to a fuzzy concept or specific idea. The problem is llms don’t usually work so nicely where a single neurons refers to some information about tennis for instance. if they do, that’s called monosemanticity, there have been experiments looking towards that direction so we can interpret more of what’s going inside a model. But right now it’d be really hard to just remove sports information without destroying other parts of the model capabilities, certain bits of sports info could be extremely correlated with medical information for instance and that would all get cut out probably.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2z8kao","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Oooh, cool idea, if we could that would be a big breakthrough or a field called mechanistic interpretability. You would need to look to see if certain neurons or circuits (clusters of specific families of neurons) correspond to a feature, a feature is roughly equivalent to a fuzzy concept or specific idea. The problem is llms don’t usually work so nicely where a single neurons refers to some information about tennis for instance. if they do, that’s called monosemanticity, there have been experiments looking towards that direction so we can interpret more of what’s going inside a model. But right now it’d be really hard to just remove sports information without destroying other parts of the model capabilities, certain bits of sports info could be extremely correlated with medical information for instance and that would all get cut out probably.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lz17w8/madness_the_ignorants_question_would_it_be/n2z8kao/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752446532,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lz17w8","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2znuu1","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"mindful_maven_25","can_mod_post":false,"created_utc":1752451734,"send_replies":true,"parent_id":"t3_1lz17w8","score":1,"author_fullname":"t2_1syhds3w2f","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Selective removal of knowledge acquired by llm through pruning or distillation is not possible. You can improve the accuracy of certain domains through training with specific areas of interest which will degrade accuracy on others. But cannot completely take that knowledge away.\\n\\nIf size is what you are worried about and don't want to train the model, quantization is only the option available along with Low rank approximation. Low rank approximation is applicable to Linear components of the models (idea behind LORA adapter). Pruning, distillation needs data and retraining of the model. If however you have a LORA adapter, removing those adapters will get back your previous model. Even in MOE model, so called experts are selected through the router component during runtime so you cannot arbitrarily remove them. \\n\\nRemoving layers which don't contribute is possible (sparsity) but removing means zeroing out the parameters and to take advantage of that you need to change your inference code which in turn requires hardware which can take advantage of sparse parameters. Many hardware doesn't support random sparsification as it makes the math computation complex.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2znuu1","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Selective removal of knowledge acquired by llm through pruning or distillation is not possible. You can improve the accuracy of certain domains through training with specific areas of interest which will degrade accuracy on others. But cannot completely take that knowledge away.&lt;/p&gt;\\n\\n&lt;p&gt;If size is what you are worried about and don&amp;#39;t want to train the model, quantization is only the option available along with Low rank approximation. Low rank approximation is applicable to Linear components of the models (idea behind LORA adapter). Pruning, distillation needs data and retraining of the model. If however you have a LORA adapter, removing those adapters will get back your previous model. Even in MOE model, so called experts are selected through the router component during runtime so you cannot arbitrarily remove them. &lt;/p&gt;\\n\\n&lt;p&gt;Removing layers which don&amp;#39;t contribute is possible (sparsity) but removing means zeroing out the parameters and to take advantage of that you need to change your inference code which in turn requires hardware which can take advantage of sparse parameters. Many hardware doesn&amp;#39;t support random sparsification as it makes the math computation complex.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lz17w8/madness_the_ignorants_question_would_it_be/n2znuu1/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752451734,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lz17w8","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n318gfe","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"SAPPHIR3ROS3","can_mod_post":false,"send_replies":true,"parent_id":"t1_n30znv1","score":1,"author_fullname":"t2_2vre9dh1","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"That’s the point","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n318gfe","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;That’s the point&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lz17w8","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lz17w8/madness_the_ignorants_question_would_it_be/n318gfe/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752475342,"author_flair_text":null,"treatment_tags":[],"created_utc":1752475342,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n30znv1","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Accomplished_Ad8465","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3005bf","score":1,"author_fullname":"t2_66ye1sqb","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Then distillatoin is the best option, train a smaller “student” model to mimic a larger “teacher.” If your downstream use cases never involve sports, you can distill only on non-sports data","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n30znv1","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Then distillatoin is the best option, train a smaller “student” model to mimic a larger “teacher.” If your downstream use cases never involve sports, you can distill only on non-sports data&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lz17w8","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lz17w8/madness_the_ignorants_question_would_it_be/n30znv1/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752470676,"author_flair_text":null,"treatment_tags":[],"created_utc":1752470676,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3005bf","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"SAPPHIR3ROS3","can_mod_post":false,"created_utc":1752456118,"send_replies":true,"parent_id":"t1_n2zt1mf","score":1,"author_fullname":"t2_2vre9dh1","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"The OP specifically said to exclude quantization","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3005bf","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The OP specifically said to exclude quantization&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lz17w8","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lz17w8/madness_the_ignorants_question_would_it_be/n3005bf/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752456118,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2zt1mf","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Accomplished_Ad8465","can_mod_post":false,"created_utc":1752453599,"send_replies":true,"parent_id":"t3_1lz17w8","score":1,"author_fullname":"t2_66ye1sqb","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"It can be quantized","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2zt1mf","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It can be quantized&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lz17w8/madness_the_ignorants_question_would_it_be/n2zt1mf/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752453599,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lz17w8","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2zzla5","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"epSos-DE","can_mod_post":false,"created_utc":1752455920,"send_replies":true,"parent_id":"t3_1lz17w8","score":1,"author_fullname":"t2_5e3ax","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Domain specific scope !\\n\\n  \\nLike for coding. it would only need the scope of the language definitions and a library of very well written code examples.\\n\\n\\n\\n  \\nSame for legal documents.\\n\\nIt would only need a library of legal books and some nice examples of legal resolutions by the courts.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2zzla5","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Domain specific scope !&lt;/p&gt;\\n\\n&lt;p&gt;Like for coding. it would only need the scope of the language definitions and a library of very well written code examples.&lt;/p&gt;\\n\\n&lt;p&gt;Same for legal documents.&lt;/p&gt;\\n\\n&lt;p&gt;It would only need a library of legal books and some nice examples of legal resolutions by the courts.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lz17w8/madness_the_ignorants_question_would_it_be/n2zzla5/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752455920,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lz17w8","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2zzxs1","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"SAPPHIR3ROS3","can_mod_post":false,"created_utc":1752456043,"send_replies":true,"parent_id":"t3_1lz17w8","score":1,"author_fullname":"t2_2vre9dh1","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Everyone’s talking about about pruning  and/or simple but i think it would be better and easier to do a general finetuning (excluding what you don’t need) and then a  selective distillation after, this way you increase the “knowledge depth” in other areas and sort of remove what you don’t need. It’s all just theoretical but it could work","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2zzxs1","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Everyone’s talking about about pruning  and/or simple but i think it would be better and easier to do a general finetuning (excluding what you don’t need) and then a  selective distillation after, this way you increase the “knowledge depth” in other areas and sort of remove what you don’t need. It’s all just theoretical but it could work&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lz17w8/madness_the_ignorants_question_would_it_be/n2zzxs1/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752456043,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lz17w8","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n303kjg","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"HealthCorrect","can_mod_post":false,"created_utc":1752457355,"send_replies":true,"parent_id":"t3_1lz17w8","score":1,"author_fullname":"t2_7w7ujxhh","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Pruning a MoE will give the thing you’re asking","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n303kjg","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Pruning a MoE will give the thing you’re asking&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lz17w8/madness_the_ignorants_question_would_it_be/n303kjg/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752457355,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lz17w8","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n308p0h","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"buildmine10","can_mod_post":false,"created_utc":1752459207,"send_replies":true,"parent_id":"t3_1lz17w8","score":1,"author_fullname":"t2_9zuu2802","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"In theory yes, but it's very abstract problem. In practice, no you don't really get to choose what to lose. It's probably easier to distill knowledge into a smaller model and the train for task specific skills, but this is akin to removing a little bit of everything and adding back what you want.\\n\\nI suppose it's possible for a mixture of experts model, but you would need a base model with millions of experts (good thing we know how to do that). The hard part would be identifying which experts have information you are willing to lose.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n308p0h","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;In theory yes, but it&amp;#39;s very abstract problem. In practice, no you don&amp;#39;t really get to choose what to lose. It&amp;#39;s probably easier to distill knowledge into a smaller model and the train for task specific skills, but this is akin to removing a little bit of everything and adding back what you want.&lt;/p&gt;\\n\\n&lt;p&gt;I suppose it&amp;#39;s possible for a mixture of experts model, but you would need a base model with millions of experts (good thing we know how to do that). The hard part would be identifying which experts have information you are willing to lose.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lz17w8/madness_the_ignorants_question_would_it_be/n308p0h/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752459207,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lz17w8","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n30oewy","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DeepWisdomGuy","can_mod_post":false,"created_utc":1752465434,"send_replies":true,"parent_id":"t3_1lz17w8","score":1,"author_fullname":"t2_lznk2wv8h","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Deep structure is weird. There are modern business processes that are commonly spoken of in sports analogies. You don't really know what information you might be deleting.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n30oewy","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Deep structure is weird. There are modern business processes that are commonly spoken of in sports analogies. You don&amp;#39;t really know what information you might be deleting.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lz17w8/madness_the_ignorants_question_would_it_be/n30oewy/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752465434,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lz17w8","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n30rffx","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"robberviet","can_mod_post":false,"created_utc":1752466763,"send_replies":true,"parent_id":"t3_1lz17w8","score":1,"author_fullname":"t2_jxc5a","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Possible but not intuitive, like reverse-engineering. Easier to train on what you want.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n30rffx","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Possible but not intuitive, like reverse-engineering. Easier to train on what you want.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lz17w8/madness_the_ignorants_question_would_it_be/n30rffx/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752466763,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lz17w8","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2ydqp3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"fizzy1242","can_mod_post":false,"created_utc":1752436956,"send_replies":true,"parent_id":"t3_1lz17w8","score":1,"author_fullname":"t2_16zcsx","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"you can prune layers from a model to make it smaller but you need to be careful to not break it","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2ydqp3","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;you can prune layers from a model to make it smaller but you need to be careful to not break it&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lz17w8/madness_the_ignorants_question_would_it_be/n2ydqp3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752436956,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lz17w8","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),r=()=>e.jsx(l,{data:t});export{r as default};
