import{j as l}from"./index-xfnGEtuL.js";import{R as e}from"./RedditPostRenderer-DAZRIZZK.js";import"./index-BaNn5-RR.js";const a=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"as the title suggests i would like to run the best llm possible for my system and i am really new to llms so i really have no idea where to start help is really appreciated . ( my system has 32gb ddr5 6000mhz cl 36 ram and a amd rx 7600 with 8gb vram )","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"best llm for 32 gb ram and 8gb vram","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lto2in","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.53,"author_flair_background_color":null,"subreddit_type":"public","ups":1,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_1o4evizu1h","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":1,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1751873903,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;as the title suggests i would like to run the best llm possible for my system and i am really new to llms so i really have no idea where to start help is really appreciated . ( my system has 32gb ddr5 6000mhz cl 36 ram and a amd rx 7600 with 8gb vram )&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":true,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1lto2in","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"diddy_stroker","discussion_type":null,"num_comments":7,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lto2in/best_llm_for_32_gb_ram_and_8gb_vram/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lto2in/best_llm_for_32_gb_ram_and_8gb_vram/","subreddit_subscribers":496034,"created_utc":1751873903,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1s3k6q","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"ilintar","can_mod_post":false,"created_utc":1751880902,"send_replies":true,"parent_id":"t3_1lto2in","score":3,"author_fullname":"t2_cctud","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Qwen3 30B-A3B MoE, Q4\\\\_K\\\\_XL from Unsloth, use with -ngl 99 -ot exps=CPU in llama.cpp.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1s3k6q","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Qwen3 30B-A3B MoE, Q4_K_XL from Unsloth, use with -ngl 99 -ot exps=CPU in llama.cpp.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lto2in/best_llm_for_32_gb_ram_and_8gb_vram/n1s3k6q/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751880902,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lto2in","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1s4pzv","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"i-eat-kittens","can_mod_post":false,"created_utc":1751881577,"send_replies":true,"parent_id":"t3_1lto2in","score":2,"author_fullname":"t2_z290n","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"The best model depends on what you're trying to do. Qwen3 4B, 8B and Gemma3 4B and 12B are good starting points. The larger models won't fit entirely on your GPU (at least when you need some context), but at these sizes running some of the model on CPU and system ram should be fine. Speed takes a hit, but quality improves a lot.\\n\\nQ4 is probably your best quant size option, but you could also try Q5-Q6 to see how it affects quality and performance.\\n\\nI suggest also giving Qwen3-30B-A3B a shot. You can run it fully on cpu/system ram and should still have a bit left for other things using Q4-Q5. baidu/ERNIE-4.5-21B-A3B-PT should be even easier to run than 30B-A3B, but currently isn't widely supported.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1s4pzv","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The best model depends on what you&amp;#39;re trying to do. Qwen3 4B, 8B and Gemma3 4B and 12B are good starting points. The larger models won&amp;#39;t fit entirely on your GPU (at least when you need some context), but at these sizes running some of the model on CPU and system ram should be fine. Speed takes a hit, but quality improves a lot.&lt;/p&gt;\\n\\n&lt;p&gt;Q4 is probably your best quant size option, but you could also try Q5-Q6 to see how it affects quality and performance.&lt;/p&gt;\\n\\n&lt;p&gt;I suggest also giving Qwen3-30B-A3B a shot. You can run it fully on cpu/system ram and should still have a bit left for other things using Q4-Q5. baidu/ERNIE-4.5-21B-A3B-PT should be even easier to run than 30B-A3B, but currently isn&amp;#39;t widely supported.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lto2in/best_llm_for_32_gb_ram_and_8gb_vram/n1s4pzv/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751881577,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lto2in","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1rv1gi","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ababana97653","can_mod_post":false,"created_utc":1751875730,"send_replies":true,"parent_id":"t3_1lto2in","score":3,"author_fullname":"t2_al30xh6j","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"LM Studio will recommend a model for you based on your system configuration","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1rv1gi","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;LM Studio will recommend a model for you based on your system configuration&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lto2in/best_llm_for_32_gb_ram_and_8gb_vram/n1rv1gi/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751875730,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lto2in","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1t230u","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"No_Edge2098","can_mod_post":false,"created_utc":1751896051,"send_replies":true,"parent_id":"t3_1lto2in","score":0,"author_fullname":"t2_uaotuj04","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"For your 32 GB RAM and AMD RX 7600 (8 GB VRAM), try Llama 3.1 8B (Q4\\\\_K\\\\_M) or Mistral 7B (Q4\\\\_K\\\\_M). Both fit in 8 GB VRAM using LM Studio or llama.cpp with ROCm for AMD GPU support. Llama’s great for versatility (10-15 tokens/s), Mistral’s faster (15-20 tokens/s). Your RAM supports \\\\~8k token context. Start with LM Studio’s GUI and grab models from Hugging Face. Got a specific task in mind?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1t230u","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;For your 32 GB RAM and AMD RX 7600 (8 GB VRAM), try Llama 3.1 8B (Q4_K_M) or Mistral 7B (Q4_K_M). Both fit in 8 GB VRAM using LM Studio or llama.cpp with ROCm for AMD GPU support. Llama’s great for versatility (10-15 tokens/s), Mistral’s faster (15-20 tokens/s). Your RAM supports ~8k token context. Start with LM Studio’s GUI and grab models from Hugging Face. Got a specific task in mind?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lto2in/best_llm_for_32_gb_ram_and_8gb_vram/n1t230u/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751896051,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lto2in","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1rug6p","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MachinaVerum","can_mod_post":false,"created_utc":1751875380,"send_replies":true,"parent_id":"t3_1lto2in","score":1,"author_fullname":"t2_6ybfgv5i","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"mimo-7b-rl-gguf, go for a q4","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1rug6p","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;mimo-7b-rl-gguf, go for a q4&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lto2in/best_llm_for_32_gb_ram_and_8gb_vram/n1rug6p/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751875380,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lto2in","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1rss7t","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"unserioustroller","can_mod_post":false,"created_utc":1751874396,"send_replies":true,"parent_id":"t3_1lto2in","score":0,"author_fullname":"t2_1iislbtdvk","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"look for models below 4b. like gemma or qwen","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1rss7t","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;look for models below 4b. like gemma or qwen&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lto2in/best_llm_for_32_gb_ram_and_8gb_vram/n1rss7t/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751874396,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lto2in","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":1,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}}]`),s=()=>l.jsx(e,{data:a});export{s as default};
