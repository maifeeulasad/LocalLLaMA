import{j as e}from"./index-DQXiEb7D.js";import{R as t}from"./RedditPostRenderer-BjndLgq8.js";import"./index-B-ILyjT1.js";const l=JSON.parse('[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"https://preview.redd.it/5wzktz5ijoaf1.png?width=1024&amp;format=png&amp;auto=webp&amp;s=b0025a13ccbd3337b1a11b34ca3ee53f5ec733d7\\n\\n# \\n\\n  \\n*On Day 8, we looked at what Rotary Positional Embeddings (RoPE) are and why they are important in transformers.*\\n\\n*Today, on Day 9, we’re going to code RoPE and see how it’s implemented in the DeepSeek Children’s Stories model, a transformer architecture optimized for generating engaging stories for kids.*\\n\\n*Quick Recap: What is RoPE?*\\n\\n*RoPE is a method for injecting positional information into transformer models, not by adding position vectors (like absolute positional embeddings), but by rotating the query and key vectors within the attention mechanism.*\\n\\n*This provides several advantages:*\\n\\n* ***Relative Position Awareness****: Understands the distance between tokens*\\n* ***Extrapolation****: Handles sequences longer than seen during training*\\n* ***Efficiency****: Doesn’t require additional embeddings — just math inside attention*\\n\\n#  Code Walkthrough\\n\\n*Let’s walk through how RoPE is implemented in the DeepSeek-Children-Stories-15M-model* [*https://github.com/ideaweaver-ai/DeepSeek-Children-Stories-15M-model*](https://github.com/ideaweaver-ai/DeepSeek-Children-Stories-15M-model) *codebase.*\\n\\n# 1: Implementation: RoPEPositionalEncoding\\n\\n*In the file src/model/deepseek.py, you’ll find the class RoPEPositionalEncoding.*\\n\\n*This class:*\\n\\n* *Precomputes rotation frequencies*\\n* *Provides an apply\\\\_rope method*\\n* *Applies RoPE to input tensors, usually the query and key vectors*\\n\\n&amp;#8203;\\n\\n    # deepseek.py\\n    class RoPEPositionalEncoding(nn.Module):\\n        def __init__(self, dim, max_len=2048):\\n            super().__init__()\\n            inv_freq = 1.0 / (10000 ** (torch.arange(0, dim, 2).float() / dim))\\n            t = torch.arange(max_len, dtype=torch.float)\\n            freqs = torch.einsum(\\"i,j-&gt;ij\\", t, inv_freq)\\n            emb = torch.cat((freqs.sin(), freqs.cos()), dim=-1)\\n            self.register_buffer(\\"positional_encoding\\", emb)\\n\\n        def apply_rope(self, x, position_ids):\\n            rope = self.positional_encoding[position_ids]\\n            x1, x2 = x[..., ::2], x[..., 1::2]\\n            rope1, rope2 = rope[..., ::2], rope[..., 1::2]\\n            return torch.cat([x1 * rope2 + x2 * rope1, x2 * rope2 - x1 * rope1], dim=-1)\\n\\n&gt;***Note****: The key idea is rotating even and odd dimensions of the query/key vectors based on sine and cosine frequencies.*\\n\\n# 2: Usage: Integrating RoPE into Attention\\n\\n*The DeepSeek model utilizes a custom attention mechanism known as Multihead Latent Attention (MLA). Here’s how RoPE is integrated:*\\n\\n    # deepseek.py\\n    q = self.q_proj(x)\\n    k = self.k_proj(x)\\n\\n    q = self.rope.apply_rope(q, position_ids)\\n    k = self.rope.apply_rope(k, position_ids)\\n\\n*What’s happening?*\\n\\n* `x` *is projected into query (*`q`*) and key (*`k`*) vectors.*\\n* *RoPE is applied to both using apply\\\\_rope, injecting position awareness.*\\n* *Attention proceeds as usual — except now the queries and keys are aware of their relative positions.*\\n\\n# 3: Where RoPE is Used\\n\\n* ***Every Transformer Block****: Each block in the DeepSeek model uses MLA and applies RoPE.*\\n* ***During Both Training and Inference****: RoPE is always on, helping the model understand the token sequence no matter the mode.*\\n\\n# Why RoPE is Perfect for Story Generation\\n\\n*In story generation, especially for children’s stories, context is everything.*\\n\\n*RoPE enables the model to:*\\n\\n* *Track who did what across paragraphs*\\n* *Maintain chronological consistency*\\n* *Preserve narrative flow even in long outputs*\\n\\n*This is crucial when the model must remember that “the dragon flew over the mountain” five paragraphs ago.*\\n\\n# Conclusion\\n\\n*Rotary Positional Embeddings (RoPE) are not just a theoretical improvement; they offer practical performance and generalization benefits.*\\n\\n*If you’re working on any transformer-based task with long sequences, story generation, document QA, or chat history modeling, you should absolutely consider using RoPE.*\\n\\n*Next Up (Day 10): We’ll dive into one of my favorite topics , model distillation: what it is, how it works, and why it’s so powerful.*\\n\\n*Codebase:* [*https://github.com/ideaweaver-ai/DeepSeek-Children-Stories-15M-model*](https://github.com/ideaweaver-ai/DeepSeek-Children-Stories-15M-model)","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Day 9/50: Building a Small Language Model from Scratch — Coding Rotary Positional Embeddings (RoPE)","link_flair_richtext":[{"e":"text","t":"Discussion"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":70,"top_awarded_type":null,"hide_score":false,"media_metadata":{"5wzktz5ijoaf1":{"status":"valid","e":"Image","m":"image/png","p":[{"y":108,"x":108,"u":"https://preview.redd.it/5wzktz5ijoaf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=7ee71cc035bbb52ca909828163bda5b700206248"},{"y":216,"x":216,"u":"https://preview.redd.it/5wzktz5ijoaf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=5e24bae63fa7b1f4370b715903e1f5a396a9d526"},{"y":320,"x":320,"u":"https://preview.redd.it/5wzktz5ijoaf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=a128849d5dd8f60b1fd8797320b64ca086a52958"},{"y":640,"x":640,"u":"https://preview.redd.it/5wzktz5ijoaf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=f3a06d4af88d05b20c654cb11c9bab89e85592f4"},{"y":960,"x":960,"u":"https://preview.redd.it/5wzktz5ijoaf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=9de890d9da4436870159e3dd2fee03f2b049f914"}],"s":{"y":1024,"x":1024,"u":"https://preview.redd.it/5wzktz5ijoaf1.png?width=1024&amp;format=png&amp;auto=webp&amp;s=b0025a13ccbd3337b1a11b34ca3ee53f5ec733d7"},"id":"5wzktz5ijoaf1"}},"name":"t3_1lqsvmf","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.85,"author_flair_background_color":null,"ups":17,"total_awards_received":0,"media_embed":{},"thumbnail_width":140,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_8ht7a116","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Discussion","can_mod_post":false,"score":17,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"https://external-preview.redd.it/_5Ra3RoNdh4C2mkNryyCaQAOI7vzi8pOsjW50OxYvoo.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=d67d337b55965c2b5d71b5eef82e922385ee69a1","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"post_hint":"self","content_categories":null,"is_self":true,"subreddit_type":"public","created":1751557433,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;&lt;a href=\\"https://preview.redd.it/5wzktz5ijoaf1.png?width=1024&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b0025a13ccbd3337b1a11b34ca3ee53f5ec733d7\\"&gt;https://preview.redd.it/5wzktz5ijoaf1.png?width=1024&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=b0025a13ccbd3337b1a11b34ca3ee53f5ec733d7&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;&lt;em&gt;On Day 8, we looked at what Rotary Positional Embeddings (RoPE) are and why they are important in transformers.&lt;/em&gt;&lt;/p&gt;\\n\\n&lt;p&gt;&lt;em&gt;Today, on Day 9, we’re going to code RoPE and see how it’s implemented in the DeepSeek Children’s Stories model, a transformer architecture optimized for generating engaging stories for kids.&lt;/em&gt;&lt;/p&gt;\\n\\n&lt;p&gt;&lt;em&gt;Quick Recap: What is RoPE?&lt;/em&gt;&lt;/p&gt;\\n\\n&lt;p&gt;&lt;em&gt;RoPE is a method for injecting positional information into transformer models, not by adding position vectors (like absolute positional embeddings), but by rotating the query and key vectors within the attention mechanism.&lt;/em&gt;&lt;/p&gt;\\n\\n&lt;p&gt;&lt;em&gt;This provides several advantages:&lt;/em&gt;&lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;&lt;strong&gt;&lt;em&gt;Relative Position Awareness&lt;/em&gt;&lt;/strong&gt;&lt;em&gt;: Understands the distance between tokens&lt;/em&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;strong&gt;&lt;em&gt;Extrapolation&lt;/em&gt;&lt;/strong&gt;&lt;em&gt;: Handles sequences longer than seen during training&lt;/em&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;strong&gt;&lt;em&gt;Efficiency&lt;/em&gt;&lt;/strong&gt;&lt;em&gt;: Doesn’t require additional embeddings — just math inside attention&lt;/em&gt;&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;h1&gt; Code Walkthrough&lt;/h1&gt;\\n\\n&lt;p&gt;&lt;em&gt;Let’s walk through how RoPE is implemented in the DeepSeek-Children-Stories-15M-model&lt;/em&gt; &lt;a href=\\"https://github.com/ideaweaver-ai/DeepSeek-Children-Stories-15M-model\\"&gt;&lt;em&gt;https://github.com/ideaweaver-ai/DeepSeek-Children-Stories-15M-model&lt;/em&gt;&lt;/a&gt; &lt;em&gt;codebase.&lt;/em&gt;&lt;/p&gt;\\n\\n&lt;h1&gt;1: Implementation: RoPEPositionalEncoding&lt;/h1&gt;\\n\\n&lt;p&gt;&lt;em&gt;In the file src/model/deepseek.py, you’ll find the class RoPEPositionalEncoding.&lt;/em&gt;&lt;/p&gt;\\n\\n&lt;p&gt;&lt;em&gt;This class:&lt;/em&gt;&lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;&lt;em&gt;Precomputes rotation frequencies&lt;/em&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;em&gt;Provides an apply_rope method&lt;/em&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;em&gt;Applies RoPE to input tensors, usually the query and key vectors&lt;/em&gt;&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;p&gt;&amp;#8203;&lt;/p&gt;\\n\\n&lt;pre&gt;&lt;code&gt;# deepseek.py\\nclass RoPEPositionalEncoding(nn.Module):\\n    def __init__(self, dim, max_len=2048):\\n        super().__init__()\\n        inv_freq = 1.0 / (10000 ** (torch.arange(0, dim, 2).float() / dim))\\n        t = torch.arange(max_len, dtype=torch.float)\\n        freqs = torch.einsum(&amp;quot;i,j-&amp;gt;ij&amp;quot;, t, inv_freq)\\n        emb = torch.cat((freqs.sin(), freqs.cos()), dim=-1)\\n        self.register_buffer(&amp;quot;positional_encoding&amp;quot;, emb)\\n\\n    def apply_rope(self, x, position_ids):\\n        rope = self.positional_encoding[position_ids]\\n        x1, x2 = x[..., ::2], x[..., 1::2]\\n        rope1, rope2 = rope[..., ::2], rope[..., 1::2]\\n        return torch.cat([x1 * rope2 + x2 * rope1, x2 * rope2 - x1 * rope1], dim=-1)\\n&lt;/code&gt;&lt;/pre&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;Note&lt;/em&gt;&lt;/strong&gt;&lt;em&gt;: The key idea is rotating even and odd dimensions of the query/key vectors based on sine and cosine frequencies.&lt;/em&gt;&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;h1&gt;2: Usage: Integrating RoPE into Attention&lt;/h1&gt;\\n\\n&lt;p&gt;&lt;em&gt;The DeepSeek model utilizes a custom attention mechanism known as Multihead Latent Attention (MLA). Here’s how RoPE is integrated:&lt;/em&gt;&lt;/p&gt;\\n\\n&lt;pre&gt;&lt;code&gt;# deepseek.py\\nq = self.q_proj(x)\\nk = self.k_proj(x)\\n\\nq = self.rope.apply_rope(q, position_ids)\\nk = self.rope.apply_rope(k, position_ids)\\n&lt;/code&gt;&lt;/pre&gt;\\n\\n&lt;p&gt;&lt;em&gt;What’s happening?&lt;/em&gt;&lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;&lt;code&gt;x&lt;/code&gt; &lt;em&gt;is projected into query (&lt;/em&gt;&lt;code&gt;q&lt;/code&gt;&lt;em&gt;) and key (&lt;/em&gt;&lt;code&gt;k&lt;/code&gt;&lt;em&gt;) vectors.&lt;/em&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;em&gt;RoPE is applied to both using apply_rope, injecting position awareness.&lt;/em&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;em&gt;Attention proceeds as usual — except now the queries and keys are aware of their relative positions.&lt;/em&gt;&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;h1&gt;3: Where RoPE is Used&lt;/h1&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;&lt;strong&gt;&lt;em&gt;Every Transformer Block&lt;/em&gt;&lt;/strong&gt;&lt;em&gt;: Each block in the DeepSeek model uses MLA and applies RoPE.&lt;/em&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;strong&gt;&lt;em&gt;During Both Training and Inference&lt;/em&gt;&lt;/strong&gt;&lt;em&gt;: RoPE is always on, helping the model understand the token sequence no matter the mode.&lt;/em&gt;&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;h1&gt;Why RoPE is Perfect for Story Generation&lt;/h1&gt;\\n\\n&lt;p&gt;&lt;em&gt;In story generation, especially for children’s stories, context is everything.&lt;/em&gt;&lt;/p&gt;\\n\\n&lt;p&gt;&lt;em&gt;RoPE enables the model to:&lt;/em&gt;&lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;&lt;em&gt;Track who did what across paragraphs&lt;/em&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;em&gt;Maintain chronological consistency&lt;/em&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;em&gt;Preserve narrative flow even in long outputs&lt;/em&gt;&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;p&gt;&lt;em&gt;This is crucial when the model must remember that “the dragon flew over the mountain” five paragraphs ago.&lt;/em&gt;&lt;/p&gt;\\n\\n&lt;h1&gt;Conclusion&lt;/h1&gt;\\n\\n&lt;p&gt;&lt;em&gt;Rotary Positional Embeddings (RoPE) are not just a theoretical improvement; they offer practical performance and generalization benefits.&lt;/em&gt;&lt;/p&gt;\\n\\n&lt;p&gt;&lt;em&gt;If you’re working on any transformer-based task with long sequences, story generation, document QA, or chat history modeling, you should absolutely consider using RoPE.&lt;/em&gt;&lt;/p&gt;\\n\\n&lt;p&gt;&lt;em&gt;Next Up (Day 10): We’ll dive into one of my favorite topics , model distillation: what it is, how it works, and why it’s so powerful.&lt;/em&gt;&lt;/p&gt;\\n\\n&lt;p&gt;&lt;em&gt;Codebase:&lt;/em&gt; &lt;a href=\\"https://github.com/ideaweaver-ai/DeepSeek-Children-Stories-15M-model\\"&gt;&lt;em&gt;https://github.com/ideaweaver-ai/DeepSeek-Children-Stories-15M-model&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"preview":{"images":[{"source":{"url":"https://external-preview.redd.it/_5Ra3RoNdh4C2mkNryyCaQAOI7vzi8pOsjW50OxYvoo.png?auto=webp&amp;s=0c27ab9b5764679c50e0adc71710dfe3f9448763","width":1200,"height":600},"resolutions":[{"url":"https://external-preview.redd.it/_5Ra3RoNdh4C2mkNryyCaQAOI7vzi8pOsjW50OxYvoo.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=e02674ee596709cdf5dd29ebf3417f363bd55eda","width":108,"height":54},{"url":"https://external-preview.redd.it/_5Ra3RoNdh4C2mkNryyCaQAOI7vzi8pOsjW50OxYvoo.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=c7c566f65ff67005168e511f30aefa29a58f5d6b","width":216,"height":108},{"url":"https://external-preview.redd.it/_5Ra3RoNdh4C2mkNryyCaQAOI7vzi8pOsjW50OxYvoo.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=9345b7ed424e9f4076418fc4b52f3cd0080f8083","width":320,"height":160},{"url":"https://external-preview.redd.it/_5Ra3RoNdh4C2mkNryyCaQAOI7vzi8pOsjW50OxYvoo.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=438b67c1d9deb5dd0f97a1c609b743a0ba02da9a","width":640,"height":320},{"url":"https://external-preview.redd.it/_5Ra3RoNdh4C2mkNryyCaQAOI7vzi8pOsjW50OxYvoo.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=da546090177ad66dad893ac1b08290d8c72a16af","width":960,"height":480},{"url":"https://external-preview.redd.it/_5Ra3RoNdh4C2mkNryyCaQAOI7vzi8pOsjW50OxYvoo.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=140c7aa532bc0b58e2d5708f61190e9112bc36e0","width":1080,"height":540}],"variants":{},"id":"_5Ra3RoNdh4C2mkNryyCaQAOI7vzi8pOsjW50OxYvoo"}],"enabled":false},"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"5f921ea4-c7bc-11ed-9c23-3a00622979b4","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"mod_note":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"num_reports":null,"removal_reason":null,"link_flair_background_color":"#646d73","id":"1lqsvmf","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"Prashant-Lakhera","discussion_type":null,"num_comments":3,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lqsvmf/day_950_building_a_small_language_model_from/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lqsvmf/day_950_building_a_small_language_model_from/","subreddit_subscribers":494198,"created_utc":1751557433,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n169ssq","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Russ_Dill","can_mod_post":false,"created_utc":1751568144,"send_replies":true,"parent_id":"t3_1lqsvmf","score":1,"author_fullname":"t2_99roi","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"These are great, but seem to assume the reader already well versed in ML to the point of not really needing this guide.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n169ssq","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;These are great, but seem to assume the reader already well versed in ML to the point of not really needing this guide.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqsvmf/day_950_building_a_small_language_model_from/n169ssq/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751568144,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lqsvmf","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n16aian","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Limp_Classroom_2645","can_mod_post":false,"created_utc":1751568349,"send_replies":true,"parent_id":"t3_1lqsvmf","score":1,"author_fullname":"t2_1lwf5vg68e","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"love these","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n16aian","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;love these&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqsvmf/day_950_building_a_small_language_model_from/n16aian/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751568349,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lqsvmf","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n16s2ne","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Secure_Reflection409","can_mod_post":false,"created_utc":1751573547,"send_replies":true,"parent_id":"t3_1lqsvmf","score":1,"author_fullname":"t2_by77ogdhr","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"These are awesome, thanks.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n16s2ne","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;These are awesome, thanks.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqsvmf/day_950_building_a_small_language_model_from/n16s2ne/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751573547,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lqsvmf","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]'),i=()=>e.jsx(t,{data:l});export{i as default};
