import{j as e}from"./index-Cd3v0jxz.js";import{R as l}from"./RedditPostRenderer-cI96YLhy.js";import"./index-DGBoKyQm.js";const t=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Hey everyone,\\n\\nI'm trying to get an LLM to analyze a bunch of documents (around 30 PDFs or TXT files), but I’m running into some issues. These are pretty sensitive communications, so keeping everything local is a must – no sending them off to online services!\\n\\nI've been playing around with LM Studio, but it seems like it can only handle a few files at a time. It processes 2 or 3 PDFs, grabs some info from them, and then just stops. I really want the LLM to look at all my documents every time I ask it something, re-checking everything as needed.  I'm not worried about how long it takes to respond – I just need it to be thorough.\\n\\nDoes anyone have any suggestions for other local LLM tools that can handle a larger document set? Something that doesn’t get overwhelmed by 30 files. Or, are there any online LLM services out there that actually guarantee data privacy and security?  I'm looking for something more than just the usual \\"we protect your data\\" – I need real assurances.\\n\\nAny advice would be appreciated!   \\nThanks","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Need Help - Local LLM &amp; Lots of Files! (Privacy Concerns)","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1m73q8n","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.83,"author_flair_background_color":null,"subreddit_type":"public","ups":4,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_1bvl9ir2ne","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":4,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1753259199,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;m trying to get an LLM to analyze a bunch of documents (around 30 PDFs or TXT files), but I’m running into some issues. These are pretty sensitive communications, so keeping everything local is a must – no sending them off to online services!&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;ve been playing around with LM Studio, but it seems like it can only handle a few files at a time. It processes 2 or 3 PDFs, grabs some info from them, and then just stops. I really want the LLM to look at all my documents every time I ask it something, re-checking everything as needed.  I&amp;#39;m not worried about how long it takes to respond – I just need it to be thorough.&lt;/p&gt;\\n\\n&lt;p&gt;Does anyone have any suggestions for other local LLM tools that can handle a larger document set? Something that doesn’t get overwhelmed by 30 files. Or, are there any online LLM services out there that actually guarantee data privacy and security?  I&amp;#39;m looking for something more than just the usual &amp;quot;we protect your data&amp;quot; – I need real assurances.&lt;/p&gt;\\n\\n&lt;p&gt;Any advice would be appreciated!&lt;br/&gt;\\nThanks&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1m73q8n","is_robot_indexable":true,"num_duplicates":2,"report_reasons":null,"author":"AreBee73","discussion_type":null,"num_comments":5,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1m73q8n/need_help_local_llm_lots_of_files_privacy_concerns/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1m73q8n/need_help_local_llm_lots_of_files_privacy_concerns/","subreddit_subscribers":503518,"created_utc":1753259199,"num_crossposts":2,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4oqzqp","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Fit-Investment-7543","can_mod_post":false,"created_utc":1753265898,"send_replies":true,"parent_id":"t3_1m73q8n","score":2,"author_fullname":"t2_ax5zij2n","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"ok a) ollama+ OpenWebUI or b) ollama + n8n(community edition) for RAG (+ small 7b model)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4oqzqp","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;ok a) ollama+ OpenWebUI or b) ollama + n8n(community edition) for RAG (+ small 7b model)&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m73q8n/need_help_local_llm_lots_of_files_privacy_concerns/n4oqzqp/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753265898,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m73q8n","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4ogbr1","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"steezy13312","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4og2qv","score":3,"author_fullname":"t2_rfjj2","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I’ve used AnythingLLM with decent success on a similar setup. I usually host via docker but they also have a desktop install version too.\\nIf you can install and host Ollama and download a decent 7B model and a small embedding model like nomic-embed then this setup should work for you. ","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n4ogbr1","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I’ve used AnythingLLM with decent success on a similar setup. I usually host via docker but they also have a desktop install version too.\\nIf you can install and host Ollama and download a decent 7B model and a small embedding model like nomic-embed then this setup should work for you. &lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m73q8n","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m73q8n/need_help_local_llm_lots_of_files_privacy_concerns/n4ogbr1/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753259919,"author_flair_text":null,"treatment_tags":[],"created_utc":1753259919,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n4og2qv","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AreBee73","can_mod_post":false,"created_utc":1753259773,"send_replies":true,"parent_id":"t1_n4ofj5i","score":1,"author_fullname":"t2_1bvl9ir2ne","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I'm running Windows 11, CPU, Ryzen 5 5600X,I've got 32GB of Dual-Channel DDR4 and i'm using an 8GB MSI AMD Radeon RX 6600. \\n\\nI'm aware my hardware might be considered \\"dated,\\" but I assume this mainly affects speed, not the amount of documentation the LLM can consult, right?\\n\\nThanks,","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4og2qv","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;m running Windows 11, CPU, Ryzen 5 5600X,I&amp;#39;ve got 32GB of Dual-Channel DDR4 and i&amp;#39;m using an 8GB MSI AMD Radeon RX 6600. &lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;m aware my hardware might be considered &amp;quot;dated,&amp;quot; but I assume this mainly affects speed, not the amount of documentation the LLM can consult, right?&lt;/p&gt;\\n\\n&lt;p&gt;Thanks,&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m73q8n","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m73q8n/need_help_local_llm_lots_of_files_privacy_concerns/n4og2qv/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753259773,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4ofj5i","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"steezy13312","can_mod_post":false,"created_utc":1753259457,"send_replies":true,"parent_id":"t3_1m73q8n","score":1,"author_fullname":"t2_rfjj2","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"What’s your hardware? And what’s the average size of the files you’re talking about?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4ofj5i","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;What’s your hardware? And what’s the average size of the files you’re talking about?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m73q8n/need_help_local_llm_lots_of_files_privacy_concerns/n4ofj5i/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753259457,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m73q8n","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4qofb5","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Reason_is_Key","can_mod_post":false,"created_utc":1753288843,"send_replies":true,"parent_id":"t3_1m73q8n","score":2,"author_fullname":"t2_15j6i9","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I’d recommend trying Retab, it’s designed for structured extraction from PDFs and large file batches, even with messy or scanned docs.\\n\\nIt’s not a local LLM tool (so maybe not exactly what you’re asking for), but it *does* offer strong enterprise-grade privacy: GDPR, SOC2, ISO, and no data ever used for training.\\n\\nI’ve used it on 50+ docs at once, and it handled them way better than most local tools I tried. There is a free trial if you want to check it out !","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4qofb5","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I’d recommend trying Retab, it’s designed for structured extraction from PDFs and large file batches, even with messy or scanned docs.&lt;/p&gt;\\n\\n&lt;p&gt;It’s not a local LLM tool (so maybe not exactly what you’re asking for), but it &lt;em&gt;does&lt;/em&gt; offer strong enterprise-grade privacy: GDPR, SOC2, ISO, and no data ever used for training.&lt;/p&gt;\\n\\n&lt;p&gt;I’ve used it on 50+ docs at once, and it handled them way better than most local tools I tried. There is a free trial if you want to check it out !&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m73q8n/need_help_local_llm_lots_of_files_privacy_concerns/n4qofb5/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753288843,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m73q8n","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}}]`),r=()=>e.jsx(l,{data:t});export{r as default};
