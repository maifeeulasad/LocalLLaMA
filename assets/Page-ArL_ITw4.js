import{j as e}from"./index-DLSqWzaI.js";import{R as l}from"./RedditPostRenderer-CysRo2D_.js";import"./index-COXiL3Lo.js";const a=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Hi,  \\nI'm looking for a solid open-source coding agent that can run entirely with local models. I haven’t come across anything that really fits that need yet.\\n\\nI'm planning to build a lightweight CLI tool to handle everyday tasks like debugging, semantic search, and general code assistance.\\n\\nIf you know of any suitable small language models (SLMs) that could power something like this locally—ideally something that runs efficiently on CPU or modest GPU setups—I’d really appreciate the recommendations.","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"SLM for local coding assistance","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1m09rbh","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.67,"author_flair_background_color":null,"subreddit_type":"public","ups":2,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_intoh3lv","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":2,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1752558365,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Hi,&lt;br/&gt;\\nI&amp;#39;m looking for a solid open-source coding agent that can run entirely with local models. I haven’t come across anything that really fits that need yet.&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;m planning to build a lightweight CLI tool to handle everyday tasks like debugging, semantic search, and general code assistance.&lt;/p&gt;\\n\\n&lt;p&gt;If you know of any suitable small language models (SLMs) that could power something like this locally—ideally something that runs efficiently on CPU or modest GPU setups—I’d really appreciate the recommendations.&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1m09rbh","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"callmedevilthebad","discussion_type":null,"num_comments":4,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1m09rbh/slm_for_local_coding_assistance/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1m09rbh/slm_for_local_coding_assistance/","subreddit_subscribers":499295,"created_utc":1752558365,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n37vgn1","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"callmedevilthebad","can_mod_post":false,"send_replies":true,"parent_id":"t1_n37tqr2","score":1,"author_fullname":"t2_intoh3lv","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Thanks this helps. :)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n37vgn1","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thanks this helps. :)&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m09rbh","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m09rbh/slm_for_local_coding_assistance/n37vgn1/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752561638,"author_flair_text":null,"treatment_tags":[],"created_utc":1752561638,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n37tqr2","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"SoAp9035","can_mod_post":false,"send_replies":true,"parent_id":"t1_n37sxqi","score":2,"author_fullname":"t2_78asa0m0","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Hmm... Below that parameter is not very usable, but you can try new models. What about EXAONE 4.0 1.2B? It is a new model and seems to outperform Qwen3 0.6B and Qwen3 1.7B in coding.\\n\\nIf you are going for GGUFs, don't forget to use high quantization because small models suffer a lot from quantization; go for Q8.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n37tqr2","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Hmm... Below that parameter is not very usable, but you can try new models. What about EXAONE 4.0 1.2B? It is a new model and seems to outperform Qwen3 0.6B and Qwen3 1.7B in coding.&lt;/p&gt;\\n\\n&lt;p&gt;If you are going for GGUFs, don&amp;#39;t forget to use high quantization because small models suffer a lot from quantization; go for Q8.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m09rbh","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m09rbh/slm_for_local_coding_assistance/n37tqr2/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752560699,"author_flair_text":null,"treatment_tags":[],"created_utc":1752560699,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n37sxqi","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"callmedevilthebad","can_mod_post":false,"created_utc":1752560260,"send_replies":true,"parent_id":"t1_n37rr6n","score":1,"author_fullname":"t2_intoh3lv","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Hi thanks for your response, really appreciate. I am willing to open source this as well, so that my teammates and other people can use this too.  so 4B still sounds like a big model to me.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n37sxqi","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Hi thanks for your response, really appreciate. I am willing to open source this as well, so that my teammates and other people can use this too.  so 4B still sounds like a big model to me.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m09rbh","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m09rbh/slm_for_local_coding_assistance/n37sxqi/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752560260,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n37rr6n","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"SoAp9035","can_mod_post":false,"created_utc":1752559634,"send_replies":true,"parent_id":"t3_1m09rbh","score":2,"author_fullname":"t2_78asa0m0","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I recommend Qwen3 4B and go for the highest quantization you can afford.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n37rr6n","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I recommend Qwen3 4B and go for the highest quantization you can afford.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m09rbh/slm_for_local_coding_assistance/n37rr6n/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752559634,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m09rbh","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}}]`),n=()=>e.jsx(l,{data:a});export{n as default};
