import{j as e}from"./index-Cd3v0jxz.js";import{R as a}from"./RedditPostRenderer-cI96YLhy.js";import"./index-DGBoKyQm.js";const t=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"I've been reading papers where the main contribution is creating a synthetic dataset for a specific task, followed by fine-tuning an LLM on it. One thing I keep noticing: most of them don't seem to perform hyperparameter tuning (e.g., learning rate, epochs, weight decay) using a validation set. Instead, they just reuse common/default values.\\n\\nI'm wondering—why is this so common?\\n\\n* Is it because hyperparameter tuning is considered less important, so they did search but skipped reporting it?\\n* Or is it because the main contribution is in data creation, so they just don't care much about the fine-tuning details?","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Why do many papers skip hyperparameter search?","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1m7503r","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.87,"author_flair_background_color":null,"subreddit_type":"public","ups":11,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_5z9ud297u","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":11,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1753264251,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;ve been reading papers where the main contribution is creating a synthetic dataset for a specific task, followed by fine-tuning an LLM on it. One thing I keep noticing: most of them don&amp;#39;t seem to perform hyperparameter tuning (e.g., learning rate, epochs, weight decay) using a validation set. Instead, they just reuse common/default values.&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;m wondering—why is this so common?&lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;Is it because hyperparameter tuning is considered less important, so they did search but skipped reporting it?&lt;/li&gt;\\n&lt;li&gt;Or is it because the main contribution is in data creation, so they just don&amp;#39;t care much about the fine-tuning details?&lt;/li&gt;\\n&lt;/ul&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1m7503r","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"hwanchang","discussion_type":null,"num_comments":4,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1m7503r/why_do_many_papers_skip_hyperparameter_search/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1m7503r/why_do_many_papers_skip_hyperparameter_search/","subreddit_subscribers":503757,"created_utc":1753264251,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4orbfo","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"hwanchang","can_mod_post":false,"created_utc":1753266068,"send_replies":true,"parent_id":"t1_n4op3kx","score":1,"author_fullname":"t2_5z9ud297u","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I’ve never trained at that scale, so I didn’t realize that—makes sense now. Thanks a lot!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4orbfo","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I’ve never trained at that scale, so I didn’t realize that—makes sense now. Thanks a lot!&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m7503r","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7503r/why_do_many_papers_skip_hyperparameter_search/n4orbfo/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753266068,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4q8rgx","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"indicava","can_mod_post":false,"created_utc":1753284496,"send_replies":true,"parent_id":"t1_n4op3kx","score":1,"author_fullname":"t2_4dvff","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"As OP stated, most papers fine tune rather then train a model from scratch. Nobody fine tunes on 15T tokens, datasets are significantly smaller in the 50M-500M token range. Hyper parameters still matter, even the most basic thing like batch size can affect loss significantly. Also, for RL, they matter much more. Even a 0.01 change to the kl coefficient can produce dramatically different results.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4q8rgx","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;As OP stated, most papers fine tune rather then train a model from scratch. Nobody fine tunes on 15T tokens, datasets are significantly smaller in the 50M-500M token range. Hyper parameters still matter, even the most basic thing like batch size can affect loss significantly. Also, for RL, they matter much more. Even a 0.01 change to the kl coefficient can produce dramatically different results.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":true,"can_gild":false,"link_id":"t3_1m7503r","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7503r/why_do_many_papers_skip_hyperparameter_search/n4q8rgx/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753284496,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4op3kx","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Amgadoz","can_mod_post":false,"created_utc":1753264888,"send_replies":true,"parent_id":"t3_1m7503r","score":14,"author_fullname":"t2_3el21u3z","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"1. LLM training is very expensive, there's only a budget for a few training runs.\\n2. The industry has converged on very good values for the most popular hyper parameters, changing the lr from. 3e-5 to 5e-5 wouldn't make that much difference when you are training on 15T tokens as these big models are very good function approximators\\n3. Most performance improvements come from working on the data. This is true for all ML models, but especially important for language models as we have many sources of data that can be used to train the model.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4op3kx","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;ol&gt;\\n&lt;li&gt;LLM training is very expensive, there&amp;#39;s only a budget for a few training runs.&lt;/li&gt;\\n&lt;li&gt;The industry has converged on very good values for the most popular hyper parameters, changing the lr from. 3e-5 to 5e-5 wouldn&amp;#39;t make that much difference when you are training on 15T tokens as these big models are very good function approximators&lt;/li&gt;\\n&lt;li&gt;Most performance improvements come from working on the data. This is true for all ML models, but especially important for language models as we have many sources of data that can be used to train the model.&lt;/li&gt;\\n&lt;/ol&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7503r/why_do_many_papers_skip_hyperparameter_search/n4op3kx/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753264888,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m7503r","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":14}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4p3iu3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DepthHour1669","can_mod_post":false,"created_utc":1753271574,"send_replies":true,"parent_id":"t3_1m7503r","score":2,"author_fullname":"t2_t6glzswk","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Learning rate is basically standardized since chinchilla","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4p3iu3","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Learning rate is basically standardized since chinchilla&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7503r/why_do_many_papers_skip_hyperparameter_search/n4p3iu3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753271574,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m7503r","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}}]`),s=()=>e.jsx(a,{data:t});export{s as default};
