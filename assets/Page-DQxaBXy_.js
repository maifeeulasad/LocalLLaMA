import{j as e}from"./index-M4edQi1P.js";import{R as l}from"./RedditPostRenderer-CESBGIIy.js";import"./index-DFpL1mt4.js";const a=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Hey folks, I‚Äôm getting serious AI fever.\\n\\nI know there are a lot of enthusiasts here, so I‚Äôm looking for advice on budget-friendly options. I am focused on running large LLMs, not training them.\\n\\nIs it currently worth investing in a Mac Studio M1 128GB RAM? Can it run 70B models with decent quantization and a reasonable tokens/s rate? Or is the only real option for running large LLMs building a monster rig like 4x 3090s?\\n\\nI know there‚Äôs that mini PC from NVIDIA (DGX Spark), but it‚Äôs pretty weak. The memory bandwidth is a terrible joke.\\n\\nIs it worth waiting for better options? Are there any happy or unhappy owners of the Mac Studio M1 here?\\n\\nShould I just retreat to my basement and build a monster out of a dozen P40s and never be the same person again?\\n\\n","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"AI fever D:","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lyu7bf","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.47,"author_flair_background_color":null,"subreddit_type":"public","ups":0,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_qq6spcu23","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":0,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1752417048,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Hey folks, I‚Äôm getting serious AI fever.&lt;/p&gt;\\n\\n&lt;p&gt;I know there are a lot of enthusiasts here, so I‚Äôm looking for advice on budget-friendly options. I am focused on running large LLMs, not training them.&lt;/p&gt;\\n\\n&lt;p&gt;Is it currently worth investing in a Mac Studio M1 128GB RAM? Can it run 70B models with decent quantization and a reasonable tokens/s rate? Or is the only real option for running large LLMs building a monster rig like 4x 3090s?&lt;/p&gt;\\n\\n&lt;p&gt;I know there‚Äôs that mini PC from NVIDIA (DGX Spark), but it‚Äôs pretty weak. The memory bandwidth is a terrible joke.&lt;/p&gt;\\n\\n&lt;p&gt;Is it worth waiting for better options? Are there any happy or unhappy owners of the Mac Studio M1 here?&lt;/p&gt;\\n\\n&lt;p&gt;Should I just retreat to my basement and build a monster out of a dozen P40s and never be the same person again?&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":true,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1lyu7bf","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"Czydera","discussion_type":null,"num_comments":34,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lyu7bf/ai_fever_d/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lyu7bf/ai_fever_d/","subreddit_subscribers":498850,"created_utc":1752417048,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2wo3mw","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Czydera","can_mod_post":false,"created_utc":1752418585,"send_replies":true,"parent_id":"t1_n2wli3f","score":3,"author_fullname":"t2_qq6spcu23","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yup, I heard that. Unified memory isn't everything :/","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2wo3mw","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yup, I heard that. Unified memory isn&amp;#39;t everything :/&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lyu7bf","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyu7bf/ai_fever_d/n2wo3mw/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752418585,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n2wli3f","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"And-Bee","can_mod_post":false,"created_utc":1752417793,"send_replies":true,"parent_id":"t3_1lyu7bf","score":9,"author_fullname":"t2_a81fjhk","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Terrible prompt processing speed on Mac. If your prompt is large then you will be waiting a few minutes for the full response. Factor that into your decision.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2wli3f","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Terrible prompt processing speed on Mac. If your prompt is large then you will be waiting a few minutes for the full response. Factor that into your decision.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyu7bf/ai_fever_d/n2wli3f/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752417793,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lyu7bf","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":9}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2xhzjr","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Peterianer","can_mod_post":false,"created_utc":1752427544,"send_replies":true,"parent_id":"t1_n2x8yrb","score":3,"author_fullname":"t2_3tstfjy0","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Add to that a board with a beefy CPU like the higher end ones of the AMD Epyc family, as they also come with many PCI-E and RAM Channels, then you might even be able to run Kimi-K2 with it's 1000B base footprint at a decent speed by loading the active params into VRAM.\\n\\nIf you really dish out for some big ram sticks, perhaps you could even run it at fp16, if slowly.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2xhzjr","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Add to that a board with a beefy CPU like the higher end ones of the AMD Epyc family, as they also come with many PCI-E and RAM Channels, then you might even be able to run Kimi-K2 with it&amp;#39;s 1000B base footprint at a decent speed by loading the active params into VRAM.&lt;/p&gt;\\n\\n&lt;p&gt;If you really dish out for some big ram sticks, perhaps you could even run it at fp16, if slowly.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lyu7bf","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyu7bf/ai_fever_d/n2xhzjr/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752427544,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n2x8yrb","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Lemgon-Ultimate","can_mod_post":false,"created_utc":1752424897,"send_replies":true,"parent_id":"t3_1lyu7bf","score":3,"author_fullname":"t2_fqbzjd3xn","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Running multiple RTX 3090 is the best way imo. Not only do you have a configurable system you can also run pretty any AI model on it's already build CUDA framework. It's quite fast and gets support for years to come. By adding cards you can run even bigger models like 120b or you can train it on your dataset. You can never be wrong choosing this route.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2x8yrb","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Running multiple RTX 3090 is the best way imo. Not only do you have a configurable system you can also run pretty any AI model on it&amp;#39;s already build CUDA framework. It&amp;#39;s quite fast and gets support for years to come. By adding cards you can run even bigger models like 120b or you can train it on your dataset. You can never be wrong choosing this route.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyu7bf/ai_fever_d/n2x8yrb/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752424897,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lyu7bf","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2xmutp","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"jacek2023","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2xhw5z","score":2,"author_fullname":"t2_vqgbql9w","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"good luck with your pierogi ;)","edited":false,"author_flair_css_class":null,"name":"t1_n2xmutp","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;good luck with your pierogi ;)&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lyu7bf","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":"light","treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyu7bf/ai_fever_d/n2xmutp/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752428933,"author_flair_text":"llama.cpp","collapsed":false,"created_utc":1752428933,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n2xhw5z","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Czydera","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2xgmwq","score":1,"author_fullname":"t2_qq6spcu23","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Kurde, Polak :D Pierogi, bigos, siema! Dziƒôki wielkie za link! üòÅ","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2xhw5z","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Kurde, Polak :D Pierogi, bigos, siema! Dziƒôki wielkie za link! üòÅ&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lyu7bf","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyu7bf/ai_fever_d/n2xhw5z/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752427517,"author_flair_text":null,"treatment_tags":[],"created_utc":1752427517,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2xgmwq","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"jacek2023","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2xfmz5","score":3,"author_fullname":"t2_vqgbql9w","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"[https://www.reddit.com/r/LocalLLaMA/comments/1kooyfx/llamacpp\\\\_benchmarks\\\\_on\\\\_72gb\\\\_vram\\\\_setup\\\\_2x\\\\_3090\\\\_2x/](https://www.reddit.com/r/LocalLLaMA/comments/1kooyfx/llamacpp_benchmarks_on_72gb_vram_setup_2x_3090_2x/)\\n\\nhave fun!","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2xgmwq","is_submitter":false,"collapsed":false,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;&lt;a href=\\"https://www.reddit.com/r/LocalLLaMA/comments/1kooyfx/llamacpp_benchmarks_on_72gb_vram_setup_2x_3090_2x/\\"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1kooyfx/llamacpp_benchmarks_on_72gb_vram_setup_2x_3090_2x/&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;have fun!&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lyu7bf","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyu7bf/ai_fever_d/n2xgmwq/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752427154,"author_flair_text":"llama.cpp","treatment_tags":[],"created_utc":1752427154,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n2xfmz5","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Czydera","can_mod_post":false,"created_utc":1752426866,"send_replies":true,"parent_id":"t1_n2x736k","score":2,"author_fullname":"t2_qq6spcu23","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I know. I'm fighting myself on this, but I'll probably end up with a Frankenstein monster built in the basement. Can I ask what your specs are for the 2x3090 setup?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2xfmz5","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I know. I&amp;#39;m fighting myself on this, but I&amp;#39;ll probably end up with a Frankenstein monster built in the basement. Can I ask what your specs are for the 2x3090 setup?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lyu7bf","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyu7bf/ai_fever_d/n2xfmz5/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752426866,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n2x736k","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"jacek2023","can_mod_post":false,"created_utc":1752424321,"send_replies":true,"parent_id":"t3_1lyu7bf","score":3,"author_fullname":"t2_vqgbql9w","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I can run 70B models on 2x3090 in Q4, if I want to go to Q6 I use 2x3090+2x3060, but you will be running mostly 32B dense models, larger memory is usable for MoE","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2x736k","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I can run 70B models on 2x3090 in Q4, if I want to go to Q6 I use 2x3090+2x3060, but you will be running mostly 32B dense models, larger memory is usable for MoE&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyu7bf/ai_fever_d/n2x736k/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752424321,"author_flair_text":"llama.cpp","treatment_tags":[],"link_id":"t3_1lyu7bf","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2xf5zs","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Lissanro","can_mod_post":false,"created_utc":1752426728,"send_replies":true,"parent_id":"t3_1lyu7bf","score":4,"author_fullname":"t2_fpfao9g","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"For 70B, a pair of 3090 in a normal gaming PC with a pair of x16 slots (typically x8 when both cards are inserted) is the cheapest way to run them, with reasonably good prompt processing speed. Any other option (including DGX Spark) will be more expensive and not necessary will have a better performance unless you go really expensive.\\n\\nLike you mentioned P40 also can work, and some people report success with MI50 and some other cards, but if you have sufficient budget to buy a pair or even one 3090, it probably will be better (with one 3090 you can run 32B models fully in VRAM, and add the second one later if needed).","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2xf5zs","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;For 70B, a pair of 3090 in a normal gaming PC with a pair of x16 slots (typically x8 when both cards are inserted) is the cheapest way to run them, with reasonably good prompt processing speed. Any other option (including DGX Spark) will be more expensive and not necessary will have a better performance unless you go really expensive.&lt;/p&gt;\\n\\n&lt;p&gt;Like you mentioned P40 also can work, and some people report success with MI50 and some other cards, but if you have sufficient budget to buy a pair or even one 3090, it probably will be better (with one 3090 you can run 32B models fully in VRAM, and add the second one later if needed).&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyu7bf/ai_fever_d/n2xf5zs/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752426728,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lyu7bf","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2wrm78","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"SuddenOutlandishness","can_mod_post":false,"created_utc":1752419644,"send_replies":true,"parent_id":"t3_1lyu7bf","score":2,"author_fullname":"t2_wko7p7u","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I got the M4 Max MBP w/ 128GB. You can run quantized 70b models and also have a few smaller models loaded in memory as well for fast switching (think agent workflows).","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2wrm78","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I got the M4 Max MBP w/ 128GB. You can run quantized 70b models and also have a few smaller models loaded in memory as well for fast switching (think agent workflows).&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyu7bf/ai_fever_d/n2wrm78/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752419644,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lyu7bf","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2wnysn","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Toooooool","can_mod_post":false,"created_utc":1752418544,"send_replies":true,"parent_id":"t3_1lyu7bf","score":2,"author_fullname":"t2_8llornh4","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"You can do like this one guy did and invest in MI50's for cheap\\n\\n[https://www.reddit.com/r/LocalLLaMA/s/U98WeACokQ](https://www.reddit.com/r/LocalLLaMA/s/U98WeACokQ)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2wnysn","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You can do like this one guy did and invest in MI50&amp;#39;s for cheap&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://www.reddit.com/r/LocalLLaMA/s/U98WeACokQ\\"&gt;https://www.reddit.com/r/LocalLLaMA/s/U98WeACokQ&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyu7bf/ai_fever_d/n2wnysn/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752418544,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lyu7bf","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"7d1f04e6-4920-11ef-b2e1-2e580594e1a1","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2x6wm6","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"InvertedVantage","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2wqj37","score":2,"author_fullname":"t2_4me51","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Like I said in response to the other poster; ChatGPT had that whole sycophantic episode and Claude has already sunset 3.7 for non API (at least in Cursor) and I don't know when they'll sunset 3.5 - and I really don't like 4.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2x6wm6","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Like I said in response to the other poster; ChatGPT had that whole sycophantic episode and Claude has already sunset 3.7 for non API (at least in Cursor) and I don&amp;#39;t know when they&amp;#39;ll sunset 3.5 - and I really don&amp;#39;t like 4.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lyu7bf","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyu7bf/ai_fever_d/n2x6wm6/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752424263,"author_flair_text":null,"treatment_tags":[],"created_utc":1752424263,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n2wqj37","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"SlowFail2433","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2wpjmc","score":3,"author_fullname":"t2_131eezppgs","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"The API models don‚Äôt ever change. Only the clients like ChatGPT and Gemini app change over time.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2wqj37","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The API models don‚Äôt ever change. Only the clients like ChatGPT and Gemini app change over time.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lyu7bf","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyu7bf/ai_fever_d/n2wqj37/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752419318,"author_flair_text":null,"treatment_tags":[],"created_utc":1752419318,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"7d1f04e6-4920-11ef-b2e1-2e580594e1a1","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"7d1f04e6-4920-11ef-b2e1-2e580594e1a1","distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"7d1f04e6-4920-11ef-b2e1-2e580594e1a1","distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2ylcrx","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"InvertedVantage","can_mod_post":false,"created_utc":1752439196,"send_replies":true,"parent_id":"t1_n2xkzi0","score":1,"author_fullname":"t2_4me51","approved_by":null,"mod_note":null,"all_awardings":[],"body":"I haven't really ran local models for valid uses yet. My main use case is code and I'm only now able to see smaller models that are useful.","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n2ylcrx","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I haven&amp;#39;t really ran local models for valid uses yet. My main use case is code and I&amp;#39;m only now able to see smaller models that are useful.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lyu7bf","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyu7bf/ai_fever_d/n2ylcrx/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752439196,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":7,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2xkzi0","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AI_Tonic","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2x7rj4","score":1,"author_fullname":"t2_100b8v9zrg","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"yeaah i suppose i am ... for making datasets though ... what are you running these \\"local models\\" on ... because ... just the time it takes to make 50K rows of data i would expect it to make more sense on cloud, just to save you the time ...","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n2xkzi0","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"Llama 3.1"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;yeaah i suppose i am ... for making datasets though ... what are you running these &amp;quot;local models&amp;quot; on ... because ... just the time it takes to make 50K rows of data i would expect it to make more sense on cloud, just to save you the time ...&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lyu7bf","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyu7bf/ai_fever_d/n2xkzi0/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752428399,"author_flair_text":"Llama 3.1","treatment_tags":[],"created_utc":1752428399,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":6,"author_flair_background_color":"#93b1ba","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2x7rj4","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"InvertedVantage","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2x7msb","score":1,"author_fullname":"t2_4me51","approved_by":null,"mod_note":null,"all_awardings":[],"body":"I was saying that cloud models (i.e. GPT, Claude) can change. That's why I like local. I think you're thinking I meant open weight models deployed to the cloud?","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n2x7rj4","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I was saying that cloud models (i.e. GPT, Claude) can change. That&amp;#39;s why I like local. I think you&amp;#39;re thinking I meant open weight models deployed to the cloud?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lyu7bf","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyu7bf/ai_fever_d/n2x7rj4/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752424527,"author_flair_text":null,"treatment_tags":[],"created_utc":1752424527,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2x7msb","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AI_Tonic","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2x6oh4","score":1,"author_fullname":"t2_100b8v9zrg","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"none of those can be deployed on cloud , i guess i'm missing the point","edited":false,"author_flair_css_class":null,"name":"t1_n2x7msb","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"Llama 3.1"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;none of those can be deployed on cloud , i guess i&amp;#39;m missing the point&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lyu7bf","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":"light","treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyu7bf/ai_fever_d/n2x7msb/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752424487,"author_flair_text":"Llama 3.1","collapsed":false,"created_utc":1752424487,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":"#93b1ba","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2x6oh4","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"InvertedVantage","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2wqr36","score":2,"author_fullname":"t2_4me51","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Well for example, ChatGPT had that whole psychophantic episode...and I really don't like Claude 4 and have no idea when they will sunset Claude 3.5 (and they already sunset Claude 3.7, at least when I'm not using the API).","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2x6oh4","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Well for example, ChatGPT had that whole psychophantic episode...and I really don&amp;#39;t like Claude 4 and have no idea when they will sunset Claude 3.5 (and they already sunset Claude 3.7, at least when I&amp;#39;m not using the API).&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lyu7bf","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyu7bf/ai_fever_d/n2x6oh4/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752424194,"author_flair_text":null,"treatment_tags":[],"created_utc":1752424194,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n2wqr36","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AI_Tonic","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2wpjmc","score":1,"author_fullname":"t2_100b8v9zrg","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"yeah i have to agree with slowfail below , deploying models on cloud doesnt have this problem at all (my personal experience, i think i might not understand what you mean)","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2wqr36","is_submitter":false,"collapsed":false,"author_flair_richtext":[{"e":"text","t":"Llama 3.1"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;yeah i have to agree with slowfail below , deploying models on cloud doesnt have this problem at all (my personal experience, i think i might not understand what you mean)&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lyu7bf","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyu7bf/ai_fever_d/n2wqr36/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752419385,"author_flair_text":"Llama 3.1","treatment_tags":[],"created_utc":1752419385,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":"#93b1ba","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2wpjmc","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"InvertedVantage","can_mod_post":false,"created_utc":1752419024,"send_replies":true,"parent_id":"t1_n2wk6dg","score":4,"author_fullname":"t2_4me51","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"The one other reason I would say local is worth it is because retail models can degrade or change over time and having a local model means you have a known quantity that will not change.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2wpjmc","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The one other reason I would say local is worth it is because retail models can degrade or change over time and having a local model means you have a known quantity that will not change.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lyu7bf","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyu7bf/ai_fever_d/n2wpjmc/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752419024,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"7d1f04e6-4920-11ef-b2e1-2e580594e1a1","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2xtenu","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"simracerman","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2wpt6h","score":1,"author_fullname":"t2_vbzgnic","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"There‚Äôs no bad guy thing at all. I use ChatGPT, Claude almost daily aside from everything I have setup on my local machine. There‚Äôs use case for everything and everyone. Thanks for sharing your setup. Looks cool.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2xtenu","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;There‚Äôs no bad guy thing at all. I use ChatGPT, Claude almost daily aside from everything I have setup on my local machine. There‚Äôs use case for everything and everyone. Thanks for sharing your setup. Looks cool.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lyu7bf","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyu7bf/ai_fever_d/n2xtenu/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752430863,"author_flair_text":null,"treatment_tags":[],"created_utc":1752430863,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2wpt6h","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AI_Tonic","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2wn94i","score":1,"author_fullname":"t2_100b8v9zrg","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"i share my model weights on remote providers like huggingface , currently serving \\"localmodels\\" on 9 h200s there , and more on other clouds ... local is cool but the reason it even exists is that open source is discoverable , and it's hosted on cloud. since i cant deal with the threat model using my 2GB sound card i do what i can another way , and everything i host is just a docker deploy away for local llama folks ;-) here's an example : https://huggingface.co/spaces/Tonic/Math?docker=true am i a bad guy for it ? (hope not ;-) )","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2wpt6h","is_submitter":false,"collapsed":false,"author_flair_richtext":[{"e":"text","t":"Llama 3.1"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;i share my model weights on remote providers like huggingface , currently serving &amp;quot;localmodels&amp;quot; on 9 h200s there , and more on other clouds ... local is cool but the reason it even exists is that open source is discoverable , and it&amp;#39;s hosted on cloud. since i cant deal with the threat model using my 2GB sound card i do what i can another way , and everything i host is just a docker deploy away for local llama folks ;-) here&amp;#39;s an example : &lt;a href=\\"https://huggingface.co/spaces/Tonic/Math?docker=true\\"&gt;https://huggingface.co/spaces/Tonic/Math?docker=true&lt;/a&gt; am i a bad guy for it ? (hope not ;-) )&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lyu7bf","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyu7bf/ai_fever_d/n2wpt6h/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752419104,"author_flair_text":"Llama 3.1","treatment_tags":[],"created_utc":1752419104,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":"#93b1ba","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2wn94i","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"simracerman","can_mod_post":false,"created_utc":1752418330,"send_replies":true,"parent_id":"t1_n2wk6dg","score":5,"author_fullname":"t2_vbzgnic","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I fully agree that retail hardware is always more expensive to procure and maintain than any cloud provider. That said, cloud providers are NEVER local.\\n\\nThe word local means you have physical access to the hardware and software. In your case, you have access to the software, but not hardware per se. it‚Äôs like asking your friend Bob to give you remote access to his powerful home PC to install LLMs. Well, Bob is a great friend, but if ¬†one day he decides to look your my data, or accidentally leaks it because it‚Äôs local to him and crap happens, your data is no longer safe.\\n\\nAll that said, every threat model calls for different defense measures. If your threat model is basic, then no worries at all. To know what‚Äôs your threat model, ask a Large LLM to rate it for you, and make sure to provide it with all necessary context like the data you feed AI, and the data it generates out for you.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2wn94i","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I fully agree that retail hardware is always more expensive to procure and maintain than any cloud provider. That said, cloud providers are NEVER local.&lt;/p&gt;\\n\\n&lt;p&gt;The word local means you have physical access to the hardware and software. In your case, you have access to the software, but not hardware per se. it‚Äôs like asking your friend Bob to give you remote access to his powerful home PC to install LLMs. Well, Bob is a great friend, but if ¬†one day he decides to look your my data, or accidentally leaks it because it‚Äôs local to him and crap happens, your data is no longer safe.&lt;/p&gt;\\n\\n&lt;p&gt;All that said, every threat model calls for different defense measures. If your threat model is basic, then no worries at all. To know what‚Äôs your threat model, ask a Large LLM to rate it for you, and make sure to provide it with all necessary context like the data you feed AI, and the data it generates out for you.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lyu7bf","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyu7bf/ai_fever_d/n2wn94i/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752418330,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"7d1f04e6-4920-11ef-b2e1-2e580594e1a1","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"7d1f04e6-4920-11ef-b2e1-2e580594e1a1","distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2wyr10","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AI_Tonic","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2wvnrv","score":1,"author_fullname":"t2_100b8v9zrg","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"i would actually suggest those are the reasons why cloud is going to be where those self-improving agents and models will show up first for the same reasons as you outline . \\n\\nbut it sounds really interesting what you're doing , i'd love to follow along if there's anywhere where i can do that :-)","edited":false,"author_flair_css_class":null,"name":"t1_n2wyr10","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"Llama 3.1"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;i would actually suggest those are the reasons why cloud is going to be where those self-improving agents and models will show up first for the same reasons as you outline . &lt;/p&gt;\\n\\n&lt;p&gt;but it sounds really interesting what you&amp;#39;re doing , i&amp;#39;d love to follow along if there&amp;#39;s anywhere where i can do that :-)&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lyu7bf","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":"light","treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyu7bf/ai_fever_d/n2wyr10/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752421795,"author_flair_text":"Llama 3.1","collapsed":false,"created_utc":1752421795,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":"#93b1ba","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2wvnrv","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Ok_Appearance3584","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2wqhcl","score":1,"author_fullname":"t2_oyxj85n1","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Well, I'm not doing it professionally but as part of my work and as a curiosity.\\n\\n\\nFor my use case, you could even use DGX Spark and get good results, you'd break even in half a year or so.\\n\\nThe problem I have is that APIs are too expensive for the amount of input/output tokens I consume for relatively trivial stuff and rental is too expensive for some of the 24/7 agents I run locally. Also, APIs only offer off-the-shelf models, I need custom models.\\n\\nBig batch processing is OK for rental but too inflexible if you want more complex dynamics and APIs are OK for more lightweight stuff but cannot handle high token counts on a budget.\\n\\nAlso, I am experimenting with continuous finetuning agents that train themselves after every response. They're the next big thing IMO. But 10x compute costs. Not gonna get them from cloud APIs any time soon, doesn't scale well for them.","edited":1752421164,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2wvnrv","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Well, I&amp;#39;m not doing it professionally but as part of my work and as a curiosity.&lt;/p&gt;\\n\\n&lt;p&gt;For my use case, you could even use DGX Spark and get good results, you&amp;#39;d break even in half a year or so.&lt;/p&gt;\\n\\n&lt;p&gt;The problem I have is that APIs are too expensive for the amount of input/output tokens I consume for relatively trivial stuff and rental is too expensive for some of the 24/7 agents I run locally. Also, APIs only offer off-the-shelf models, I need custom models.&lt;/p&gt;\\n\\n&lt;p&gt;Big batch processing is OK for rental but too inflexible if you want more complex dynamics and APIs are OK for more lightweight stuff but cannot handle high token counts on a budget.&lt;/p&gt;\\n\\n&lt;p&gt;Also, I am experimenting with continuous finetuning agents that train themselves after every response. They&amp;#39;re the next big thing IMO. But 10x compute costs. Not gonna get them from cloud APIs any time soon, doesn&amp;#39;t scale well for them.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lyu7bf","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyu7bf/ai_fever_d/n2wvnrv/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752420864,"author_flair_text":null,"treatment_tags":[],"created_utc":1752420864,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2wqhcl","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AI_Tonic","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2wokim","score":1,"author_fullname":"t2_100b8v9zrg","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"at a million tokens a day you would pay off your 8 node H100s in several years (are you actually selling datasets??) compared to using the deepseek api for just a few dollars a day ... i'd love to learn more about which models you're using though , i make quite a lot of datasets too and cloud/renting is way cheaper than buying in my experience, but i'm not selling datasets , just open sourcing them...","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2wqhcl","is_submitter":false,"collapsed":false,"author_flair_richtext":[{"e":"text","t":"Llama 3.1"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;at a million tokens a day you would pay off your 8 node H100s in several years (are you actually selling datasets??) compared to using the deepseek api for just a few dollars a day ... i&amp;#39;d love to learn more about which models you&amp;#39;re using though , i make quite a lot of datasets too and cloud/renting is way cheaper than buying in my experience, but i&amp;#39;m not selling datasets , just open sourcing them...&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lyu7bf","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyu7bf/ai_fever_d/n2wqhcl/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752419303,"author_flair_text":"Llama 3.1","treatment_tags":[],"created_utc":1752419303,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":"#93b1ba","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2wokim","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Ok_Appearance3584","can_mod_post":false,"created_utc":1752418726,"send_replies":true,"parent_id":"t1_n2wk6dg","score":2,"author_fullname":"t2_oyxj85n1","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Not true if you're running serious AI workloads (such as processing data with LLMs), local hardware will pay off eventually or sometimes pretty soon (in a matter of months).\\n\\n\\nI've calculated my use cases process a minimum of about a million output tokens per day, sometimes multiples of this. Input tokens would be a similar amount, if not more so.\\n\\n\\nAn example of my use case is generating a synthetic instruct dataset for a low resource language. Other use cases include more traditional data processing and problem solving with expanded, agentic thinking. One prompt with recursive agent decomposition might produce a hundred thousand outout tokens or more.\\n\\n\\nBut I agree, if all you do is chat and code, probably cheaper to not go local. If you do high token quantity data processing, or don't want to give away your private data, maybe cheaper to go local.¬†","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2wokim","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Not true if you&amp;#39;re running serious AI workloads (such as processing data with LLMs), local hardware will pay off eventually or sometimes pretty soon (in a matter of months).&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;ve calculated my use cases process a minimum of about a million output tokens per day, sometimes multiples of this. Input tokens would be a similar amount, if not more so.&lt;/p&gt;\\n\\n&lt;p&gt;An example of my use case is generating a synthetic instruct dataset for a low resource language. Other use cases include more traditional data processing and problem solving with expanded, agentic thinking. One prompt with recursive agent decomposition might produce a hundred thousand outout tokens or more.&lt;/p&gt;\\n\\n&lt;p&gt;But I agree, if all you do is chat and code, probably cheaper to not go local. If you do high token quantity data processing, or don&amp;#39;t want to give away your private data, maybe cheaper to go local.¬†&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lyu7bf","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyu7bf/ai_fever_d/n2wokim/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752418726,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2xtxkl","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Czydera","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2wy2qt","score":3,"author_fullname":"t2_qq6spcu23","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Exactly! How else can you ask what the capital of France is and how cannibals cook their dinnerwait what","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2xtxkl","is_submitter":true,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Exactly! How else can you ask what the capital of France is and how cannibals cook their dinnerwait what&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lyu7bf","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyu7bf/ai_fever_d/n2xtxkl/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752431021,"author_flair_text":null,"treatment_tags":[],"created_utc":1752431021,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n2wy2qt","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"stoppableDissolution","can_mod_post":false,"created_utc":1752421592,"send_replies":true,"parent_id":"t1_n2wk6dg","score":2,"author_fullname":"t2_1n0su21k4z","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I mean, cmon, its _local_llama, not _openweight_llama or anything. The whole point of it being local is complete control. Yes, it comes at a price, but its up to each individual to figure whether its worth it.\\n\\nPrivacy and all aside, you are guaranteed to not get rugpulled if (when) regulations start clamping down.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2wy2qt","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I mean, cmon, its _local_llama, not _openweight_llama or anything. The whole point of it being local is complete control. Yes, it comes at a price, but its up to each individual to figure whether its worth it.&lt;/p&gt;\\n\\n&lt;p&gt;Privacy and all aside, you are guaranteed to not get rugpulled if (when) regulations start clamping down.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lyu7bf","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyu7bf/ai_fever_d/n2wy2qt/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752421592,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2ymwyt","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"CMDR-Bugsbunny","can_mod_post":false,"created_utc":1752439659,"send_replies":true,"parent_id":"t1_n2wk6dg","score":2,"author_fullname":"t2_lj6an","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I'm only going to disagree as you said \\"Never\\" and that's not true! As there are cases for when to use cloud resources and times when outsourcing to cloud is not a viable solution.\\n\\nDepending on the use case, some small models are very capable and I've gotten performance with the right prompt and model (again, use case) to get results that compare to cloud AI. I got amazing content generation with Gemma 3 27b QAT, I even had Claude and ChatGPT evaluate the responses that confirmed my observation using a personal rubric for the results. \\n\\nI have a local LLM stack on my MacBook M2 Max 96GB RAM (running Llama 4 Scout 4-bit) for architecture design using Context7 and have no issues with context that I had with Claude on their Max plan! Not to mention having the latest documentation that even Claude has missed for me in the past.\\n\\nFor coding I'm quite happy with Qwen 2.5 Coder Q6 (upgraded to 5090, because gaming and AI nonsense), but the 4-bit was good on my older GPU or even the 14B with higher quant.\\n\\nCurrently, I get the following on my Windows PC with the 5090:  \\nQwen 2.5 Coder 32B Q6: 45+ T/s  \\nGemma 3 27B QAT: 60+ T/s\\n\\nFor the MacBook:  \\nLlama 4 - Scout (4-bit): 26+ T/s  \\nLlama 3.3 70b Instruct (4-bit): 7 T/s  \\nGemma 3 27B BF16: 18+ T/s\\n\\nSo depending on the use case \\"Retail\\" can make a lot of sense.\\n\\nOther considerations:  \\n1) Privacy of my data  \\n2) Performance is consistent on local and with the right model generates faster than the cloud solutions and greatly diminish depending on cloud usage at the time  \\n3) I can RAG and train/tune local models to perform even better - just do a google search \\"small LLM outperform large LLM\\"  \\n4) Enshitification - this is huge as SaaS has a trend and once users are subscribing to these online models, the companies will need to start improving their profits to make back their investment to get customers. This cycle keeps repeating and will for AI, too.   \\n  \\nSo, \\"retail computing is never worth it if you do the math\\" is absolutely wrong and greatly depends on the use case!\\n\\nTo answer the OP, sure a Mac is a good personal LLM for local and good if the model you want with quat is in the MLX format. Otherwise, look to upgrade your GPU or even consider one of the AMD solutions, but again make sure the model you want is available (it won't be GGUF as that's Nvidia)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2ymwyt","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;m only going to disagree as you said &amp;quot;Never&amp;quot; and that&amp;#39;s not true! As there are cases for when to use cloud resources and times when outsourcing to cloud is not a viable solution.&lt;/p&gt;\\n\\n&lt;p&gt;Depending on the use case, some small models are very capable and I&amp;#39;ve gotten performance with the right prompt and model (again, use case) to get results that compare to cloud AI. I got amazing content generation with Gemma 3 27b QAT, I even had Claude and ChatGPT evaluate the responses that confirmed my observation using a personal rubric for the results. &lt;/p&gt;\\n\\n&lt;p&gt;I have a local LLM stack on my MacBook M2 Max 96GB RAM (running Llama 4 Scout 4-bit) for architecture design using Context7 and have no issues with context that I had with Claude on their Max plan! Not to mention having the latest documentation that even Claude has missed for me in the past.&lt;/p&gt;\\n\\n&lt;p&gt;For coding I&amp;#39;m quite happy with Qwen 2.5 Coder Q6 (upgraded to 5090, because gaming and AI nonsense), but the 4-bit was good on my older GPU or even the 14B with higher quant.&lt;/p&gt;\\n\\n&lt;p&gt;Currently, I get the following on my Windows PC with the 5090:&lt;br/&gt;\\nQwen 2.5 Coder 32B Q6: 45+ T/s&lt;br/&gt;\\nGemma 3 27B QAT: 60+ T/s&lt;/p&gt;\\n\\n&lt;p&gt;For the MacBook:&lt;br/&gt;\\nLlama 4 - Scout (4-bit): 26+ T/s&lt;br/&gt;\\nLlama 3.3 70b Instruct (4-bit): 7 T/s&lt;br/&gt;\\nGemma 3 27B BF16: 18+ T/s&lt;/p&gt;\\n\\n&lt;p&gt;So depending on the use case &amp;quot;Retail&amp;quot; can make a lot of sense.&lt;/p&gt;\\n\\n&lt;p&gt;Other considerations:&lt;br/&gt;\\n1) Privacy of my data&lt;br/&gt;\\n2) Performance is consistent on local and with the right model generates faster than the cloud solutions and greatly diminish depending on cloud usage at the time&lt;br/&gt;\\n3) I can RAG and train/tune local models to perform even better - just do a google search &amp;quot;small LLM outperform large LLM&amp;quot;&lt;br/&gt;\\n4) Enshitification - this is huge as SaaS has a trend and once users are subscribing to these online models, the companies will need to start improving their profits to make back their investment to get customers. This cycle keeps repeating and will for AI, too.   &lt;/p&gt;\\n\\n&lt;p&gt;So, &amp;quot;retail computing is never worth it if you do the math&amp;quot; is absolutely wrong and greatly depends on the use case!&lt;/p&gt;\\n\\n&lt;p&gt;To answer the OP, sure a Mac is a good personal LLM for local and good if the model you want with quat is in the MLX format. Otherwise, look to upgrade your GPU or even consider one of the AMD solutions, but again make sure the model you want is available (it won&amp;#39;t be GGUF as that&amp;#39;s Nvidia)&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lyu7bf","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyu7bf/ai_fever_d/n2ymwyt/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752439659,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n2wk6dg","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AI_Tonic","can_mod_post":false,"created_utc":1752417386,"send_replies":true,"parent_id":"t3_1lyu7bf","score":0,"author_fullname":"t2_100b8v9zrg","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"retail computing is never worth it if you do the math , until we get a lot of small models (which i do hope might happen soon) there's literally no financial sense in buying something to \\"run locally\\" because it will never perform well enough and you're always better off using providers or cloud solutions (just my two cents) . \\n\\nbtw i host and run a lot of models at any given time on cloud , but i still consider it local , because it's ... local to me . so please dont give me hate , i just have a very old and lame computer :-) \\n\\nbut to answer your question , check the ONNX models and see what works for you , that's what you'll be running on a mac (128GB can run basically 7-16B models with fairly sized context windows)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2wk6dg","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"Llama 3.1"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;retail computing is never worth it if you do the math , until we get a lot of small models (which i do hope might happen soon) there&amp;#39;s literally no financial sense in buying something to &amp;quot;run locally&amp;quot; because it will never perform well enough and you&amp;#39;re always better off using providers or cloud solutions (just my two cents) . &lt;/p&gt;\\n\\n&lt;p&gt;btw i host and run a lot of models at any given time on cloud , but i still consider it local , because it&amp;#39;s ... local to me . so please dont give me hate , i just have a very old and lame computer :-) &lt;/p&gt;\\n\\n&lt;p&gt;but to answer your question , check the ONNX models and see what works for you , that&amp;#39;s what you&amp;#39;ll be running on a mac (128GB can run basically 7-16B models with fairly sized context windows)&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyu7bf/ai_fever_d/n2wk6dg/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752417386,"author_flair_text":"Llama 3.1","treatment_tags":[],"link_id":"t3_1lyu7bf","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":1,"depth":0,"author_flair_background_color":"#93b1ba","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}}]`),n=()=>e.jsx(l,{data:a});export{n as default};
