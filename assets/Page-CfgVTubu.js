import{j as e}from"./index-DQXiEb7D.js";import{R as l}from"./RedditPostRenderer-BjndLgq8.js";import"./index-B-ILyjT1.js";const a=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Just starting into AI, ComfyUI. Using a 7900XTX 24GB. It goes not as smooth as I had hoped. Now I want to buy a nVidia GPU with 24GB.\\n\\nQ: Can I only use the nVidia to compute and VRAM of both cards combined? Do both cards needs to have the same amount of VRAM?","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Can VRAM be combined of 2 brands","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lze20x","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.76,"author_flair_background_color":null,"subreddit_type":"public","ups":9,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_1fp7huwh","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":9,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1752470433,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Just starting into AI, ComfyUI. Using a 7900XTX 24GB. It goes not as smooth as I had hoped. Now I want to buy a nVidia GPU with 24GB.&lt;/p&gt;\\n\\n&lt;p&gt;Q: Can I only use the nVidia to compute and VRAM of both cards combined? Do both cards needs to have the same amount of VRAM?&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1lze20x","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"tonyleungnl","discussion_type":null,"num_comments":85,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lze20x/can_vram_be_combined_of_2_brands/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lze20x/can_vram_be_combined_of_2_brands/","subreddit_subscribers":499297,"created_utc":1752470433,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n34d5x2","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"fallingdowndizzyvr","can_mod_post":false,"send_replies":true,"parent_id":"t1_n31l80v","score":4,"author_fullname":"t2_o65i6kx","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yes. Yes, you can. I do it all the time. I recently posted numbers again doing it.\\n\\n    **7900xtx + 3060 + 2070**\\n    | model                          |       size |     params | backend    | ngl | mmap |            test |                  t/s |\\n    | ------------------------------ | ---------: | ---------: | ---------- | --: | ---: | --------------: | -------------------: |\\n    | qwen2 32B Q8_0                 |  32.42 GiB |    32.76 B | RPC,Vulkan | 999 |    0 |           pp512 |       342.35 ± 17.21 |\\n    | qwen2 32B Q8_0                 |  32.42 GiB |    32.76 B | RPC,Vulkan | 999 |    0 |           tg128 |         11.52 ± 0.18 |\\n    | qwen2 32B Q8_0                 |  32.42 GiB |    32.76 B | RPC,Vulkan | 999 |    0 |  pp512 @ d10000 |        213.81 ± 3.92 |\\n    | qwen2 32B Q8_0                 |  32.42 GiB |    32.76 B | RPC,Vulkan | 999 |    0 |  tg128 @ d10000 |          8.27 ± 0.02 |\\n\\nhttps://www.reddit.com/r/LocalLLaMA/comments/1le951x/gmk_x2amd_max_395_w128gb_first_impressions/","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n34d5x2","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yes. Yes, you can. I do it all the time. I recently posted numbers again doing it.&lt;/p&gt;\\n\\n&lt;pre&gt;&lt;code&gt;**7900xtx + 3060 + 2070**\\n| model                          |       size |     params | backend    | ngl | mmap |            test |                  t/s |\\n| ------------------------------ | ---------: | ---------: | ---------- | --: | ---: | --------------: | -------------------: |\\n| qwen2 32B Q8_0                 |  32.42 GiB |    32.76 B | RPC,Vulkan | 999 |    0 |           pp512 |       342.35 ± 17.21 |\\n| qwen2 32B Q8_0                 |  32.42 GiB |    32.76 B | RPC,Vulkan | 999 |    0 |           tg128 |         11.52 ± 0.18 |\\n| qwen2 32B Q8_0                 |  32.42 GiB |    32.76 B | RPC,Vulkan | 999 |    0 |  pp512 @ d10000 |        213.81 ± 3.92 |\\n| qwen2 32B Q8_0                 |  32.42 GiB |    32.76 B | RPC,Vulkan | 999 |    0 |  tg128 @ d10000 |          8.27 ± 0.02 |\\n&lt;/code&gt;&lt;/pre&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://www.reddit.com/r/LocalLLaMA/comments/1le951x/gmk_x2amd_max_395_w128gb_first_impressions/\\"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1le951x/gmk_x2amd_max_395_w128gb_first_impressions/&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lze20x","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lze20x/can_vram_be_combined_of_2_brands/n34d5x2/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752517602,"author_flair_text":null,"treatment_tags":[],"created_utc":1752517602,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"more","data":{"count":0,"name":"t1__","id":"_","parent_id":"t1_n357mb5","depth":10,"children":[]}}],"before":null}},"user_reports":[],"saved":false,"id":"n357mb5","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"a_beautiful_rhind","can_mod_post":false,"created_utc":1752526295,"send_replies":true,"parent_id":"t1_n3524zk","score":2,"author_fullname":"t2_h5utwre7","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Now now.. that's quite the caveat. RPC has overhead. Not the same as running *one* llama.cpp and it using both cards to split the same model. If you can't do that, then it's still kinda like it was.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n357mb5","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Now now.. that&amp;#39;s quite the caveat. RPC has overhead. Not the same as running &lt;em&gt;one&lt;/em&gt; llama.cpp and it using both cards to split the same model. If you can&amp;#39;t do that, then it&amp;#39;s still kinda like it was.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lze20x/can_vram_be_combined_of_2_brands/n357mb5/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752526295,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lze20x","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":9,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n3524zk","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"fallingdowndizzyvr","can_mod_post":false,"created_utc":1752524760,"send_replies":true,"parent_id":"t1_n34z9lj","score":3,"author_fullname":"t2_o65i6kx","approved_by":null,"mod_note":null,"all_awardings":[],"body":"What are my favorite things about llama.ccp? Vulkan and RPC. You can use CUDA and ROCm together through RPC. Spin up a RPC server using CUDA and then run the master llama-cli using ROCm.\\n\\nThat's how I can use AMD, Intel, Nvidia and Mac altogether.","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n3524zk","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;What are my favorite things about llama.ccp? Vulkan and RPC. You can use CUDA and ROCm together through RPC. Spin up a RPC server using CUDA and then run the master llama-cli using ROCm.&lt;/p&gt;\\n\\n&lt;p&gt;That&amp;#39;s how I can use AMD, Intel, Nvidia and Mac altogether.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lze20x/can_vram_be_combined_of_2_brands/n3524zk/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752524760,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lze20x","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":8,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n34z9lj","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"a_beautiful_rhind","can_mod_post":false,"created_utc":1752523967,"send_replies":true,"parent_id":"t1_n34rebp","score":1,"author_fullname":"t2_h5utwre7","approved_by":null,"mod_note":null,"all_awardings":[],"body":"It's news to me that you can do it without using vulkan.","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n34z9lj","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It&amp;#39;s news to me that you can do it without using vulkan.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lze20x","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lze20x/can_vram_be_combined_of_2_brands/n34z9lj/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752523967,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":7,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n34rebp","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"fallingdowndizzyvr","can_mod_post":false,"send_replies":true,"parent_id":"t1_n34l1hu","score":4,"author_fullname":"t2_o65i6kx","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Yes. You can split a model between a GPU running CUDA and a GPU running ROCm. I've posted that so many times. I'm surprise this is news to you.","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n34rebp","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yes. You can split a model between a GPU running CUDA and a GPU running ROCm. I&amp;#39;ve posted that so many times. I&amp;#39;m surprise this is news to you.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lze20x","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lze20x/can_vram_be_combined_of_2_brands/n34rebp/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752521709,"author_flair_text":null,"treatment_tags":[],"created_utc":1752521709,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":6,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n34zufj","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"a_beautiful_rhind","can_mod_post":false,"created_utc":1752524126,"send_replies":true,"parent_id":"t1_n34tk36","score":3,"author_fullname":"t2_h5utwre7","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Makefile thing was at the end of april I think. I remember having to switch to ccmake to save build parameters.\\n\\nDocs say you can build it with all backends included but I didn't know they'd play nice with the same weights.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n34zufj","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Makefile thing was at the end of april I think. I remember having to switch to ccmake to save build parameters.&lt;/p&gt;\\n\\n&lt;p&gt;Docs say you can build it with all backends included but I didn&amp;#39;t know they&amp;#39;d play nice with the same weights.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lze20x/can_vram_be_combined_of_2_brands/n34zufj/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752524126,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lze20x","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":9,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n34tyco","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"fallingdowndizzyvr","can_mod_post":false,"created_utc":1752522457,"send_replies":true,"parent_id":"t1_n34tk36","score":1,"author_fullname":"t2_o65i6kx","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"&gt; I think this was added back when they deprecated Makefile support.\\n\\nAh... that would explain it. I haven't tried in a while. Definitely pre cmake.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n34tyco","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;I think this was added back when they deprecated Makefile support.&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;Ah... that would explain it. I haven&amp;#39;t tried in a while. Definitely pre cmake.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lze20x/can_vram_be_combined_of_2_brands/n34tyco/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752522457,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lze20x","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":9,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n34tk36","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"m18coppola","can_mod_post":false,"created_utc":1752522343,"send_replies":true,"parent_id":"t1_n34rssr","score":4,"author_fullname":"t2_3w4hfbq9","approved_by":null,"mod_note":null,"all_awardings":[],"body":"It works because it would just build the shared library multiple times. You'd have one .so/.dll file for CUDA ifdefs and another .so/.dll file for ROCm ifdefs. See the \\"Notes about GPU-accelerated backends\\" [here](https://github.com/ggml-org/llama.cpp/blob/master/docs/build.md#notes-about-gpu-accelerated-backends). Pinging u/a_beautiful_rhind too, I think this was added back when they deprecated Makefile support.","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n34tk36","is_submitter":false,"collapsed":false,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It works because it would just build the shared library multiple times. You&amp;#39;d have one .so/.dll file for CUDA ifdefs and another .so/.dll file for ROCm ifdefs. See the &amp;quot;Notes about GPU-accelerated backends&amp;quot; &lt;a href=\\"https://github.com/ggml-org/llama.cpp/blob/master/docs/build.md#notes-about-gpu-accelerated-backends\\"&gt;here&lt;/a&gt;. Pinging &lt;a href=\\"/u/a_beautiful_rhind\\"&gt;u/a_beautiful_rhind&lt;/a&gt; too, I think this was added back when they deprecated Makefile support.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lze20x/can_vram_be_combined_of_2_brands/n34tk36/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752522343,"author_flair_text":"llama.cpp","treatment_tags":[],"link_id":"t3_1lze20x","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":8,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}}],"before":null}},"user_reports":[],"saved":false,"id":"n34rssr","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"fallingdowndizzyvr","can_mod_post":false,"created_utc":1752521827,"send_replies":true,"parent_id":"t1_n34lt00","score":2,"author_fullname":"t2_o65i6kx","approved_by":null,"mod_note":null,"all_awardings":[],"body":"I've never been able to get that to work. Have you? It doesn't seem like it should work since llama.cpp is very ifdef. So if it's ifdef CUDA then that overrides the ifdef for ROCm.","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n34rssr","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;ve never been able to get that to work. Have you? It doesn&amp;#39;t seem like it should work since llama.cpp is very ifdef. So if it&amp;#39;s ifdef CUDA then that overrides the ifdef for ROCm.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lze20x","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lze20x/can_vram_be_combined_of_2_brands/n34rssr/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752521827,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":7,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n34nfk9","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"a_beautiful_rhind","can_mod_post":false,"created_utc":1752520559,"send_replies":true,"parent_id":"t1_n34lt00","score":1,"author_fullname":"t2_h5utwre7","approved_by":null,"mod_note":null,"all_awardings":[],"body":"When did they add that? Wouldn't stuff like FA be incompatible across kernels?","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n34nfk9","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;When did they add that? Wouldn&amp;#39;t stuff like FA be incompatible across kernels?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lze20x","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lze20x/can_vram_be_combined_of_2_brands/n34nfk9/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752520559,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":7,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n34lt00","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"m18coppola","can_mod_post":false,"send_replies":true,"parent_id":"t1_n34l1hu","score":3,"author_fullname":"t2_3w4hfbq9","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Yeah, you just have to enable all the needed backends in the cmake flags, and then they will show up as available devices in llama.cpp","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n34lt00","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yeah, you just have to enable all the needed backends in the cmake flags, and then they will show up as available devices in llama.cpp&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lze20x","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lze20x/can_vram_be_combined_of_2_brands/n34lt00/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752520101,"author_flair_text":"llama.cpp","treatment_tags":[],"created_utc":1752520101,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":6,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n34l1hu","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"a_beautiful_rhind","can_mod_post":false,"send_replies":true,"parent_id":"t1_n34ecop","score":1,"author_fullname":"t2_h5utwre7","approved_by":null,"mod_note":null,"all_awardings":[],"body":"Splitting the same model?","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n34l1hu","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Splitting the same model?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lze20x","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lze20x/can_vram_be_combined_of_2_brands/n34l1hu/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752519887,"author_flair_text":null,"treatment_tags":[],"created_utc":1752519887,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n34ecop","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"fallingdowndizzyvr","can_mod_post":false,"send_replies":true,"parent_id":"t1_n325iho","score":1,"author_fullname":"t2_o65i6kx","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"&gt; Only with vulkan.\\n\\nNo. You can do it running CUDA on Nvidia and ROCm on AMD. It's not only with Vulkan.","edited":false,"author_flair_css_class":null,"name":"t1_n34ecop","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;Only with vulkan.&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;No. You can do it running CUDA on Nvidia and ROCm on AMD. It&amp;#39;s not only with Vulkan.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lze20x","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lze20x/can_vram_be_combined_of_2_brands/n34ecop/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752517943,"author_flair_text":null,"collapsed":false,"created_utc":1752517943,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n325iho","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"a_beautiful_rhind","can_mod_post":false,"send_replies":true,"parent_id":"t1_n31loc8","score":2,"author_fullname":"t2_h5utwre7","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Only with vulkan. I dunno what pytorch does if you split across amd + nvida. Probably fails.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n325iho","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Only with vulkan. I dunno what pytorch does if you split across amd + nvida. Probably fails.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lze20x","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lze20x/can_vram_be_combined_of_2_brands/n325iho/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752493349,"author_flair_text":null,"treatment_tags":[],"created_utc":1752493349,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n31loc8","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"CatalyticDragon","can_mod_post":false,"send_replies":true,"parent_id":"t1_n31l80v","score":2,"author_fullname":"t2_3h1nb","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"You indeed can!  Pipeline, tensor, data, expert. There are many types of parallelism which will all work with a mix of GPUs.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n31loc8","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You indeed can!  Pipeline, tensor, data, expert. There are many types of parallelism which will all work with a mix of GPUs.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lze20x","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lze20x/can_vram_be_combined_of_2_brands/n31loc8/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752482994,"author_flair_text":null,"treatment_tags":[],"created_utc":1752482994,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n31l80v","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"CommunityTough1","can_mod_post":false,"created_utc":1752482730,"send_replies":true,"parent_id":"t1_n30zyrt","score":1,"author_fullname":"t2_1iuzpxw7eg","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Correct me if I'm wrong, but I don't think you can combine the VRAM across AMD and Nvidia cards.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n31l80v","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Correct me if I&amp;#39;m wrong, but I don&amp;#39;t think you can combine the VRAM across AMD and Nvidia cards.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lze20x","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lze20x/can_vram_be_combined_of_2_brands/n31l80v/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752482730,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3424pe","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Evening_Ad6637","can_mod_post":false,"send_replies":true,"parent_id":"t1_n31czx6","score":1,"author_fullname":"t2_p45er6oo","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"No, of course not! The text generation speed is slightly slower under vulcan, but really acceptable.\\n\\n\\nBut the prompt processing speed will suffer immensely.","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n3424pe","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;No, of course not! The text generation speed is slightly slower under vulcan, but really acceptable.&lt;/p&gt;\\n\\n&lt;p&gt;But the prompt processing speed will suffer immensely.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lze20x","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lze20x/can_vram_be_combined_of_2_brands/n3424pe/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752514579,"author_flair_text":"llama.cpp","treatment_tags":[],"created_utc":1752514579,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":6,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"more","data":{"count":0,"name":"t1__","id":"_","parent_id":"t1_n31fals","depth":10,"children":[]}}],"before":null}},"user_reports":[],"saved":false,"id":"n31fals","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"SashaUsesReddit","can_mod_post":false,"created_utc":1752479241,"send_replies":true,"parent_id":"t1_n31eqbt","score":2,"author_fullname":"t2_57wafqev","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"So now he's making stuff up too since you don't know what vllm is or how tensor parallelism works?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n31fals","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;So now he&amp;#39;s making stuff up too since you don&amp;#39;t know what vllm is or how tensor parallelism works?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lze20x/can_vram_be_combined_of_2_brands/n31fals/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752479241,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lze20x","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":9,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n31eqbt","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"fallingdowndizzyvr","can_mod_post":false,"created_utc":1752478909,"send_replies":true,"parent_id":"t1_n31dnuj","score":1,"author_fullname":"t2_o65i6kx","approved_by":null,"mod_note":null,"all_awardings":[],"body":"So you don't have any experience with using multiple types of GPUs then do you? You are just making stuff up.","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n31eqbt","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;So you don&amp;#39;t have any experience with using multiple types of GPUs then do you? You are just making stuff up.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lze20x/can_vram_be_combined_of_2_brands/n31eqbt/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752478909,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lze20x","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":8,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n31dnuj","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Such_Advantage_6949","can_mod_post":false,"created_utc":1752478272,"send_replies":true,"parent_id":"t1_n31dgjl","score":2,"author_fullname":"t2_a548b491","approved_by":null,"mod_note":null,"all_awardings":[],"body":"Nah i am happy with vllm and tensor parallel. Dont think vllm support vulkan. So it will be slower regardless","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n31dnuj","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Nah i am happy with vllm and tensor parallel. Dont think vllm support vulkan. So it will be slower regardless&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lze20x","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lze20x/can_vram_be_combined_of_2_brands/n31dnuj/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752478272,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":7,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n31dgjl","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"fallingdowndizzyvr","can_mod_post":false,"send_replies":true,"parent_id":"t1_n31czx6","score":1,"author_fullname":"t2_o65i6kx","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Yes, sometimes even better. There have been threads about it. Go look.","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n31dgjl","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yes, sometimes even better. There have been threads about it. Go look.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lze20x","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lze20x/can_vram_be_combined_of_2_brands/n31dgjl/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752478154,"author_flair_text":null,"treatment_tags":[],"created_utc":1752478154,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":6,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n31czx6","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Such_Advantage_6949","can_mod_post":false,"send_replies":true,"parent_id":"t1_n31cwgo","score":2,"author_fullname":"t2_a548b491","approved_by":null,"mod_note":null,"all_awardings":[],"body":"Does running vulkan give same speed as cuda for nvidia card?","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n31czx6","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Does running vulkan give same speed as cuda for nvidia card?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lze20x","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lze20x/can_vram_be_combined_of_2_brands/n31czx6/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752477888,"author_flair_text":null,"treatment_tags":[],"created_utc":1752477888,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n31cwgo","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"fallingdowndizzyvr","can_mod_post":false,"send_replies":true,"parent_id":"t1_n31cdbc","score":3,"author_fullname":"t2_o65i6kx","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"&gt; How do u run it, how many backend actually support this? \\n\\nThe easy thing to do is to use Vulkan on all of them except the Mac. For that use Metal. If you must, you could run ROCm/CUDA instead but why?\\n\\n&gt; Is the speed as fast as running with same brand?\\n\\nHaving the same brand doesn't really change anything. Having the same card doesn't really change anything. What does is if you do tensor parallel. For that you would need identical cards and a MB with enough at least x4 slots to hosts those cards. But that's not what OP is asking about.","edited":false,"author_flair_css_class":null,"name":"t1_n31cwgo","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;How do u run it, how many backend actually support this? &lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;The easy thing to do is to use Vulkan on all of them except the Mac. For that use Metal. If you must, you could run ROCm/CUDA instead but why?&lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;Is the speed as fast as running with same brand?&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;Having the same brand doesn&amp;#39;t really change anything. Having the same card doesn&amp;#39;t really change anything. What does is if you do tensor parallel. For that you would need identical cards and a MB with enough at least x4 slots to hosts those cards. But that&amp;#39;s not what OP is asking about.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lze20x","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lze20x/can_vram_be_combined_of_2_brands/n31cwgo/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752477833,"author_flair_text":null,"collapsed":false,"created_utc":1752477833,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n31cdbc","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Such_Advantage_6949","can_mod_post":false,"send_replies":true,"parent_id":"t1_n31c7jc","score":2,"author_fullname":"t2_a548b491","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"How do u run it, how many backend actually support this? Is the speed as fast as running with same brand?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n31cdbc","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;How do u run it, how many backend actually support this? Is the speed as fast as running with same brand?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lze20x","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lze20x/can_vram_be_combined_of_2_brands/n31cdbc/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752477532,"author_flair_text":null,"treatment_tags":[],"created_utc":1752477532,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n31c7jc","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"fallingdowndizzyvr","can_mod_post":false,"send_replies":true,"parent_id":"t1_n317org","score":3,"author_fullname":"t2_o65i6kx","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"That's not true at all. I run AMD, Intel, Nvidia and for a bit of spice a Mac all together to run big models. It couldn't be easier.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n31c7jc","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;That&amp;#39;s not true at all. I run AMD, Intel, Nvidia and for a bit of spice a Mac all together to run big models. It couldn&amp;#39;t be easier.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lze20x","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lze20x/can_vram_be_combined_of_2_brands/n31c7jc/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752477440,"author_flair_text":null,"treatment_tags":[],"created_utc":1752477440,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n317org","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":"LOW_SCORE","no_follow":true,"author":"Such_Advantage_6949","can_mod_post":false,"created_utc":1752474915,"send_replies":true,"parent_id":"t1_n30zyrt","score":-12,"author_fullname":"t2_a548b491","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Even for llm not really. Even within single brand, there are a lot of driver issue and compatibility","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n317org","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Even for llm not really. Even within single brand, there are a lot of driver issue and compatibility&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":"comment score below threshold","distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lze20x","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lze20x/can_vram_be_combined_of_2_brands/n317org/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752474915,"author_flair_text":null,"treatment_tags":[],"collapsed":true,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-12}}],"before":null}},"user_reports":[],"saved":false,"id":"n30zyrt","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"fallingdowndizzyvr","can_mod_post":false,"created_utc":1752470827,"send_replies":true,"parent_id":"t3_1lze20x","score":14,"author_fullname":"t2_o65i6kx","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"For LLM yes, you can \\"combine\\" the RAM and run larger models. They do not have to be the same anything.\\n\\nBut, since you are saying ComfyUI, I take it you want to do image/video gen too. It won't help for that. Other than maybe Wan, I don't know of a model that can be split across GPUs for image/video gen. You might be able to do things like run different parts of the workflow on different GPUs to conserve RAM but you might as well do offloading.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n30zyrt","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;For LLM yes, you can &amp;quot;combine&amp;quot; the RAM and run larger models. They do not have to be the same anything.&lt;/p&gt;\\n\\n&lt;p&gt;But, since you are saying ComfyUI, I take it you want to do image/video gen too. It won&amp;#39;t help for that. Other than maybe Wan, I don&amp;#39;t know of a model that can be split across GPUs for image/video gen. You might be able to do things like run different parts of the workflow on different GPUs to conserve RAM but you might as well do offloading.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lze20x/can_vram_be_combined_of_2_brands/n30zyrt/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752470827,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lze20x","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":14}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n325vru","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"a_beautiful_rhind","can_mod_post":false,"created_utc":1752493506,"send_replies":true,"parent_id":"t1_n314iye","score":2,"author_fullname":"t2_h5utwre7","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Run comfyui in AMD environment and LLM in opposite environment. Install both drivers to host system.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n325vru","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Run comfyui in AMD environment and LLM in opposite environment. Install both drivers to host system.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lze20x","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lze20x/can_vram_be_combined_of_2_brands/n325vru/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752493506,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n34j9cx","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AppearanceHeavy6724","can_mod_post":false,"send_replies":true,"parent_id":"t1_n34dbf4","score":1,"author_fullname":"t2_uz37qfx5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I need to check myself.","edited":false,"author_flair_css_class":null,"name":"t1_n34j9cx","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I need to check myself.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lze20x","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lze20x/can_vram_be_combined_of_2_brands/n34j9cx/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752519359,"author_flair_text":null,"collapsed":false,"created_utc":1752519359,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n34dbf4","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"fallingdowndizzyvr","can_mod_post":false,"send_replies":true,"parent_id":"t1_n31ks9u","score":1,"author_fullname":"t2_o65i6kx","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Yes. Yes it is.\\n\\nhttps://www.reddit.com/r/LocalLLaMA/comments/1kabje8/vulkan_is_faster_tan_cuda_currently_with_llamacpp/","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n34dbf4","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yes. Yes it is.&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://www.reddit.com/r/LocalLLaMA/comments/1kabje8/vulkan_is_faster_tan_cuda_currently_with_llamacpp/\\"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1kabje8/vulkan_is_faster_tan_cuda_currently_with_llamacpp/&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lze20x","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lze20x/can_vram_be_combined_of_2_brands/n34dbf4/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752517646,"author_flair_text":null,"treatment_tags":[],"created_utc":1752517646,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n31ks9u","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AppearanceHeavy6724","can_mod_post":false,"send_replies":true,"parent_id":"t1_n31cgqt","score":3,"author_fullname":"t2_uz37qfx5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"not on Nvidia.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n31ks9u","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;not on Nvidia.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lze20x","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lze20x/can_vram_be_combined_of_2_brands/n31ks9u/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752482469,"author_flair_text":null,"treatment_tags":[],"created_utc":1752482469,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n35p06f","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Evening_Ad6637","can_mod_post":false,"created_utc":1752531431,"send_replies":true,"parent_id":"t1_n353kcf","score":1,"author_fullname":"t2_p45er6oo","approved_by":null,"mod_note":null,"all_awardings":[],"body":"I known i know, it was just a quick and dirty vibe check","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n35p06f","is_submitter":false,"collapsed":false,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I known i know, it was just a quick and dirty vibe check&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lze20x/can_vram_be_combined_of_2_brands/n35p06f/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752531431,"author_flair_text":"llama.cpp","treatment_tags":[],"link_id":"t3_1lze20x","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":8,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n353kcf","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"fallingdowndizzyvr","can_mod_post":false,"created_utc":1752525155,"send_replies":true,"parent_id":"t1_n34ybk9","score":1,"author_fullname":"t2_o65i6kx","approved_by":null,"mod_note":null,"all_awardings":[],"body":"&gt;  Tomorrow I am going trying with llama.cpp directly and with other models. \\n\\nPlease use llama-bench. That's the point of it. To keep as many variables constant as possible. Ideally only one variable should change, Vulkan vs CUDA. That's how benchmarking is done. You can't do that by using LM Studio.","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n353kcf","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;Tomorrow I am going trying with llama.cpp directly and with other models. &lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;Please use llama-bench. That&amp;#39;s the point of it. To keep as many variables constant as possible. Ideally only one variable should change, Vulkan vs CUDA. That&amp;#39;s how benchmarking is done. You can&amp;#39;t do that by using LM Studio.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lze20x","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lze20x/can_vram_be_combined_of_2_brands/n353kcf/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752525155,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":7,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n34ybk9","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Evening_Ad6637","can_mod_post":false,"send_replies":true,"parent_id":"t1_n34qn91","score":1,"author_fullname":"t2_p45er6oo","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Okay so at least I could reproduce results for one card, for another another unfortunately not. But I have to mention that for convenience I've used LM Studio. Tomorrow I am going trying with llama.cpp directly and with other models. But it’s indeed very interesting already now. Here the results from my 'quicky':\\n\\n- - -\\n\\nOn an old mining card, Vulkan is approximately 5% **FASTER** than CUDA in text-generation. \\n\\n### Device - NVIDIA CMP 30HX\\n\\n## Vulkan\\n\\n**Time-to-first-token 0.44s**\\n\\n**Text-generation 49.5 tok/sec**\\n\\n## CUDA\\n\\n**Time-to-first-token 0.07**\\n\\n**Text-generation 46.2 tok/sec**\\n\\n- - -\\n\\nOn an 3090, Vulkan is approximately 13% **SLOWER** than CUDA in text-generation.\\n\\n### Device - NVIDIA RTX 3090 Ti\\n\\n## Vulkan\\n\\n**Time-to-first-token 0.14**\\n\\n**Text-generation 136.0 tok/sec**\\n\\n## CUDA\\n\\n**Time-to-first-token 0.02**\\n\\n**Text-generation 154.1 tok/sec**\\n\\n- - -\\n\\n_Note_\\n\\n- Always using Model - gemma-3-1b-qat (Q4_0)\\n- Always have 2 runs\\n- average value for TG\\n- first value for TTFT\\n- in both cases, the cards get hotter and the fans louder when running with CUDA","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n34ybk9","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Okay so at least I could reproduce results for one card, for another another unfortunately not. But I have to mention that for convenience I&amp;#39;ve used LM Studio. Tomorrow I am going trying with llama.cpp directly and with other models. But it’s indeed very interesting already now. Here the results from my &amp;#39;quicky&amp;#39;:&lt;/p&gt;\\n\\n&lt;hr/&gt;\\n\\n&lt;p&gt;On an old mining card, Vulkan is approximately 5% &lt;strong&gt;FASTER&lt;/strong&gt; than CUDA in text-generation. &lt;/p&gt;\\n\\n&lt;h3&gt;Device - NVIDIA CMP 30HX&lt;/h3&gt;\\n\\n&lt;h2&gt;Vulkan&lt;/h2&gt;\\n\\n&lt;p&gt;&lt;strong&gt;Time-to-first-token 0.44s&lt;/strong&gt;&lt;/p&gt;\\n\\n&lt;p&gt;&lt;strong&gt;Text-generation 49.5 tok/sec&lt;/strong&gt;&lt;/p&gt;\\n\\n&lt;h2&gt;CUDA&lt;/h2&gt;\\n\\n&lt;p&gt;&lt;strong&gt;Time-to-first-token 0.07&lt;/strong&gt;&lt;/p&gt;\\n\\n&lt;p&gt;&lt;strong&gt;Text-generation 46.2 tok/sec&lt;/strong&gt;&lt;/p&gt;\\n\\n&lt;hr/&gt;\\n\\n&lt;p&gt;On an 3090, Vulkan is approximately 13% &lt;strong&gt;SLOWER&lt;/strong&gt; than CUDA in text-generation.&lt;/p&gt;\\n\\n&lt;h3&gt;Device - NVIDIA RTX 3090 Ti&lt;/h3&gt;\\n\\n&lt;h2&gt;Vulkan&lt;/h2&gt;\\n\\n&lt;p&gt;&lt;strong&gt;Time-to-first-token 0.14&lt;/strong&gt;&lt;/p&gt;\\n\\n&lt;p&gt;&lt;strong&gt;Text-generation 136.0 tok/sec&lt;/strong&gt;&lt;/p&gt;\\n\\n&lt;h2&gt;CUDA&lt;/h2&gt;\\n\\n&lt;p&gt;&lt;strong&gt;Time-to-first-token 0.02&lt;/strong&gt;&lt;/p&gt;\\n\\n&lt;p&gt;&lt;strong&gt;Text-generation 154.1 tok/sec&lt;/strong&gt;&lt;/p&gt;\\n\\n&lt;hr/&gt;\\n\\n&lt;p&gt;&lt;em&gt;Note&lt;/em&gt;&lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;Always using Model - gemma-3-1b-qat (Q4_0)&lt;/li&gt;\\n&lt;li&gt;Always have 2 runs&lt;/li&gt;\\n&lt;li&gt;average value for TG&lt;/li&gt;\\n&lt;li&gt;first value for TTFT&lt;/li&gt;\\n&lt;li&gt;in both cases, the cards get hotter and the fans louder when running with CUDA&lt;/li&gt;\\n&lt;/ul&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lze20x","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lze20x/can_vram_be_combined_of_2_brands/n34ybk9/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752523701,"author_flair_text":"llama.cpp","treatment_tags":[],"created_utc":1752523701,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":6,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n34qn91","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"fallingdowndizzyvr","can_mod_post":false,"send_replies":true,"parent_id":"t1_n34lazx","score":2,"author_fullname":"t2_o65i6kx","approved_by":null,"mod_note":null,"all_awardings":[],"body":"&gt; If I talked some bullshit, then sorry, my fault.\\n\\nDude, it's totally cool. In fact, props for posting that. Not many people would.\\n\\n&gt; But that would mean NVIDIA users only need cuda for training, otherwise obsolet, right?\\n\\nFor most people, yes.","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n34qn91","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;If I talked some bullshit, then sorry, my fault.&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;Dude, it&amp;#39;s totally cool. In fact, props for posting that. Not many people would.&lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;But that would mean NVIDIA users only need cuda for training, otherwise obsolet, right?&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;For most people, yes.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lze20x","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lze20x/can_vram_be_combined_of_2_brands/n34qn91/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752521488,"author_flair_text":null,"treatment_tags":[],"created_utc":1752521488,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n34lazx","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Evening_Ad6637","can_mod_post":false,"send_replies":true,"parent_id":"t1_n34b2cw","score":2,"author_fullname":"t2_p45er6oo","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Okay wtf, i even upvoted your post from the first link, so I must have tested it myself to agree. Still can’t believe it xD\\n\\n\\nI have to test it myself again lol\\n\\n\\nIf I talked some bullshit, then sorry, my fault.\\nBut that would mean NVIDIA users only need cuda for training, otherwise obsolet, right?","edited":false,"author_flair_css_class":null,"name":"t1_n34lazx","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Okay wtf, i even upvoted your post from the first link, so I must have tested it myself to agree. Still can’t believe it xD&lt;/p&gt;\\n\\n&lt;p&gt;I have to test it myself again lol&lt;/p&gt;\\n\\n&lt;p&gt;If I talked some bullshit, then sorry, my fault.\\nBut that would mean NVIDIA users only need cuda for training, otherwise obsolet, right?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lze20x","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":"light","treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lze20x/can_vram_be_combined_of_2_brands/n34lazx/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752519963,"author_flair_text":"llama.cpp","collapsed":false,"created_utc":1752519963,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n34b2cw","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"fallingdowndizzyvr","can_mod_post":false,"send_replies":true,"parent_id":"t1_n342mc0","score":1,"author_fullname":"t2_o65i6kx","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"&gt; No, that’s not true! The text generation speed is slightly slower under vulcan, but acceptable.\\n\\nIt is true. I and others have shown it to be true multiple times.\\n\\nhttps://www.reddit.com/r/LocalLLaMA/comments/1kabje8/vulkan_is_faster_tan_cuda_currently_with_llamacpp/\\n\\nhttps://www.reddit.com/r/LocalLLaMA/comments/1iw9m8r/amd_inference_using_amdvlk_driver_is_40_faster/\\n\\nVulkan is even faster now than it was then.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n34b2cw","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;No, that’s not true! The text generation speed is slightly slower under vulcan, but acceptable.&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;It is true. I and others have shown it to be true multiple times.&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://www.reddit.com/r/LocalLLaMA/comments/1kabje8/vulkan_is_faster_tan_cuda_currently_with_llamacpp/\\"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1kabje8/vulkan_is_faster_tan_cuda_currently_with_llamacpp/&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://www.reddit.com/r/LocalLLaMA/comments/1iw9m8r/amd_inference_using_amdvlk_driver_is_40_faster/\\"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1iw9m8r/amd_inference_using_amdvlk_driver_is_40_faster/&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;Vulkan is even faster now than it was then.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lze20x","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lze20x/can_vram_be_combined_of_2_brands/n34b2cw/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752517013,"author_flair_text":null,"treatment_tags":[],"created_utc":1752517013,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n342mc0","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Evening_Ad6637","can_mod_post":false,"send_replies":true,"parent_id":"t1_n31cgqt","score":1,"author_fullname":"t2_p45er6oo","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"No, that’s not true! The text generation speed is slightly slower under vulcan, but acceptable.\\n\\nBut the prompt processing speed will suffer immensely.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n342mc0","is_submitter":false,"collapsed":false,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;No, that’s not true! The text generation speed is slightly slower under vulcan, but acceptable.&lt;/p&gt;\\n\\n&lt;p&gt;But the prompt processing speed will suffer immensely.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lze20x","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lze20x/can_vram_be_combined_of_2_brands/n342mc0/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752514708,"author_flair_text":"llama.cpp","treatment_tags":[],"created_utc":1752514708,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n31cgqt","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"fallingdowndizzyvr","can_mod_post":false,"created_utc":1752477586,"send_replies":true,"parent_id":"t1_n314iye","score":1,"author_fullname":"t2_o65i6kx","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"&gt; But the backend is not fully optimized.\\n\\nThe llama.cpp Vulkan backend is as fast or faster than ROCm/CUDA.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n31cgqt","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;But the backend is not fully optimized.&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;The llama.cpp Vulkan backend is as fast or faster than ROCm/CUDA.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lze20x","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lze20x/can_vram_be_combined_of_2_brands/n31cgqt/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752477586,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":1,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n314iye","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"lly0571","can_mod_post":false,"created_utc":1752473189,"send_replies":true,"parent_id":"t3_1lze20x","score":3,"author_fullname":"t2_70vzcleel","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"ComfyUI may not work.\\n\\nFor llms, maybe Llama.cpp vulkan backend can make both GPUs working together. But the backend is not fully optimized.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n314iye","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;ComfyUI may not work.&lt;/p&gt;\\n\\n&lt;p&gt;For llms, maybe Llama.cpp vulkan backend can make both GPUs working together. But the backend is not fully optimized.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lze20x/can_vram_be_combined_of_2_brands/n314iye/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752473189,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lze20x","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n34s9k6","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"FieldProgrammable","can_mod_post":false,"send_replies":true,"parent_id":"t1_n34r91q","score":1,"author_fullname":"t2_moet0t","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"You rather spew subjective statements like \\"this is trivial\\". Have you even asked what OS OP is running? You seem to have a high opinion of your knowledge, perhaps when OP has bought both an AMD and Nvidia card and is struggling to get it running you might provide him with technical support in getting it running.","edited":false,"author_flair_css_class":null,"name":"t1_n34s9k6","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You rather spew subjective statements like &amp;quot;this is trivial&amp;quot;. Have you even asked what OS OP is running? You seem to have a high opinion of your knowledge, perhaps when OP has bought both an AMD and Nvidia card and is struggling to get it running you might provide him with technical support in getting it running.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lze20x","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lze20x/can_vram_be_combined_of_2_brands/n34s9k6/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752521966,"author_flair_text":null,"collapsed":false,"created_utc":1752521966,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n34r91q","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"fallingdowndizzyvr","can_mod_post":false,"send_replies":true,"parent_id":"t1_n34l0pj","score":1,"author_fullname":"t2_o65i6kx","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I rather people speak truth from a position of arrogance than made up lies from a position of ignorance.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n34r91q","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I rather people speak truth from a position of arrogance than made up lies from a position of ignorance.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lze20x","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lze20x/can_vram_be_combined_of_2_brands/n34r91q/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752521665,"author_flair_text":null,"treatment_tags":[],"created_utc":1752521665,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n34l0pj","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"FieldProgrammable","can_mod_post":false,"send_replies":true,"parent_id":"t1_n34fckf","score":1,"author_fullname":"t2_moet0t","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"And it's pretty clear you speaking from a position of arrogance.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n34l0pj","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;And it&amp;#39;s pretty clear you speaking from a position of arrogance.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lze20x","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lze20x/can_vram_be_combined_of_2_brands/n34l0pj/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752519881,"author_flair_text":null,"treatment_tags":[],"created_utc":1752519881,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n34fckf","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"fallingdowndizzyvr","can_mod_post":false,"created_utc":1752518229,"send_replies":true,"parent_id":"t1_n324xdr","score":1,"author_fullname":"t2_o65i6kx","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"&gt; I would say that if you want an easy time of setting this up then absolutely do not mix brands.\\n\\nThat's absolutely not true. It's trivially simple to mix brands. Even if you must use CUDA/ROCm. In fact, the hardest part if you must use CUDA/ROCm is installing CUDA/ROCm.\\n\\n&gt; Just mixing different generations of the same brand can be a problem, let alone getting two very different compute platforms to behave optimally with each other.\\n\\nHave you ever tried? I do it all the time. It's trivial.\\n\\n&gt; It might be theoretically possible with hours of research on getting an LLM running in one particular configuration with Vulkan.\\n\\nAh... what? It's trivial to get Vulkan working on one GPU or a gaggle of GPUs together. It's far easier to get Vulkan working than CUDA or ROCm. Vulkan is built into the driver for pretty much any GPU. There's nothing to install. Just download your LLM program that supports Vulkan and go. It's the closest thing to \\"plug and play\\".\\n\\n&gt;  But do yourself a favour and save that time, money and energy doing something you enjoy rather than fighting obscure driver and library conflicts based on random anonymous forums.\\n\\nDo yourself a favor and give Vulkan a try. Stopped struggling. It's clear you have never even tried and thus are speaking from a position of ignorance.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n34fckf","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;I would say that if you want an easy time of setting this up then absolutely do not mix brands.&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;That&amp;#39;s absolutely not true. It&amp;#39;s trivially simple to mix brands. Even if you must use CUDA/ROCm. In fact, the hardest part if you must use CUDA/ROCm is installing CUDA/ROCm.&lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;Just mixing different generations of the same brand can be a problem, let alone getting two very different compute platforms to behave optimally with each other.&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;Have you ever tried? I do it all the time. It&amp;#39;s trivial.&lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;It might be theoretically possible with hours of research on getting an LLM running in one particular configuration with Vulkan.&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;Ah... what? It&amp;#39;s trivial to get Vulkan working on one GPU or a gaggle of GPUs together. It&amp;#39;s far easier to get Vulkan working than CUDA or ROCm. Vulkan is built into the driver for pretty much any GPU. There&amp;#39;s nothing to install. Just download your LLM program that supports Vulkan and go. It&amp;#39;s the closest thing to &amp;quot;plug and play&amp;quot;.&lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;But do yourself a favour and save that time, money and energy doing something you enjoy rather than fighting obscure driver and library conflicts based on random anonymous forums.&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;Do yourself a favor and give Vulkan a try. Stopped struggling. It&amp;#39;s clear you have never even tried and thus are speaking from a position of ignorance.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lze20x","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lze20x/can_vram_be_combined_of_2_brands/n34fckf/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752518229,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n324xdr","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"FieldProgrammable","can_mod_post":false,"created_utc":1752493096,"send_replies":true,"parent_id":"t3_1lze20x","score":3,"author_fullname":"t2_moet0t","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"There seem to be some strange comments in this thread. I would say that if you want an easy time of setting this up then absolutely do not mix brands. Just mixing different generations of the same brand can be a problem, let alone getting two very different compute platforms to behave optimally with each other. My advice is if you want more VRAM, stick with AMD and live with the consequences (namely that it has less support than CUDA for many ML tasks beyond LLM). If you now want a CUDA card for that reason, then expect to not be able to share a model between them.\\n\\nIn terms of ComfyUI diffusion models are much less tolerant of mult-GPU setups than LLMs. You would need a special set of \\"Mult-GPU\\" nodes just to do anything and those are really designed for putting VAE and embedding models to a separate GPU to the latent space and diffusion model. Splitting the diffusion model itself can be done with something like the DisTorch multi-GPU node but this isn't particularly stable and won't perform nearly as well as a single GPU.\\n\\nIt might be theoretically possible with hours of research on getting an LLM running in one particular configuration with Vulkan. But do yourself a favour and save that time, money and energy doing something you enjoy rather than fighting obscure driver and library conflicts based on random anonymous forums.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n324xdr","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;There seem to be some strange comments in this thread. I would say that if you want an easy time of setting this up then absolutely do not mix brands. Just mixing different generations of the same brand can be a problem, let alone getting two very different compute platforms to behave optimally with each other. My advice is if you want more VRAM, stick with AMD and live with the consequences (namely that it has less support than CUDA for many ML tasks beyond LLM). If you now want a CUDA card for that reason, then expect to not be able to share a model between them.&lt;/p&gt;\\n\\n&lt;p&gt;In terms of ComfyUI diffusion models are much less tolerant of mult-GPU setups than LLMs. You would need a special set of &amp;quot;Mult-GPU&amp;quot; nodes just to do anything and those are really designed for putting VAE and embedding models to a separate GPU to the latent space and diffusion model. Splitting the diffusion model itself can be done with something like the DisTorch multi-GPU node but this isn&amp;#39;t particularly stable and won&amp;#39;t perform nearly as well as a single GPU.&lt;/p&gt;\\n\\n&lt;p&gt;It might be theoretically possible with hours of research on getting an LLM running in one particular configuration with Vulkan. But do yourself a favour and save that time, money and energy doing something you enjoy rather than fighting obscure driver and library conflicts based on random anonymous forums.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lze20x/can_vram_be_combined_of_2_brands/n324xdr/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752493096,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lze20x","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n31aiq2","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Rich_Repeat_22","can_mod_post":false,"created_utc":1752476479,"send_replies":true,"parent_id":"t3_1lze20x","score":3,"author_fullname":"t2_viufiki6","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"If you are using Windows, before you buy another card, please have a look at this guide to use ROCm with the 7900XTX on Windows with ComfyUI. \\n\\n[https://youtu.be/gfcOt1-3zYk](https://youtu.be/gfcOt1-3zYk)\\n\\nUsed it and works on the 7900XT, as you see the comments can be used for all 7000 and 9000 series within 10 minutes.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n31aiq2","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;If you are using Windows, before you buy another card, please have a look at this guide to use ROCm with the 7900XTX on Windows with ComfyUI. &lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://youtu.be/gfcOt1-3zYk\\"&gt;https://youtu.be/gfcOt1-3zYk&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;Used it and works on the 7900XT, as you see the comments can be used for all 7000 and 9000 series within 10 minutes.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lze20x/can_vram_be_combined_of_2_brands/n31aiq2/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752476479,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lze20x","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"more","data":{"count":0,"name":"t1__","id":"_","parent_id":"t1_n31f84p","depth":10,"children":[]}}],"before":null}},"user_reports":[],"saved":false,"id":"n31f84p","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"fallingdowndizzyvr","can_mod_post":false,"created_utc":1752479202,"send_replies":true,"parent_id":"t1_n31eksy","score":2,"author_fullname":"t2_o65i6kx","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Still making stuff up I see. What you said doesn't even make any sense. You don't have any understanding of how multi-gpu works do you?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n31f84p","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Still making stuff up I see. What you said doesn&amp;#39;t even make any sense. You don&amp;#39;t have any understanding of how multi-gpu works do you?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lze20x/can_vram_be_combined_of_2_brands/n31f84p/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752479202,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lze20x","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":9,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n31eksy","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"SashaUsesReddit","can_mod_post":false,"created_utc":1752478818,"send_replies":true,"parent_id":"t1_n31egzl","score":1,"author_fullname":"t2_57wafqev","approved_by":null,"mod_note":null,"all_awardings":[],"body":"Im sure you have a car with 4 different size wheels also and are happy it gets up to 10mph\\n\\nGrow up. This person is looking to actively spend money lol","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n31eksy","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Im sure you have a car with 4 different size wheels also and are happy it gets up to 10mph&lt;/p&gt;\\n\\n&lt;p&gt;Grow up. This person is looking to actively spend money lol&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lze20x/can_vram_be_combined_of_2_brands/n31eksy/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752478818,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lze20x","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":8,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n31egzl","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"fallingdowndizzyvr","can_mod_post":false,"created_utc":1752478755,"send_replies":true,"parent_id":"t1_n31dnca","score":3,"author_fullname":"t2_o65i6kx","approved_by":null,"mod_note":null,"all_awardings":[],"body":"&gt; There are drivers and libs that will break all over the place.\\n\\nThat is absolutely not the case. Please stop making stuff up.","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n31egzl","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;There are drivers and libs that will break all over the place.&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;That is absolutely not the case. Please stop making stuff up.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lze20x","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lze20x/can_vram_be_combined_of_2_brands/n31egzl/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752478755,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":7,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n31dnca","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"SashaUsesReddit","can_mod_post":false,"send_replies":true,"parent_id":"t1_n31dboe","score":1,"author_fullname":"t2_57wafqev","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"That's absolutely not the case. There are drivers and libs that will break all over the place. P2p memory won't function correctly without heavy system root load, there will be serious function level issues for trying to do fp16 or fp8 functions, tensor parallelism will negatively scale if you can even it it to actually work (real parallelism, not just slow mem sharding)\\n\\nBeing broken to me includes the perf being a total waste of time and money.","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n31dnca","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;That&amp;#39;s absolutely not the case. There are drivers and libs that will break all over the place. P2p memory won&amp;#39;t function correctly without heavy system root load, there will be serious function level issues for trying to do fp16 or fp8 functions, tensor parallelism will negatively scale if you can even it it to actually work (real parallelism, not just slow mem sharding)&lt;/p&gt;\\n\\n&lt;p&gt;Being broken to me includes the perf being a total waste of time and money.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lze20x","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lze20x/can_vram_be_combined_of_2_brands/n31dnca/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752478264,"author_flair_text":null,"treatment_tags":[],"created_utc":1752478264,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":6,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n31dboe","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"fallingdowndizzyvr","can_mod_post":false,"send_replies":true,"parent_id":"t1_n31cgpc","score":2,"author_fullname":"t2_o65i6kx","approved_by":null,"mod_note":null,"all_awardings":[],"body":"LOL. You said you couldn't even do it because of \\"drivers and libs will have conflicts all over the place\\". Now you say the \\"performance is just terrible\\". How would you know? You've never been able to do it.\\n\\nThere are no \\"Device drivers and libs\\" conflicts. Let alone all over the place. And the performance is just fine. There is a performance penalty for going multi-gpu. But that's because it's multi-gpu and thus there is a loss of efficiency.\\n\\n&gt; Edit: we as a community should steer people in the right direction.\\n\\nAs a community, we should speak about things we know about. Things we have experience doing. Not making stuff up when we have no idea what we are talking about.","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n31dboe","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;LOL. You said you couldn&amp;#39;t even do it because of &amp;quot;drivers and libs will have conflicts all over the place&amp;quot;. Now you say the &amp;quot;performance is just terrible&amp;quot;. How would you know? You&amp;#39;ve never been able to do it.&lt;/p&gt;\\n\\n&lt;p&gt;There are no &amp;quot;Device drivers and libs&amp;quot; conflicts. Let alone all over the place. And the performance is just fine. There is a performance penalty for going multi-gpu. But that&amp;#39;s because it&amp;#39;s multi-gpu and thus there is a loss of efficiency.&lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;Edit: we as a community should steer people in the right direction.&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;As a community, we should speak about things we know about. Things we have experience doing. Not making stuff up when we have no idea what we are talking about.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lze20x","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lze20x/can_vram_be_combined_of_2_brands/n31dboe/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752478076,"author_flair_text":null,"treatment_tags":[],"created_utc":1752478076,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n31cgpc","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"SashaUsesReddit","can_mod_post":false,"send_replies":true,"parent_id":"t1_n31cdv3","score":-2,"author_fullname":"t2_57wafqev","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"So.. your performance is just terrible as a consequence\\n\\nEdit: we as a community should steer people in the right direction. Buying new parts to intentionally mix and match is different than working with what you have. Just because you can technically get a model to load does NOT make it a good idea to spend money going down this road.","edited":1752478052,"author_flair_css_class":null,"name":"t1_n31cgpc","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;So.. your performance is just terrible as a consequence&lt;/p&gt;\\n\\n&lt;p&gt;Edit: we as a community should steer people in the right direction. Buying new parts to intentionally mix and match is different than working with what you have. Just because you can technically get a model to load does NOT make it a good idea to spend money going down this road.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lze20x","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lze20x/can_vram_be_combined_of_2_brands/n31cgpc/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752477585,"author_flair_text":null,"collapsed":false,"created_utc":1752477585,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-2}}],"before":null}},"user_reports":[],"saved":false,"id":"n31cdv3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"fallingdowndizzyvr","can_mod_post":false,"send_replies":true,"parent_id":"t1_n310wkt","score":2,"author_fullname":"t2_o65i6kx","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"That's just user error. I don't have those problems.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n31cdv3","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;That&amp;#39;s just user error. I don&amp;#39;t have those problems.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lze20x","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lze20x/can_vram_be_combined_of_2_brands/n31cdv3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752477541,"author_flair_text":null,"treatment_tags":[],"created_utc":1752477541,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n310wkt","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"SashaUsesReddit","can_mod_post":false,"send_replies":true,"parent_id":"t1_n310qgm","score":1,"author_fullname":"t2_57wafqev","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Device drivers and libs will have conflicts all over the place. If you had trouble just with AMD, this would be hell","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n310wkt","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Device drivers and libs will have conflicts all over the place. If you had trouble just with AMD, this would be hell&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lze20x","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lze20x/can_vram_be_combined_of_2_brands/n310wkt/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752471310,"author_flair_text":null,"treatment_tags":[],"created_utc":1752471310,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n310qgm","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"reacusn","can_mod_post":false,"created_utc":1752471221,"send_replies":true,"parent_id":"t1_n31033z","score":4,"author_fullname":"t2_1ppg6hcqm8","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"What if you use vulkan on the nvidia gpu? Is that possible?\\n\\n---\\n\\nOkay, so I found this post: https://old.reddit.com/r/LocalLLaMA/comments/1dt367v/is_it_possible_to_use_both_and_nvidia_and_amd_gpu/\\n\\nu/kirill32 says:\\n&gt;Tested RX 7900 XTX and 4060 Ti (16GB) running together in LM Studio via Vulkan. Tried it with two models:  \\nDS r1 70B Q5 — 10.05 tok/sec  \\nQWQ 32b — 15.67 tok/sec  \\nFor comparison, RX 7900 XTX solo gets around 24.55 tok/sec in QWQ 32b.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n310qgm","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;What if you use vulkan on the nvidia gpu? Is that possible?&lt;/p&gt;\\n\\n&lt;hr/&gt;\\n\\n&lt;p&gt;Okay, so I found this post: &lt;a href=\\"https://old.reddit.com/r/LocalLLaMA/comments/1dt367v/is_it_possible_to_use_both_and_nvidia_and_amd_gpu/\\"&gt;https://old.reddit.com/r/LocalLLaMA/comments/1dt367v/is_it_possible_to_use_both_and_nvidia_and_amd_gpu/&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"/u/kirill32\\"&gt;u/kirill32&lt;/a&gt; says:&lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;Tested RX 7900 XTX and 4060 Ti (16GB) running together in LM Studio via Vulkan. Tried it with two models:&lt;br/&gt;\\nDS r1 70B Q5 — 10.05 tok/sec&lt;br/&gt;\\nQWQ 32b — 15.67 tok/sec&lt;br/&gt;\\nFor comparison, RX 7900 XTX solo gets around 24.55 tok/sec in QWQ 32b.&lt;/p&gt;\\n&lt;/blockquote&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lze20x","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lze20x/can_vram_be_combined_of_2_brands/n310qgm/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752471221,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n31emqz","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"fallingdowndizzyvr","can_mod_post":false,"send_replies":true,"parent_id":"t1_n31dqsb","score":2,"author_fullname":"t2_o65i6kx","approved_by":null,"mod_note":null,"all_awardings":[],"body":"You only seem to be here to make up stuff about things you know nothing about.","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n31emqz","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You only seem to be here to make up stuff about things you know nothing about.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lze20x","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lze20x/can_vram_be_combined_of_2_brands/n31emqz/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752478850,"author_flair_text":null,"treatment_tags":[],"created_utc":1752478850,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n31dqsb","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"SashaUsesReddit","can_mod_post":false,"send_replies":true,"parent_id":"t1_n31de4a","score":1,"author_fullname":"t2_57wafqev","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Good comment, enjoy your duct tape.\\n\\nIm here to make and suggest good purchases for the community. Why encourage him to do this when you know it'll be crap?","edited":false,"author_flair_css_class":null,"name":"t1_n31dqsb","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Good comment, enjoy your duct tape.&lt;/p&gt;\\n\\n&lt;p&gt;Im here to make and suggest good purchases for the community. Why encourage him to do this when you know it&amp;#39;ll be crap?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lze20x","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lze20x/can_vram_be_combined_of_2_brands/n31dqsb/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752478320,"author_flair_text":null,"collapsed":false,"created_utc":1752478320,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n31de4a","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"fallingdowndizzyvr","can_mod_post":false,"send_replies":true,"parent_id":"t1_n31cj59","score":2,"author_fullname":"t2_o65i6kx","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"How would you know? You've never done it.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n31de4a","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;How would you know? You&amp;#39;ve never done it.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lze20x","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lze20x/can_vram_be_combined_of_2_brands/n31de4a/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752478115,"author_flair_text":null,"treatment_tags":[],"created_utc":1752478115,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n31cj59","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"SashaUsesReddit","can_mod_post":false,"send_replies":true,"parent_id":"t1_n31cbnv","score":-1,"author_fullname":"t2_57wafqev","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Oof. Sorry for your performance.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n31cj59","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Oof. Sorry for your performance.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lze20x","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lze20x/can_vram_be_combined_of_2_brands/n31cj59/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752477624,"author_flair_text":null,"treatment_tags":[],"created_utc":1752477624,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-1}}],"before":null}},"user_reports":[],"saved":false,"id":"n31cbnv","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"fallingdowndizzyvr","can_mod_post":false,"created_utc":1752477506,"send_replies":true,"parent_id":"t1_n31033z","score":4,"author_fullname":"t2_o65i6kx","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"&gt; No mix and match of brands.\\n\\nThat's not true at all. I run AMD, Intel, Nvidia and for a bit of spice a Mac all together to run big models.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n31cbnv","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;No mix and match of brands.&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;That&amp;#39;s not true at all. I run AMD, Intel, Nvidia and for a bit of spice a Mac all together to run big models.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lze20x","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lze20x/can_vram_be_combined_of_2_brands/n31cbnv/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752477506,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}}],"before":null}},"user_reports":[],"saved":false,"id":"n31033z","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"SashaUsesReddit","can_mod_post":false,"created_utc":1752470887,"send_replies":true,"parent_id":"t3_1lze20x","score":4,"author_fullname":"t2_57wafqev","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"No mix and match of brands.\\n\\nAlso some mix and match can work with same brand GPUs... but its hit or miss depending on the application and compute level required (fp16, fp8 etc)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n31033z","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;No mix and match of brands.&lt;/p&gt;\\n\\n&lt;p&gt;Also some mix and match can work with same brand GPUs... but its hit or miss depending on the application and compute level required (fp16, fp8 etc)&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lze20x/can_vram_be_combined_of_2_brands/n31033z/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752470887,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lze20x","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n317bya","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Threatening-Silence-","can_mod_post":false,"created_utc":1752474719,"send_replies":true,"parent_id":"t3_1lze20x","score":2,"author_fullname":"t2_15wqsifdjf","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Vulcan.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n317bya","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Vulcan.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lze20x/can_vram_be_combined_of_2_brands/n317bya/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752474719,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lze20x","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}}]`),o=()=>e.jsx(l,{data:a});export{o as default};
