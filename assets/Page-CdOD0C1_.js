import{j as e}from"./index-CNyNkRpk.js";import{R as t}from"./RedditPostRenderer-Dza0u9i2.js";import"./index-BUchu_-K.js";const l=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Sharing some experiences here. Mostly vibes, but maybe someone will find this helpful:\\n\\n**CPU:** Ryzen 9 3950x (16c/32t)\\n\\n**GPU(s):** two Rx 6800's (2x16GB at ~520GB/s for 32GB total)\\n\\n**RAM:** 64GB 2700mhz DDR4 in dual channel \\n\\n**OS:** Ubuntu 24.04\\n\\n**Inference Software:** Llama-CPP (llama-server specifically) built to use ROCm\\n\\n**Weights:** Qwen3-235b-a22b Q2 (Unsloth Quant), ~85GB. ~32GB into VRAM, 53GB to memory before context \\n\\n**Performance (Speed):** Inference speed was anywhere from 4 to 6 tokens per second with 8K max context (have not tested much higher). I offload 34 layers to GPU. I tried offloading experts to CPU (which allowed me to set this to ~75 layers) but did not experience a speed boost of any sort.\\n\\n**Speculative Decoding:** I tried using a few quants of Qwen3 0.6b, 1.7b, and 4b .. none had good accuracy and all slowed things down.\\n\\n**Intelligence:** I'm convinced this is the absolute best model that this machine can run, *but am diving deeper to determine if that's worth the speed penalty to my use cases*. It beats the previous champs (Qwen3-32B larger quants, Llama 3.3 70B Q5) for sure, even at Western history/trivia (Llama usually has an unfair advantage over Qwen here in my tests), but not tremendously so. There is no doubt in my mind that this is the most intelligent LLM I can run shut off from the open web with my current hardware (before inviting my SSD and some insane wait-times into the equation..). The intelligence gain doesn't appear to be night-and-day, but the speed loss absolutely is.\\n\\n**Vulkan** Vulkan briefly uses more VRAM on startup it seems. By the time I can get it to start using Vulkan (without crashing) I've sent so many layers back to CPU that it'd be impossible for it to keep up with ROCm in speed.\\n\\n**Vs Llama 4 Scout:** - Llama4 Scout fits IQ2XSS fully on GPU's and Q5 (!) on the same VRAM+CPU hybrid. It also inferences faster due to smaller experts. That's where the good news stops though. It's a complete win for Qwen3-235b to the point where I found IQ3 Llama 3.3 70B (fits neatly on GPU) better than it.\\n\\n**Drawbacks:** - For memory/context constraints' sake, quantizing cache on a Q2 model meant that coding performance was pretty underwhelming. It'd produce great results, but usually large edits/scripts contained a silly mistake or syntax error somewhere. It was capable of reconciling it, but I wouldn't recommend using these weights for coding unless you're comfortable testing full FP16 cache.\\n\\n**Thinking:** - All of the above impressive performance is from disabling thinking using \`/no_think\` in the prompt. Thinking improves a lot of this, but like all Qwen3 models, this thing likes to think *A LOT* (not quite QwQ level, but much more than deepseek or its distills) - and alas my patience could not survive that many thinking tokens at what would get down to 4 t/s\\n\\n### Command Used\\n\\n    HSA_OVERRIDE_GFX_VERSION=10.3.0 ./llama-server \\\\\\n    -m \\"\${MODEL_PATH}\\" \\\\\\n    --ctx-size 8000 \\\\\\n    -v \\\\\\n    --split-mode row \\\\\\n    --gpu-layers 34 \\\\\\n    --flash-attn \\\\\\n    --host 0.0.0.0 \\\\\\n    --mlock \\\\\\n    --no-mmap \\\\\\n    --cache-type-k q8_0 \\\\\\n    --cache-type-v q8_0 \\\\\\n    --no-warmup \\\\\\n    --threads 30 \\\\\\n    --temp 0.7 \\\\\\n    --top-p 0.8 \\\\\\n    --top-k 20 \\\\\\n    --min-p 0 \\\\\\n    --tensor-split 0.47,0.53\\n\\n-the awkward tensor split is to account for a bit of VRAM being used by my desktop environment. Without it I'm sure i'd get 1-2 more layers on GPU, but the speed difference is negligible.","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Qwen3-235B-Q2 running locally on my 64GB (DDR4) and 32GB VRAM machine","link_flair_richtext":[{"e":"text","t":"Discussion"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lue5xt","quarantine":false,"link_flair_text_color":"light","upvote_ratio":1,"author_flair_background_color":null,"subreddit_type":"public","ups":41,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_w2gxqd6i2","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Discussion","can_mod_post":false,"score":41,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":1751950246,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1751944511,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Sharing some experiences here. Mostly vibes, but maybe someone will find this helpful:&lt;/p&gt;\\n\\n&lt;p&gt;&lt;strong&gt;CPU:&lt;/strong&gt; Ryzen 9 3950x (16c/32t)&lt;/p&gt;\\n\\n&lt;p&gt;&lt;strong&gt;GPU(s):&lt;/strong&gt; two Rx 6800&amp;#39;s (2x16GB at ~520GB/s for 32GB total)&lt;/p&gt;\\n\\n&lt;p&gt;&lt;strong&gt;RAM:&lt;/strong&gt; 64GB 2700mhz DDR4 in dual channel &lt;/p&gt;\\n\\n&lt;p&gt;&lt;strong&gt;OS:&lt;/strong&gt; Ubuntu 24.04&lt;/p&gt;\\n\\n&lt;p&gt;&lt;strong&gt;Inference Software:&lt;/strong&gt; Llama-CPP (llama-server specifically) built to use ROCm&lt;/p&gt;\\n\\n&lt;p&gt;&lt;strong&gt;Weights:&lt;/strong&gt; Qwen3-235b-a22b Q2 (Unsloth Quant), ~85GB. ~32GB into VRAM, 53GB to memory before context &lt;/p&gt;\\n\\n&lt;p&gt;&lt;strong&gt;Performance (Speed):&lt;/strong&gt; Inference speed was anywhere from 4 to 6 tokens per second with 8K max context (have not tested much higher). I offload 34 layers to GPU. I tried offloading experts to CPU (which allowed me to set this to ~75 layers) but did not experience a speed boost of any sort.&lt;/p&gt;\\n\\n&lt;p&gt;&lt;strong&gt;Speculative Decoding:&lt;/strong&gt; I tried using a few quants of Qwen3 0.6b, 1.7b, and 4b .. none had good accuracy and all slowed things down.&lt;/p&gt;\\n\\n&lt;p&gt;&lt;strong&gt;Intelligence:&lt;/strong&gt; I&amp;#39;m convinced this is the absolute best model that this machine can run, &lt;em&gt;but am diving deeper to determine if that&amp;#39;s worth the speed penalty to my use cases&lt;/em&gt;. It beats the previous champs (Qwen3-32B larger quants, Llama 3.3 70B Q5) for sure, even at Western history/trivia (Llama usually has an unfair advantage over Qwen here in my tests), but not tremendously so. There is no doubt in my mind that this is the most intelligent LLM I can run shut off from the open web with my current hardware (before inviting my SSD and some insane wait-times into the equation..). The intelligence gain doesn&amp;#39;t appear to be night-and-day, but the speed loss absolutely is.&lt;/p&gt;\\n\\n&lt;p&gt;&lt;strong&gt;Vulkan&lt;/strong&gt; Vulkan briefly uses more VRAM on startup it seems. By the time I can get it to start using Vulkan (without crashing) I&amp;#39;ve sent so many layers back to CPU that it&amp;#39;d be impossible for it to keep up with ROCm in speed.&lt;/p&gt;\\n\\n&lt;p&gt;&lt;strong&gt;Vs Llama 4 Scout:&lt;/strong&gt; - Llama4 Scout fits IQ2XSS fully on GPU&amp;#39;s and Q5 (!) on the same VRAM+CPU hybrid. It also inferences faster due to smaller experts. That&amp;#39;s where the good news stops though. It&amp;#39;s a complete win for Qwen3-235b to the point where I found IQ3 Llama 3.3 70B (fits neatly on GPU) better than it.&lt;/p&gt;\\n\\n&lt;p&gt;&lt;strong&gt;Drawbacks:&lt;/strong&gt; - For memory/context constraints&amp;#39; sake, quantizing cache on a Q2 model meant that coding performance was pretty underwhelming. It&amp;#39;d produce great results, but usually large edits/scripts contained a silly mistake or syntax error somewhere. It was capable of reconciling it, but I wouldn&amp;#39;t recommend using these weights for coding unless you&amp;#39;re comfortable testing full FP16 cache.&lt;/p&gt;\\n\\n&lt;p&gt;&lt;strong&gt;Thinking:&lt;/strong&gt; - All of the above impressive performance is from disabling thinking using &lt;code&gt;/no_think&lt;/code&gt; in the prompt. Thinking improves a lot of this, but like all Qwen3 models, this thing likes to think &lt;em&gt;A LOT&lt;/em&gt; (not quite QwQ level, but much more than deepseek or its distills) - and alas my patience could not survive that many thinking tokens at what would get down to 4 t/s&lt;/p&gt;\\n\\n&lt;h3&gt;Command Used&lt;/h3&gt;\\n\\n&lt;pre&gt;&lt;code&gt;HSA_OVERRIDE_GFX_VERSION=10.3.0 ./llama-server \\\\\\n-m &amp;quot;\${MODEL_PATH}&amp;quot; \\\\\\n--ctx-size 8000 \\\\\\n-v \\\\\\n--split-mode row \\\\\\n--gpu-layers 34 \\\\\\n--flash-attn \\\\\\n--host 0.0.0.0 \\\\\\n--mlock \\\\\\n--no-mmap \\\\\\n--cache-type-k q8_0 \\\\\\n--cache-type-v q8_0 \\\\\\n--no-warmup \\\\\\n--threads 30 \\\\\\n--temp 0.7 \\\\\\n--top-p 0.8 \\\\\\n--top-k 20 \\\\\\n--min-p 0 \\\\\\n--tensor-split 0.47,0.53\\n&lt;/code&gt;&lt;/pre&gt;\\n\\n&lt;p&gt;-the awkward tensor split is to account for a bit of VRAM being used by my desktop environment. Without it I&amp;#39;m sure i&amp;#39;d get 1-2 more layers on GPU, but the speed difference is negligible.&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"5f921ea4-c7bc-11ed-9c23-3a00622979b4","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#646d73","id":"1lue5xt","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"EmPips","discussion_type":null,"num_comments":8,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lue5xt/qwen3235bq2_running_locally_on_my_64gb_ddr4_and/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lue5xt/qwen3235bq2_running_locally_on_my_64gb_ddr4_and/","subreddit_subscribers":496034,"created_utc":1751944511,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1xvnzi","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Threatening-Silence-","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1xhiii","score":4,"author_fullname":"t2_15wqsifdjf","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Offload tensors, not layers. \\n\\nHere's a guide:\\n\\nhttps://medium.com/p/13dc15287bed","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n1xvnzi","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Offload tensors, not layers. &lt;/p&gt;\\n\\n&lt;p&gt;Here&amp;#39;s a guide:&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://medium.com/p/13dc15287bed\\"&gt;https://medium.com/p/13dc15287bed&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lue5xt","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lue5xt/qwen3235bq2_running_locally_on_my_64gb_ddr4_and/n1xvnzi/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751954758,"author_flair_text":null,"treatment_tags":[],"created_utc":1751954758,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}}],"before":null}},"user_reports":[],"saved":false,"id":"n1xhiii","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"EmPips","can_mod_post":false,"created_utc":1751947962,"send_replies":true,"parent_id":"t1_n1xh7my","score":6,"author_fullname":"t2_w2gxqd6i2","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I was trying with quite a few, but I didn't notice any performance gain even after re-tuning the GPU layers (ended up with ~75 iirc)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1xhiii","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I was trying with quite a few, but I didn&amp;#39;t notice any performance gain even after re-tuning the GPU layers (ended up with ~75 iirc)&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lue5xt","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lue5xt/qwen3235bq2_running_locally_on_my_64gb_ddr4_and/n1xhiii/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751947962,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}}],"before":null}},"user_reports":[],"saved":false,"id":"n1xh7my","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Red_Redditor_Reddit","can_mod_post":false,"created_utc":1751947832,"send_replies":true,"parent_id":"t3_1lue5xt","score":7,"author_fullname":"t2_8eelmfjg","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"You might want to look into the llama.cpp arguments.  There's ones specifically for moe models like qwen when offloading to CPU like -ot \\".ffn_.*_exps.=CPU\\"\\n\\nBut yea, I use qwen 235B 2Q on my 64GB CPU only laptop with DDR4 and I get ~1.2 tokens a sec.  Not super great but still way faster than 70B or 100B dense.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1xh7my","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You might want to look into the llama.cpp arguments.  There&amp;#39;s ones specifically for moe models like qwen when offloading to CPU like -ot &amp;quot;.ffn_.*_exps.=CPU&amp;quot;&lt;/p&gt;\\n\\n&lt;p&gt;But yea, I use qwen 235B 2Q on my 64GB CPU only laptop with DDR4 and I get ~1.2 tokens a sec.  Not super great but still way faster than 70B or 100B dense.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lue5xt/qwen3235bq2_running_locally_on_my_64gb_ddr4_and/n1xh7my/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751947832,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lue5xt","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":7}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1xg2dz","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"EmPips","can_mod_post":false,"created_utc":1751947342,"send_replies":true,"parent_id":"t1_n1xefhq","score":1,"author_fullname":"t2_w2gxqd6i2","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Kinda similar-ish but it definitely has an edge over other models. I'm trying to determine if it's marginal or significant, but it's definitely *there*. The problem is that for my current use cases it almost definitely doesn't justify the loss of speed. Qwen3-32B IQ4-Q5 runs ~4x as fast and Llama 3.3 70B iq3 runs ~3x as fast, and can put up a fight it seems.\\n\\n**edit** -  will definitely be trying out that tensor override tool","edited":1751948082,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1xg2dz","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Kinda similar-ish but it definitely has an edge over other models. I&amp;#39;m trying to determine if it&amp;#39;s marginal or significant, but it&amp;#39;s definitely &lt;em&gt;there&lt;/em&gt;. The problem is that for my current use cases it almost definitely doesn&amp;#39;t justify the loss of speed. Qwen3-32B IQ4-Q5 runs ~4x as fast and Llama 3.3 70B iq3 runs ~3x as fast, and can put up a fight it seems.&lt;/p&gt;\\n\\n&lt;p&gt;&lt;strong&gt;edit&lt;/strong&gt; -  will definitely be trying out that tensor override tool&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lue5xt","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lue5xt/qwen3235bq2_running_locally_on_my_64gb_ddr4_and/n1xg2dz/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751947342,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1xefhq","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"kevin_1994","can_mod_post":false,"created_utc":1751946664,"send_replies":true,"parent_id":"t3_1lue5xt","score":8,"author_fullname":"t2_o015g","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I have a similar setup (60gb vram, 128 gb ddr4 quad channel) and I can run I4QXS about 7 tok/s with the help of my [tensor override tool](https://github.com/k-koehler/gguf-tensor-overrider) and ik_llama.cpp. In your setup, I would recommend at the very least allocating the attention tensors to your gpus, rather than splitting arbitrary layers--youll get a bit speedup\\n\\nHere's some random thoughts on my journey lol\\n\\n- imo this model is barely (?) smarter than Qwen3 32b Q8XL. Imo qwen3 32b feels a bit smarter\\n- prompt processing is the real killer here, anything less than 15 tok/s makes any tool use feel terrible\\n- llama-swap with such a beefy model feels unacceptably slow\\n\\nMy painful conclusion is that this model is not quite worth it without vastly more compute power haha. Curious what your experience has been?","edited":1751947026,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1xefhq","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I have a similar setup (60gb vram, 128 gb ddr4 quad channel) and I can run I4QXS about 7 tok/s with the help of my &lt;a href=\\"https://github.com/k-koehler/gguf-tensor-overrider\\"&gt;tensor override tool&lt;/a&gt; and ik_llama.cpp. In your setup, I would recommend at the very least allocating the attention tensors to your gpus, rather than splitting arbitrary layers--youll get a bit speedup&lt;/p&gt;\\n\\n&lt;p&gt;Here&amp;#39;s some random thoughts on my journey lol&lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;imo this model is barely (?) smarter than Qwen3 32b Q8XL. Imo qwen3 32b feels a bit smarter&lt;/li&gt;\\n&lt;li&gt;prompt processing is the real killer here, anything less than 15 tok/s makes any tool use feel terrible&lt;/li&gt;\\n&lt;li&gt;llama-swap with such a beefy model feels unacceptably slow&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;p&gt;My painful conclusion is that this model is not quite worth it without vastly more compute power haha. Curious what your experience has been?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":true,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lue5xt/qwen3235bq2_running_locally_on_my_64gb_ddr4_and/n1xefhq/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751946664,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lue5xt","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":8}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1xqvtv","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"EmPips","can_mod_post":false,"created_utc":1751952311,"send_replies":true,"parent_id":"t1_n1xo1hp","score":2,"author_fullname":"t2_w2gxqd6i2","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"If I did this again I would get an Rx 7900xt (20GB) or 7900xtx (24GB) for the same price as the two 6800's and spend any additional savings to get on a DDR5 platform.\\n\\nReason being:\\n\\n- dual AMD GPUs seem to have a bigger performance hit than dual Nvidia GPUs\\n\\n- I'd benefit from DDR5 and Gen5 nvme drives way more than I benefit from the extra 8 or 12GB of VRAM\\n\\n- using 32GB of context + model at 512Gb/s is doable, but 800GB-1TB/s would be way nicer\\n\\nI'd stress that my CPU is overkill and for a VM lab mainly. If you build with a 5600x budget, please do yourself the biggest favor and look into a 13400f/14400f on a DDR5 board instead!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1xqvtv","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;If I did this again I would get an Rx 7900xt (20GB) or 7900xtx (24GB) for the same price as the two 6800&amp;#39;s and spend any additional savings to get on a DDR5 platform.&lt;/p&gt;\\n\\n&lt;p&gt;Reason being:&lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;&lt;p&gt;dual AMD GPUs seem to have a bigger performance hit than dual Nvidia GPUs&lt;/p&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;p&gt;I&amp;#39;d benefit from DDR5 and Gen5 nvme drives way more than I benefit from the extra 8 or 12GB of VRAM&lt;/p&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;p&gt;using 32GB of context + model at 512Gb/s is doable, but 800GB-1TB/s would be way nicer&lt;/p&gt;&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;p&gt;I&amp;#39;d stress that my CPU is overkill and for a VM lab mainly. If you build with a 5600x budget, please do yourself the biggest favor and look into a 13400f/14400f on a DDR5 board instead!&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lue5xt","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lue5xt/qwen3235bq2_running_locally_on_my_64gb_ddr4_and/n1xqvtv/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751952311,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n1xo1hp","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"opoot_","can_mod_post":false,"created_utc":1751950931,"send_replies":true,"parent_id":"t3_1lue5xt","score":1,"author_fullname":"t2_rn6co7q5m","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Yooo your build is almost exactly what I want to build myself. Do you have any suggestions regarding your hardware? I might go with a 5600 cpu instead though but the idea is dual rx6800s for decent enough vram amount and amount.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1xo1hp","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yooo your build is almost exactly what I want to build myself. Do you have any suggestions regarding your hardware? I might go with a 5600 cpu instead though but the idea is dual rx6800s for decent enough vram amount and amount.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lue5xt/qwen3235bq2_running_locally_on_my_64gb_ddr4_and/n1xo1hp/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751950931,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lue5xt","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1y6xkp","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ArtfulGenie69","can_mod_post":false,"created_utc":1751961097,"send_replies":true,"parent_id":"t3_1lue5xt","score":1,"author_fullname":"t2_1d6ghm3sq0","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Something in me is thinking that 2bit on a 22b is like really bad, but I haven't tried it. I still think the best model is deepseek 70b r1 distill at around 4bit. I've been using q4_km a lot lately. ","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1y6xkp","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Something in me is thinking that 2bit on a 22b is like really bad, but I haven&amp;#39;t tried it. I still think the best model is deepseek 70b r1 distill at around 4bit. I&amp;#39;ve been using q4_km a lot lately. &lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lue5xt/qwen3235bq2_running_locally_on_my_64gb_ddr4_and/n1y6xkp/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751961097,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lue5xt","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),s=()=>e.jsx(t,{data:l});export{s as default};
