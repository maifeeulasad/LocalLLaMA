import{j as e}from"./index-Bqs-ekb2.js";import{R as l}from"./RedditPostRenderer-DUVdf0-i.js";import"./index-D52ORTDm.js";const a=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"So I wanted to share my experience and hear about yours.\\n\\nHardware : \\n\\nGPU : 3060 12GB\\nCPU : i5-3060\\nRAM : 32GB\\n\\nFront-end : Koboldcpp + open-webui\\n\\nUse cases : General Q&amp;A, Long context RAG, Humanities, Summarization, Translation, code. \\n\\nI've been testing quite a lot of models recently, especially when I finally realized I could run 14B quite comfortably. \\n\\nGEMMA-3N E4B and Qwen3-14B are, for me the best models one can use for these use cases. Even with an aged GPU, they're quite fast, and have a good ability to stick to the prompt. \\n\\nGemma-3 12B seems to perform worse than 3n E4B, which is surprising to me. GLM is spotting nonsense, Deepseek Distills Qwen3 seem to perform may worse than Qwen3. I was not impressed by Phi4 and it's variants. \\n\\nWhat are your experiences? Do you use other models of the same range? \\n\\nGood day everyone! \\n\\n\\n","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Yappp - Yet Another Poor Peasent Post","link_flair_richtext":[{"e":"text","t":"Discussion"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lqlsyb","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.88,"author_flair_background_color":null,"subreddit_type":"public","ups":20,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_cpgzcud","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Discussion","can_mod_post":false,"score":20,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1751537200,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;So I wanted to share my experience and hear about yours.&lt;/p&gt;\\n\\n&lt;p&gt;Hardware : &lt;/p&gt;\\n\\n&lt;p&gt;GPU : 3060 12GB\\nCPU : i5-3060\\nRAM : 32GB&lt;/p&gt;\\n\\n&lt;p&gt;Front-end : Koboldcpp + open-webui&lt;/p&gt;\\n\\n&lt;p&gt;Use cases : General Q&amp;amp;A, Long context RAG, Humanities, Summarization, Translation, code. &lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;ve been testing quite a lot of models recently, especially when I finally realized I could run 14B quite comfortably. &lt;/p&gt;\\n\\n&lt;p&gt;GEMMA-3N E4B and Qwen3-14B are, for me the best models one can use for these use cases. Even with an aged GPU, they&amp;#39;re quite fast, and have a good ability to stick to the prompt. &lt;/p&gt;\\n\\n&lt;p&gt;Gemma-3 12B seems to perform worse than 3n E4B, which is surprising to me. GLM is spotting nonsense, Deepseek Distills Qwen3 seem to perform may worse than Qwen3. I was not impressed by Phi4 and it&amp;#39;s variants. &lt;/p&gt;\\n\\n&lt;p&gt;What are your experiences? Do you use other models of the same range? &lt;/p&gt;\\n\\n&lt;p&gt;Good day everyone! &lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"5f921ea4-c7bc-11ed-9c23-3a00622979b4","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#646d73","id":"1lqlsyb","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"needthosepylons","discussion_type":null,"num_comments":34,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lqlsyb/yappp_yet_another_poor_peasent_post/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lqlsyb/yappp_yet_another_poor_peasent_post/","subreddit_subscribers":494001,"created_utc":1751537200,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1425gx","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"National_Meeting_749","can_mod_post":false,"send_replies":true,"parent_id":"t1_n13rq2z","score":1,"author_fullname":"t2_drm5tg5d","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Are you at 3-4tps with no context? If so then definitely. But when I load it up with context I get down to about 6 tos, about 12 on a fresh slate.","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n1425gx","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Are you at 3-4tps with no context? If so then definitely. But when I load it up with context I get down to about 6 tos, about 12 on a fresh slate.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lqlsyb","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqlsyb/yappp_yet_another_poor_peasent_post/n1425gx/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751544148,"author_flair_text":null,"treatment_tags":[],"created_utc":1751544148,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":6,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n13rq2z","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"needthosepylons","can_mod_post":false,"send_replies":true,"parent_id":"t1_n13rca0","score":2,"author_fullname":"t2_cpgzcud","approved_by":null,"mod_note":null,"all_awardings":[],"body":"Ouch, I suppose something is wrong with my tests then, because with optimal offloading, I'm at 3-4t/s. Hmm, interesting, thanks for letting me know!","edited":1751540048,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n13rq2z","is_submitter":true,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Ouch, I suppose something is wrong with my tests then, because with optimal offloading, I&amp;#39;m at 3-4t/s. Hmm, interesting, thanks for letting me know!&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lqlsyb","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqlsyb/yappp_yet_another_poor_peasent_post/n13rq2z/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751539542,"author_flair_text":null,"treatment_tags":[],"created_utc":1751539542,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n13rca0","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"National_Meeting_749","can_mod_post":false,"send_replies":true,"parent_id":"t1_n13ovd5","score":3,"author_fullname":"t2_drm5tg5d","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I'm running a ryzen 5 5600x with a  7600 8gb and the Qwen 3 30B A3B is my go to","edited":false,"author_flair_css_class":null,"name":"t1_n13rca0","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;m running a ryzen 5 5600x with a  7600 8gb and the Qwen 3 30B A3B is my go to&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lqlsyb","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqlsyb/yappp_yet_another_poor_peasent_post/n13rca0/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751539351,"author_flair_text":null,"collapsed":false,"created_utc":1751539351,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n13rwx4","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"GreenTreeAndBlueSky","can_mod_post":false,"send_replies":true,"parent_id":"t1_n13ovd5","score":2,"author_fullname":"t2_1p50pl73j2","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I have 8gb of vram so you'll need to offload less than me! \\nAlso i always use q4_k_m it seems the sweet spot of vast memory footprint reduction vs loss of quality. That will give you an overall footprint of about 22gb so 12 on vram and 10 on dram. \\nShould be fairly quick!","edited":false,"author_flair_css_class":null,"name":"t1_n13rwx4","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I have 8gb of vram so you&amp;#39;ll need to offload less than me! \\nAlso i always use q4_k_m it seems the sweet spot of vast memory footprint reduction vs loss of quality. That will give you an overall footprint of about 22gb so 12 on vram and 10 on dram. \\nShould be fairly quick!&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lqlsyb","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqlsyb/yappp_yet_another_poor_peasent_post/n13rwx4/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751539635,"author_flair_text":null,"collapsed":false,"created_utc":1751539635,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n13ovd5","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"needthosepylons","can_mod_post":false,"send_replies":true,"parent_id":"t1_n13opm7","score":1,"author_fullname":"t2_cpgzcud","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I tried that, I think, but maybe my CPU is just too weak? This i5-10400F ain't young anymore!\\nAlthough you're making me wonder if.. I'll try again!\\n\\nWhat GPU and quants do you use?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n13ovd5","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I tried that, I think, but maybe my CPU is just too weak? This i5-10400F ain&amp;#39;t young anymore!\\nAlthough you&amp;#39;re making me wonder if.. I&amp;#39;ll try again!&lt;/p&gt;\\n\\n&lt;p&gt;What GPU and quants do you use?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lqlsyb","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqlsyb/yappp_yet_another_poor_peasent_post/n13ovd5/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751538067,"author_flair_text":null,"treatment_tags":[],"created_utc":1751538067,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n13opm7","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"GreenTreeAndBlueSky","can_mod_post":false,"send_replies":true,"parent_id":"t1_n13olc9","score":5,"author_fullname":"t2_1p50pl73j2","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"You can! Offload some or all experts to cpu.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n13opm7","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You can! Offload some or all experts to cpu.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lqlsyb","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqlsyb/yappp_yet_another_poor_peasent_post/n13opm7/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751537983,"author_flair_text":null,"treatment_tags":[],"created_utc":1751537983,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n155j0y","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"tempetemplar","can_mod_post":false,"send_replies":true,"parent_id":"t1_n14n0lz","score":1,"author_fullname":"t2_atvw2aj","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Decrease the insanity by prompting them to simulate multiple agents (say three). Use sequential thinking (MCP). The degree of insanity is less. Not saying it's gone.","edited":false,"author_flair_css_class":null,"name":"t1_n155j0y","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Decrease the insanity by prompting them to simulate multiple agents (say three). Use sequential thinking (MCP). The degree of insanity is less. Not saying it&amp;#39;s gone.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lqlsyb","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqlsyb/yappp_yet_another_poor_peasent_post/n155j0y/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751556736,"author_flair_text":null,"collapsed":false,"created_utc":1751556736,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n14n0lz","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DragonfruitIll660","can_mod_post":false,"send_replies":true,"parent_id":"t1_n149jx9","score":2,"author_fullname":"t2_duscbn82","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Are Iq1\\\\_xxs's coherent? Last time I tried one they were going insane after a few messages.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n14n0lz","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Are Iq1_xxs&amp;#39;s coherent? Last time I tried one they were going insane after a few messages.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lqlsyb","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqlsyb/yappp_yet_another_poor_peasent_post/n14n0lz/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751551376,"author_flair_text":null,"treatment_tags":[],"created_utc":1751551376,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n149jx9","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"tempetemplar","can_mod_post":false,"send_replies":true,"parent_id":"t1_n13olc9","score":1,"author_fullname":"t2_atvw2aj","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Use the iq1_xxs from unsloth","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n149jx9","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Use the iq1_xxs from unsloth&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lqlsyb","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqlsyb/yappp_yet_another_poor_peasent_post/n149jx9/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751546934,"author_flair_text":null,"treatment_tags":[],"created_utc":1751546934,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n13olc9","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"needthosepylons","can_mod_post":false,"created_utc":1751537920,"send_replies":true,"parent_id":"t1_n13o921","score":1,"author_fullname":"t2_cpgzcud","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Oh, yeah. I wish I could run this one!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n13olc9","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Oh, yeah. I wish I could run this one!&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lqlsyb","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqlsyb/yappp_yet_another_poor_peasent_post/n13olc9/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751537920,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n13o921","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"GreenTreeAndBlueSky","can_mod_post":false,"created_utc":1751537739,"send_replies":true,"parent_id":"t3_1lqlsyb","score":13,"author_fullname":"t2_1p50pl73j2","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Quantized qwen3 30b ftw","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n13o921","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Quantized qwen3 30b ftw&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqlsyb/yappp_yet_another_poor_peasent_post/n13o921/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751537739,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lqlsyb","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":13}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n13o50o","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"j0holo","can_mod_post":false,"created_utc":1751537679,"send_replies":true,"parent_id":"t3_1lqlsyb","score":2,"author_fullname":"t2_hodrk","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"My current setup is an Intel Arc B580 with Intel's vllm with intel-ipex support.  \\n  \\nI mostly use it for generating data that looks like real data.  \\nAt the same time I'm also working on a RAG database with Elasticsearch for hybrid search.  \\n  \\nI did run ollama with open-webui but since a month or two I'm never hitting the limits of Claude anymore.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n13o50o","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;My current setup is an Intel Arc B580 with Intel&amp;#39;s vllm with intel-ipex support.  &lt;/p&gt;\\n\\n&lt;p&gt;I mostly use it for generating data that looks like real data.&lt;br/&gt;\\nAt the same time I&amp;#39;m also working on a RAG database with Elasticsearch for hybrid search.  &lt;/p&gt;\\n\\n&lt;p&gt;I did run ollama with open-webui but since a month or two I&amp;#39;m never hitting the limits of Claude anymore.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqlsyb/yappp_yet_another_poor_peasent_post/n13o50o/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751537679,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lqlsyb","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n13xpkl","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"rog-uk","can_mod_post":false,"send_replies":true,"parent_id":"t1_n13w9rx","score":1,"author_fullname":"t2_x4r6o","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Wouldn't it be weird if enough text properly indexed/linked in a rag could generate novel ideas? Like causes and effects that hadn't been explored yet?","edited":false,"author_flair_css_class":null,"name":"t1_n13xpkl","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Wouldn&amp;#39;t it be weird if enough text properly indexed/linked in a rag could generate novel ideas? Like causes and effects that hadn&amp;#39;t been explored yet?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lqlsyb","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqlsyb/yappp_yet_another_poor_peasent_post/n13xpkl/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751542310,"author_flair_text":null,"collapsed":false,"created_utc":1751542310,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n13w9rx","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"needthosepylons","can_mod_post":false,"send_replies":true,"parent_id":"t1_n13t317","score":3,"author_fullname":"t2_cpgzcud","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Yes. And actually, I'm a teacher in humanities, and I use my Llms to generate quizzes but. for me! To make sure I'm not forgetting stuff I'm not working on for a while.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n13w9rx","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yes. And actually, I&amp;#39;m a teacher in humanities, and I use my Llms to generate quizzes but. for me! To make sure I&amp;#39;m not forgetting stuff I&amp;#39;m not working on for a while.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lqlsyb","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqlsyb/yappp_yet_another_poor_peasent_post/n13w9rx/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751541673,"author_flair_text":null,"treatment_tags":[],"created_utc":1751541673,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n13t317","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"rog-uk","can_mod_post":false,"send_replies":true,"parent_id":"t1_n13qdfq","score":1,"author_fullname":"t2_x4r6o","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Although my interests are more technical, I always thought these things could do well on humanities, especially if one had a large corpus of cross referenced material.\\n\\n\\nI suspect even in academic land it's not \\"cheating\\" if you're only using it to pull up chains of references/citations and breifly explain what links them.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n13t317","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Although my interests are more technical, I always thought these things could do well on humanities, especially if one had a large corpus of cross referenced material.&lt;/p&gt;\\n\\n&lt;p&gt;I suspect even in academic land it&amp;#39;s not &amp;quot;cheating&amp;quot; if you&amp;#39;re only using it to pull up chains of references/citations and breifly explain what links them.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lqlsyb","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqlsyb/yappp_yet_another_poor_peasent_post/n13t317/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751540196,"author_flair_text":null,"treatment_tags":[],"created_utc":1751540196,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n13qdfq","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"needthosepylons","can_mod_post":false,"created_utc":1751538854,"send_replies":true,"parent_id":"t1_n13pt5f","score":2,"author_fullname":"t2_cpgzcud","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Quite well actually, I use a small embedding model, Qwen3 or nomic, create a persistent ChromaDB before querying it. It works quite well. When I'm a bit in a hurry or know my RAG database will evolve rapidly, I end up using open-webui knowledge system with those 2 tiny models, and it works well!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n13qdfq","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Quite well actually, I use a small embedding model, Qwen3 or nomic, create a persistent ChromaDB before querying it. It works quite well. When I&amp;#39;m a bit in a hurry or know my RAG database will evolve rapidly, I end up using open-webui knowledge system with those 2 tiny models, and it works well!&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lqlsyb","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqlsyb/yappp_yet_another_poor_peasent_post/n13qdfq/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751538854,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n14oje4","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"rog-uk","can_mod_post":false,"send_replies":true,"parent_id":"t1_n14mxjz","score":1,"author_fullname":"t2_x4r6o","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"What's your use case if you don't mind me asking? I am interested in having a play at a complex system, any it almost wouldn't matter what thr subject was as long as ai can get the material to work with - technical documents come with a few issues, I am warming to the idea of social sciences or humanities as a test.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n14oje4","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;What&amp;#39;s your use case if you don&amp;#39;t mind me asking? I am interested in having a play at a complex system, any it almost wouldn&amp;#39;t matter what thr subject was as long as ai can get the material to work with - technical documents come with a few issues, I am warming to the idea of social sciences or humanities as a test.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lqlsyb","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqlsyb/yappp_yet_another_poor_peasent_post/n14oje4/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751551842,"author_flair_text":null,"treatment_tags":[],"created_utc":1751551842,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n14mxjz","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"godndiogoat","can_mod_post":false,"created_utc":1751551350,"send_replies":true,"parent_id":"t1_n13pt5f","score":2,"author_fullname":"t2_1o8b7or53v","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Everything’s done in-house: I point Qwen3-14B at raw texts, it auto-labels topics, slices with recursive chunking, then spits out page ids so I’ve got built-in citations. Embeddings go into a local Chroma store; nightly job yanks any new docs, merges indexes and runs a quick cross-reference pass to catch duplicate quotes. For bulk summarisation I still bang Deepseek’s off-peak endpoint-it’s stupid cheap, just avoid anything politically spicy or it 403s. I’ve tried Pinecone and Supabase, but APIWrapper.ai keeps the token counts predictable when I need remote capacity. Works well so far.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n14mxjz","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Everything’s done in-house: I point Qwen3-14B at raw texts, it auto-labels topics, slices with recursive chunking, then spits out page ids so I’ve got built-in citations. Embeddings go into a local Chroma store; nightly job yanks any new docs, merges indexes and runs a quick cross-reference pass to catch duplicate quotes. For bulk summarisation I still bang Deepseek’s off-peak endpoint-it’s stupid cheap, just avoid anything politically spicy or it 403s. I’ve tried Pinecone and Supabase, but APIWrapper.ai keeps the token counts predictable when I need remote capacity. Works well so far.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lqlsyb","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqlsyb/yappp_yet_another_poor_peasent_post/n14mxjz/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751551350,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n13pt5f","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"rog-uk","can_mod_post":false,"created_utc":1751538559,"send_replies":true,"parent_id":"t3_1lqlsyb","score":2,"author_fullname":"t2_x4r6o","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Are you using an llm to create/prepare your rag database? Deekseek api was dirt cheap off peak, as long as you don't push stuff the CCP wouldn't like into it. I am assuming it's a humanities based database. Are you doing citation cross referencing?\\n\\n\\nI am just curious about how this is working for you.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n13pt5f","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Are you using an llm to create/prepare your rag database? Deekseek api was dirt cheap off peak, as long as you don&amp;#39;t push stuff the CCP wouldn&amp;#39;t like into it. I am assuming it&amp;#39;s a humanities based database. Are you doing citation cross referencing?&lt;/p&gt;\\n\\n&lt;p&gt;I am just curious about how this is working for you.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqlsyb/yappp_yet_another_poor_peasent_post/n13pt5f/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751538559,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lqlsyb","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n13yrt9","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"a_beautiful_rhind","can_mod_post":false,"created_utc":1751542769,"send_replies":true,"parent_id":"t3_1lqlsyb","score":2,"author_fullname":"t2_h5utwre7","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Don't use the distills. Phi generalizes poorly. You're really in a tough spot model wise, but compared to last year, these smalls have greatly improved.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n13yrt9","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Don&amp;#39;t use the distills. Phi generalizes poorly. You&amp;#39;re really in a tough spot model wise, but compared to last year, these smalls have greatly improved.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqlsyb/yappp_yet_another_poor_peasent_post/n13yrt9/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751542769,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lqlsyb","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n142s1p","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"-Ellary-","can_mod_post":false,"send_replies":true,"parent_id":"t1_n140ovq","score":1,"author_fullname":"t2_s4zzntp","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Nah, I'm good =D","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n142s1p","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Nah, I&amp;#39;m good =D&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lqlsyb","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqlsyb/yappp_yet_another_poor_peasent_post/n142s1p/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751544400,"author_flair_text":null,"treatment_tags":[],"created_utc":1751544400,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n140ovq","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"admajic","can_mod_post":false,"created_utc":1751543560,"send_replies":true,"parent_id":"t1_n13wuf8","score":1,"author_fullname":"t2_60b9farf","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I feel your pain so had to upgrade to a 3090","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n140ovq","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I feel your pain so had to upgrade to a 3090&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lqlsyb","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqlsyb/yappp_yet_another_poor_peasent_post/n140ovq/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751543560,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n13wuf8","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"-Ellary-","can_mod_post":false,"created_utc":1751541927,"send_replies":true,"parent_id":"t3_1lqlsyb","score":1,"author_fullname":"t2_s4zzntp","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I'm using 3060 12GB VRAM + 32GB RAM, I'm running:\\n\\nGemma 3 27b at 4 tps.  \\nGLM4 32b at 3 tps.  \\nMistral 3.2 24b at 8 tps.  \\nQwen 3 30b A3B - CPU only at 32k context 10 tps, Ryzen 5500.\\n\\n\\\\---\\n\\nPhi 4 is great for work and productivity tasks, it just nails stuff that it was created for.  \\nNemoMix-Unleashed-12B a fine model for even general tasks.  \\nGemma-2-Ataraxy-9B nice small model.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n13wuf8","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;m using 3060 12GB VRAM + 32GB RAM, I&amp;#39;m running:&lt;/p&gt;\\n\\n&lt;p&gt;Gemma 3 27b at 4 tps.&lt;br/&gt;\\nGLM4 32b at 3 tps.&lt;br/&gt;\\nMistral 3.2 24b at 8 tps.&lt;br/&gt;\\nQwen 3 30b A3B - CPU only at 32k context 10 tps, Ryzen 5500.&lt;/p&gt;\\n\\n&lt;p&gt;---&lt;/p&gt;\\n\\n&lt;p&gt;Phi 4 is great for work and productivity tasks, it just nails stuff that it was created for.&lt;br/&gt;\\nNemoMix-Unleashed-12B a fine model for even general tasks.&lt;br/&gt;\\nGemma-2-Ataraxy-9B nice small model.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqlsyb/yappp_yet_another_poor_peasent_post/n13wuf8/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751541927,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lqlsyb","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n140qi8","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"needthosepylons","can_mod_post":false,"created_utc":1751543578,"send_replies":true,"parent_id":"t1_n140mhf","score":1,"author_fullname":"t2_cpgzcud","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Very nice, thank you!!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n140qi8","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Very nice, thank you!!&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lqlsyb","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqlsyb/yappp_yet_another_poor_peasent_post/n140qi8/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751543578,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n140mhf","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"admajic","can_mod_post":false,"created_utc":1751543532,"send_replies":true,"parent_id":"t3_1lqlsyb","score":1,"author_fullname":"t2_60b9farf","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Tried llamacpp vs koboldcpp. On my 3090 llamacpp was 30% faster. So they're you go. Tip 1. Lol\\n\\n2. I use lmstudio it uses llamacpp back end so not screwing around with 50 command line settings\\n\\n3. For basic stuff use qwen3 8b 14b  whatever fits in vram.\\n\\n4. For coding go online via api. Use a big boy like gemini or deepseek-r1 v3 because you will get less frustrated by how bad the little models are that your machine can run...","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n140mhf","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Tried llamacpp vs koboldcpp. On my 3090 llamacpp was 30% faster. So they&amp;#39;re you go. Tip 1. Lol&lt;/p&gt;\\n\\n&lt;ol&gt;\\n&lt;li&gt;&lt;p&gt;I use lmstudio it uses llamacpp back end so not screwing around with 50 command line settings&lt;/p&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;p&gt;For basic stuff use qwen3 8b 14b  whatever fits in vram.&lt;/p&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;p&gt;For coding go online via api. Use a big boy like gemini or deepseek-r1 v3 because you will get less frustrated by how bad the little models are that your machine can run...&lt;/p&gt;&lt;/li&gt;\\n&lt;/ol&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqlsyb/yappp_yet_another_poor_peasent_post/n140mhf/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751543532,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lqlsyb","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n149fo5","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"tempetemplar","can_mod_post":false,"created_utc":1751546893,"send_replies":true,"parent_id":"t3_1lqlsyb","score":1,"author_fullname":"t2_atvw2aj","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Try phi 4 reasoning plus 14b","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n149fo5","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Try phi 4 reasoning plus 14b&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqlsyb/yappp_yet_another_poor_peasent_post/n149fo5/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751546893,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lqlsyb","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n14rng7","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AppearanceHeavy6724","can_mod_post":false,"created_utc":1751552774,"send_replies":true,"parent_id":"t3_1lqlsyb","score":1,"author_fullname":"t2_uz37qfx5","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Add $25 p104-100 and open brave new world of 21b+ models.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n14rng7","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Add $25 p104-100 and open brave new world of 21b+ models.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqlsyb/yappp_yet_another_poor_peasent_post/n14rng7/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751552774,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lqlsyb","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n153mzu","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"needthosepylons","can_mod_post":false,"created_utc":1751556207,"send_replies":true,"parent_id":"t1_n151hrd","score":1,"author_fullname":"t2_cpgzcud","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I'm always on the look for models, since my uses cases are quite.. different from math/code above all. And I didn't know this one so ty, I'll give it a try.\\n\\nBut yes, this gemma-3n-E4B vs Gemma-12B is intriguing and I wanted to compare with others' experiences .","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n153mzu","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;m always on the look for models, since my uses cases are quite.. different from math/code above all. And I didn&amp;#39;t know this one so ty, I&amp;#39;ll give it a try.&lt;/p&gt;\\n\\n&lt;p&gt;But yes, this gemma-3n-E4B vs Gemma-12B is intriguing and I wanted to compare with others&amp;#39; experiences .&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lqlsyb","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqlsyb/yappp_yet_another_poor_peasent_post/n153mzu/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751556207,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n151hrd","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"CheatCodesOfLife","can_mod_post":false,"created_utc":1751555604,"send_replies":true,"parent_id":"t3_1lqlsyb","score":1,"author_fullname":"t2_32el727b","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Are you asking for a model suggestion?\\n\\n&gt; General Q&amp;A, Long context RAG, Humanities, Summarization, Translation, code. \\n\\nGive this a try if you haven't already: [bartowski/c4ai-command-r7b-12-2024-GGUF](https://huggingface.co/bartowski/c4ai-command-r7b-12-2024-GGUF)\\n\\nIt's pretty good at most of those ^ for it's size and the Q4_K should fit easily in your 3060. (I wouldn't know about \\"humanities\\" though)\\nCohere's models excel at RAG and follow instructions really well.\\n\\n&gt; Gemma-3 12B seems to perform worse than 3n E4B\\n\\nThat's surprising","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n151hrd","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Are you asking for a model suggestion?&lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;General Q&amp;amp;A, Long context RAG, Humanities, Summarization, Translation, code. &lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;Give this a try if you haven&amp;#39;t already: &lt;a href=\\"https://huggingface.co/bartowski/c4ai-command-r7b-12-2024-GGUF\\"&gt;bartowski/c4ai-command-r7b-12-2024-GGUF&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;It&amp;#39;s pretty good at most of those ^ for it&amp;#39;s size and the Q4_K should fit easily in your 3060. (I wouldn&amp;#39;t know about &amp;quot;humanities&amp;quot; though)\\nCohere&amp;#39;s models excel at RAG and follow instructions really well.&lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;Gemma-3 12B seems to perform worse than 3n E4B&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;That&amp;#39;s surprising&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqlsyb/yappp_yet_another_poor_peasent_post/n151hrd/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751555604,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lqlsyb","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n146y7a","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"mitchins-au","can_mod_post":false,"created_utc":1751545998,"send_replies":true,"parent_id":"t1_n13rerr","score":2,"author_fullname":"t2_4hjtgq5u","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"My experience with Phi has been underwhelming. Maybe I’m using it wrong.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n146y7a","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;My experience with Phi has been underwhelming. Maybe I’m using it wrong.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lqlsyb","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqlsyb/yappp_yet_another_poor_peasent_post/n146y7a/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751545998,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n13zdke","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"wallbergai","can_mod_post":false,"send_replies":true,"parent_id":"t1_n13snl7","score":0,"author_fullname":"t2_1sqjlhrsaf","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Comparing to what, and using it for...? That compute power you have can: Write one book every day, code one app every day, make a game every day, create an entire website every day. Then seriously, create a llm from scratch, finetune under 4b models, label millions of data scraped easily, or just use it for text 2 whatever inference.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n13zdke","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Comparing to what, and using it for...? That compute power you have can: Write one book every day, code one app every day, make a game every day, create an entire website every day. Then seriously, create a llm from scratch, finetune under 4b models, label millions of data scraped easily, or just use it for text 2 whatever inference.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lqlsyb","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqlsyb/yappp_yet_another_poor_peasent_post/n13zdke/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751543023,"author_flair_text":null,"treatment_tags":[],"created_utc":1751543023,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}},"user_reports":[],"saved":false,"id":"n13snl7","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"needthosepylons","can_mod_post":false,"created_utc":1751539991,"send_replies":true,"parent_id":"t1_n13rerr","score":1,"author_fullname":"t2_cpgzcud","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yeah, but 32gb vram is not really peasant-class, is it? :)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n13snl7","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yeah, but 32gb vram is not really peasant-class, is it? :)&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lqlsyb","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqlsyb/yappp_yet_another_poor_peasent_post/n13snl7/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751539991,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n13rerr","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"wallbergai","can_mod_post":false,"created_utc":1751539386,"send_replies":true,"parent_id":"t3_1lqlsyb","score":-1,"author_fullname":"t2_1sqjlhrsaf","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Phi 4 and Mistral can run very well in 32 gb vram. They are high ranked in benchmarks and perform well universally.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n13rerr","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Phi 4 and Mistral can run very well in 32 gb vram. They are high ranked in benchmarks and perform well universally.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqlsyb/yappp_yet_another_poor_peasent_post/n13rerr/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751539386,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lqlsyb","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-1}}],"before":null}}]`),n=()=>e.jsx(l,{data:a});export{n as default};
