import{j as e}from"./index-CWmJdUH_.js";import{R as l}from"./RedditPostRenderer-D2iunoQ9.js";import"./index-BCg9RP6g.js";const t=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Interesting pattern I noticed for non-reasoning models (I am in the process of picking one to fine-tune): there is a Llama at/near the top of the intelligence index for *every* model size class *except* small models! Also interesting: the small model class is the *most* crowded model class by far.\\n\\n*Processing img fgwkkzv116af1...*\\n\\n*Processing img gcfpkrz916af1...*\\n\\n*Processing img 2nxh432b16af1...*\\n\\n*Processing img lmjustob16af1...*","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"A Llama near the top for every size except small","link_flair_richtext":[{"e":"text","t":"Discussion"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":77,"top_awarded_type":null,"hide_score":false,"name":"t3_1lop94b","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.63,"author_flair_background_color":null,"ups":11,"total_awards_received":0,"media_embed":{},"thumbnail_width":140,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_1a48h7vf","secure_media":null,"is_reddit_media_domain":true,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Discussion","can_mod_post":false,"score":11,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"https://b.thumbs.redditmedia.com/XfopAqTl3Lz8V3vkyvriP3r3Uo6UYLrKTg_hHAo74PU.jpg","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"post_hint":"image","content_categories":null,"is_self":false,"subreddit_type":"public","created":1751333577,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"i.redd.it","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Interesting pattern I noticed for non-reasoning models (I am in the process of picking one to fine-tune): there is a Llama at/near the top of the intelligence index for &lt;em&gt;every&lt;/em&gt; model size class &lt;em&gt;except&lt;/em&gt; small models! Also interesting: the small model class is the &lt;em&gt;most&lt;/em&gt; crowded model class by far.&lt;/p&gt;\\n\\n&lt;p&gt;&lt;em&gt;Processing img fgwkkzv116af1...&lt;/em&gt;&lt;/p&gt;\\n\\n&lt;p&gt;&lt;em&gt;Processing img gcfpkrz916af1...&lt;/em&gt;&lt;/p&gt;\\n\\n&lt;p&gt;&lt;em&gt;Processing img 2nxh432b16af1...&lt;/em&gt;&lt;/p&gt;\\n\\n&lt;p&gt;&lt;em&gt;Processing img lmjustob16af1...&lt;/em&gt;&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"url_overridden_by_dest":"https://i.redd.it/o941j62s16af1.png","view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"preview":{"images":[{"source":{"url":"https://preview.redd.it/o941j62s16af1.png?auto=webp&amp;s=9dd864abedf38010a5189f7886083dabeab50a79","width":3408,"height":1880},"resolutions":[{"url":"https://preview.redd.it/o941j62s16af1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=6072914d4ec2ac875329b538e9d177acdb7d6437","width":108,"height":59},{"url":"https://preview.redd.it/o941j62s16af1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=357f3f88eac9c519626d4bc4f7500c0e0034c4fe","width":216,"height":119},{"url":"https://preview.redd.it/o941j62s16af1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=6504da72b17511151208f1594a808a49dbfe7b6c","width":320,"height":176},{"url":"https://preview.redd.it/o941j62s16af1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=90263e1c05cfd35fd17eb13ba1e683470b488b2f","width":640,"height":353},{"url":"https://preview.redd.it/o941j62s16af1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=54572a712b82e0b1f4232f1d1a04560c55e321ff","width":960,"height":529},{"url":"https://preview.redd.it/o941j62s16af1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=118f864c78a9cdeb90cdfada0cb318213c75aec9","width":1080,"height":595}],"variants":{},"id":"KMuXBI0ne3F6EdTKsvKl_Nn7nawm2MTpWtu07mABO7w"}],"enabled":true},"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"5f921ea4-c7bc-11ed-9c23-3a00622979b4","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"mod_note":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"num_reports":null,"removal_reason":null,"link_flair_background_color":"#646d73","id":"1lop94b","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"entsnack","discussion_type":null,"num_comments":6,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lop94b/a_llama_near_the_top_for_every_size_except_small/","stickied":false,"url":"https://i.redd.it/o941j62s16af1.png","subreddit_subscribers":493458,"created_utc":1751333577,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0ox1z1","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"entsnack","can_mod_post":false,"created_utc":1751337223,"send_replies":true,"parent_id":"t1_n0oplld","score":1,"author_fullname":"t2_1a48h7vf","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"That makes a lot of sense.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0ox1z1","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;That makes a lot of sense.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lop94b","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lop94b/a_llama_near_the_top_for_every_size_except_small/n0ox1z1/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751337223,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0oplld","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"DinoAmino","can_mod_post":false,"created_utc":1751334541,"send_replies":true,"parent_id":"t3_1lop94b","score":17,"author_fullname":"t2_j1v7f","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"The other day someone posted SimpleQA benchmarks and the pattern I noticed was that the top performers were dense models.\\n\\nIt makes sense that the small model space is crowded since they require far fewer resources to train a base ... same reason there are so few 70B+ models.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0oplld","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The other day someone posted SimpleQA benchmarks and the pattern I noticed was that the top performers were dense models.&lt;/p&gt;\\n\\n&lt;p&gt;It makes sense that the small model space is crowded since they require far fewer resources to train a base ... same reason there are so few 70B+ models.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lop94b/a_llama_near_the_top_for_every_size_except_small/n0oplld/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751334541,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lop94b","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":17}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0pxafg","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Utoko","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0orslu","score":3,"author_fullname":"t2_6a8ry","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I mean there are prompts which get models to reason a lot, which people used before the good reasoning models came out.  \\nand they did improve logic and math scores a bit.\\n\\nOf course it isn't the same as a model intentionally training for it. Also his prompt seems to be a bad one, you need to give the framework.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0pxafg","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I mean there are prompts which get models to reason a lot, which people used before the good reasoning models came out.&lt;br/&gt;\\nand they did improve logic and math scores a bit.&lt;/p&gt;\\n\\n&lt;p&gt;Of course it isn&amp;#39;t the same as a model intentionally training for it. Also his prompt seems to be a bad one, you need to give the framework.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lop94b","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lop94b/a_llama_near_the_top_for_every_size_except_small/n0pxafg/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751354148,"author_flair_text":null,"treatment_tags":[],"created_utc":1751354148,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n0orslu","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"MDT-49","can_mod_post":false,"created_utc":1751335318,"send_replies":true,"parent_id":"t1_n0oodq4","score":37,"author_fullname":"t2_h8yrica5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yeah, also make sure to add \\"You're a really smart 2000B LLM who doesn't hallucinate\\" to make it a SOTA model that will beat every benchmark.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0orslu","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yeah, also make sure to add &amp;quot;You&amp;#39;re a really smart 2000B LLM who doesn&amp;#39;t hallucinate&amp;quot; to make it a SOTA model that will beat every benchmark.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lop94b","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lop94b/a_llama_near_the_top_for_every_size_except_small/n0orslu/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751335318,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":37}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0owuw4","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"entsnack","can_mod_post":false,"created_utc":1751337150,"send_replies":true,"parent_id":"t1_n0oodq4","score":19,"author_fullname":"t2_1a48h7vf","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"\\\\&gt; it is very easy to make it a reasoning model.\\n\\nYou're oversimplifying. The way reasoning models are made today is with reinforcement learning. Read the DeepSeek r1-Zero paper, very well written.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0owuw4","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;&amp;gt; it is very easy to make it a reasoning model.&lt;/p&gt;\\n\\n&lt;p&gt;You&amp;#39;re oversimplifying. The way reasoning models are made today is with reinforcement learning. Read the DeepSeek r1-Zero paper, very well written.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lop94b","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lop94b/a_llama_near_the_top_for_every_size_except_small/n0owuw4/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751337150,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":19}}],"before":null}},"user_reports":[],"saved":false,"id":"n0oodq4","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":"LOW_SCORE","no_follow":true,"author":"epSos-DE","can_mod_post":false,"created_utc":1751334108,"send_replies":true,"parent_id":"t3_1lop94b","score":-36,"author_fullname":"t2_5e3ax","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":true,"body":"it is very easy to make it a reasoning model.\\n\\n  \\nJust tell it to : \\n\\n&gt;Assume the role of a professional.. Think deep. Use self reasoning, self reflection and self correction for the investigation. Always compare the current state with the proposed solution. select the more beneficial one, to avoid solving problems that do not exist.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0oodq4","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;it is very easy to make it a reasoning model.&lt;/p&gt;\\n\\n&lt;p&gt;Just tell it to : &lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;Assume the role of a professional.. Think deep. Use self reasoning, self reflection and self correction for the investigation. Always compare the current state with the proposed solution. select the more beneficial one, to avoid solving problems that do not exist.&lt;/p&gt;\\n&lt;/blockquote&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":"comment score below threshold","distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lop94b/a_llama_near_the_top_for_every_size_except_small/n0oodq4/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751334108,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lop94b","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-36}}],"before":null}}]`),s=()=>e.jsx(l,{data:t});export{s as default};
