import{j as e}from"./index-BOnf-UhU.js";import{R as l}from"./RedditPostRenderer-Ce39qICS.js";import"./index-CDase6VD.js";const t=JSON.parse('[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Heelo, im making a project where llm might have to deal with geospatial data, raster like. Dealing with formalts like Map Tiles, geojason etc. (Algo RAG implementations) for this i need an LLM but an so confused which one to use. Llama and Mistral both have so many models that im confused.   \\nIt must be free to use via api or downloadable locally through ollama (light enough to run well on a gaming laptop).\\n\\nIf someone has exp with using LLMs for similar tasks i need ur help ðŸ˜¬  \\n  \\nThis LLM will be the frontface for the user. There wl be other chains to perform operations on the data.","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Help choosing LLM","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lsvff1","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.67,"author_flair_background_color":null,"subreddit_type":"public","ups":2,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_8rh26fjt","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":2,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1751786941,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Heelo, im making a project where llm might have to deal with geospatial data, raster like. Dealing with formalts like Map Tiles, geojason etc. (Algo RAG implementations) for this i need an LLM but an so confused which one to use. Llama and Mistral both have so many models that im confused.&lt;br/&gt;\\nIt must be free to use via api or downloadable locally through ollama (light enough to run well on a gaming laptop).&lt;/p&gt;\\n\\n&lt;p&gt;If someone has exp with using LLMs for similar tasks i need ur help ðŸ˜¬  &lt;/p&gt;\\n\\n&lt;p&gt;This LLM will be the frontface for the user. There wl be other chains to perform operations on the data.&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1lsvff1","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"BESTHARSH004","discussion_type":null,"num_comments":9,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lsvff1/help_choosing_llm/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lsvff1/help_choosing_llm/","subreddit_subscribers":495651,"created_utc":1751786941,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1lyqgb","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"BESTHARSH004","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1lufrk","score":1,"author_fullname":"t2_8rh26fjt","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Noted, thankyou","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1lyqgb","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Noted, thankyou&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lsvff1","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lsvff1/help_choosing_llm/n1lyqgb/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751794993,"author_flair_text":null,"treatment_tags":[],"created_utc":1751794993,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1lufrk","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"AppearanceHeavy6724","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1ltxcx","score":4,"author_fullname":"t2_uz37qfx5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"That guy stuck in 2023. What he offered are ultra ancient llms. These day no one use Mistral 7b.Use something more modern like Ministral 8b, Llama 3.1 8b, qwen 3 8b etc.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n1lufrk","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;That guy stuck in 2023. What he offered are ultra ancient llms. These day no one use Mistral 7b.Use something more modern like Ministral 8b, Llama 3.1 8b, qwen 3 8b etc.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lsvff1","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lsvff1/help_choosing_llm/n1lufrk/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751792333,"author_flair_text":null,"treatment_tags":[],"created_utc":1751792333,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}}],"before":null}},"user_reports":[],"saved":false,"id":"n1ltxcx","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"BESTHARSH004","can_mod_post":false,"created_utc":1751792020,"send_replies":true,"parent_id":"t1_n1lmyfr","score":1,"author_fullname":"t2_8rh26fjt","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Thankyou so much man ðŸ‘ðŸ»","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1ltxcx","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thankyou so much man ðŸ‘ðŸ»&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lsvff1","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lsvff1/help_choosing_llm/n1ltxcx/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751792020,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1nwt9s","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"godndiogoat","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1mucme","score":1,"author_fullname":"t2_1o8b7or53v","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"7Bâ€™s still my go-to for local ragwork; newer 20â€“30B sets look flashy but throttle the GPU right when GDAL spins up. Trim the vram hit by running the 4-bit GGUF with q5KM, bump context to 16k with rope-scale, and it keeps up fine. Where it stumbles I just chain to a remote Sonnet call. The mix keeps latency under a second while the raster ops churn. 7B still holds its own.","edited":false,"author_flair_css_class":null,"name":"t1_n1nwt9s","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;7Bâ€™s still my go-to for local ragwork; newer 20â€“30B sets look flashy but throttle the GPU right when GDAL spins up. Trim the vram hit by running the 4-bit GGUF with q5KM, bump context to 16k with rope-scale, and it keeps up fine. Where it stumbles I just chain to a remote Sonnet call. The mix keeps latency under a second while the raster ops churn. 7B still holds its own.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lsvff1","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lsvff1/help_choosing_llm/n1nwt9s/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751821774,"author_flair_text":null,"collapsed":false,"created_utc":1751821774,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1mucme","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Corana","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1mggxr","score":1,"author_fullname":"t2_4k74w","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I think they are having a go at Mistral 7b being used... A lot of people feel its been beaten out by newer models so it no longer has value.\\n\\nThat being said, I actually enjoy its output and working with it more than the newer models, and yes I love its 7b variant most of all.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1mucme","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I think they are having a go at Mistral 7b being used... A lot of people feel its been beaten out by newer models so it no longer has value.&lt;/p&gt;\\n\\n&lt;p&gt;That being said, I actually enjoy its output and working with it more than the newer models, and yes I love its 7b variant most of all.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lsvff1","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lsvff1/help_choosing_llm/n1mucme/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751809809,"author_flair_text":null,"treatment_tags":[],"created_utc":1751809809,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1mggxr","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"godndiogoat","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1lyu5w","score":1,"author_fullname":"t2_1o8b7or53v","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"7Bâ€™s still the sweet spot for local geo pipelines-quality jumped with instruct tweaks, gpu load stays chill. I push prompts via LangChain, cache tile refs in Supabase, and DreamFactory maps SQL to REST so the model can fetch meta instantly. 7B keeps things fast and free.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n1mggxr","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;7Bâ€™s still the sweet spot for local geo pipelines-quality jumped with instruct tweaks, gpu load stays chill. I push prompts via LangChain, cache tile refs in Supabase, and DreamFactory maps SQL to REST so the model can fetch meta instantly. 7B keeps things fast and free.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lsvff1","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lsvff1/help_choosing_llm/n1mggxr/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751804356,"author_flair_text":null,"treatment_tags":[],"created_utc":1751804356,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1lyu5w","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Decaf_GT","can_mod_post":false,"created_utc":1751795055,"send_replies":true,"parent_id":"t1_n1lmyfr","score":2,"author_fullname":"t2_r98e1eogc","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"...Mistral....*7B*?? in 2025?\\n\\nhttps://i.imgur.com/gzq7OLL.jpeg","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1lyu5w","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;...Mistral....&lt;em&gt;7B&lt;/em&gt;?? in 2025?&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://i.imgur.com/gzq7OLL.jpeg\\"&gt;https://i.imgur.com/gzq7OLL.jpeg&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lsvff1","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lsvff1/help_choosing_llm/n1lyu5w/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751795055,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n1lmyfr","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"godndiogoat","can_mod_post":false,"created_utc":1751787861,"send_replies":true,"parent_id":"t3_1lsvff1","score":2,"author_fullname":"t2_1o8b7or53v","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Go with Mistral 7B instruct or Llama-3 8B; theyâ€™re light, open weights, and slot straight into Ollama without hammering your GPU. For raster/geojson RAG, embed the text side (layer names, bounding boxes, tags) with an all-MiniLM model, store in pgvector, and keep the heavy pixel math in GDAL or rasterio; the LLM just resolves user intent and spits out function calls. Chunk tiles by z/x/y so the vector search stays fast, then stream the actual files from disk or S3 once the LLM picks the IDs. Iâ€™ve bounced between pgvector and Qdrant for the store, but APIWrapper.ai ended up smoother to glue the LLM, the DB, and my geoprocessing lambdas. Fine-tune isnâ€™t worth it until youâ€™ve logged a few hundred edge cases-better to iterate on your tool calls first. Stick with those two models until you really need more juice.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1lmyfr","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Go with Mistral 7B instruct or Llama-3 8B; theyâ€™re light, open weights, and slot straight into Ollama without hammering your GPU. For raster/geojson RAG, embed the text side (layer names, bounding boxes, tags) with an all-MiniLM model, store in pgvector, and keep the heavy pixel math in GDAL or rasterio; the LLM just resolves user intent and spits out function calls. Chunk tiles by z/x/y so the vector search stays fast, then stream the actual files from disk or S3 once the LLM picks the IDs. Iâ€™ve bounced between pgvector and Qdrant for the store, but APIWrapper.ai ended up smoother to glue the LLM, the DB, and my geoprocessing lambdas. Fine-tune isnâ€™t worth it until youâ€™ve logged a few hundred edge cases-better to iterate on your tool calls first. Stick with those two models until you really need more juice.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lsvff1/help_choosing_llm/n1lmyfr/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751787861,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lsvff1","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1mnz30","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"BidWestern1056","can_mod_post":false,"created_utc":1751807437,"send_replies":true,"parent_id":"t3_1lsvff1","score":2,"author_fullname":"t2_uzxql7po","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"use gemma3 (whatever biggest one your comp can support) with npcpyÂ \\nhttps://github.com/NPC-Worldwide/npcpy\\nand set up tools for your geospatial stuff that the LLM can use.\\ni havent done this myself w LLMs but i used to use OSGEO/gdal a lot in python and am an astronomer by training so am familiarÂ  w your needs. would be happy to help directly on this if youd like to get together sometime virtually.\\nhmu https://enpisi.com","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1mnz30","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;use gemma3 (whatever biggest one your comp can support) with npcpyÂ \\n&lt;a href=\\"https://github.com/NPC-Worldwide/npcpy\\"&gt;https://github.com/NPC-Worldwide/npcpy&lt;/a&gt;\\nand set up tools for your geospatial stuff that the LLM can use.\\ni havent done this myself w LLMs but i used to use OSGEO/gdal a lot in python and am an astronomer by training so am familiarÂ  w your needs. would be happy to help directly on this if youd like to get together sometime virtually.\\nhmu &lt;a href=\\"https://enpisi.com\\"&gt;https://enpisi.com&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lsvff1/help_choosing_llm/n1mnz30/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751807437,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lsvff1","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}}]'),s=()=>e.jsx(l,{data:t});export{s as default};
