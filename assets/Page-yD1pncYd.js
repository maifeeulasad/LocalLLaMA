import{j as e}from"./index-CeRg6Q3f.js";import{R as l}from"./RedditPostRenderer-D7n1g-D8.js";import"./index-DPToWe3n.js";const t=JSON.parse('[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":" \\nHey folks,\\nI’m exploring some ideas around running small LLMs entirely **in the browser**, and wanted to ask for suggestions or experiences with lightweight inference frameworks.\\n\\nThe main use case I’m playing with is:\\n\\n1. **(Priority)** Taking a JSON object and generating a valid [Chart.js](https://www.chartjs.org/) config to visualize it.\\n2. **(Secondary)** Producing a natural language explanation of the data — like a brief summary or insight.\\n\\nI\'d like the whole thing to run locally in the browser — no backend — so I\'m looking for tools or runtimes that support:\\n\\n* Small quantized models (ideally &lt;100MB)\\n* WebGPU or WASM support\\n* Quick startup and decent performance for structured JSON reasoning\\n\\nI’ve started looking into [MLC.ai](https://mlc.ai), which seems promising, but curious if anyone here has:\\n\\n* Tried MLC.ai recently for browser-based LLM tasks?\\n* Found any newer/easier runtimes that support small models?\\n* Used models that are particularly good at structured JSON-to-JSON transformations?\\n* Prompting tips for clean Chart.js output?\\n\\nExample:\\n\\n```json\\n{ \\"sales\\": [100, 200, 300], \\"months\\": [\\"Jan\\", \\"Feb\\", \\"Mar\\"] }\\n```\\n\\nExpected output: A full Chart.js config for a bar or line chart.\\nBonus: An optional summary like *“Sales increased steadily from January to March.”*\\n\\nWould love to hear what folks have tried or recommend for running small models client-side. Thanks!\\n\\nEdit: Anything under 500mb is good\\nEdit 2: Since this is a side project / experiment. I am looking for OSS projects with permissive license","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"What are some good in-browser inference tools for small LLMs? (Use case: JSON to Chart.js config)","link_flair_richtext":[{"e":"text","t":"Discussion"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lswkv4","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.67,"author_flair_background_color":null,"subreddit_type":"public","ups":4,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_intoh3lv","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Discussion","can_mod_post":false,"score":4,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":1751796936,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"post_hint":"self","content_categories":null,"is_self":true,"mod_note":null,"created":1751791715,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Hey folks,\\nI’m exploring some ideas around running small LLMs entirely &lt;strong&gt;in the browser&lt;/strong&gt;, and wanted to ask for suggestions or experiences with lightweight inference frameworks.&lt;/p&gt;\\n\\n&lt;p&gt;The main use case I’m playing with is:&lt;/p&gt;\\n\\n&lt;ol&gt;\\n&lt;li&gt;&lt;strong&gt;(Priority)&lt;/strong&gt; Taking a JSON object and generating a valid &lt;a href=\\"https://www.chartjs.org/\\"&gt;Chart.js&lt;/a&gt; config to visualize it.&lt;/li&gt;\\n&lt;li&gt;&lt;strong&gt;(Secondary)&lt;/strong&gt; Producing a natural language explanation of the data — like a brief summary or insight.&lt;/li&gt;\\n&lt;/ol&gt;\\n\\n&lt;p&gt;I&amp;#39;d like the whole thing to run locally in the browser — no backend — so I&amp;#39;m looking for tools or runtimes that support:&lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;Small quantized models (ideally &amp;lt;100MB)&lt;/li&gt;\\n&lt;li&gt;WebGPU or WASM support&lt;/li&gt;\\n&lt;li&gt;Quick startup and decent performance for structured JSON reasoning&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;p&gt;I’ve started looking into &lt;a href=\\"https://mlc.ai\\"&gt;MLC.ai&lt;/a&gt;, which seems promising, but curious if anyone here has:&lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;Tried MLC.ai recently for browser-based LLM tasks?&lt;/li&gt;\\n&lt;li&gt;Found any newer/easier runtimes that support small models?&lt;/li&gt;\\n&lt;li&gt;Used models that are particularly good at structured JSON-to-JSON transformations?&lt;/li&gt;\\n&lt;li&gt;Prompting tips for clean Chart.js output?&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;p&gt;Example:&lt;/p&gt;\\n\\n&lt;p&gt;&lt;code&gt;json\\n{ &amp;quot;sales&amp;quot;: [100, 200, 300], &amp;quot;months&amp;quot;: [&amp;quot;Jan&amp;quot;, &amp;quot;Feb&amp;quot;, &amp;quot;Mar&amp;quot;] }\\n&lt;/code&gt;&lt;/p&gt;\\n\\n&lt;p&gt;Expected output: A full Chart.js config for a bar or line chart.\\nBonus: An optional summary like &lt;em&gt;“Sales increased steadily from January to March.”&lt;/em&gt;&lt;/p&gt;\\n\\n&lt;p&gt;Would love to hear what folks have tried or recommend for running small models client-side. Thanks!&lt;/p&gt;\\n\\n&lt;p&gt;Edit: Anything under 500mb is good\\nEdit 2: Since this is a side project / experiment. I am looking for OSS projects with permissive license&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"preview":{"images":[{"source":{"url":"https://external-preview.redd.it/-uEak470UG8Alrel1Gf3FXYft0yuiaCfUYkcL8CkATo.png?auto=webp&amp;s=8c02eb66ee920483b21a1d912634b784d11ec657","width":1200,"height":630},"resolutions":[{"url":"https://external-preview.redd.it/-uEak470UG8Alrel1Gf3FXYft0yuiaCfUYkcL8CkATo.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=627603f6e59ec463c19125b1400307e37eed7cdf","width":108,"height":56},{"url":"https://external-preview.redd.it/-uEak470UG8Alrel1Gf3FXYft0yuiaCfUYkcL8CkATo.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=b479069501d30190bf3e493343f6badc2cb222c6","width":216,"height":113},{"url":"https://external-preview.redd.it/-uEak470UG8Alrel1Gf3FXYft0yuiaCfUYkcL8CkATo.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=9cb38886c07c94f4dc05d9d38acc774bcf279a0a","width":320,"height":168},{"url":"https://external-preview.redd.it/-uEak470UG8Alrel1Gf3FXYft0yuiaCfUYkcL8CkATo.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=61d12e3174210bd97e5353e7f232f185229e2940","width":640,"height":336},{"url":"https://external-preview.redd.it/-uEak470UG8Alrel1Gf3FXYft0yuiaCfUYkcL8CkATo.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=749c64789dceb7126eac7be9c3d87e56b00f8ff6","width":960,"height":504},{"url":"https://external-preview.redd.it/-uEak470UG8Alrel1Gf3FXYft0yuiaCfUYkcL8CkATo.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b819f933e3ee73d29cc1af835175a8ee8d692c7b","width":1080,"height":567}],"variants":{},"id":"-uEak470UG8Alrel1Gf3FXYft0yuiaCfUYkcL8CkATo"}],"enabled":false},"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"5f921ea4-c7bc-11ed-9c23-3a00622979b4","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#646d73","id":"1lswkv4","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"callmedevilthebad","discussion_type":null,"num_comments":6,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lswkv4/what_are_some_good_inbrowser_inference_tools_for/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lswkv4/what_are_some_good_inbrowser_inference_tools_for/","subreddit_subscribers":495651,"created_utc":1751791715,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"d2642412-d9ce-11ed-ae30-32b11309f5bd","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1lvswv","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Everlier","can_mod_post":false,"created_utc":1751793173,"send_replies":true,"parent_id":"t3_1lswkv4","score":3,"author_fullname":"t2_o7p5m","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Not many options, you know them all already. Wllama, transformers.js, Web LLM","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1lvswv","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"Alpaca"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Not many options, you know them all already. Wllama, transformers.js, Web LLM&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lswkv4/what_are_some_good_inbrowser_inference_tools_for/n1lvswv/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751793173,"author_flair_text":"Alpaca","treatment_tags":[],"link_id":"t3_1lswkv4","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bd9e9e","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1lzqwo","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"callmedevilthebad","can_mod_post":false,"created_utc":1751795600,"send_replies":true,"parent_id":"t1_n1luh42","score":1,"author_fullname":"t2_intoh3lv","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I think under 500MB should also be fine.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1lzqwo","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I think under 500MB should also be fine.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lswkv4","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lswkv4/what_are_some_good_inbrowser_inference_tools_for/n1lzqwo/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751795600,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1luh42","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Afraid-Act424","can_mod_post":false,"created_utc":1751792357,"send_replies":true,"parent_id":"t3_1lswkv4","score":1,"author_fullname":"t2_1bqf8k3r59","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I doubt you can find a capable SLM under 100MB. You can explore other types of models on Hugging Face, many of them are compatible with Transformers.js.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1luh42","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I doubt you can find a capable SLM under 100MB. You can explore other types of models on Hugging Face, many of them are compatible with Transformers.js.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lswkv4/what_are_some_good_inbrowser_inference_tools_for/n1luh42/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751792357,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lswkv4","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1lwcyp","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"help_all","can_mod_post":false,"created_utc":1751793517,"send_replies":true,"parent_id":"t3_1lswkv4","score":1,"author_fullname":"t2_64rf6xn7","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"WebAssembly binding for llama.cpp - Enabling on-browser LLM inference. I have not tried it though\\n\\n\\n\\n[https://github.com/ngxson/wllama](https://github.com/ngxson/wllama)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1lwcyp","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;WebAssembly binding for llama.cpp - Enabling on-browser LLM inference. I have not tried it though&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://github.com/ngxson/wllama\\"&gt;https://github.com/ngxson/wllama&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lswkv4/what_are_some_good_inbrowser_inference_tools_for/n1lwcyp/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751793517,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lswkv4","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1m1qih","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"BidWestern1056","can_mod_post":false,"created_utc":1751796778,"send_replies":true,"parent_id":"t3_1lswkv4","score":1,"author_fullname":"t2_uzxql7po","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"maybe [horsy.ai](http://horsy.ai) ?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1m1qih","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;maybe &lt;a href=\\"http://horsy.ai\\"&gt;horsy.ai&lt;/a&gt; ?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lswkv4/what_are_some_good_inbrowser_inference_tools_for/n1m1qih/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751796778,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lswkv4","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1ntpua","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"synw_","can_mod_post":false,"created_utc":1751820835,"send_replies":true,"parent_id":"t3_1lswkv4","score":2,"author_fullname":"t2_ecqgod","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Qwen 0.6b with a few example shots might be able to do it, using something like Wllama","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1ntpua","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Qwen 0.6b with a few example shots might be able to do it, using something like Wllama&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lswkv4/what_are_some_good_inbrowser_inference_tools_for/n1ntpua/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751820835,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lswkv4","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}}]'),n=()=>e.jsx(l,{data:t});export{n as default};
