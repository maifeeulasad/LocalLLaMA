import{j as e}from"./index-Bu7qcPAU.js";import{R as t}from"./RedditPostRenderer-CbHA7O5q.js";import"./index-BKgbfxhf.js";const a=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Hey everyone,\\nIâ€™ve been working on fine-tuning open-source LLMs like Phi-3 and LLaMA 3 using Unsloth in Google Colab, targeting a chatbot for customer support (around 500 prompt-response examples).\\n\\nIâ€™m facing the same recurring issues no matter what I do:\\n\\nâ¸»\\n\\nâ— The problems:\\n\\t1.\\tThe model often responds with the exact same prompt I gave it, instead of the intended response.\\n\\t2.\\tSometimes it returns blank output.\\n\\t3.\\tWhen it does respond, it gives very generic or off-topic answers, not the specific ones from my training data.\\n\\nâ¸»\\n\\nðŸ› ï¸ My Setup:\\n\\tâ€¢\\tUsing Unsloth + FastLanguageModel\\n\\tâ€¢\\tTrained on a .json or .jsonl dataset with format:\\n\\n{\\n  \\"prompt\\": \\"How long does it take to get a refund?\\",\\n  \\"response\\": \\"Refunds typically take 5â€“7 business days.\\"\\n}\\n\\n\\nWrapped in training with:\\n\\nf\\"### Input: {prompt}\\\\n### Output: {response}&lt;|endoftext|&gt;\\"\\n\\n\\nInference via:\\n\\nmessages = [{\\"role\\": \\"user\\", \\"content\\": \\"How long does it take to get a refund?\\"}]\\ntokenizer.apply_chat_template(...)\\n\\n\\nWhat Iâ€™ve tried:\\n\\tâ€¢\\tTraining with both 3 and 10 epochs\\n\\tâ€¢\\tTraining both Phi-3-mini and LLaMA 3 8B with LoRA (4-bit)\\n\\tâ€¢\\tTesting with correct Modelfile templates in Ollama like:\\n\\n\\nTEMPLATE \\"\\"\\"### Input: {{ .Prompt }}\\\\n### Output:\\"\\"\\"\\n\\nWhy is the model not learning my input-output structure properly?\\n\\tâ€¢\\tIs there a better way to format the prompts or structure the dataset?\\n\\tâ€¢\\tCould the model size (like Phi-3) be a bottleneck?\\n\\tâ€¢\\tShould I be adding system prompts or few-shot examples at inference?\\n\\nAny advice, shared experiences, or working examples would help a lot.\\nThanks in advance!\\n","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"ðŸ†˜ [Help] My Fine-Tuned Model Keeps Echoing Prompts or Giving Blank/Generic Responses","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1m4j0sa","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.4,"author_flair_background_color":null,"subreddit_type":"public","ups":0,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_1hyyc3o6lb","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":0,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1752996921,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Hey everyone,\\nIâ€™ve been working on fine-tuning open-source LLMs like Phi-3 and LLaMA 3 using Unsloth in Google Colab, targeting a chatbot for customer support (around 500 prompt-response examples).&lt;/p&gt;\\n\\n&lt;p&gt;Iâ€™m facing the same recurring issues no matter what I do:&lt;/p&gt;\\n\\n&lt;p&gt;â¸»&lt;/p&gt;\\n\\n&lt;p&gt;â— The problems:\\n    1.  The model often responds with the exact same prompt I gave it, instead of the intended response.\\n    2.  Sometimes it returns blank output.\\n    3.  When it does respond, it gives very generic or off-topic answers, not the specific ones from my training data.&lt;/p&gt;\\n\\n&lt;p&gt;â¸»&lt;/p&gt;\\n\\n&lt;p&gt;ðŸ› ï¸ My Setup:\\n    â€¢ Using Unsloth + FastLanguageModel\\n    â€¢ Trained on a .json or .jsonl dataset with format:&lt;/p&gt;\\n\\n&lt;p&gt;{\\n  &amp;quot;prompt&amp;quot;: &amp;quot;How long does it take to get a refund?&amp;quot;,\\n  &amp;quot;response&amp;quot;: &amp;quot;Refunds typically take 5â€“7 business days.&amp;quot;\\n}&lt;/p&gt;\\n\\n&lt;p&gt;Wrapped in training with:&lt;/p&gt;\\n\\n&lt;p&gt;f&amp;quot;### Input: {prompt}\\\\n### Output: {response}&amp;lt;|endoftext|&amp;gt;&amp;quot;&lt;/p&gt;\\n\\n&lt;p&gt;Inference via:&lt;/p&gt;\\n\\n&lt;p&gt;messages = [{&amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;, &amp;quot;content&amp;quot;: &amp;quot;How long does it take to get a refund?&amp;quot;}]\\ntokenizer.apply_chat_template(...)&lt;/p&gt;\\n\\n&lt;p&gt;What Iâ€™ve tried:\\n    â€¢ Training with both 3 and 10 epochs\\n    â€¢ Training both Phi-3-mini and LLaMA 3 8B with LoRA (4-bit)\\n    â€¢ Testing with correct Modelfile templates in Ollama like:&lt;/p&gt;\\n\\n&lt;p&gt;TEMPLATE &amp;quot;&amp;quot;&amp;quot;### Input: {{ .Prompt }}\\\\n### Output:&amp;quot;&amp;quot;&amp;quot;&lt;/p&gt;\\n\\n&lt;p&gt;Why is the model not learning my input-output structure properly?\\n    â€¢ Is there a better way to format the prompts or structure the dataset?\\n    â€¢ Could the model size (like Phi-3) be a bottleneck?\\n    â€¢ Should I be adding system prompts or few-shot examples at inference?&lt;/p&gt;\\n\\n&lt;p&gt;Any advice, shared experiences, or working examples would help a lot.\\nThanks in advance!&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":true,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1m4j0sa","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"Srmxz","discussion_type":null,"num_comments":10,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1m4j0sa/help_my_finetuned_model_keeps_echoing_prompts_or/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1m4j0sa/help_my_finetuned_model_keeps_echoing_prompts_or/","subreddit_subscribers":502274,"created_utc":1752996921,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n44tsey","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"QFGTrialByFire","can_mod_post":false,"created_utc":1752999376,"send_replies":true,"parent_id":"t3_1m4j0sa","score":1,"author_fullname":"t2_1h4o7f23eh","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Hi we'd probably need a bit more detail when you say you've trained the model:\\n\\n1. Did you start with a base model or one already trained on instruction sets? base models will need some partial instruction training before with something like alpaca to understand q/a type prompts\\n\\n2. Your training - how many samples/learning rate etc?\\n\\nAssuming you trained on an already pretrained model and did something like say 1000 samples of your own training it should start responding as per your training. How long is your max tokens - I've noticed that if i put max tokens to much larger than the input token it starts looping and repeating so i usually scale max tokens for each input size to be a scaler of the input. The repetition penalty and temperature also needed fine tuning to work with specific datasets so you might have to fiddle with those as well.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n44tsey","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Hi we&amp;#39;d probably need a bit more detail when you say you&amp;#39;ve trained the model:&lt;/p&gt;\\n\\n&lt;ol&gt;\\n&lt;li&gt;&lt;p&gt;Did you start with a base model or one already trained on instruction sets? base models will need some partial instruction training before with something like alpaca to understand q/a type prompts&lt;/p&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;p&gt;Your training - how many samples/learning rate etc?&lt;/p&gt;&lt;/li&gt;\\n&lt;/ol&gt;\\n\\n&lt;p&gt;Assuming you trained on an already pretrained model and did something like say 1000 samples of your own training it should start responding as per your training. How long is your max tokens - I&amp;#39;ve noticed that if i put max tokens to much larger than the input token it starts looping and repeating so i usually scale max tokens for each input size to be a scaler of the input. The repetition penalty and temperature also needed fine tuning to work with specific datasets so you might have to fiddle with those as well.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4j0sa/help_my_finetuned_model_keeps_echoing_prompts_or/n44tsey/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752999376,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m4j0sa","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4bur5m","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"rnosov","can_mod_post":false,"created_utc":1753100487,"send_replies":true,"parent_id":"t1_n4btb0v","score":1,"author_fullname":"t2_18x6fa","approved_by":null,"mod_note":null,"all_awardings":[],"body":"you can try QLoRA - might fit 3B model. You can also target only say down\\\\_proj mlp layers where factual knowledge is thought to reside","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n4bur5m","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;you can try QLoRA - might fit 3B model. You can also target only say down_proj mlp layers where factual knowledge is thought to reside&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4j0sa/help_my_finetuned_model_keeps_echoing_prompts_or/n4bur5m/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753100487,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m4j0sa","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":8,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4btb0v","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Srmxz","can_mod_post":false,"created_utc":1753099930,"send_replies":true,"parent_id":"t1_n4bq1xs","score":1,"author_fullname":"t2_1hyyc3o6lb","approved_by":null,"mod_note":null,"all_awardings":[],"body":"What about Phi-3 model or Llama 3 3B with that spec","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n4btb0v","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;What about Phi-3 model or Llama 3 3B with that spec&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1m4j0sa","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4j0sa/help_my_finetuned_model_keeps_echoing_prompts_or/n4btb0v/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753099930,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":7,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4bq1xs","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"rnosov","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4boufn","score":1,"author_fullname":"t2_18x6fa","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"6GB VRAM is too tight for 8b model. You can try finetuning 0.6B Qwen. For simple customer support style queries it should work just fine. Local is generally better as you can leave it training for several days if you need too. Free Colab can randomly kick you out.","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n4bq1xs","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;6GB VRAM is too tight for 8b model. You can try finetuning 0.6B Qwen. For simple customer support style queries it should work just fine. Local is generally better as you can leave it training for several days if you need too. Free Colab can randomly kick you out.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1m4j0sa","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4j0sa/help_my_finetuned_model_keeps_echoing_prompts_or/n4bq1xs/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753098627,"author_flair_text":null,"treatment_tags":[],"created_utc":1753098627,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":6,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4boufn","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Srmxz","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4alyqg","score":1,"author_fullname":"t2_1hyyc3o6lb","approved_by":null,"mod_note":null,"all_awardings":[],"body":"What about fine tuning in ma local machine !? \\nWith 16GB ram and RTX 3050 6GB \\nWould that be okay with llama 3 8B model!??\\nWill i face the same issue?","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n4boufn","is_submitter":true,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;What about fine tuning in ma local machine !? \\nWith 16GB ram and RTX 3050 6GB \\nWould that be okay with llama 3 8B model!??\\nWill i face the same issue?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1m4j0sa","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4j0sa/help_my_finetuned_model_keeps_echoing_prompts_or/n4boufn/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753098121,"author_flair_text":null,"treatment_tags":[],"created_utc":1753098121,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4alyqg","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"rnosov","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4aiyrs","score":1,"author_fullname":"t2_18x6fa","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Free tier Colab is not the best platform out there but if your compute budget is 0 I guess you don't have much choice, do you?","edited":false,"author_flair_css_class":null,"name":"t1_n4alyqg","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Free tier Colab is not the best platform out there but if your compute budget is 0 I guess you don&amp;#39;t have much choice, do you?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1m4j0sa","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4j0sa/help_my_finetuned_model_keeps_echoing_prompts_or/n4alyqg/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753077118,"author_flair_text":null,"collapsed":false,"created_utc":1753077118,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4aiyrs","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Srmxz","can_mod_post":false,"send_replies":true,"parent_id":"t1_n46u72i","score":1,"author_fullname":"t2_1hyyc3o6lb","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Is using a colab a issue!? Am i facing any data loss issues?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4aiyrs","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Is using a colab a issue!? Am i facing any data loss issues?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m4j0sa","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4j0sa/help_my_finetuned_model_keeps_echoing_prompts_or/n4aiyrs/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753075566,"author_flair_text":null,"treatment_tags":[],"created_utc":1753075566,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n46u72i","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"rnosov","can_mod_post":false,"send_replies":true,"parent_id":"t1_n45hw41","score":2,"author_fullname":"t2_18x6fa","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"100k is ok - these things are trained on datasets with billions of rows. But start with 10-15 pairs and 100 variations first to dial hyper parameters in. If it answers correctly at least 20%-30% times then you can either continue adding variations via SFT or treat it as a cold start and finish it off with GRPO. GRPO is a much more \\"mild\\" form of training that you can run without ill effects for many epochs whereas SFT gives you quick results but might damage your model. If you're planning to use it in production some form of RL training like GRPO would likely be mandatory. This is not a beginner stuff so if you succeed feel free to DM me resulting training script.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n46u72i","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;100k is ok - these things are trained on datasets with billions of rows. But start with 10-15 pairs and 100 variations first to dial hyper parameters in. If it answers correctly at least 20%-30% times then you can either continue adding variations via SFT or treat it as a cold start and finish it off with GRPO. GRPO is a much more &amp;quot;mild&amp;quot; form of training that you can run without ill effects for many epochs whereas SFT gives you quick results but might damage your model. If you&amp;#39;re planning to use it in production some form of RL training like GRPO would likely be mandatory. This is not a beginner stuff so if you succeed feel free to DM me resulting training script.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m4j0sa","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4j0sa/help_my_finetuned_model_keeps_echoing_prompts_or/n46u72i/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753029279,"author_flair_text":null,"treatment_tags":[],"created_utc":1753029279,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n45hw41","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Srmxz","can_mod_post":false,"created_utc":1753012753,"send_replies":true,"parent_id":"t1_n44sd6v","score":1,"author_fullname":"t2_1hyyc3o6lb","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"If i use a dataset which has nearly 100k variations will that be okay? \\nIâ€™m a beginner not an expert \\nSo i was given a task to create a Chatbot  \\nMy Dataset has only 500 prompt response pair\\nAnd facing this issue for 3 days","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n45hw41","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;If i use a dataset which has nearly 100k variations will that be okay? \\nIâ€™m a beginner not an expert \\nSo i was given a task to create a Chatbot&lt;br/&gt;\\nMy Dataset has only 500 prompt response pair\\nAnd facing this issue for 3 days&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m4j0sa","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4j0sa/help_my_finetuned_model_keeps_echoing_prompts_or/n45hw41/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753012753,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n44sd6v","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"rnosov","can_mod_post":false,"created_utc":1752998555,"send_replies":true,"parent_id":"t3_1m4j0sa","score":1,"author_fullname":"t2_18x6fa","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"If you're training base model it is expected result. What you want to do is to have several hundred variations of the same refund prompt like \\"Refund how long?\\", \\"I need refund now!\\" etc for every separate question (and answer) so you'd get say 500\\\\*250=125000 rows that you train for 1 epoch. You can use LLM for that. You'd probably want to mask out questions and train on answers only. Add validation loss and try to make sure that training loss doesn't drop to zero. Also, this a textbook case for RAG which you might want to use in conjunction or instead of fine-tuning.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n44sd6v","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;If you&amp;#39;re training base model it is expected result. What you want to do is to have several hundred variations of the same refund prompt like &amp;quot;Refund how long?&amp;quot;, &amp;quot;I need refund now!&amp;quot; etc for every separate question (and answer) so you&amp;#39;d get say 500*250=125000 rows that you train for 1 epoch. You can use LLM for that. You&amp;#39;d probably want to mask out questions and train on answers only. Add validation loss and try to make sure that training loss doesn&amp;#39;t drop to zero. Also, this a textbook case for RAG which you might want to use in conjunction or instead of fine-tuning.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4j0sa/help_my_finetuned_model_keeps_echoing_prompts_or/n44sd6v/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752998555,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m4j0sa","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),r=()=>e.jsx(t,{data:a});export{r as default};
