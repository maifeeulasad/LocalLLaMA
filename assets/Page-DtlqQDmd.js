import{j as e}from"./index-DFOnUtq9.js";import{R as t}from"./RedditPostRenderer-B-dx19nm.js";import"./index-CUOQn61u.js";const a=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Over the past year, weâ€™ve learned a lot from this community while exploring model merging. Now weâ€™re giving back with **Mergenetic**, an open-source library that makes *evolutionary* merging practical without needing big hardware.\\n\\nWhat it does:\\n\\n* Evolves high-quality LLM merges using evolutionary algorithms\\n* Supports SLERP, TIES, DARE, Task Arithmetic, and more\\n* Efficient: search happens in parameter space, not gradient needed\\n* Modular, hackable, and built on familiar tools (\`mergekit\`, \`pymoo\`, \`lm-eval-harness\`)\\n\\nRun it via Python, CLI, or GUI â€” and try some wild merge experiments on your own GPU.\\n\\nFor details, check out our papers:\\n\\n* ACL 2025 Demo: [arxiv.org/abs/2505.11427](https://arxiv.org/pdf/2505.11427)\\n* ICML 2025: [arxiv.org/abs/2502.10436](https://arxiv.org/pdf/2502.10436)\\n\\nðŸ”— [GitHub: tommasomncttn/mergenetic](https://github.com/tommasomncttn/mergenetic)\\n\\nWould love feedback or contributions â€” hope itâ€™s useful to some of you!","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Hey r/LocalLLaMA! We made evolutionary model merging feasible on consumer GPUs â€“ meet Mergenetic ðŸ§¬","link_flair_richtext":[{"e":"text","t":"Resources"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lqndyy","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.93,"author_flair_background_color":null,"subreddit_type":"public","ups":23,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_81hdual0","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Resources","can_mod_post":false,"score":23,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1751542836,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Over the past year, weâ€™ve learned a lot from this community while exploring model merging. Now weâ€™re giving back with &lt;strong&gt;Mergenetic&lt;/strong&gt;, an open-source library that makes &lt;em&gt;evolutionary&lt;/em&gt; merging practical without needing big hardware.&lt;/p&gt;\\n\\n&lt;p&gt;What it does:&lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;Evolves high-quality LLM merges using evolutionary algorithms&lt;/li&gt;\\n&lt;li&gt;Supports SLERP, TIES, DARE, Task Arithmetic, and more&lt;/li&gt;\\n&lt;li&gt;Efficient: search happens in parameter space, not gradient needed&lt;/li&gt;\\n&lt;li&gt;Modular, hackable, and built on familiar tools (&lt;code&gt;mergekit&lt;/code&gt;, &lt;code&gt;pymoo&lt;/code&gt;, &lt;code&gt;lm-eval-harness&lt;/code&gt;)&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;p&gt;Run it via Python, CLI, or GUI â€” and try some wild merge experiments on your own GPU.&lt;/p&gt;\\n\\n&lt;p&gt;For details, check out our papers:&lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;ACL 2025 Demo: &lt;a href=\\"https://arxiv.org/pdf/2505.11427\\"&gt;arxiv.org/abs/2505.11427&lt;/a&gt;&lt;/li&gt;\\n&lt;li&gt;ICML 2025: &lt;a href=\\"https://arxiv.org/pdf/2502.10436\\"&gt;arxiv.org/abs/2502.10436&lt;/a&gt;&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;p&gt;ðŸ”— &lt;a href=\\"https://github.com/tommasomncttn/mergenetic\\"&gt;GitHub: tommasomncttn/mergenetic&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;Would love feedback or contributions â€” hope itâ€™s useful to some of you!&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#ccac2b","id":"1lqndyy","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"leviatan0","discussion_type":null,"num_comments":2,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lqndyy/hey_rlocalllama_we_made_evolutionary_model/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lqndyy/hey_rlocalllama_we_made_evolutionary_model/","subreddit_subscribers":494198,"created_utc":1751542836,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n14203v","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"a_beautiful_rhind","can_mod_post":false,"created_utc":1751544087,"send_replies":true,"parent_id":"t3_1lqndyy","score":2,"author_fullname":"t2_h5utwre7","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I played with merging some vision and RP models with patched mergekit. It definitely works but I am cockblocked by the size of the weights for anything I'd like to run.\\n\\nYou still need \\"big\\" hardware if you have to run the model quantized. Process would be merge, quant and *then* test. Best case scenario is finding a recipe on a smaller model and hoping it works for the larger one.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n14203v","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I played with merging some vision and RP models with patched mergekit. It definitely works but I am cockblocked by the size of the weights for anything I&amp;#39;d like to run.&lt;/p&gt;\\n\\n&lt;p&gt;You still need &amp;quot;big&amp;quot; hardware if you have to run the model quantized. Process would be merge, quant and &lt;em&gt;then&lt;/em&gt; test. Best case scenario is finding a recipe on a smaller model and hoping it works for the larger one.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqndyy/hey_rlocalllama_we_made_evolutionary_model/n14203v/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751544087,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lqndyy","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n14671a","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"RobotRobotWhatDoUSee","can_mod_post":false,"created_utc":1751545714,"send_replies":true,"parent_id":"t3_1lqndyy","score":5,"author_fullname":"t2_m78cdz1nv","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Very interested to see this. I just cracked your paper open, but since you're here I'll just ask you ask well: what is the objective function you are minimizing to score the evolutionary algo step?\\n\\nThat is, if I understand correctly, you're doing a merge where you choose parameter or hyperparameters of the merge to minimize some objective -- what's the objective? (I'm skimming the paper now, but in my phone.)\\n\\nI'm interested in model merging to create bespoke research assistant LLMs that have expertise in niche academic research areas.\\n\\nHere's a follow-up Q -- I think the answer is no from my quick skim, but will ask anyway -- does your merge^3 handle mixture-of-experts style merging as in \`mergekit-moe\`? The reason I'm interested in that,  of course, is because one could potentially:\\n1. Fine tune (or CPT+SFT) a few small models to be good at different parts of niche task, then\\n2. \`merge-moe\` them into a bigger expert model with multiple specialties\\n\\nWhen I read over the \\"gates without training\\" section of [Goddard's \\"Clown MoE\\" post](https://goddard.blog/posts/clown-moe/#moe-gates-without-training), it struck me that choosing good positive/negative seed phrases was exactly the type of problem I might want to throw an evolutionary algorithm at. Of course one is then seaeching in \\"prompt space,\\" which itself has to be hard and messy -- I don't know if there is good off-the-shelf solutions fo that (I know DSPy is supposed to have functionality to search prompt space, but haven't looked into it much yet).\\n\\n\\nRegardless,  this is very interesting,  thanks!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n14671a","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Very interested to see this. I just cracked your paper open, but since you&amp;#39;re here I&amp;#39;ll just ask you ask well: what is the objective function you are minimizing to score the evolutionary algo step?&lt;/p&gt;\\n\\n&lt;p&gt;That is, if I understand correctly, you&amp;#39;re doing a merge where you choose parameter or hyperparameters of the merge to minimize some objective -- what&amp;#39;s the objective? (I&amp;#39;m skimming the paper now, but in my phone.)&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;m interested in model merging to create bespoke research assistant LLMs that have expertise in niche academic research areas.&lt;/p&gt;\\n\\n&lt;p&gt;Here&amp;#39;s a follow-up Q -- I think the answer is no from my quick skim, but will ask anyway -- does your merge&lt;sup&gt;3&lt;/sup&gt; handle mixture-of-experts style merging as in &lt;code&gt;mergekit-moe&lt;/code&gt;? The reason I&amp;#39;m interested in that,  of course, is because one could potentially:\\n1. Fine tune (or CPT+SFT) a few small models to be good at different parts of niche task, then\\n2. &lt;code&gt;merge-moe&lt;/code&gt; them into a bigger expert model with multiple specialties&lt;/p&gt;\\n\\n&lt;p&gt;When I read over the &amp;quot;gates without training&amp;quot; section of &lt;a href=\\"https://goddard.blog/posts/clown-moe/#moe-gates-without-training\\"&gt;Goddard&amp;#39;s &amp;quot;Clown MoE&amp;quot; post&lt;/a&gt;, it struck me that choosing good positive/negative seed phrases was exactly the type of problem I might want to throw an evolutionary algorithm at. Of course one is then seaeching in &amp;quot;prompt space,&amp;quot; which itself has to be hard and messy -- I don&amp;#39;t know if there is good off-the-shelf solutions fo that (I know DSPy is supposed to have functionality to search prompt space, but haven&amp;#39;t looked into it much yet).&lt;/p&gt;\\n\\n&lt;p&gt;Regardless,  this is very interesting,  thanks!&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqndyy/hey_rlocalllama_we_made_evolutionary_model/n14671a/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751545714,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lqndyy","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}}],"before":null}}]`),r=()=>e.jsx(t,{data:a});export{r as default};
