import{j as e}from"./index-BgwOAK4-.js";import{R as t}from"./RedditPostRenderer-BOBjDTFu.js";import"./index-BL22wVg5.js";const a=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"I told my brother I can help him build an AI workstation since he wants to run Llama 3.1 locally and train it or build a RAG or whatever. Since he's a software guy and I'm a gamer who built 2 gaming PCs in my entire life, he agreed to trust me with picking the parts and putting everything together (I was shocked too). I got him to order all the parts, including an expensive nvlink bridge 4 slot from eBay that is crucial for the build since he needs a 48GB of pooled vram from the two 3090s he was able to buy very cheaply from friends.\\n\\nLong story short, we ended buying Gigabyte trx50 aero D and the nvlink 4 slot bridge is too short and doesn't reach the second GPU.. I messed up big time and now I'm trying to find a solution without switching the entire setup because everything is already built, wired for air flow etc, PCU and AIO connected and PSU. The primary card I'm using in the PCIe slot 1 is ASUS ROG STRIX 3090 OC, the secondary is MSI VENTUS 3X 3090 OC which right now is in PCIe slot 3. Slot 2 is too close to the Asus GPU and besides it also doesn't allow for the nvlink to fit because then it'll be too long.  \\nI then had the idea of getting a GPU stand that can hold my MSI GPU at the correct height to accommodate the nvlink, and a PCIe riser cable to connect from either slot 2 or 3 to the card - the problem is all riser cables are way too long and I can't bend them enough to fit.  \\nI measured 17mm between the center of slot 2 and the fingers of the MSI GPU at the optimal position for the nvlink, and 23mm between the center of slot 3 and the fingers of the MSI GPU. Can't find a riser cable this short and even if do I don't know that it'll work very well at that length. I'm starting to lose hope and I'm not sure what to tell my brother.. now I'm on AliExpress looking for a PCB for a 16 pin PCIe that can offset by one slot up or down but it's looking like a lost cause.. I'm desperate. Any help would be much appreciated.  \\nMore specifically for the folks on this sub - Should my brother accept working with the 2 3090s without the Nvlink? Would it be dramatically lower performance on all counts of running local LLms or only for fine-tuning?\\n\\nThings I've already tried that don't work (that the good folks at PCBuild Help suggested):\\n\\n1. Switching GPU fans to water block won't help - the problem is that there is no PCIe configuration in this mobo that allows appropriate distance to accommodate the 4-slot NVlink.\\n2. They don't make a 5 or 3 slot NVlink for the 3090s. If anyone here has a lead on something like this from a third party I'll be all over it, but thus far was not able to find it.\\n3. Riser cables are 10-30cm where I need a 25mm that goes from PCIe slot 3 to the optimal position of the MSI GPU to accommodate the NVlink - no one makes that and if I get it custom I don't know that performance will justify it. Anyone know of more flexible riser type solutions that can bend more?\\n4. I know switching the MOBO will solve it. Trying to avoid that to not spend more money and redo the build, also trying to save some of what's left of my dignity in front of my brother.\\n5. My case can't fit both cards vertically.\\n\\n","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"I messed up my brother's Llama AI workstation.. looking for advice","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1m5ojym","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.6,"author_flair_background_color":null,"subreddit_type":"public","ups":1,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_dfnw6xdeo","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":1,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1753117487,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;I told my brother I can help him build an AI workstation since he wants to run Llama 3.1 locally and train it or build a RAG or whatever. Since he&amp;#39;s a software guy and I&amp;#39;m a gamer who built 2 gaming PCs in my entire life, he agreed to trust me with picking the parts and putting everything together (I was shocked too). I got him to order all the parts, including an expensive nvlink bridge 4 slot from eBay that is crucial for the build since he needs a 48GB of pooled vram from the two 3090s he was able to buy very cheaply from friends.&lt;/p&gt;\\n\\n&lt;p&gt;Long story short, we ended buying Gigabyte trx50 aero D and the nvlink 4 slot bridge is too short and doesn&amp;#39;t reach the second GPU.. I messed up big time and now I&amp;#39;m trying to find a solution without switching the entire setup because everything is already built, wired for air flow etc, PCU and AIO connected and PSU. The primary card I&amp;#39;m using in the PCIe slot 1 is ASUS ROG STRIX 3090 OC, the secondary is MSI VENTUS 3X 3090 OC which right now is in PCIe slot 3. Slot 2 is too close to the Asus GPU and besides it also doesn&amp;#39;t allow for the nvlink to fit because then it&amp;#39;ll be too long.&lt;br/&gt;\\nI then had the idea of getting a GPU stand that can hold my MSI GPU at the correct height to accommodate the nvlink, and a PCIe riser cable to connect from either slot 2 or 3 to the card - the problem is all riser cables are way too long and I can&amp;#39;t bend them enough to fit.&lt;br/&gt;\\nI measured 17mm between the center of slot 2 and the fingers of the MSI GPU at the optimal position for the nvlink, and 23mm between the center of slot 3 and the fingers of the MSI GPU. Can&amp;#39;t find a riser cable this short and even if do I don&amp;#39;t know that it&amp;#39;ll work very well at that length. I&amp;#39;m starting to lose hope and I&amp;#39;m not sure what to tell my brother.. now I&amp;#39;m on AliExpress looking for a PCB for a 16 pin PCIe that can offset by one slot up or down but it&amp;#39;s looking like a lost cause.. I&amp;#39;m desperate. Any help would be much appreciated.&lt;br/&gt;\\nMore specifically for the folks on this sub - Should my brother accept working with the 2 3090s without the Nvlink? Would it be dramatically lower performance on all counts of running local LLms or only for fine-tuning?&lt;/p&gt;\\n\\n&lt;p&gt;Things I&amp;#39;ve already tried that don&amp;#39;t work (that the good folks at PCBuild Help suggested):&lt;/p&gt;\\n\\n&lt;ol&gt;\\n&lt;li&gt;Switching GPU fans to water block won&amp;#39;t help - the problem is that there is no PCIe configuration in this mobo that allows appropriate distance to accommodate the 4-slot NVlink.&lt;/li&gt;\\n&lt;li&gt;They don&amp;#39;t make a 5 or 3 slot NVlink for the 3090s. If anyone here has a lead on something like this from a third party I&amp;#39;ll be all over it, but thus far was not able to find it.&lt;/li&gt;\\n&lt;li&gt;Riser cables are 10-30cm where I need a 25mm that goes from PCIe slot 3 to the optimal position of the MSI GPU to accommodate the NVlink - no one makes that and if I get it custom I don&amp;#39;t know that performance will justify it. Anyone know of more flexible riser type solutions that can bend more?&lt;/li&gt;\\n&lt;li&gt;I know switching the MOBO will solve it. Trying to avoid that to not spend more money and redo the build, also trying to save some of what&amp;#39;s left of my dignity in front of my brother.&lt;/li&gt;\\n&lt;li&gt;My case can&amp;#39;t fit both cards vertically.&lt;/li&gt;\\n&lt;/ol&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":true,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1m5ojym","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"spherical-aspiration","discussion_type":null,"num_comments":10,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1m5ojym/i_messed_up_my_brothers_llama_ai_workstation/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1m5ojym/i_messed_up_my_brothers_llama_ai_workstation/","subreddit_subscribers":502515,"created_utc":1753117487,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4dw24u","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"ShengrenR","can_mod_post":false,"created_utc":1753122335,"send_replies":true,"parent_id":"t3_1m5ojym","score":11,"author_fullname":"t2_ji4n4","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"More to the point - unless your brother is doing serious training runs that NVlink won't be all that important; it's a small inference speed bump, but it's really useful for training specifically.  Unless he wants to train a ton, the 6th option may just be 'oops, oh well' and don't worry about it.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4dw24u","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;More to the point - unless your brother is doing serious training runs that NVlink won&amp;#39;t be all that important; it&amp;#39;s a small inference speed bump, but it&amp;#39;s really useful for training specifically.  Unless he wants to train a ton, the 6th option may just be &amp;#39;oops, oh well&amp;#39; and don&amp;#39;t worry about it.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5ojym/i_messed_up_my_brothers_llama_ai_workstation/n4dw24u/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753122335,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m5ojym","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":11}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4dxqln","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"koushd","can_mod_post":false,"created_utc":1753122806,"send_replies":true,"parent_id":"t3_1m5ojym","score":7,"author_fullname":"t2_4yut6","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"nvlink improvement between two cards is going to be minimal for training, and inconsequential for inference.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4dxqln","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;nvlink improvement between two cards is going to be minimal for training, and inconsequential for inference.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5ojym/i_messed_up_my_brothers_llama_ai_workstation/n4dxqln/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753122806,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m5ojym","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":7}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4dt8pw","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"xchaos4ux","can_mod_post":false,"created_utc":1753121543,"send_replies":true,"parent_id":"t3_1m5ojym","score":6,"author_fullname":"t2_qys5z","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"switching the case to one of the gpu mining racks cases on amazon and riser cables should be the solution.   allowing you to mount the  motherboard underneath the cards, and postion the cards as needed to make the build work. \\n\\n  \\nthose cases are not too terribly expensive.   but the trick will be finding good pcie riser cables , i have not used any so im not sure  what to recommend  but i would read reviews carefully and ensure the risers work on 5090, 3090s. looking at one brand that seems reliable cost around 120.00 per cable.  a bit expensive.  there are others that cost around 40.00   maybe there is a nice middle ground.  still  if you spend  &lt;460.00 for the needed parts to make the case switch. it would be cheaper than than swapping the mother board \\n\\n  \\nas long as aesthetics are not a primary concern, you should be able to make that work.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4dt8pw","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;switching the case to one of the gpu mining racks cases on amazon and riser cables should be the solution.   allowing you to mount the  motherboard underneath the cards, and postion the cards as needed to make the build work. &lt;/p&gt;\\n\\n&lt;p&gt;those cases are not too terribly expensive.   but the trick will be finding good pcie riser cables , i have not used any so im not sure  what to recommend  but i would read reviews carefully and ensure the risers work on 5090, 3090s. looking at one brand that seems reliable cost around 120.00 per cable.  a bit expensive.  there are others that cost around 40.00   maybe there is a nice middle ground.  still  if you spend  &amp;lt;460.00 for the needed parts to make the case switch. it would be cheaper than than swapping the mother board &lt;/p&gt;\\n\\n&lt;p&gt;as long as aesthetics are not a primary concern, you should be able to make that work.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5ojym/i_messed_up_my_brothers_llama_ai_workstation/n4dt8pw/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753121543,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m5ojym","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4e6usv","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"abnormal_human","can_mod_post":false,"created_utc":1753125382,"send_replies":true,"parent_id":"t3_1m5ojym","score":5,"author_fullname":"t2_5y02z","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"If you have the money, buy different components that fit. If you don't, just don't do NVLink--it's not critical. Up to you. \\n\\nFor perspective, 3090 is the last GPU with NVLink and has the same PCIe interconnect performance as the 4090. If you'd built with 4090s NVLink wouldn't even be a consideration and things would be a-ok.\\n\\nInterconnect is really important when you have 8-1024 GPUs training in batch. It makes very little practical difference for r/LocalLLama kind of use cases.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4e6usv","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;If you have the money, buy different components that fit. If you don&amp;#39;t, just don&amp;#39;t do NVLink--it&amp;#39;s not critical. Up to you. &lt;/p&gt;\\n\\n&lt;p&gt;For perspective, 3090 is the last GPU with NVLink and has the same PCIe interconnect performance as the 4090. If you&amp;#39;d built with 4090s NVLink wouldn&amp;#39;t even be a consideration and things would be a-ok.&lt;/p&gt;\\n\\n&lt;p&gt;Interconnect is really important when you have 8-1024 GPUs training in batch. It makes very little practical difference for &lt;a href=\\"/r/LocalLLama\\"&gt;r/LocalLLama&lt;/a&gt; kind of use cases.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5ojym/i_messed_up_my_brothers_llama_ai_workstation/n4e6usv/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753125382,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m5ojym","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4f68kw","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"WHY_CAN_I_NOT_LIFE","can_mod_post":false,"created_utc":1753135778,"send_replies":true,"parent_id":"t1_n4eaqlf","score":2,"author_fullname":"t2_6dkk52oc","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I agree. NVlink on 3090s isn't really beneficial for AI/ML work. OP mentioned that their brother needed it for memory pooling, but 3090s don't support memory pooling over NVlink, that feature is reserved for Quadros and Teslas.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4f68kw","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I agree. NVlink on 3090s isn&amp;#39;t really beneficial for AI/ML work. OP mentioned that their brother needed it for memory pooling, but 3090s don&amp;#39;t support memory pooling over NVlink, that feature is reserved for Quadros and Teslas.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m5ojym","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5ojym/i_messed_up_my_brothers_llama_ai_workstation/n4f68kw/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753135778,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n4eaqlf","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Jcarlough","can_mod_post":false,"created_utc":1753126497,"send_replies":true,"parent_id":"t3_1m5ojym","score":4,"author_fullname":"t2_2dh7u9c","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Don’t think you need to bother with the NVLink.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4eaqlf","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Don’t think you need to bother with the NVLink.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5ojym/i_messed_up_my_brothers_llama_ai_workstation/n4eaqlf/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753126497,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m5ojym","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4ejmxg","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"raiffuvar","can_mod_post":false,"created_utc":1753129031,"send_replies":true,"parent_id":"t3_1m5ojym","score":1,"author_fullname":"t2_4a6mfq67","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Mine some bitcoin with 3090 and buy a proper link. Easy.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4ejmxg","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Mine some bitcoin with 3090 and buy a proper link. Easy.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5ojym/i_messed_up_my_brothers_llama_ai_workstation/n4ejmxg/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753129031,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m5ojym","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4fbcdy","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"michaelsoft__binbows","can_mod_post":false,"created_utc":1753137447,"send_replies":true,"parent_id":"t3_1m5ojym","score":2,"author_fullname":"t2_iifi6ul2l","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I spent a lot of effort getting NVlink enabled for my dual 3090 build. I have a 3 slots separation between my main PCIe slots. I used a x16 riser and a heaping dose of creativity to get both GPUs mounted wiht one slot in the middle (i used a tall 3090 (an FTW3) and a short 3090 (an XLR8) so the solution had to be really bespoke). I wanted to mount them 4 slots separated anyway, so top card could breathe.\\n\\nturns out.... the NVLink doesn't help at all for inference speed, not only that, but 70B models aren't any more useful today than 30B models, and my workstation stopped doing the very annoying thing of hard locking up (average once every 4 months, that shit sucked, impossible to diagnose) once i took the 2nd GPU out.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4fbcdy","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I spent a lot of effort getting NVlink enabled for my dual 3090 build. I have a 3 slots separation between my main PCIe slots. I used a x16 riser and a heaping dose of creativity to get both GPUs mounted wiht one slot in the middle (i used a tall 3090 (an FTW3) and a short 3090 (an XLR8) so the solution had to be really bespoke). I wanted to mount them 4 slots separated anyway, so top card could breathe.&lt;/p&gt;\\n\\n&lt;p&gt;turns out.... the NVLink doesn&amp;#39;t help at all for inference speed, not only that, but 70B models aren&amp;#39;t any more useful today than 30B models, and my workstation stopped doing the very annoying thing of hard locking up (average once every 4 months, that shit sucked, impossible to diagnose) once i took the 2nd GPU out.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5ojym/i_messed_up_my_brothers_llama_ai_workstation/n4fbcdy/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753137447,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m5ojym","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4dpc6j","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Some_thing_like_vr","can_mod_post":false,"created_utc":1753120463,"send_replies":true,"parent_id":"t3_1m5ojym","score":-2,"author_fullname":"t2_icn1q0kn","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I noticed that nobody is responding to your post... Maybe try asking Gemino or some other AI?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4dpc6j","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I noticed that nobody is responding to your post... Maybe try asking Gemino or some other AI?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5ojym/i_messed_up_my_brothers_llama_ai_workstation/n4dpc6j/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753120463,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m5ojym","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-2}}],"before":null}}]`),r=()=>e.jsx(t,{data:a});export{r as default};
