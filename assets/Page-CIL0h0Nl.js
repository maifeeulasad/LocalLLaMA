import{j as e}from"./index-BpC9hjVs.js";import{R as l}from"./RedditPostRenderer-BEo6AnSR.js";import"./index-DwkJHX1_.js";const t=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Hey,  \\nI'm looking for a LLM to run on my shitty laptop (DELL UltraSharp U2422H, 24–32GB RAM, 4GB VRAM). The model should support tool use (like a calculator or \`DuckDuckGoSearchRun()\`), and decent reasoning ability would be a bonus, though I know that's probably pushing it with my hardware.\\n\\nI’ve triedllama3.2:3b , which runs fast, but the outputs are pretty weak and it tends to hallucinate instead of actually using tools. I also tested qwen3:8b , which gives better responses but is way too slow on my setup.\\n\\nIdeally looking for something that runs through Ollama. Appreciate any suggestions, thanks.","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"LLM model recommendation for poor HW","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lojtq3","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.5,"author_flair_background_color":null,"subreddit_type":"public","ups":0,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_7hrz72dl","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":0,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1751318920,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Hey,&lt;br/&gt;\\nI&amp;#39;m looking for a LLM to run on my shitty laptop (DELL UltraSharp U2422H, 24–32GB RAM, 4GB VRAM). The model should support tool use (like a calculator or &lt;code&gt;DuckDuckGoSearchRun()&lt;/code&gt;), and decent reasoning ability would be a bonus, though I know that&amp;#39;s probably pushing it with my hardware.&lt;/p&gt;\\n\\n&lt;p&gt;I’ve triedllama3.2:3b , which runs fast, but the outputs are pretty weak and it tends to hallucinate instead of actually using tools. I also tested qwen3:8b , which gives better responses but is way too slow on my setup.&lt;/p&gt;\\n\\n&lt;p&gt;Ideally looking for something that runs through Ollama. Appreciate any suggestions, thanks.&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":true,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1lojtq3","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"ReputationMindless32","discussion_type":null,"num_comments":5,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lojtq3/llm_model_recommendation_for_poor_hw/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lojtq3/llm_model_recommendation_for_poor_hw/","subreddit_subscribers":493458,"created_utc":1751318920,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0pxq91","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ReputationMindless32","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0oasw1","score":1,"author_fullname":"t2_7hrz72dl","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Actually, I set the temperature to 0, but it still keeps generating nonsense. Anyway, I think I figured out what’s going on with the Qwen3 model. I’m building a multimodal app (web search, image analysis, calculator, PDF/txt parsing), and since the model has some \\"reasoning,\\" it tries to decide whether to use a tool or just respond directly and it ends up getting stuck. At least that’s what it looks like to me. Both 4B and even 8B models run surprisingly smoothly in the terminal.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0pxq91","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Actually, I set the temperature to 0, but it still keeps generating nonsense. Anyway, I think I figured out what’s going on with the Qwen3 model. I’m building a multimodal app (web search, image analysis, calculator, PDF/txt parsing), and since the model has some &amp;quot;reasoning,&amp;quot; it tries to decide whether to use a tool or just respond directly and it ends up getting stuck. At least that’s what it looks like to me. Both 4B and even 8B models run surprisingly smoothly in the terminal.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lojtq3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lojtq3/llm_model_recommendation_for_poor_hw/n0pxq91/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751354409,"author_flair_text":null,"treatment_tags":[],"created_utc":1751354409,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0oasw1","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"LicensedTerrapin","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0niv5d","score":1,"author_fullname":"t2_97zi8wea","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"That doesn't sound right. The hallucinations might be settings related like high temp etc","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0oasw1","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;That doesn&amp;#39;t sound right. The hallucinations might be settings related like high temp etc&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lojtq3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lojtq3/llm_model_recommendation_for_poor_hw/n0oasw1/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751329297,"author_flair_text":null,"treatment_tags":[],"created_utc":1751329297,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0niv5d","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ReputationMindless32","can_mod_post":false,"created_utc":1751320081,"send_replies":true,"parent_id":"t1_n0nhdhe","score":2,"author_fullname":"t2_7hrz72dl","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"yeah, I tried it as well - for some reason, not a very big different compare to the 8b.  I will check the  leaderboard anyway. Thanks!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0niv5d","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;yeah, I tried it as well - for some reason, not a very big different compare to the 8b.  I will check the  leaderboard anyway. Thanks!&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lojtq3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lojtq3/llm_model_recommendation_for_poor_hw/n0niv5d/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751320081,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n0nhdhe","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"SM8085","can_mod_post":false,"created_utc":1751319617,"send_replies":true,"parent_id":"t3_1lojtq3","score":3,"author_fullname":"t2_14vikjao97","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"&gt;I’ve tried llama3.2:3b\\n\\nllama3.2 3B is fine to chat with but with tool calling it's not very coherent, [https://gorilla.cs.berkeley.edu/leaderboard.html](https://gorilla.cs.berkeley.edu/leaderboard.html) ranked 89th on the Berkeley leaderboard.\\n\\nQwen3 4B is ranked 28th.  8B that you tried is 18th.  Even the Qwen3 0.6B model ranks higher than Llama 3.2 3B, currently 87th.  \\n\\nSo if an 8B is too slow on your setup try the Qwen3 4B, which should be faster and only a small step down in tool calling performance.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0nhdhe","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;I’ve tried llama3.2:3b&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;llama3.2 3B is fine to chat with but with tool calling it&amp;#39;s not very coherent, &lt;a href=\\"https://gorilla.cs.berkeley.edu/leaderboard.html\\"&gt;https://gorilla.cs.berkeley.edu/leaderboard.html&lt;/a&gt; ranked 89th on the Berkeley leaderboard.&lt;/p&gt;\\n\\n&lt;p&gt;Qwen3 4B is ranked 28th.  8B that you tried is 18th.  Even the Qwen3 0.6B model ranks higher than Llama 3.2 3B, currently 87th.  &lt;/p&gt;\\n\\n&lt;p&gt;So if an 8B is too slow on your setup try the Qwen3 4B, which should be faster and only a small step down in tool calling performance.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lojtq3/llm_model_recommendation_for_poor_hw/n0nhdhe/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751319617,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lojtq3","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0nrrzb","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"ilintar","can_mod_post":false,"created_utc":1751322947,"send_replies":true,"parent_id":"t3_1lojtq3","score":4,"author_fullname":"t2_cctud","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Qwen3 30B-A3B MoE with -ot exps=CPU.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0nrrzb","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Qwen3 30B-A3B MoE with -ot exps=CPU.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lojtq3/llm_model_recommendation_for_poor_hw/n0nrrzb/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751322947,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lojtq3","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}}],"before":null}}]`),n=()=>e.jsx(l,{data:t});export{n as default};
