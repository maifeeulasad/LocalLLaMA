import{j as e}from"./index-Cx8kh0ir.js";import{R as l}from"./RedditPostRenderer-DwODJGFH.js";import"./index-CGE9oYHK.js";const t=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"I've gotten UD 2 bit quants to work with llama.cpp. I've merged the split ggufs and tried to load that into vllm (v0.9.1) and it says qwen3moe architecture isn't supported for gguf. So I guess my real question here is done anyone repackage unsloth quants in a format that vllm can load? Or is it possible for me to do that?","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Qwen3 tiny/unsloth quants with vllm?","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lmggiz","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.75,"author_flair_background_color":null,"subreddit_type":"public","ups":2,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_mcvyi","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":2,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1751093817,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;ve gotten UD 2 bit quants to work with llama.cpp. I&amp;#39;ve merged the split ggufs and tried to load that into vllm (v0.9.1) and it says qwen3moe architecture isn&amp;#39;t supported for gguf. So I guess my real question here is done anyone repackage unsloth quants in a format that vllm can load? Or is it possible for me to do that?&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1lmggiz","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"MengerianMango","discussion_type":null,"num_comments":25,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lmggiz/qwen3_tinyunsloth_quants_with_vllm/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lmggiz/qwen3_tinyunsloth_quants_with_vllm/","subreddit_subscribers":492627,"created_utc":1751093817,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"total_awards_received":0,"approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"ups":1,"removal_reason":null,"link_id":"t3_1lmggiz","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n07qasi","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"thirteen-bit","can_mod_post":false,"created_utc":1751102550,"send_replies":true,"parent_id":"t1_n07q382","score":1,"author_fullname":"t2_9l12dgc5","approved_by":null,"mod_note":null,"all_awardings":[],"body":"With your VRAM you may play with speculative decoding too. Try Qwen3 dense and 30B MoE models at lower quants. With 24Gb I've got no improvements, --draft-model actually made it slower","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n07qasi","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;With your VRAM you may play with speculative decoding too. Try Qwen3 dense and 30B MoE models at lower quants. With 24Gb I&amp;#39;ve got no improvements, --draft-model actually made it slower&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lmggiz","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmggiz/qwen3_tinyunsloth_quants_with_vllm/n07qasi/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751102550,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":7,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n07q382","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":"DELETED","no_follow":true,"author":"[deleted]","can_mod_post":false,"send_replies":true,"parent_id":"t1_n07p9eb","score":1,"approved_by":null,"report_reasons":null,"all_awardings":[],"subreddit_id":"t5_81eyvm","body":"[removed]","edited":false,"downs":0,"author_flair_css_class":null,"collapsed":true,"is_submitter":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;[removed]&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"associated_award":null,"stickied":false,"subreddit_type":"public","can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":"dark","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmggiz/qwen3_tinyunsloth_quants_with_vllm/n07q382/","num_reports":null,"locked":false,"name":"t1_n07q382","created":1751102421,"subreddit":"LocalLLaMA","author_flair_text":null,"treatment_tags":[],"created_utc":1751102421,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":6,"author_flair_background_color":"","collapsed_because_crowd_control":null,"mod_reports":[],"mod_note":null,"distinguished":null}}],"before":null}},"user_reports":[],"saved":false,"id":"n07p9eb","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MengerianMango","can_mod_post":false,"send_replies":true,"parent_id":"t1_n07ox23","score":1,"author_fullname":"t2_mcvyi","approved_by":null,"mod_note":null,"all_awardings":[],"body":"Mind showing me your exact llama.cpp command? I'm always wondering if there are flags I'm missing/unaware of.","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n07p9eb","is_submitter":true,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Mind showing me your exact llama.cpp command? I&amp;#39;m always wondering if there are flags I&amp;#39;m missing/unaware of.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lmggiz","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmggiz/qwen3_tinyunsloth_quants_with_vllm/n07p9eb/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751101919,"author_flair_text":null,"treatment_tags":[],"created_utc":1751101919,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n07ox23","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"thirteen-bit","can_mod_post":false,"send_replies":true,"parent_id":"t1_n07micn","score":1,"author_fullname":"t2_9l12dgc5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Ok, I'd not look at vLLM at all until the speed is critical - it may be faster but you'll have to dig through its documentation, github issues and source code for days to optimize it.\\n\\nRegarding llama.cpp: I'd start with Q3 or even Q4 of 235B for RTX 6000 Pro - I'm getting 3.6 tps on small prompts with unsloth's Qwen3-235B-A22B-UD-Q3\\\\_K\\\\_XL on 250W power limited RTX 3090 + i5-12400 w/ 96 Gb of slow DDR4 (unmatched RAM so running at 2133 MHz) and adjust the layers offloaded to CPU.","edited":false,"author_flair_css_class":null,"name":"t1_n07ox23","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Ok, I&amp;#39;d not look at vLLM at all until the speed is critical - it may be faster but you&amp;#39;ll have to dig through its documentation, github issues and source code for days to optimize it.&lt;/p&gt;\\n\\n&lt;p&gt;Regarding llama.cpp: I&amp;#39;d start with Q3 or even Q4 of 235B for RTX 6000 Pro - I&amp;#39;m getting 3.6 tps on small prompts with unsloth&amp;#39;s Qwen3-235B-A22B-UD-Q3_K_XL on 250W power limited RTX 3090 + i5-12400 w/ 96 Gb of slow DDR4 (unmatched RAM so running at 2133 MHz) and adjust the layers offloaded to CPU.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lmggiz","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmggiz/qwen3_tinyunsloth_quants_with_vllm/n07ox23/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751101711,"author_flair_text":null,"collapsed":false,"created_utc":1751101711,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n07micn","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MengerianMango","can_mod_post":false,"send_replies":true,"parent_id":"t1_n07lynw","score":1,"author_fullname":"t2_mcvyi","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Single user. I have an RTX Pro 6000 Blackwell and I'm just trying to get the most speed out of it I can so I can use it for agentic coding. It's already fast enough for chat under llama, but speed matters a lot more when you're having the llm actually do the work, yk.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n07micn","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Single user. I have an RTX Pro 6000 Blackwell and I&amp;#39;m just trying to get the most speed out of it I can so I can use it for agentic coding. It&amp;#39;s already fast enough for chat under llama, but speed matters a lot more when you&amp;#39;re having the llm actually do the work, yk.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lmggiz","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmggiz/qwen3_tinyunsloth_quants_with_vllm/n07micn/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751100276,"author_flair_text":null,"treatment_tags":[],"created_utc":1751100276,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n07lynw","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"thirteen-bit","can_mod_post":false,"send_replies":true,"parent_id":"t1_n07kjn8","score":2,"author_fullname":"t2_9l12dgc5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Ah, 235b is a large one.\\n\\nLooking at [https://github.com/vllm-project/vllm/issues/17327](https://github.com/vllm-project/vllm/issues/17327) it does not seem to work with GGUF.\\n\\nWhat is your target? Do you plan to serve multiple users or do you want to improve single user performance?\\n\\nIf multiple users is a target or vLLM is required for some other reason then you'll probably have to look for increased VRAM to fit at least 4-bit quantization and some context.\\n\\nIf you're targeting (somewhat) improved performance with your existing hardware look at ik\\\\_llama and this quantization: [https://huggingface.co/ubergarm/Qwen3-235B-A22B-GGUF](https://huggingface.co/ubergarm/Qwen3-235B-A22B-GGUF)","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n07lynw","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Ah, 235b is a large one.&lt;/p&gt;\\n\\n&lt;p&gt;Looking at &lt;a href=\\"https://github.com/vllm-project/vllm/issues/17327\\"&gt;https://github.com/vllm-project/vllm/issues/17327&lt;/a&gt; it does not seem to work with GGUF.&lt;/p&gt;\\n\\n&lt;p&gt;What is your target? Do you plan to serve multiple users or do you want to improve single user performance?&lt;/p&gt;\\n\\n&lt;p&gt;If multiple users is a target or vLLM is required for some other reason then you&amp;#39;ll probably have to look for increased VRAM to fit at least 4-bit quantization and some context.&lt;/p&gt;\\n\\n&lt;p&gt;If you&amp;#39;re targeting (somewhat) improved performance with your existing hardware look at ik_llama and this quantization: &lt;a href=\\"https://huggingface.co/ubergarm/Qwen3-235B-A22B-GGUF\\"&gt;https://huggingface.co/ubergarm/Qwen3-235B-A22B-GGUF&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lmggiz","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmggiz/qwen3_tinyunsloth_quants_with_vllm/n07lynw/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751099953,"author_flair_text":null,"treatment_tags":[],"created_utc":1751099953,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n07kjn8","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MengerianMango","can_mod_post":false,"created_utc":1751099108,"send_replies":true,"parent_id":"t1_n07js68","score":2,"author_fullname":"t2_mcvyi","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"&gt;Why are you looking at GGUF at all if you're using vLLM?\\n\\nI don't really know what I'm doing. I just want to run Qwen3 235b with a 2 bit quant, under vllm if possible since ofc I'd prefer to get the most performance I can.\\n\\n&gt;Wasn't AWQ best for vLLM?\\n\\nYou might be right. I hadn't heard of AWQ before now. Seems like it is strictly 4 bit. I don't have enough vram for that.","edited":1751099325,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n07kjn8","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;Why are you looking at GGUF at all if you&amp;#39;re using vLLM?&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;I don&amp;#39;t really know what I&amp;#39;m doing. I just want to run Qwen3 235b with a 2 bit quant, under vllm if possible since ofc I&amp;#39;d prefer to get the most performance I can.&lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;Wasn&amp;#39;t AWQ best for vLLM?&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;You might be right. I hadn&amp;#39;t heard of AWQ before now. Seems like it is strictly 4 bit. I don&amp;#39;t have enough vram for that.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lmggiz","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmggiz/qwen3_tinyunsloth_quants_with_vllm/n07kjn8/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751099108,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n07js68","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"thirteen-bit","can_mod_post":false,"created_utc":1751098656,"send_replies":true,"parent_id":"t3_1lmggiz","score":3,"author_fullname":"t2_9l12dgc5","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Why are you looking at GGUF at all if you're using vLLM?\\n\\nWasn't AWQ best for vLLM?\\n\\n[https://docs.vllm.ai/en/latest/features/quantization/index.html](https://docs.vllm.ai/en/latest/features/quantization/index.html)\\n\\n[https://www.reddit.com/r/LocalLLaMA/comments/1ieoxk0/vllm\\\\_quantization\\\\_performance\\\\_which\\\\_kinds\\\\_work/](https://www.reddit.com/r/LocalLLaMA/comments/1ieoxk0/vllm_quantization_performance_which_kinds_work/)\\n\\n  \\nOtherwise if you want some more meaningful answers here please at least specify the model? There are quite a few Qwen 3 models. [https://huggingface.co/models?search=Qwen/Qwen3](https://huggingface.co/models?search=Qwen/Qwen3)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n07js68","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Why are you looking at GGUF at all if you&amp;#39;re using vLLM?&lt;/p&gt;\\n\\n&lt;p&gt;Wasn&amp;#39;t AWQ best for vLLM?&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://docs.vllm.ai/en/latest/features/quantization/index.html\\"&gt;https://docs.vllm.ai/en/latest/features/quantization/index.html&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://www.reddit.com/r/LocalLLaMA/comments/1ieoxk0/vllm_quantization_performance_which_kinds_work/\\"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1ieoxk0/vllm_quantization_performance_which_kinds_work/&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;Otherwise if you want some more meaningful answers here please at least specify the model? There are quite a few Qwen 3 models. &lt;a href=\\"https://huggingface.co/models?search=Qwen/Qwen3\\"&gt;https://huggingface.co/models?search=Qwen/Qwen3&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmggiz/qwen3_tinyunsloth_quants_with_vllm/n07js68/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751098656,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lmggiz","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"more","data":{"count":0,"name":"t1__","id":"_","parent_id":"t1_n0anqlo","depth":10,"children":[]}}],"before":null}},"user_reports":[],"saved":false,"id":"n0anqlo","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DinoAmino","can_mod_post":false,"created_utc":1751141267,"send_replies":true,"parent_id":"t1_n0a6s3f","score":1,"author_fullname":"t2_j1v7f","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Ada supports FP8 natively - it does not require Marlin. Not sure what the problem is with qwen's quant unless it requires specific configuration or something. Rather than trying to puzzle it out I'd try the RedHat FP8 first.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0anqlo","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Ada supports FP8 natively - it does not require Marlin. Not sure what the problem is with qwen&amp;#39;s quant unless it requires specific configuration or something. Rather than trying to puzzle it out I&amp;#39;d try the RedHat FP8 first.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmggiz/qwen3_tinyunsloth_quants_with_vllm/n0anqlo/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751141267,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lmggiz","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":9,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0a6s3f","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ahmetegesel","can_mod_post":false,"created_utc":1751135796,"send_replies":true,"parent_id":"t1_n0a65wj","score":1,"author_fullname":"t2_69skhb61","approved_by":null,"mod_note":null,"all_awardings":[],"body":"Wait, just checked that ours is A6000 Ada, would that make a difference? I suspect they are fundamentally different\\n\\nEdit: According to the article below, Ada has different arch, and it is not Ampere","edited":1751137426,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n0a6s3f","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Wait, just checked that ours is A6000 Ada, would that make a difference? I suspect they are fundamentally different&lt;/p&gt;\\n\\n&lt;p&gt;Edit: According to the article below, Ada has different arch, and it is not Ampere&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmggiz/qwen3_tinyunsloth_quants_with_vllm/n0a6s3f/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751135796,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lmggiz","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":8,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0a65wj","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DinoAmino","can_mod_post":false,"created_utc":1751135597,"send_replies":true,"parent_id":"t1_n0a2z86","score":1,"author_fullname":"t2_j1v7f","approved_by":null,"mod_note":null,"all_awardings":[],"body":"I can't tell you for sure what the technical differences are. I know that llm-compressor is part of vLLM and it's also used for dynamic quantization at startup on full size models. I suspect Qwen uses a different tool and vLLM can't use Marlin on their FP8 quant ü§∑‚Äç‚ôÇÔ∏è All I know is Redhat or NM FP8 quants work reliably on Ampere using vLLM.","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n0a65wj","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I can&amp;#39;t tell you for sure what the technical differences are. I know that llm-compressor is part of vLLM and it&amp;#39;s also used for dynamic quantization at startup on full size models. I suspect Qwen uses a different tool and vLLM can&amp;#39;t use Marlin on their FP8 quant ü§∑‚Äç‚ôÇÔ∏è All I know is Redhat or NM FP8 quants work reliably on Ampere using vLLM.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lmggiz","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmggiz/qwen3_tinyunsloth_quants_with_vllm/n0a65wj/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751135597,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":7,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0a2z86","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ahmetegesel","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0a1xi2","score":1,"author_fullname":"t2_69skhb61","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Am I reading this correct, this is different FP8 quantization technique? Can you give me some explanation or keywords so I can dig a little deeper? Why exactly Qwen‚Äôs FP8 doesn‚Äôt work with A6000 but this one would work?","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n0a2z86","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Am I reading this correct, this is different FP8 quantization technique? Can you give me some explanation or keywords so I can dig a little deeper? Why exactly Qwen‚Äôs FP8 doesn‚Äôt work with A6000 but this one would work?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lmggiz","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmggiz/qwen3_tinyunsloth_quants_with_vllm/n0a2z86/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751134584,"author_flair_text":null,"treatment_tags":[],"created_utc":1751134584,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":6,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0a1xi2","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DinoAmino","can_mod_post":false,"send_replies":true,"parent_id":"t1_n09gqwy","score":1,"author_fullname":"t2_j1v7f","approved_by":null,"mod_note":null,"all_awardings":[],"body":"No, I read the issue. It may be qwen's quant isn't Marlin friendly, if that makes sense. You should give this quant a try then - IBM/RedHat bought Neural Magic, the naintainers of vLLM. They use llm-compressor on all their quants so this one _should_ work.\\n\\nhttps://huggingface.co/RedHatAI/Qwen3-30B-A3B-FP8-dynamic","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n0a1xi2","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;No, I read the issue. It may be qwen&amp;#39;s quant isn&amp;#39;t Marlin friendly, if that makes sense. You should give this quant a try then - IBM/RedHat bought Neural Magic, the naintainers of vLLM. They use llm-compressor on all their quants so this one &lt;em&gt;should&lt;/em&gt; work.&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://huggingface.co/RedHatAI/Qwen3-30B-A3B-FP8-dynamic\\"&gt;https://huggingface.co/RedHatAI/Qwen3-30B-A3B-FP8-dynamic&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lmggiz","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmggiz/qwen3_tinyunsloth_quants_with_vllm/n0a1xi2/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751134257,"author_flair_text":null,"treatment_tags":[],"created_utc":1751134257,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n09gqwy","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ahmetegesel","can_mod_post":false,"send_replies":true,"parent_id":"t1_n09gnaj","score":1,"author_fullname":"t2_69skhb61","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Qwen‚Äôs official GGUF\\n\\nEdit: I suspect you didn‚Äôt read the issue\\n\\nEdit2: I mistyped it is qwen‚Äôs official FP8\\n\\nhttps://huggingface.co/Qwen/Qwen3-30B-A3B-FP8","edited":1751128079,"author_flair_css_class":null,"name":"t1_n09gqwy","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Qwen‚Äôs official GGUF&lt;/p&gt;\\n\\n&lt;p&gt;Edit: I suspect you didn‚Äôt read the issue&lt;/p&gt;\\n\\n&lt;p&gt;Edit2: I mistyped it is qwen‚Äôs official FP8&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://huggingface.co/Qwen/Qwen3-30B-A3B-FP8\\"&gt;https://huggingface.co/Qwen/Qwen3-30B-A3B-FP8&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lmggiz","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmggiz/qwen3_tinyunsloth_quants_with_vllm/n09gqwy/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751127715,"author_flair_text":null,"collapsed":false,"created_utc":1751127715,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n09gnaj","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DinoAmino","can_mod_post":false,"send_replies":true,"parent_id":"t1_n09f4qd","score":1,"author_fullname":"t2_j1v7f","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"No I haven't. And I don't use sglang. Maybe a bad quantization? Who quantized yours?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n09gnaj","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;No I haven&amp;#39;t. And I don&amp;#39;t use sglang. Maybe a bad quantization? Who quantized yours?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lmggiz","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmggiz/qwen3_tinyunsloth_quants_with_vllm/n09gnaj/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751127684,"author_flair_text":null,"treatment_tags":[],"created_utc":1751127684,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n09f4qd","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ahmetegesel","can_mod_post":false,"send_replies":true,"parent_id":"t1_n09exeq","score":1,"author_fullname":"t2_69skhb61","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Did you try running Qwen3 30B A3B FP8?\\n\\nEdit: check this out - https://github.com/sgl-project/sglang/issues/5871","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n09f4qd","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Did you try running Qwen3 30B A3B FP8?&lt;/p&gt;\\n\\n&lt;p&gt;Edit: check this out - &lt;a href=\\"https://github.com/sgl-project/sglang/issues/5871\\"&gt;https://github.com/sgl-project/sglang/issues/5871&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lmggiz","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmggiz/qwen3_tinyunsloth_quants_with_vllm/n09f4qd/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751127197,"author_flair_text":null,"treatment_tags":[],"created_utc":1751127197,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n09exeq","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DinoAmino","can_mod_post":false,"created_utc":1751127132,"send_replies":true,"parent_id":"t1_n07wasq","score":1,"author_fullname":"t2_j1v7f","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"vLLM will use the Marlin kernel libraries on ampere cards. I use FP8 all the time on A6000s. Check your configuration options.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n09exeq","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;vLLM will use the Marlin kernel libraries on ampere cards. I use FP8 all the time on A6000s. Check your configuration options.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lmggiz","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmggiz/qwen3_tinyunsloth_quants_with_vllm/n09exeq/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751127132,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n07wasq","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ahmetegesel","can_mod_post":false,"created_utc":1751106069,"send_replies":true,"parent_id":"t3_1lmggiz","score":1,"author_fullname":"t2_69skhb61","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Welcome to the club. I have been trying to run 30B A3B UD 8bit on A6000 Ada with no luck. It looks like the support is missing on transformers side. I saw a PR for bringing qwen3 support but nobody is trying to bring qwen3moe support. I tried to fork transformers myself and tried a few things but couldn‚Äôt manage. \\n\\nFP8 is not working on A6000 apparently, it is a new architecture that old gpus do not support. INT4 was stupid, so was AWQ. I tried gguf but no luck. \\n\\nNow I am back to llamacpp but not sure how it would its concurrency performance be compared to vLLM.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n07wasq","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Welcome to the club. I have been trying to run 30B A3B UD 8bit on A6000 Ada with no luck. It looks like the support is missing on transformers side. I saw a PR for bringing qwen3 support but nobody is trying to bring qwen3moe support. I tried to fork transformers myself and tried a few things but couldn‚Äôt manage. &lt;/p&gt;\\n\\n&lt;p&gt;FP8 is not working on A6000 apparently, it is a new architecture that old gpus do not support. INT4 was stupid, so was AWQ. I tried gguf but no luck. &lt;/p&gt;\\n\\n&lt;p&gt;Now I am back to llamacpp but not sure how it would its concurrency performance be compared to vLLM.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmggiz/qwen3_tinyunsloth_quants_with_vllm/n07wasq/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751106069,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lmggiz","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0bjvqa","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MengerianMango","can_mod_post":false,"created_utc":1751152262,"send_replies":true,"parent_id":"t1_n08nyd4","score":1,"author_fullname":"t2_mcvyi","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Not sure how to benchmark. I'm not using ollama rn, used to just use ollama run --verbose.\\n\\nIt's fast.\\n\\nAny suggestions for benchmarking with llama.cpp?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0bjvqa","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Not sure how to benchmark. I&amp;#39;m not using ollama rn, used to just use ollama run --verbose.&lt;/p&gt;\\n\\n&lt;p&gt;It&amp;#39;s fast.&lt;/p&gt;\\n\\n&lt;p&gt;Any suggestions for benchmarking with llama.cpp?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lmggiz","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmggiz/qwen3_tinyunsloth_quants_with_vllm/n0bjvqa/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751152262,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0droow","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"djdeniro","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0ce3hc","score":2,"author_fullname":"t2_1epwrhsm","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Wow it is super fast! Crazy!¬†","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0droow","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Wow it is super fast! Crazy!¬†&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lmggiz","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmggiz/qwen3_tinyunsloth_quants_with_vllm/n0droow/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751189487,"author_flair_text":null,"treatment_tags":[],"created_utc":1751189487,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0dupza","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"djdeniro","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0ce3hc","score":1,"author_fullname":"t2_1epwrhsm","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"How match cost your GPU?","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0dupza","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;How match cost your GPU?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lmggiz","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmggiz/qwen3_tinyunsloth_quants_with_vllm/n0dupza/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751191328,"author_flair_text":null,"treatment_tags":[],"created_utc":1751191328,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0ce3hc","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MengerianMango","can_mod_post":false,"created_utc":1751163806,"send_replies":true,"parent_id":"t1_n08nyd4","score":1,"author_fullname":"t2_mcvyi","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"\`\`\`\\nllama-cli  -hfr unsloth/Qwen3-235B-A22B-GGUF:UD-Q2_K_XL   --ctx-size $((1024*128))   --gpu-layers 92   --threads 32   --mlock -fa -ctk q8_0 -ctv q8_0\\n...\\nllama_perf_sampler_print:    sampling time =     103.84 ms /  1738 runs   (    0.06 ms per token, 16738.09 tokens per second)\\nllama_perf_context_print:        load time =    7585.62 ms\\nllama_perf_context_print: prompt eval time =     313.25 ms /    20 tokens (   15.66 ms per token,    63.85 tokens per second)\\nllama_perf_context_print:        eval time =   77677.01 ms /  1717 runs   (   45.24 ms per token,    22.10 tokens per second)\\nllama_perf_context_print:       total time =  218813.27 ms /  1737 tokens\\n\\n\`\`\`\\nIf I use 128k context, I have to offload 3 layers and TPS drops a ton.\\n\\n\`\`\`\\nllama_perf_sampler_print:    sampling time =      26.28 ms /   734 runs   (    0.04 ms per token, 27927.86 tokens per second)\\nllama_perf_context_print:        load time =    7576.59 ms\\nllama_perf_context_print: prompt eval time =     352.98 ms /    31 tokens (   11.39 ms per token,    87.82 tokens per second)\\nllama_perf_context_print:        eval time =   16826.08 ms /   946 runs   (   17.79 ms per token,    56.22 tokens per second)\\nllama_perf_context_print:       total time =   56032.48 ms /   977 tokens\\n\`\`\`\\nThis is 64k context with no offload.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0ce3hc","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;\`\`\`\\nllama-cli  -hfr unsloth/Qwen3-235B-A22B-GGUF:UD-Q2_K_XL   --ctx-size $((1024*128))   --gpu-layers 92   --threads 32   --mlock -fa -ctk q8_0 -ctv q8_0\\n...\\nllama_perf_sampler_print:    sampling time =     103.84 ms /  1738 runs   (    0.06 ms per token, 16738.09 tokens per second)\\nllama_perf_context_print:        load time =    7585.62 ms\\nllama_perf_context_print: prompt eval time =     313.25 ms /    20 tokens (   15.66 ms per token,    63.85 tokens per second)\\nllama_perf_context_print:        eval time =   77677.01 ms /  1717 runs   (   45.24 ms per token,    22.10 tokens per second)\\nllama_perf_context_print:       total time =  218813.27 ms /  1737 tokens&lt;/p&gt;\\n\\n&lt;p&gt;\`\`\`\\nIf I use 128k context, I have to offload 3 layers and TPS drops a ton.&lt;/p&gt;\\n\\n&lt;p&gt;&lt;code&gt;\\nllama_perf_sampler_print:    sampling time =      26.28 ms /   734 runs   (    0.04 ms per token, 27927.86 tokens per second)\\nllama_perf_context_print:        load time =    7576.59 ms\\nllama_perf_context_print: prompt eval time =     352.98 ms /    31 tokens (   11.39 ms per token,    87.82 tokens per second)\\nllama_perf_context_print:        eval time =   16826.08 ms /   946 runs   (   17.79 ms per token,    56.22 tokens per second)\\nllama_perf_context_print:       total time =   56032.48 ms /   977 tokens\\n&lt;/code&gt;\\nThis is 64k context with no offload.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lmggiz","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmggiz/qwen3_tinyunsloth_quants_with_vllm/n0ce3hc/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751163806,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n08nyd4","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"djdeniro","can_mod_post":false,"created_utc":1751118379,"send_replies":true,"parent_id":"t3_1lmggiz","score":1,"author_fullname":"t2_1epwrhsm","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"q2\\\\_x\\\\_xl most likely wins in quality over awq 4bit and gptq 4bit. Maybe you will got better speed but lower quallity.  \\nI've been looking for ways to run it on vllm for a month now, but for the agent, the best solution is to use qwen3 when you need to think, and 24-32b models for fast \\"agent\\" work where you don't need to make creative decisions.\\n\\nAlso, AWQ will not give any speed boost, in one thread, compared to GGUF which you already have!\\n\\nCan you tell me how many tokens per second you get?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n08nyd4","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;q2_x_xl most likely wins in quality over awq 4bit and gptq 4bit. Maybe you will got better speed but lower quallity.&lt;br/&gt;\\nI&amp;#39;ve been looking for ways to run it on vllm for a month now, but for the agent, the best solution is to use qwen3 when you need to think, and 24-32b models for fast &amp;quot;agent&amp;quot; work where you don&amp;#39;t need to make creative decisions.&lt;/p&gt;\\n\\n&lt;p&gt;Also, AWQ will not give any speed boost, in one thread, compared to GGUF which you already have!&lt;/p&gt;\\n\\n&lt;p&gt;Can you tell me how many tokens per second you get?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmggiz/qwen3_tinyunsloth_quants_with_vllm/n08nyd4/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751118379,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lmggiz","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n07bk98","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MengerianMango","can_mod_post":false,"created_utc":1751093916,"send_replies":true,"parent_id":"t3_1lmggiz","score":-4,"author_fullname":"t2_mcvyi","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Pinging u/danielhanchen","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n07bk98","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Pinging &lt;a href=\\"/u/danielhanchen\\"&gt;u/danielhanchen&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmggiz/qwen3_tinyunsloth_quants_with_vllm/n07bk98/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751093916,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lmggiz","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-4}}],"before":null}}]`),o=()=>e.jsx(l,{data:t});export{o as default};
