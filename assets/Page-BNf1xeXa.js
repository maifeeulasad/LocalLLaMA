import{j as e}from"./index-CjwP30j7.js";import{R as t}from"./RedditPostRenderer-BbYuEq_V.js";import"./index-C-yxLSPN.js";const l=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Hey everyone. So I'll keep it short. I've written a Claude Code \\"clone\\", [cli-agent](https://github.com/amranu/cli-agent) which allows tool use for arbitrary LLMs (though they have to support tool use, I'm not using any templating). Currently it has tested support for Deepseek, Gemini, OpenAI and Anthropic APIs but I want it to work with ollama. Main problem is I don't have a setup that can work with ollama (I have an old AMD card, no nvidia). So I need someone to test out the ollama support I've added and see if it works.\\n\\nmcp-agent exposes all the tools Claude Code has, along with arbitrary subagent support. It also has an mcp server, similar to Zen MCP to allow any LLM to talk to any other LLM you have configured. Except unlike Zen MCP, the LLMs have access to tools.\\n\\nAnyone willing to help me out and test ollama support would be greatly appreciated!","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"I need help testing my agentic wrapper for LLMs","link_flair_richtext":[{"e":"text","t":"Other"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lm66fy","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.6,"author_flair_background_color":null,"subreddit_type":"public","ups":1,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_3yvyd","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Other","can_mod_post":false,"score":1,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":1751140914,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"post_hint":"self","content_categories":null,"is_self":true,"mod_note":null,"created":1751061087,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Hey everyone. So I&amp;#39;ll keep it short. I&amp;#39;ve written a Claude Code &amp;quot;clone&amp;quot;, &lt;a href=\\"https://github.com/amranu/cli-agent\\"&gt;cli-agent&lt;/a&gt; which allows tool use for arbitrary LLMs (though they have to support tool use, I&amp;#39;m not using any templating). Currently it has tested support for Deepseek, Gemini, OpenAI and Anthropic APIs but I want it to work with ollama. Main problem is I don&amp;#39;t have a setup that can work with ollama (I have an old AMD card, no nvidia). So I need someone to test out the ollama support I&amp;#39;ve added and see if it works.&lt;/p&gt;\\n\\n&lt;p&gt;mcp-agent exposes all the tools Claude Code has, along with arbitrary subagent support. It also has an mcp server, similar to Zen MCP to allow any LLM to talk to any other LLM you have configured. Except unlike Zen MCP, the LLMs have access to tools.&lt;/p&gt;\\n\\n&lt;p&gt;Anyone willing to help me out and test ollama support would be greatly appreciated!&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":true,"is_crosspostable":false,"pinned":false,"over_18":false,"preview":{"images":[{"source":{"url":"https://external-preview.redd.it/ky4q-HJ1F3S2UdCuaGkloWjj4Ru8GaNbo0jpnr086rM.png?auto=webp&amp;s=73d3e4c6457f8e6bbd5981b68295bab611fbfe97","width":1200,"height":600},"resolutions":[{"url":"https://external-preview.redd.it/ky4q-HJ1F3S2UdCuaGkloWjj4Ru8GaNbo0jpnr086rM.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=3b376e1aa1902b7556bb3536cbf55124d2711777","width":108,"height":54},{"url":"https://external-preview.redd.it/ky4q-HJ1F3S2UdCuaGkloWjj4Ru8GaNbo0jpnr086rM.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=6dfe7ffd40bbab26c0485457d0a2fff177288a62","width":216,"height":108},{"url":"https://external-preview.redd.it/ky4q-HJ1F3S2UdCuaGkloWjj4Ru8GaNbo0jpnr086rM.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=1337de2e84c021178d03af21aca3ecc6af91df4e","width":320,"height":160},{"url":"https://external-preview.redd.it/ky4q-HJ1F3S2UdCuaGkloWjj4Ru8GaNbo0jpnr086rM.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=1bf7ee1161ce602fb904e807403f07417ec4d842","width":640,"height":320},{"url":"https://external-preview.redd.it/ky4q-HJ1F3S2UdCuaGkloWjj4Ru8GaNbo0jpnr086rM.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=f17963fe73bb28a9dad1f18d1e3455c063524ece","width":960,"height":480},{"url":"https://external-preview.redd.it/ky4q-HJ1F3S2UdCuaGkloWjj4Ru8GaNbo0jpnr086rM.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b3c52c8d7e5cfd23bac748411d52d2f33ea125f0","width":1080,"height":540}],"variants":{},"id":"ky4q-HJ1F3S2UdCuaGkloWjj4Ru8GaNbo0jpnr086rM"}],"enabled":false},"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"7a7848d2-bf8e-11ed-8c2f-765d15199f78","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#94e044","id":"1lm66fy","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"amranu","discussion_type":null,"num_comments":5,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lm66fy/i_need_help_testing_my_agentic_wrapper_for_llms/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lm66fy/i_need_help_testing_my_agentic_wrapper_for_llms/","subreddit_subscribers":492627,"created_utc":1751061087,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0778t2","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ToxiCookies","can_mod_post":false,"send_replies":true,"parent_id":"t1_n075ufl","score":1,"author_fullname":"t2_l4v9b2h","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Sounds good! I'll probably text again in the morning, no rush","edited":false,"author_flair_css_class":null,"name":"t1_n0778t2","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Sounds good! I&amp;#39;ll probably text again in the morning, no rush&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lm66fy","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lm66fy/i_need_help_testing_my_agentic_wrapper_for_llms/n0778t2/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751091584,"author_flair_text":null,"collapsed":false,"created_utc":1751091584,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n075ufl","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"amranu","can_mod_post":false,"send_replies":true,"parent_id":"t1_n070e7y","score":2,"author_fullname":"t2_3yvyd","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Alright, looks like there was a flaw in the detection logic. I've setup ollama on my machine (can't run models worth anything but can test detection) and, after fixing the issue, it's working.\\n\\nI've pushed the change to github. Good luck!\\n\\nEDIT: Okay, detection works but model output doesn't. Hold up for a bit and I'll get that fixed.\\n\\nEDIT2: Model output should now work. Tool use doesn't seem to be working with qwen3:1.7b. Might work for other models though.","edited":1751092231,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n075ufl","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Alright, looks like there was a flaw in the detection logic. I&amp;#39;ve setup ollama on my machine (can&amp;#39;t run models worth anything but can test detection) and, after fixing the issue, it&amp;#39;s working.&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;ve pushed the change to github. Good luck!&lt;/p&gt;\\n\\n&lt;p&gt;EDIT: Okay, detection works but model output doesn&amp;#39;t. Hold up for a bit and I&amp;#39;ll get that fixed.&lt;/p&gt;\\n\\n&lt;p&gt;EDIT2: Model output should now work. Tool use doesn&amp;#39;t seem to be working with qwen3:1.7b. Might work for other models though.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lm66fy","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lm66fy/i_need_help_testing_my_agentic_wrapper_for_llms/n075ufl/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751090845,"author_flair_text":null,"treatment_tags":[],"created_utc":1751090845,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n070e7y","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ToxiCookies","can_mod_post":false,"send_replies":true,"parent_id":"t1_n06zenh","score":1,"author_fullname":"t2_l4v9b2h","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"executing \\"curl http://localhost:11434\\" from inside WSL returns \\"Ollama is running\\", and curling the url with the provided API tags successfully returns my list of installed models. Exporting the URL didn't change anything (I assume localhost is default anyway).","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n070e7y","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;executing &amp;quot;curl http://localhost:11434&amp;quot; from inside WSL returns &amp;quot;Ollama is running&amp;quot;, and curling the url with the provided API tags successfully returns my list of installed models. Exporting the URL didn&amp;#39;t change anything (I assume localhost is default anyway).&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lm66fy","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lm66fy/i_need_help_testing_my_agentic_wrapper_for_llms/n070e7y/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751088022,"author_flair_text":null,"treatment_tags":[],"created_utc":1751088022,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n06zenh","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"amranu","can_mod_post":false,"created_utc":1751087516,"send_replies":true,"parent_id":"t1_n06utjm","score":2,"author_fullname":"t2_3yvyd","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Ah okay, yeah it'll only be checking the WSL image for ollama, not your windows host. \\n\\nHere's what Claude suggests you try:\\n  Solution for WSL + Windows Ollama\\n\\n  1. Set the correct Ollama base URL for WSL:\\n  export OLLAMA_BASE_URL=\\"http://host.docker.internal:11434\\"\\n  2. Test the connection first:\\n  curl http://host.docker.internal:11434/api/tags\\n  3. If that doesn't work, find your Windows IP:\\n  # Get Windows host IP from WSL\\n  cat /etc/resolv.conf | grep nameserver | awk '{print $2}'\\n  # Or try\\n  ip route show | grep default | awk '{print $3}'\\n  4. Then use that IP:\\n  export OLLAMA_BASE_URL=\\"http://YOUR_WINDOWS_IP:11434\\"","edited":1751087777,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n06zenh","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Ah okay, yeah it&amp;#39;ll only be checking the WSL image for ollama, not your windows host. &lt;/p&gt;\\n\\n&lt;p&gt;Here&amp;#39;s what Claude suggests you try:\\n  Solution for WSL + Windows Ollama&lt;/p&gt;\\n\\n&lt;ol&gt;\\n&lt;li&gt;Set the correct Ollama base URL for WSL:\\nexport OLLAMA_BASE_URL=&amp;quot;&lt;a href=\\"http://host.docker.internal:11434\\"&gt;http://host.docker.internal:11434&lt;/a&gt;&amp;quot;&lt;/li&gt;\\n&lt;li&gt;Test the connection first:\\ncurl &lt;a href=\\"http://host.docker.internal:11434/api/tags\\"&gt;http://host.docker.internal:11434/api/tags&lt;/a&gt;&lt;/li&gt;\\n&lt;li&gt;If that doesn&amp;#39;t work, find your Windows IP:\\n# Get Windows host IP from WSL\\ncat /etc/resolv.conf | grep nameserver | awk &amp;#39;{print $2}&amp;#39;\\n# Or try\\nip route show | grep default | awk &amp;#39;{print $3}&amp;#39;&lt;/li&gt;\\n&lt;li&gt;Then use that IP:\\nexport OLLAMA_BASE_URL=&amp;quot;http://YOUR_WINDOWS_IP:11434&amp;quot;&lt;/li&gt;\\n&lt;/ol&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lm66fy","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lm66fy/i_need_help_testing_my_agentic_wrapper_for_llms/n06zenh/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751087516,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n06utjm","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ToxiCookies","can_mod_post":false,"created_utc":1751085228,"send_replies":true,"parent_id":"t3_1lm66fy","score":1,"author_fullname":"t2_l4v9b2h","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Hey, I've tried my hand at getting this to work, but so far it does not seem to find Ollama. I'll keep tinkering, using WSL mcp-agent install with Ollama on the Windows host.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n06utjm","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Hey, I&amp;#39;ve tried my hand at getting this to work, but so far it does not seem to find Ollama. I&amp;#39;ll keep tinkering, using WSL mcp-agent install with Ollama on the Windows host.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lm66fy/i_need_help_testing_my_agentic_wrapper_for_llms/n06utjm/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751085228,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lm66fy","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),n=()=>e.jsx(t,{data:l});export{n as default};
