import{j as e}from"./index-DOAmItP2.js";import{R as t}from"./RedditPostRenderer-KKgzpPpv.js";import"./index-YSfz60vQ.js";const l=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"I’ve been working on a local document Q\\\\\\\\&amp;A pipeline using LLaMA (mainly 7B and Mixtral variants), and a big bottleneck for me is handling scanned PDFs or image-based documents. Most of what I’m working with isn’t born-digital, stuff like manuals, invoices, policy documents, etc., usually scanned from print.\\n\\n\\n\\nBefore pushing these into a vector store or embedding pipeline, I need a preprocessor that can handle:\\n\\n\\\\- OCR (ideally layout-aware)\\n\\n\\\\- Tables and multi-column text\\n\\n\\\\- Some basic structure retention (headings, sections, etc.)\\n\\n\\\\- Minimal hallucination or text merging\\n\\n\\n\\nTesseract works okay, but it often butchers formatting or outputs noisy segments that don’t embed well. I’ve tried some DIY solutions with OpenCV + Tesseract + some Python logic, but it gets pretty messy.\\n\\n\\n\\nAre there any tools you’ve had success with for preprocessing scanned documents before feeding them into Local LLaMA setups? Open to open-source tools or minimal local deployments - privacy is important here, so I’m avoiding cloud APIs.\\n\\n","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"What are some good preprocessors for scanned documents in the LocalLLaMA use case?","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lp1nn5","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":1,"author_flair_background_color":null,"subreddit_type":"public","ups":14,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_14buuu1e74","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":14,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1751376420,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;I’ve been working on a local document Q\\\\&amp;amp;A pipeline using LLaMA (mainly 7B and Mixtral variants), and a big bottleneck for me is handling scanned PDFs or image-based documents. Most of what I’m working with isn’t born-digital, stuff like manuals, invoices, policy documents, etc., usually scanned from print.&lt;/p&gt;\\n\\n&lt;p&gt;Before pushing these into a vector store or embedding pipeline, I need a preprocessor that can handle:&lt;/p&gt;\\n\\n&lt;p&gt;- OCR (ideally layout-aware)&lt;/p&gt;\\n\\n&lt;p&gt;- Tables and multi-column text&lt;/p&gt;\\n\\n&lt;p&gt;- Some basic structure retention (headings, sections, etc.)&lt;/p&gt;\\n\\n&lt;p&gt;- Minimal hallucination or text merging&lt;/p&gt;\\n\\n&lt;p&gt;Tesseract works okay, but it often butchers formatting or outputs noisy segments that don’t embed well. I’ve tried some DIY solutions with OpenCV + Tesseract + some Python logic, but it gets pretty messy.&lt;/p&gt;\\n\\n&lt;p&gt;Are there any tools you’ve had success with for preprocessing scanned documents before feeding them into Local LLaMA setups? Open to open-source tools or minimal local deployments - privacy is important here, so I’m avoiding cloud APIs.&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1lp1nn5","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"Abelmageto","discussion_type":null,"num_comments":2,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lp1nn5/what_are_some_good_preprocessors_for_scanned/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lp1nn5/what_are_some_good_preprocessors_for_scanned/","subreddit_subscribers":493458,"created_utc":1751376420,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0rb423","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"DigiDadaist","can_mod_post":false,"created_utc":1751377774,"send_replies":true,"parent_id":"t3_1lp1nn5","score":4,"author_fullname":"t2_tgjhzp7a9","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Check out [Docling](https://docling-project.github.io/docling/usage/#model-prefetching-and-offline-usage)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0rb423","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Check out &lt;a href=\\"https://docling-project.github.io/docling/usage/#model-prefetching-and-offline-usage\\"&gt;Docling&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp1nn5/what_are_some_good_preprocessors_for_scanned/n0rb423/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751377774,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lp1nn5","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0rfz31","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"EeKy_YaYoH","can_mod_post":false,"created_utc":1751379295,"send_replies":true,"parent_id":"t3_1lp1nn5","score":1,"author_fullname":"t2_g8ywk5zc","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I’ve been in a similar spot, and recently started with OCRFlux which might be worth looking into. I think it's kind of a modern alternative to the Tesseract + patchwork approach.\\n\\nIt preserves tables, column layouts, and headings surprisingly well, outputs structured JSON or plain text with some visual segmentation logic (which helps for chunking). It also plays nicely with downstream workflows - I’ve used it as the first step before embedding chunks into a local vector DB for RAG with LLaMA.\\n\\nIf you go this route, I'd suggest:\\n1. Post-processing the OCRFlux output to break into logical sections (based on layout or headers)\\n2. Removing boilerplate headers/footers if your docs have them. They can pollute embeddings\\n3. Running a QA test set against your local LLaMA after preprocessing to catch subtle formatting losses.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0rfz31","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I’ve been in a similar spot, and recently started with OCRFlux which might be worth looking into. I think it&amp;#39;s kind of a modern alternative to the Tesseract + patchwork approach.&lt;/p&gt;\\n\\n&lt;p&gt;It preserves tables, column layouts, and headings surprisingly well, outputs structured JSON or plain text with some visual segmentation logic (which helps for chunking). It also plays nicely with downstream workflows - I’ve used it as the first step before embedding chunks into a local vector DB for RAG with LLaMA.&lt;/p&gt;\\n\\n&lt;p&gt;If you go this route, I&amp;#39;d suggest:\\n1. Post-processing the OCRFlux output to break into logical sections (based on layout or headers)\\n2. Removing boilerplate headers/footers if your docs have them. They can pollute embeddings\\n3. Running a QA test set against your local LLaMA after preprocessing to catch subtle formatting losses.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp1nn5/what_are_some_good_preprocessors_for_scanned/n0rfz31/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751379295,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lp1nn5","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),s=()=>e.jsx(t,{data:l});export{s as default};
