import{j as e}from"./index-BlGsFJYy.js";import{R as t}from"./RedditPostRenderer-B6uvq_Zl.js";import"./index-DDvVtNwD.js";const l=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"I'm trying to figure out a formula to calculate the tokens/s when I run an LLM on a CPU. I always deploy small models on different devices, and I know that RAM MHz is the most important factor, but is it the only one? What about the CPU single/multi core benchmark? Does AMD's GPU have anything to do with this? Can I just have a function that, given the hardware, LLM size, and quantization parameters, can give me an estimate of the speed in tokens per second?\\n","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"How can I figure out the speed in tokens per second that my model will run on the CPU?","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lyt372","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.67,"author_flair_background_color":null,"subreddit_type":"public","ups":2,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_9wlqkcl5","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":2,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1752414028,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;m trying to figure out a formula to calculate the tokens/s when I run an LLM on a CPU. I always deploy small models on different devices, and I know that RAM MHz is the most important factor, but is it the only one? What about the CPU single/multi core benchmark? Does AMD&amp;#39;s GPU have anything to do with this? Can I just have a function that, given the hardware, LLM size, and quantization parameters, can give me an estimate of the speed in tokens per second?&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1lyt372","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"Holiday-Picture6796","discussion_type":null,"num_comments":4,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lyt372/how_can_i_figure_out_the_speed_in_tokens_per/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lyt372/how_can_i_figure_out_the_speed_in_tokens_per/","subreddit_subscribers":498850,"created_utc":1752414028,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2wcddh","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Karyo_Ten","can_mod_post":false,"created_utc":1752414844,"send_replies":true,"parent_id":"t3_1lyt372","score":2,"author_fullname":"t2_tbdqg","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"As long as your CPU perf is over a threshold easily cleared by any CPU from the past 5 years (AVX2 or AVX512), you'll be memory-bandwidth-limited.\\n\\nMulticore doesn't really matter as long as you have 6~8 cores or more and even then.\\n\\nLocal LLMs use matrix-vector multiplication (unlike batched inference for cloud multiplication) which is memory-bound as there is only a single stream of token to predict.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2wcddh","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;As long as your CPU perf is over a threshold easily cleared by any CPU from the past 5 years (AVX2 or AVX512), you&amp;#39;ll be memory-bandwidth-limited.&lt;/p&gt;\\n\\n&lt;p&gt;Multicore doesn&amp;#39;t really matter as long as you have 6~8 cores or more and even then.&lt;/p&gt;\\n\\n&lt;p&gt;Local LLMs use matrix-vector multiplication (unlike batched inference for cloud multiplication) which is memory-bound as there is only a single stream of token to predict.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyt372/how_can_i_figure_out_the_speed_in_tokens_per/n2wcddh/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752414844,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lyt372","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2wjt4k","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AppearanceHeavy6724","can_mod_post":false,"created_utc":1752417272,"send_replies":true,"parent_id":"t3_1lyt372","score":1,"author_fullname":"t2_uz37qfx5","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"&gt; I know that RAM MHz is the most important factor, but is it the only one\\n\\nPrompt processing is faster on more powerful cpus, still very slow. Token Generation on anything more powerful than i5 of 10 years ago will depend only on memory bandwidth, period.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2wjt4k","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;I know that RAM MHz is the most important factor, but is it the only one&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;Prompt processing is faster on more powerful cpus, still very slow. Token Generation on anything more powerful than i5 of 10 years ago will depend only on memory bandwidth, period.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyt372/how_can_i_figure_out_the_speed_in_tokens_per/n2wjt4k/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752417272,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lyt372","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2x7gac","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MelodicRecognition7","can_mod_post":false,"created_utc":1752424431,"send_replies":true,"parent_id":"t3_1lyt372","score":1,"author_fullname":"t2_1eex9ug5","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"you need more cores for faster prompt processing but at some moment (~at some specific amount of threads) you will hit the memory speed bottleneck. So the memory bandwidth is still the most important factor.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2x7gac","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;you need more cores for faster prompt processing but at some moment (~at some specific amount of threads) you will hit the memory speed bottleneck. So the memory bandwidth is still the most important factor.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyt372/how_can_i_figure_out_the_speed_in_tokens_per/n2x7gac/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752424431,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lyt372","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n314en4","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"triynizzles1","can_mod_post":false,"created_utc":1752473126,"send_replies":true,"parent_id":"t3_1lyt372","score":1,"author_fullname":"t2_zr0g49ixt","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Take your system memory bandwidth and divided by the size if the model in memory.\\n\\n50gb per second (average speed for DDR4) divided by 5gb sized model = 10 tokens per second.\\n\\n50/5=10\\n\\nLets say you have 128gb of ddr4 at 50gb/s and are running a 70b mode at q4:\\n\\n50gb/s divided by 42gb (rough size of model with decent context window) 1.1 tokens per second than.\\n\\n50/42=1.19\\n\\nDdr5 is a little faster bandwidth.\\n\\nThe main bottleneck for inferencing is memory bandwidth. not cores, compute or AI tops.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n314en4","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Take your system memory bandwidth and divided by the size if the model in memory.&lt;/p&gt;\\n\\n&lt;p&gt;50gb per second (average speed for DDR4) divided by 5gb sized model = 10 tokens per second.&lt;/p&gt;\\n\\n&lt;p&gt;50/5=10&lt;/p&gt;\\n\\n&lt;p&gt;Lets say you have 128gb of ddr4 at 50gb/s and are running a 70b mode at q4:&lt;/p&gt;\\n\\n&lt;p&gt;50gb/s divided by 42gb (rough size of model with decent context window) 1.1 tokens per second than.&lt;/p&gt;\\n\\n&lt;p&gt;50/42=1.19&lt;/p&gt;\\n\\n&lt;p&gt;Ddr5 is a little faster bandwidth.&lt;/p&gt;\\n\\n&lt;p&gt;The main bottleneck for inferencing is memory bandwidth. not cores, compute or AI tops.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyt372/how_can_i_figure_out_the_speed_in_tokens_per/n314en4/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752473126,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lyt372","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),r=()=>e.jsx(t,{data:l});export{r as default};
