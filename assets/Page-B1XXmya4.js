import{j as e}from"./index-BpC9hjVs.js";import{R as l}from"./RedditPostRenderer-BEo6AnSR.js";import"./index-DwkJHX1_.js";const a=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"I apologize if this has been asked before, or asked often but i personally couldn't find anything solid through self-research or scrolling through this reddit feed. Maybe I just don't know what i'm looking for, idk. **Are there any GOOD local AI text to voice models that can work independently/and with a local SLM/LLM?** I'm really trying to give my home assistant a voice/have web articles, pdfs, and ebooks read to me. **MUST** be able to run **LOCALLY**. Preferably free or non-subscription payment. Thank you all in advance and I hope you all are having a good day/night.","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Best Model For Text-To-Audio &amp; Voice Assistant?","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lnxml5","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.67,"author_flair_background_color":null,"subreddit_type":"public","ups":2,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_1fu4ornofy","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":2,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":1751254891,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1751254683,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;I apologize if this has been asked before, or asked often but i personally couldn&amp;#39;t find anything solid through self-research or scrolling through this reddit feed. Maybe I just don&amp;#39;t know what i&amp;#39;m looking for, idk. &lt;strong&gt;Are there any GOOD local AI text to voice models that can work independently/and with a local SLM/LLM?&lt;/strong&gt; I&amp;#39;m really trying to give my home assistant a voice/have web articles, pdfs, and ebooks read to me. &lt;strong&gt;MUST&lt;/strong&gt; be able to run &lt;strong&gt;LOCALLY&lt;/strong&gt;. Preferably free or non-subscription payment. Thank you all in advance and I hope you all are having a good day/night.&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1lnxml5","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"ExcogitationMG","discussion_type":null,"num_comments":11,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lnxml5/best_model_for_texttoaudio_voice_assistant/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lnxml5/best_model_for_texttoaudio_voice_assistant/","subreddit_subscribers":493240,"created_utc":1751254683,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0lner6","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ExcogitationMG","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0k110k","score":1,"author_fullname":"t2_1fu4ornofy","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"thank you very much","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0lner6","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;thank you very much&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnxml5","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnxml5/best_model_for_texttoaudio_voice_assistant/n0lner6/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751300651,"author_flair_text":null,"treatment_tags":[],"created_utc":1751300651,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0k110k","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Evening_Ad6637","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0jkdbc","score":2,"author_fullname":"t2_p45er6oo","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yes","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0k110k","is_submitter":false,"collapsed":false,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yes&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnxml5","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnxml5/best_model_for_texttoaudio_voice_assistant/n0k110k/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751280548,"author_flair_text":"llama.cpp","treatment_tags":[],"created_utc":1751280548,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n0jkdbc","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ExcogitationMG","can_mod_post":false,"created_utc":1751270716,"send_replies":true,"parent_id":"t1_n0j174w","score":1,"author_fullname":"t2_1fu4ornofy","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"i see it says Open AI Compatible. To be clear, it would work with something like a Llama 70B Model right?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0jkdbc","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;i see it says Open AI Compatible. To be clear, it would work with something like a Llama 70B Model right?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnxml5","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnxml5/best_model_for_texttoaudio_voice_assistant/n0jkdbc/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751270716,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0j174w","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"miki4242","can_mod_post":false,"created_utc":1751259842,"send_replies":true,"parent_id":"t3_1lnxml5","score":3,"author_fullname":"t2_fj5ar","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I'm using Speaches (the successor to faster-whisper-server automatic speech recognition), which now also offers text-to-speech using Piper and Kokoro. I think the Kokoro 82M v1.0 ONNX model has a nice selection of good quality voices. Check it out on GitHub: https://github.com/speaches-ai/speaches .","edited":1751260208,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0j174w","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;m using Speaches (the successor to faster-whisper-server automatic speech recognition), which now also offers text-to-speech using Piper and Kokoro. I think the Kokoro 82M v1.0 ONNX model has a nice selection of good quality voices. Check it out on GitHub: &lt;a href=\\"https://github.com/speaches-ai/speaches\\"&gt;https://github.com/speaches-ai/speaches&lt;/a&gt; .&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnxml5/best_model_for_texttoaudio_voice_assistant/n0j174w/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751259842,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnxml5","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0nfy9b","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ArsNeph","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0n17bh","score":1,"author_fullname":"t2_vt0xkv60d","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"No unfortunately that's not how it works. A Strix Halo only has about 212 GB/s of memory bandwidth, comparative to something like an RTX 3090 with 936GB/s, making it nearly five times slower. Running inference with multiple GPUs in parallel, Tensor parallelism can help with speed, but I don't believe that's supported on Strix Halo. You should expect about 5 tk/s at a 4 bit quant, with low context. Instead of running dense models, you would be better off running MoEs, such as Qwen 3 30B A3 MoE, Qwen 235B MoE, or even the new Hunyuan 80B A13 MoE. That said, for the price of $1700 a piece, I'm not sure that they're worth it.","edited":false,"author_flair_css_class":null,"name":"t1_n0nfy9b","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;No unfortunately that&amp;#39;s not how it works. A Strix Halo only has about 212 GB/s of memory bandwidth, comparative to something like an RTX 3090 with 936GB/s, making it nearly five times slower. Running inference with multiple GPUs in parallel, Tensor parallelism can help with speed, but I don&amp;#39;t believe that&amp;#39;s supported on Strix Halo. You should expect about 5 tk/s at a 4 bit quant, with low context. Instead of running dense models, you would be better off running MoEs, such as Qwen 3 30B A3 MoE, Qwen 235B MoE, or even the new Hunyuan 80B A13 MoE. That said, for the price of $1700 a piece, I&amp;#39;m not sure that they&amp;#39;re worth it.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lnxml5","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnxml5/best_model_for_texttoaudio_voice_assistant/n0nfy9b/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751319184,"author_flair_text":null,"collapsed":false,"created_utc":1751319184,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0n17bh","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ExcogitationMG","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0ms2b0","score":1,"author_fullname":"t2_1fu4ornofy","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I have no issues with making a cluster to speed things up, I knew one Strix Halo Mainboard could run a 70B Model, albeit slowly, so adding another Mainboard should speed that up to normal speeds. SD &amp; Kokoro can share the third Strix Halo Mainboard. I have other things to run like Security Camera's with AI facial recognition, so i'll figure out how many i need for the cluster in the end.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0n17bh","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I have no issues with making a cluster to speed things up, I knew one Strix Halo Mainboard could run a 70B Model, albeit slowly, so adding another Mainboard should speed that up to normal speeds. SD &amp;amp; Kokoro can share the third Strix Halo Mainboard. I have other things to run like Security Camera&amp;#39;s with AI facial recognition, so i&amp;#39;ll figure out how many i need for the cluster in the end.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnxml5","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnxml5/best_model_for_texttoaudio_voice_assistant/n0n17bh/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751314875,"author_flair_text":null,"treatment_tags":[],"created_utc":1751314875,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0ms2b0","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ArsNeph","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0m20jp","score":2,"author_fullname":"t2_vt0xkv60d","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"No, so not exactly. A 70B parameter LLM would require about 48GB of VRAM to run at a decent speed. SD 1.5 would require about 8GB, SDXL about 12-16GB, depending on controlnets and upscaling. That's 60-64GB. XTTS is likely about 2-4GB, Dia and Zonos are more like 6-8GB. Kokoro is 82 million parameters, not billion, requiring maybe 0.5GB at most. Kokoro can easily run on a raspberry Pi if you want it to. \\n\\nYou technically only need a single AMD Halo strix for your whole setup, but the memory bandwidth is incredibly low on them, meaning a 70B model will be very slow. Diffusion models are also very compute-intensive, so they will also be slow. Only Kokoro will run quickly.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0ms2b0","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;No, so not exactly. A 70B parameter LLM would require about 48GB of VRAM to run at a decent speed. SD 1.5 would require about 8GB, SDXL about 12-16GB, depending on controlnets and upscaling. That&amp;#39;s 60-64GB. XTTS is likely about 2-4GB, Dia and Zonos are more like 6-8GB. Kokoro is 82 million parameters, not billion, requiring maybe 0.5GB at most. Kokoro can easily run on a raspberry Pi if you want it to. &lt;/p&gt;\\n\\n&lt;p&gt;You technically only need a single AMD Halo strix for your whole setup, but the memory bandwidth is incredibly low on them, meaning a 70B model will be very slow. Diffusion models are also very compute-intensive, so they will also be slow. Only Kokoro will run quickly.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnxml5","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnxml5/best_model_for_texttoaudio_voice_assistant/n0ms2b0/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751312172,"author_flair_text":null,"treatment_tags":[],"created_utc":1751312172,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n0m20jp","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ExcogitationMG","can_mod_post":false,"created_utc":1751304739,"send_replies":true,"parent_id":"t1_n0ltmjm","score":1,"author_fullname":"t2_1fu4ornofy","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"truly thank you. I'm liking XTTS for a project i have in mind that'll require training, but for the rest of them, Kokoro, Dia, &amp; Zonos sound great based on samples. How VRAM intensive are we talking here?\\n\\nCause one of these has to work in tandem with a 70B LLM Model &amp; Stable Diffusion, so if SD requires 16GB, &amp; 70B requires 256GB \\\\[Two Framework AMD Halo Strix's in a Cluster to run fast\\\\], then Kokoro 82M is 82 Billion parameter (according to what i read on Hugging face), which i think would require two more AMD Mainboards to run fast. But you tell me if I'm off or not in this estimation.","edited":1751305614,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0m20jp","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;truly thank you. I&amp;#39;m liking XTTS for a project i have in mind that&amp;#39;ll require training, but for the rest of them, Kokoro, Dia, &amp;amp; Zonos sound great based on samples. How VRAM intensive are we talking here?&lt;/p&gt;\\n\\n&lt;p&gt;Cause one of these has to work in tandem with a 70B LLM Model &amp;amp; Stable Diffusion, so if SD requires 16GB, &amp;amp; 70B requires 256GB [Two Framework AMD Halo Strix&amp;#39;s in a Cluster to run fast], then Kokoro 82M is 82 Billion parameter (according to what i read on Hugging face), which i think would require two more AMD Mainboards to run fast. But you tell me if I&amp;#39;m off or not in this estimation.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnxml5","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnxml5/best_model_for_texttoaudio_voice_assistant/n0m20jp/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751304739,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0ltmjm","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ArsNeph","can_mod_post":false,"created_utc":1751302438,"send_replies":true,"parent_id":"t3_1lnxml5","score":2,"author_fullname":"t2_vt0xkv60d","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"You're probably searching for the wrong keyword, they're generally abbreviated as TTS, and there are tons. XTTS, Zonos, Dia, and Kokoro are some of the newer ones. Many of these are VRAM intensive though, so for a lightweight model I recommend Kokoro","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0ltmjm","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You&amp;#39;re probably searching for the wrong keyword, they&amp;#39;re generally abbreviated as TTS, and there are tons. XTTS, Zonos, Dia, and Kokoro are some of the newer ones. Many of these are VRAM intensive though, so for a lightweight model I recommend Kokoro&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnxml5/best_model_for_texttoaudio_voice_assistant/n0ltmjm/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751302438,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnxml5","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0mkd13","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ExcogitationMG","can_mod_post":false,"created_utc":1751309934,"send_replies":true,"parent_id":"t1_n0mgm7h","score":1,"author_fullname":"t2_1fu4ornofy","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"well speed &amp; reliability. I am using this as a Alexa Replacement/Office Personal Assisstant. So it needs to be reasonably quick and responsive and give accurate information. The server will be attached to various IoT devices, majority of which will be used simultaneously.","edited":1751310943,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0mkd13","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;well speed &amp;amp; reliability. I am using this as a Alexa Replacement/Office Personal Assisstant. So it needs to be reasonably quick and responsive and give accurate information. The server will be attached to various IoT devices, majority of which will be used simultaneously.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnxml5","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnxml5/best_model_for_texttoaudio_voice_assistant/n0mkd13/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751309934,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0mgm7h","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MaruluVR","can_mod_post":false,"created_utc":1751308847,"send_replies":true,"parent_id":"t3_1lnxml5","score":2,"author_fullname":"t2_10ryluzwb5","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"If you are going only for speed the best combinations is piper for tts + Qwen 3 30b A3B, that should keep the latency down. You can also use slower TTS models as long as their software supports streaming.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0mgm7h","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;If you are going only for speed the best combinations is piper for tts + Qwen 3 30b A3B, that should keep the latency down. You can also use slower TTS models as long as their software supports streaming.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnxml5/best_model_for_texttoaudio_voice_assistant/n0mgm7h/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751308847,"author_flair_text":"llama.cpp","treatment_tags":[],"link_id":"t3_1lnxml5","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}}]`),n=()=>e.jsx(l,{data:a});export{n as default};
