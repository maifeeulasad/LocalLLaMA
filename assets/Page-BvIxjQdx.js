import{j as e}from"./index-BgwOAK4-.js";import{R as l}from"./RedditPostRenderer-BOBjDTFu.js";import"./index-BL22wVg5.js";const t=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Benchmarking Inference Engines and talking about metrics like TTFT, TPOT, and ITL.","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"vLLM vs SGLang vs MAX â€” Who's the fastest?","link_flair_richtext":[{"e":"text","t":"Resources"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":73,"top_awarded_type":null,"hide_score":false,"name":"t3_1lvglk7","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.92,"author_flair_background_color":null,"ups":28,"total_awards_received":0,"media_embed":{},"thumbnail_width":140,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_6ort7d94","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Resources","can_mod_post":false,"score":28,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"https://external-preview.redd.it/pRoQV1isngBk6d9PokHUBrsWFKXDdEPabl0qiiWLOq0.png?width=140&amp;height=73&amp;crop=140:73,smart&amp;auto=webp&amp;s=7213745313f6282ed8d97a9461d3203fa8b93b47","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"post_hint":"link","content_categories":null,"is_self":false,"subreddit_type":"public","created":1752061332,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"ersteiger.com","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Benchmarking Inference Engines and talking about metrics like TTFT, TPOT, and ITL.&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"url_overridden_by_dest":"https://www.ersteiger.com/posts/vllm-vs-max/","view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"preview":{"images":[{"source":{"url":"https://external-preview.redd.it/pRoQV1isngBk6d9PokHUBrsWFKXDdEPabl0qiiWLOq0.png?auto=webp&amp;s=07b7750f7f07c57f909c2e58365c9545864a9bd6","width":1200,"height":630},"resolutions":[{"url":"https://external-preview.redd.it/pRoQV1isngBk6d9PokHUBrsWFKXDdEPabl0qiiWLOq0.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=372cf45b99b811f00477201d8509803c02ad5701","width":108,"height":56},{"url":"https://external-preview.redd.it/pRoQV1isngBk6d9PokHUBrsWFKXDdEPabl0qiiWLOq0.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=0357d9d8ce069daeaebd8339c89c4474809a9e86","width":216,"height":113},{"url":"https://external-preview.redd.it/pRoQV1isngBk6d9PokHUBrsWFKXDdEPabl0qiiWLOq0.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=f5dee7f01d95844ae616fb9a02dda03d3792b254","width":320,"height":168},{"url":"https://external-preview.redd.it/pRoQV1isngBk6d9PokHUBrsWFKXDdEPabl0qiiWLOq0.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=e424426a6b9ec4c92da8acc5c9c81fb4ecc20805","width":640,"height":336},{"url":"https://external-preview.redd.it/pRoQV1isngBk6d9PokHUBrsWFKXDdEPabl0qiiWLOq0.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=b42e77f502000ed0f3eb155266e4158533a3cf97","width":960,"height":504},{"url":"https://external-preview.redd.it/pRoQV1isngBk6d9PokHUBrsWFKXDdEPabl0qiiWLOq0.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=c8bd55c4d861d06343f61d261e5b22b43bacd0b0","width":1080,"height":567}],"variants":{},"id":"pRoQV1isngBk6d9PokHUBrsWFKXDdEPabl0qiiWLOq0"}],"enabled":false},"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"mod_note":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"num_reports":null,"removal_reason":null,"link_flair_background_color":"#ccac2b","id":"1lvglk7","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"rkstgr","discussion_type":null,"num_comments":13,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lvglk7/vllm_vs_sglang_vs_max_whos_the_fastest/","stickied":false,"url":"https://www.ersteiger.com/posts/vllm-vs-max/","subreddit_subscribers":497025,"created_utc":1752061332,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n28x2pp","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"plankalkul-z1","can_mod_post":false,"send_replies":true,"parent_id":"t1_n28sw8j","score":3,"author_fullname":"t2_w73n3yrsx","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Thank you for the detailed answer.\\n\\n\\nIt seems to me the only sure way to find out what works and what doesn't is to install MAX and try it out...\\n\\n\\nThanks for the link as well (https://docs.modular.com/max/changelog/#v252-2025-03-25). The impression that I get from what's there is that TP is supported on a per model architecture basis. Again, will have to try it out to confirm.\\n\\n\\nI am well aware of Mojo, but didn't know about its association with Modular, so thanks again.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n28x2pp","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thank you for the detailed answer.&lt;/p&gt;\\n\\n&lt;p&gt;It seems to me the only sure way to find out what works and what doesn&amp;#39;t is to install MAX and try it out...&lt;/p&gt;\\n\\n&lt;p&gt;Thanks for the link as well (&lt;a href=\\"https://docs.modular.com/max/changelog/#v252-2025-03-25\\"&gt;https://docs.modular.com/max/changelog/#v252-2025-03-25&lt;/a&gt;). The impression that I get from what&amp;#39;s there is that TP is supported on a per model architecture basis. Again, will have to try it out to confirm.&lt;/p&gt;\\n\\n&lt;p&gt;I am well aware of Mojo, but didn&amp;#39;t know about its association with Modular, so thanks again.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lvglk7","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lvglk7/vllm_vs_sglang_vs_max_whos_the_fastest/n28x2pp/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752094776,"author_flair_text":null,"treatment_tags":[],"created_utc":1752094776,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2bvgie","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"rkstgr","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2bvbor","score":1,"author_fullname":"t2_6ort7d94","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"[https://www.youtube.com/watch?v=04\\\\_gN-C9IAo](https://www.youtube.com/watch?v=04_gN-C9IAo)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2bvgie","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;&lt;a href=\\"https://www.youtube.com/watch?v=04_gN-C9IAo\\"&gt;https://www.youtube.com/watch?v=04_gN-C9IAo&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lvglk7","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lvglk7/vllm_vs_sglang_vs_max_whos_the_fastest/n2bvgie/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752136306,"author_flair_text":null,"treatment_tags":[],"created_utc":1752136306,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2bvbor","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"troposfer","can_mod_post":false,"send_replies":true,"parent_id":"t1_n28sw8j","score":1,"author_fullname":"t2_51jnxfio","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Which podcast? Can we run MAX on m4 max ?","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2bvbor","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Which podcast? Can we run MAX on m4 max ?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lvglk7","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lvglk7/vllm_vs_sglang_vs_max_whos_the_fastest/n2bvbor/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752136226,"author_flair_text":null,"treatment_tags":[],"created_utc":1752136226,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n28sw8j","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"rkstgr","can_mod_post":false,"created_utc":1752093639,"send_replies":true,"parent_id":"t1_n27ttjv","score":4,"author_fullname":"t2_6ort7d94","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"MAX is by Modular the company behind Mojo, known for claiming being 10.000x faster than Python. Mojo is a new language that is (kinda) compatible with Python, but compiles to machine-code (you can also run it via JIT compilation). A few months ago there where some shady benchmarks where they claimed being faster that Rust but then they did not compile using the release flag. nevertheless, Modular is on a mission to re-build the AI inference stack without CUDA, they have demos where they can run LLMs on AMD and NVIDIA hardware on their native mojo/max stack without CUDA (which is nice because the container images are like 1Gb compared to 5Gb)\\n\\nThat being sad, they are Python compatible in so far as it should be possible to download any HF model and run it.\\n\\nThey have their own model library because these models are (presumably) optimized and reimplemented in Mojo/Max and should show improved performance. No clue so far which quants are supported atm / and also multi-modality. But very nice questions.\\n\\nI just recently watched a podcast with Chris Lattner (CEO of Modular, Creator of LLVM, Clang, MLIR) and they claimed being faster with MAX than vLLM on A100 and H100 and I wanted to check that out\\n\\nAbout TP, looks like it has this since [v25.2](https://docs.modular.com/max/changelog/#v252-2025-03-25), haven't tested that myself.\\n\\nOne advantage that MAX has over vLLM is that is more composable/future-proof in regards to their LLM kernels. vLLM has to handcraft kernels in CUDA for every hardware and architecture, whereas MAX can compile a lot of their kernels down for the specific hardware, which means faster iteration speed.\\n\\nedit: I agree that their documentation could be improved upon. Took awhile to figure out what certain flags are doing.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n28sw8j","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;MAX is by Modular the company behind Mojo, known for claiming being 10.000x faster than Python. Mojo is a new language that is (kinda) compatible with Python, but compiles to machine-code (you can also run it via JIT compilation). A few months ago there where some shady benchmarks where they claimed being faster that Rust but then they did not compile using the release flag. nevertheless, Modular is on a mission to re-build the AI inference stack without CUDA, they have demos where they can run LLMs on AMD and NVIDIA hardware on their native mojo/max stack without CUDA (which is nice because the container images are like 1Gb compared to 5Gb)&lt;/p&gt;\\n\\n&lt;p&gt;That being sad, they are Python compatible in so far as it should be possible to download any HF model and run it.&lt;/p&gt;\\n\\n&lt;p&gt;They have their own model library because these models are (presumably) optimized and reimplemented in Mojo/Max and should show improved performance. No clue so far which quants are supported atm / and also multi-modality. But very nice questions.&lt;/p&gt;\\n\\n&lt;p&gt;I just recently watched a podcast with Chris Lattner (CEO of Modular, Creator of LLVM, Clang, MLIR) and they claimed being faster with MAX than vLLM on A100 and H100 and I wanted to check that out&lt;/p&gt;\\n\\n&lt;p&gt;About TP, looks like it has this since &lt;a href=\\"https://docs.modular.com/max/changelog/#v252-2025-03-25\\"&gt;v25.2&lt;/a&gt;, haven&amp;#39;t tested that myself.&lt;/p&gt;\\n\\n&lt;p&gt;One advantage that MAX has over vLLM is that is more composable/future-proof in regards to their LLM kernels. vLLM has to handcraft kernels in CUDA for every hardware and architecture, whereas MAX can compile a lot of their kernels down for the specific hardware, which means faster iteration speed.&lt;/p&gt;\\n\\n&lt;p&gt;edit: I agree that their documentation could be improved upon. Took awhile to figure out what certain flags are doing.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lvglk7","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lvglk7/vllm_vs_sglang_vs_max_whos_the_fastest/n28sw8j/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752093639,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}}],"before":null}},"user_reports":[],"saved":false,"id":"n27ttjv","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"plankalkul-z1","can_mod_post":false,"created_utc":1752083933,"send_replies":true,"parent_id":"t3_1lvglk7","score":6,"author_fullname":"t2_w73n3yrsx","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Thanks for the article.\\n\\n\\nIt's the very first time that I heard about MAX inference engine, and I have to say I'm intrigued, but also... confused.\\n\\n\\nTheir docs do not help; typically bad: *look* extensive, but do not answer major questions...\\n\\n\\nWhy do they have their own model library? Can I just run models from huggingface? If yes, what architectures (and formats/quants) are supported? What about VLMs? And so on, and so forth.\\n\\n\\nThe example they have in their Github readme looks like a dream came true:\\n\\n\\n\`max serve --model-path=modularai/Llama-3.1-8B-Instruct-GGUF\`\\n\\n\\nSince you seriously compared MAX to vLLM and SGLang, I assume MAX supports tensor parallelism? It didn't seem like you tested it (you ran single L40)... But if TP is not there, your comparison is moot.\\n\\n\\nSo, do we have TP with *arbitrary* GGUFs in MAX, or not? What are supported architectures?\\n\\n\\nCan you please comment on that?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n27ttjv","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thanks for the article.&lt;/p&gt;\\n\\n&lt;p&gt;It&amp;#39;s the very first time that I heard about MAX inference engine, and I have to say I&amp;#39;m intrigued, but also... confused.&lt;/p&gt;\\n\\n&lt;p&gt;Their docs do not help; typically bad: &lt;em&gt;look&lt;/em&gt; extensive, but do not answer major questions...&lt;/p&gt;\\n\\n&lt;p&gt;Why do they have their own model library? Can I just run models from huggingface? If yes, what architectures (and formats/quants) are supported? What about VLMs? And so on, and so forth.&lt;/p&gt;\\n\\n&lt;p&gt;The example they have in their Github readme looks like a dream came true:&lt;/p&gt;\\n\\n&lt;p&gt;&lt;code&gt;max serve --model-path=modularai/Llama-3.1-8B-Instruct-GGUF&lt;/code&gt;&lt;/p&gt;\\n\\n&lt;p&gt;Since you seriously compared MAX to vLLM and SGLang, I assume MAX supports tensor parallelism? It didn&amp;#39;t seem like you tested it (you ran single L40)... But if TP is not there, your comparison is moot.&lt;/p&gt;\\n\\n&lt;p&gt;So, do we have TP with &lt;em&gt;arbitrary&lt;/em&gt; GGUFs in MAX, or not? What are supported architectures?&lt;/p&gt;\\n\\n&lt;p&gt;Can you please comment on that?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lvglk7/vllm_vs_sglang_vs_max_whos_the_fastest/n27ttjv/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752083933,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lvglk7","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n27p0if","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"rkstgr","can_mod_post":false,"send_replies":true,"parent_id":"t1_n26t5t9","score":3,"author_fullname":"t2_6ort7d94","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Updated it. thx for mentioning it","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n27p0if","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Updated it. thx for mentioning it&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lvglk7","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lvglk7/vllm_vs_sglang_vs_max_whos_the_fastest/n27p0if/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752082654,"author_flair_text":null,"treatment_tags":[],"created_utc":1752082654,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n26t5t9","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Prestigious_Thing797","can_mod_post":false,"send_replies":true,"parent_id":"t1_n26lf4s","score":3,"author_fullname":"t2_1anh6qztwr","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"No that would cover it. Ty. Would be nice to call that out in the article.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n26t5t9","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;No that would cover it. Ty. Would be nice to call that out in the article.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lvglk7","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lvglk7/vllm_vs_sglang_vs_max_whos_the_fastest/n26t5t9/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752073993,"author_flair_text":null,"treatment_tags":[],"created_utc":1752073993,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n26lf4s","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"rkstgr","can_mod_post":false,"created_utc":1752071828,"send_replies":true,"parent_id":"t1_n25xmyz","score":4,"author_fullname":"t2_6ort7d94","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I warmed every engine with 500 prompts before doing the seeded benchmark run. I am not sure if you are referring to sth else.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n26lf4s","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I warmed every engine with 500 prompts before doing the seeded benchmark run. I am not sure if you are referring to sth else.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lvglk7","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lvglk7/vllm_vs_sglang_vs_max_whos_the_fastest/n26lf4s/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752071828,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}}],"before":null}},"user_reports":[],"saved":false,"id":"n25xmyz","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Prestigious_Thing797","can_mod_post":false,"created_utc":1752064419,"send_replies":true,"parent_id":"t3_1lvglk7","score":5,"author_fullname":"t2_1anh6qztwr","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"The variability in vllm you are seeing is likely the warmup happening when it receives its first batch of requests. If you put one prompt through it and then run the benchmark after you'd likely see different results for it.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n25xmyz","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The variability in vllm you are seeing is likely the warmup happening when it receives its first batch of requests. If you put one prompt through it and then run the benchmark after you&amp;#39;d likely see different results for it.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lvglk7/vllm_vs_sglang_vs_max_whos_the_fastest/n25xmyz/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752064419,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lvglk7","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n28ugak","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"RunPersonal6993","can_mod_post":false,"created_utc":1752094063,"send_replies":true,"parent_id":"t3_1lvglk7","score":2,"author_fullname":"t2_5q2qgkzi","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Sglang is good for structured output. IT would be fair to run structured output tests. and also include ExlamaV2.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n28ugak","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Sglang is good for structured output. IT would be fair to run structured output tests. and also include ExlamaV2.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lvglk7/vllm_vs_sglang_vs_max_whos_the_fastest/n28ugak/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752094063,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lvglk7","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"d2642412-d9ce-11ed-ae30-32b11309f5bd","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2blw7v","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"rkstgr","can_mod_post":false,"created_utc":1752130874,"send_replies":true,"parent_id":"t1_n2bjbr3","score":1,"author_fullname":"t2_6ort7d94","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Very true. The benchmark is also not completely accurate to real world scenarios as you would set a specific rpm target for your servers, tune the settings, and have a load balancer in front.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2blw7v","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Very true. The benchmark is also not completely accurate to real world scenarios as you would set a specific rpm target for your servers, tune the settings, and have a load balancer in front.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lvglk7","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lvglk7/vllm_vs_sglang_vs_max_whos_the_fastest/n2blw7v/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752130874,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2bjbr3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ortegaalfredo","can_mod_post":false,"created_utc":1752129494,"send_replies":true,"parent_id":"t3_1lvglk7","score":2,"author_fullname":"t2_g177e","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"In all those benchmarks there is always one missing \\"How much time can the server be up without crashing\\" a very useful metric that can surprisingly be very low.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2bjbr3","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"Alpaca"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;In all those benchmarks there is always one missing &amp;quot;How much time can the server be up without crashing&amp;quot; a very useful metric that can surprisingly be very low.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lvglk7/vllm_vs_sglang_vs_max_whos_the_fastest/n2bjbr3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752129494,"author_flair_text":"Alpaca","treatment_tags":[],"link_id":"t3_1lvglk7","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bd9e9e","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n25zkcv","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Leflakk","can_mod_post":false,"created_utc":1752065092,"send_replies":true,"parent_id":"t3_1lvglk7","score":-1,"author_fullname":"t2_udr659irv","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Ad","edited":1752089446,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n25zkcv","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Ad&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lvglk7/vllm_vs_sglang_vs_max_whos_the_fastest/n25zkcv/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752065092,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lvglk7","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-1}}],"before":null}}]`),s=()=>e.jsx(l,{data:t});export{s as default};
