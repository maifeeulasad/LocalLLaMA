import{j as e}from"./index-cvG704yx.js";import{R as l}from"./RedditPostRenderer-CBthLTAH.js";import"./index-D-GavSZU.js";const a=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"I'm thinking about buying a GMKTEK Evo-2.\\nWhich models (in terms of B parameters) can I expect to run at a decent speed (&gt; 10tk/s)? I'm undecided between the 64 GB and 128 GB RAM versions, but I'm leaning towards the 64 GB since even slightly larger models (Llama 3.1 70B) run at a painfully slow speed.\\n\\nEDIT: Thank you all so much for the great answers! I'm new to this, and, to be honest, my main concern is privacy. I plan to use a local AI for research purposes, ( e.g., Which were the causes of WWI) and perhaps for some coding assistance. If I understand the comments correctly, MoE (mixture of experts) models are larger models but only part of the model is activated and can therefore run faster. If so, then maybe the 128 GB is worth it. Thanks again to everyone! ","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"What modes can expect I run on an AMD Ryzen AI Max+ 395?","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lvh87a","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.93,"author_flair_background_color":null,"subreddit_type":"public","ups":24,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_bk6b6yhm","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":24,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":1752066643,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1752063277,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;m thinking about buying a GMKTEK Evo-2.\\nWhich models (in terms of B parameters) can I expect to run at a decent speed (&amp;gt; 10tk/s)? I&amp;#39;m undecided between the 64 GB and 128 GB RAM versions, but I&amp;#39;m leaning towards the 64 GB since even slightly larger models (Llama 3.1 70B) run at a painfully slow speed.&lt;/p&gt;\\n\\n&lt;p&gt;EDIT: Thank you all so much for the great answers! I&amp;#39;m new to this, and, to be honest, my main concern is privacy. I plan to use a local AI for research purposes, ( e.g., Which were the causes of WWI) and perhaps for some coding assistance. If I understand the comments correctly, MoE (mixture of experts) models are larger models but only part of the model is activated and can therefore run faster. If so, then maybe the 128 GB is worth it. Thanks again to everyone! &lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1lvh87a","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"electrickangaroo31","discussion_type":null,"num_comments":25,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lvh87a/what_modes_can_expect_i_run_on_an_amd_ryzen_ai/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lvh87a/what_modes_can_expect_i_run_on_an_amd_ryzen_ai/","subreddit_subscribers":497025,"created_utc":1752063277,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n25z57o","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"simracerman","can_mod_post":false,"created_utc":1752064947,"send_replies":true,"parent_id":"t1_n25xx8u","score":14,"author_fullname":"t2_vbzgnic","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"That’s the answer. MoE is the future (at least from emerging trends), and so far it’s keeping up with dense models just fine. Helps that MoE or 22B active is slightly slower than a dense 22B active. This makes VRAM size your only constraint.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n25z57o","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;That’s the answer. MoE is the future (at least from emerging trends), and so far it’s keeping up with dense models just fine. Helps that MoE or 22B active is slightly slower than a dense 22B active. This makes VRAM size your only constraint.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lvh87a","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lvh87a/what_modes_can_expect_i_run_on_an_amd_ryzen_ai/n25z57o/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752064947,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":14}}],"before":null}},"user_reports":[],"saved":false,"id":"n25xx8u","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Krowken","can_mod_post":false,"created_utc":1752064520,"send_replies":true,"parent_id":"t3_1lvh87a","score":30,"author_fullname":"t2_1k5i09iuq0","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I think the biggest advantage of getting the 128GB version  would be that you could run some of the larger mixture of expert models. Any dense model taking up more than 64GB of vram would likely be limited by the low memory bandwidth anyways. ","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n25xx8u","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I think the biggest advantage of getting the 128GB version  would be that you could run some of the larger mixture of expert models. Any dense model taking up more than 64GB of vram would likely be limited by the low memory bandwidth anyways. &lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lvh87a/what_modes_can_expect_i_run_on_an_amd_ryzen_ai/n25xx8u/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752064520,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lvh87a","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":30}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n25zjvj","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"MaxKruse96","can_mod_post":false,"created_utc":1752065087,"send_replies":true,"parent_id":"t3_1lvh87a","score":12,"author_fullname":"t2_pfi81","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Get the 128gb one, you want to be future-proofed for new MoE models that run fast and have good knowledge/capabilities. If you want to buy it for inference now and then never touch it again, might as well build a cluster of 3060Ti","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n25zjvj","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Get the 128gb one, you want to be future-proofed for new MoE models that run fast and have good knowledge/capabilities. If you want to buy it for inference now and then never touch it again, might as well build a cluster of 3060Ti&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lvh87a/what_modes_can_expect_i_run_on_an_amd_ryzen_ai/n25zjvj/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752065087,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lvh87a","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":12}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n285zw7","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Mushoz","can_mod_post":false,"send_replies":true,"parent_id":"t1_n27q20s","score":3,"author_fullname":"t2_gwpq7","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I am, yes","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n285zw7","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I am, yes&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lvh87a","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lvh87a/what_modes_can_expect_i_run_on_an_amd_ryzen_ai/n285zw7/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752087318,"author_flair_text":null,"treatment_tags":[],"created_utc":1752087318,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n27q20s","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"oxygen_addiction","can_mod_post":false,"created_utc":1752082930,"send_replies":true,"parent_id":"t1_n263c3i","score":3,"author_fullname":"t2_66k6x","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Q3\\\\_K\\\\_XL should eat up around 104GB of VRAM with the default context length.  \\nAre you running under Linux?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n27q20s","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Q3_K_XL should eat up around 104GB of VRAM with the default context length.&lt;br/&gt;\\nAre you running under Linux?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lvh87a","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lvh87a/what_modes_can_expect_i_run_on_an_amd_ryzen_ai/n27q20s/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752082930,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n26q304","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Secure_Reflection409","can_mod_post":false,"created_utc":1752073136,"send_replies":true,"parent_id":"t1_n263c3i","score":2,"author_fullname":"t2_by77ogdhr","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I'm surprised it's as high as 14, very nice.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n26q304","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;m surprised it&amp;#39;s as high as 14, very nice.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lvh87a","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lvh87a/what_modes_can_expect_i_run_on_an_amd_ryzen_ai/n26q304/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752073136,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n263c3i","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Mushoz","can_mod_post":false,"created_utc":1752066360,"send_replies":true,"parent_id":"t3_1lvh87a","score":10,"author_fullname":"t2_gwpq7","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Qwen3-235B-Q3\\\\_K\\\\_XL UD quants fit perfectly, and give me a very acceptable 14 tokens/s. Dots.lm1 in Q4\\\\_K\\\\_XL UD quants easily fits as well, and is a really fun model. Around 22 tokens/s. I would go with 128GB. I am loving this laptop :)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n263c3i","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Qwen3-235B-Q3_K_XL UD quants fit perfectly, and give me a very acceptable 14 tokens/s. Dots.lm1 in Q4_K_XL UD quants easily fits as well, and is a really fun model. Around 22 tokens/s. I would go with 128GB. I am loving this laptop :)&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lvh87a/what_modes_can_expect_i_run_on_an_amd_ryzen_ai/n263c3i/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752066360,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lvh87a","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":10}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n27uqf6","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"fallingdowndizzyvr","can_mod_post":false,"send_replies":true,"parent_id":"t1_n27r3bt","score":3,"author_fullname":"t2_o65i6kx","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Under Linux it can use 111GB of RAM once you include in GTT.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n27uqf6","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Under Linux it can use 111GB of RAM once you include in GTT.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lvh87a","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lvh87a/what_modes_can_expect_i_run_on_an_amd_ryzen_ai/n27uqf6/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752084181,"author_flair_text":null,"treatment_tags":[],"created_utc":1752084181,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n27r3bt","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"oxygen_addiction","can_mod_post":false,"created_utc":1752083204,"send_replies":true,"parent_id":"t1_n26fpys","score":2,"author_fullname":"t2_66k6x","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I wonder what the highest quant + 256K context length would be.\\n\\nMaybe Q6\\\\_K\\\\_XL (70.5GB) under Linux to allocate more than 96GB of VRAM?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n27r3bt","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I wonder what the highest quant + 256K context length would be.&lt;/p&gt;\\n\\n&lt;p&gt;Maybe Q6_K_XL (70.5GB) under Linux to allocate more than 96GB of VRAM?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lvh87a","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lvh87a/what_modes_can_expect_i_run_on_an_amd_ryzen_ai/n27r3bt/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752083204,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n26fpys","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"pkmxtw","can_mod_post":false,"created_utc":1752070186,"send_replies":true,"parent_id":"t3_1lvh87a","score":8,"author_fullname":"t2_a2gtk","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"The new Hunyuan-80B-A13B is about the perfect size for AI Max+ 395 128GB.","edited":1752094300,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n26fpys","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The new Hunyuan-80B-A13B is about the perfect size for AI Max+ 395 128GB.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lvh87a/what_modes_can_expect_i_run_on_an_amd_ryzen_ai/n26fpys/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752070186,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lvh87a","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":8}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n27sbw4","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"randomfoo2","can_mod_post":false,"send_replies":true,"parent_id":"t1_n27f4yb","score":2,"author_fullname":"t2_eztox","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"You will take a hit for token generation with extended context, but the post important thing is that if you don't have proper caching, you are going to have issues with prefill itself - that's the pp512 in llama-bench results.\\n\\nYou can take a look at the benchmarks I've done for some more info and raw benchmark results. I will probably revisit in the coming weeks have I have some time to sit down to run more sweeps/testing (including non-llama.cpp engines which have more advanced caching featuers): [https://llm-tracker.info/\\\\_TOORG/Strix-Halo](https://llm-tracker.info/_TOORG/Strix-Halo)","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n27sbw4","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You will take a hit for token generation with extended context, but the post important thing is that if you don&amp;#39;t have proper caching, you are going to have issues with prefill itself - that&amp;#39;s the pp512 in llama-bench results.&lt;/p&gt;\\n\\n&lt;p&gt;You can take a look at the benchmarks I&amp;#39;ve done for some more info and raw benchmark results. I will probably revisit in the coming weeks have I have some time to sit down to run more sweeps/testing (including non-llama.cpp engines which have more advanced caching featuers): &lt;a href=\\"https://llm-tracker.info/_TOORG/Strix-Halo\\"&gt;https://llm-tracker.info/_TOORG/Strix-Halo&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lvh87a","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lvh87a/what_modes_can_expect_i_run_on_an_amd_ryzen_ai/n27sbw4/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752083533,"author_flair_text":null,"treatment_tags":[],"created_utc":1752083533,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2b4bvm","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"SillyLilBear","can_mod_post":false,"send_replies":true,"parent_id":"t1_n27f4yb","score":1,"author_fullname":"t2_wjjtz","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":" performance is atrocious on the 395+.  Dense models are unusable, MoE will give you usable performance, but there are better options.\\n\\nhttps://preview.redd.it/xh2wlyth6zbf1.png?width=566&amp;format=png&amp;auto=webp&amp;s=0bdab7c81f0d8af0405a9e07f25b42e7ba26be09","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2b4bvm","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;performance is atrocious on the 395+.  Dense models are unusable, MoE will give you usable performance, but there are better options.&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://preview.redd.it/xh2wlyth6zbf1.png?width=566&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0bdab7c81f0d8af0405a9e07f25b42e7ba26be09\\"&gt;https://preview.redd.it/xh2wlyth6zbf1.png?width=566&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0bdab7c81f0d8af0405a9e07f25b42e7ba26be09&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lvh87a","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lvh87a/what_modes_can_expect_i_run_on_an_amd_ryzen_ai/n2b4bvm/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752122077,"media_metadata":{"xh2wlyth6zbf1":{"status":"valid","e":"Image","m":"image/png","p":[{"y":48,"x":108,"u":"https://preview.redd.it/xh2wlyth6zbf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=9d7e37452df0033966296af07da7b8e9e6540d1d"},{"y":97,"x":216,"u":"https://preview.redd.it/xh2wlyth6zbf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=31534da104ed4873864941b49751598fe8897c27"},{"y":144,"x":320,"u":"https://preview.redd.it/xh2wlyth6zbf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=8b66a6b87d6bad2e3ea498e55263e1df90f8fac9"}],"s":{"y":255,"x":566,"u":"https://preview.redd.it/xh2wlyth6zbf1.png?width=566&amp;format=png&amp;auto=webp&amp;s=0bdab7c81f0d8af0405a9e07f25b42e7ba26be09"},"id":"xh2wlyth6zbf1"}},"author_flair_text":null,"treatment_tags":[],"created_utc":1752122077,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n27f4yb","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"bopcrane","can_mod_post":false,"created_utc":1752080010,"send_replies":true,"parent_id":"t1_n26uo0y","score":2,"author_fullname":"t2_wo04m","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Thanks for the concrete numbers - have you been able to test the speed of either of those models with a bit of context?   \\n  \\nI'm considering a build using the AI Max+ 395, but performance with a good bit of context (for agentic tasks and coding) is my main concern. I just can't seem to find any benchmarks or data on tokens per second at various context sizes","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n27f4yb","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thanks for the concrete numbers - have you been able to test the speed of either of those models with a bit of context?   &lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;m considering a build using the AI Max+ 395, but performance with a good bit of context (for agentic tasks and coding) is my main concern. I just can&amp;#39;t seem to find any benchmarks or data on tokens per second at various context sizes&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lvh87a","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lvh87a/what_modes_can_expect_i_run_on_an_amd_ryzen_ai/n27f4yb/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752080010,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n27r7f7","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"oxygen_addiction","can_mod_post":false,"created_utc":1752083233,"send_replies":true,"parent_id":"t1_n26uo0y","score":2,"author_fullname":"t2_66k6x","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Can you please try this with 256k context?\\n\\n[https://huggingface.co/unsloth/Hunyuan-A13B-Instruct-GGUF](https://huggingface.co/unsloth/Hunyuan-A13B-Instruct-GGUF)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n27r7f7","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Can you please try this with 256k context?&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://huggingface.co/unsloth/Hunyuan-A13B-Instruct-GGUF\\"&gt;https://huggingface.co/unsloth/Hunyuan-A13B-Instruct-GGUF&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lvh87a","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lvh87a/what_modes_can_expect_i_run_on_an_amd_ryzen_ai/n27r7f7/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752083233,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n26uo0y","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"randomfoo2","can_mod_post":false,"created_utc":1752074407,"send_replies":true,"parent_id":"t3_1lvh87a","score":7,"author_fullname":"t2_eztox","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"In my testing:\\n\\n* Qwen3-235B-A22B-UD-Q3\\\\_K\\\\_XL (96.59 GiB) can run at 12.3 tok/s\\n* Llama-4-Scout-17B-16E-Instruct-UD-Q4\\\\_K\\\\_XL (57.73 GiB) can run at 19.95 tok/s\\n\\nThere are other MoEs to test, at least that should give you a ballpark of what's possible.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n26uo0y","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;In my testing:&lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;Qwen3-235B-A22B-UD-Q3_K_XL (96.59 GiB) can run at 12.3 tok/s&lt;/li&gt;\\n&lt;li&gt;Llama-4-Scout-17B-16E-Instruct-UD-Q4_K_XL (57.73 GiB) can run at 19.95 tok/s&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;p&gt;There are other MoEs to test, at least that should give you a ballpark of what&amp;#39;s possible.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lvh87a/what_modes_can_expect_i_run_on_an_amd_ryzen_ai/n26uo0y/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752074407,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lvh87a","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":7}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2b1voy","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"SillyLilBear","can_mod_post":false,"created_utc":1752121001,"send_replies":true,"parent_id":"t3_1lvh87a","score":3,"author_fullname":"t2_wjjtz","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"The 395+ is really bad at dense models, so right away give up on that dream.  MoE models it does fairly well, but still not really worth it as there are better options.\\n\\nIf you do go the 395+, look at Qwen3 30B A3B as good baseline model.  Hunyuan is a new model that is getting popular quick and runs ok on the 395+.\\n\\nIf you do want to run dense models, expect 1-9 tokens per second.  MoE models you can expect 20-50 tokens/second depending on the size and quant.\\n\\nI can run Qwen3 32B Q8 on my 395+ 128G, but it is around 2-6 tokens/second depending on how full the contexts window is.\\n\\nYou can find a lot of great information here:\\n\\n[https://strixhalo-homelab.d7.wtf/](https://strixhalo-homelab.d7.wtf/)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2b1voy","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The 395+ is really bad at dense models, so right away give up on that dream.  MoE models it does fairly well, but still not really worth it as there are better options.&lt;/p&gt;\\n\\n&lt;p&gt;If you do go the 395+, look at Qwen3 30B A3B as good baseline model.  Hunyuan is a new model that is getting popular quick and runs ok on the 395+.&lt;/p&gt;\\n\\n&lt;p&gt;If you do want to run dense models, expect 1-9 tokens per second.  MoE models you can expect 20-50 tokens/second depending on the size and quant.&lt;/p&gt;\\n\\n&lt;p&gt;I can run Qwen3 32B Q8 on my 395+ 128G, but it is around 2-6 tokens/second depending on how full the contexts window is.&lt;/p&gt;\\n\\n&lt;p&gt;You can find a lot of great information here:&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://strixhalo-homelab.d7.wtf/\\"&gt;https://strixhalo-homelab.d7.wtf/&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lvh87a/what_modes_can_expect_i_run_on_an_amd_ryzen_ai/n2b1voy/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752121001,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lvh87a","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n25xskt","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"rorowhat","can_mod_post":false,"created_utc":1752064474,"send_replies":true,"parent_id":"t3_1lvh87a","score":5,"author_fullname":"t2_yq51a","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Of course the 128gb will run larger models, but if you want good speeds the 64gb might be the sweet spot.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n25xskt","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Of course the 128gb will run larger models, but if you want good speeds the 64gb might be the sweet spot.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lvh87a/what_modes_can_expect_i_run_on_an_amd_ryzen_ai/n25xskt/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752064474,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lvh87a","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n266bf6","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"uti24","can_mod_post":false,"created_utc":1752067321,"send_replies":true,"parent_id":"t3_1lvh87a","score":7,"author_fullname":"t2_13hbro","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"It depends on the speed you can tolerate.\\n\\nMemory bandwidth of AMD Ryzen AI Max+ 395 is 250GB/s, but in practice it is about 200GB/s\\n\\nSo if you load model that uses all 96GB of VRAM you will have about 2t/s.\\n\\nAnd anything less will run faster proportionally.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n266bf6","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It depends on the speed you can tolerate.&lt;/p&gt;\\n\\n&lt;p&gt;Memory bandwidth of AMD Ryzen AI Max+ 395 is 250GB/s, but in practice it is about 200GB/s&lt;/p&gt;\\n\\n&lt;p&gt;So if you load model that uses all 96GB of VRAM you will have about 2t/s.&lt;/p&gt;\\n\\n&lt;p&gt;And anything less will run faster proportionally.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lvh87a/what_modes_can_expect_i_run_on_an_amd_ryzen_ai/n266bf6/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752067321,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lvh87a","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":7}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n27puzs","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Caffdy","can_mod_post":false,"created_utc":1752082879,"send_replies":true,"parent_id":"t1_n263qad","score":2,"author_fullname":"t2_ql2vu0wz","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"don't forget about context length; the models could fit in 64GB, but more often than not you end up with a laughable workable context. 128GB of memory would take care of that as well.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n27puzs","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;don&amp;#39;t forget about context length; the models could fit in 64GB, but more often than not you end up with a laughable workable context. 128GB of memory would take care of that as well.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lvh87a","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lvh87a/what_modes_can_expect_i_run_on_an_amd_ryzen_ai/n27puzs/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752082879,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n263qad","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"electrickangaroo31","can_mod_post":false,"created_utc":1752066491,"send_replies":true,"parent_id":"t3_1lvh87a","score":2,"author_fullname":"t2_bk6b6yhm","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Thank you all so much for the great answers! I'm new to this, and, to be honest, my main concern is privacy.\\nI plan to use a local AI for research purposes, ( e.g., Which were the causes of WWI) and perhaps for some coding assistance. If I understand the comments correctly, MoE (mixture of experts) models are larger models but only part of the model is activated and can therefore run faster. If so, then maybe the 128 GB is worth it. Thanks again to everyone!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n263qad","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thank you all so much for the great answers! I&amp;#39;m new to this, and, to be honest, my main concern is privacy.\\nI plan to use a local AI for research purposes, ( e.g., Which were the causes of WWI) and perhaps for some coding assistance. If I understand the comments correctly, MoE (mixture of experts) models are larger models but only part of the model is activated and can therefore run faster. If so, then maybe the 128 GB is worth it. Thanks again to everyone!&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lvh87a/what_modes_can_expect_i_run_on_an_amd_ryzen_ai/n263qad/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752066491,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lvh87a","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n27nbov","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Wild_Requirement8902","can_mod_post":false,"created_utc":1752082204,"send_replies":true,"parent_id":"t3_1lvh87a","score":2,"author_fullname":"t2_98yqzbqx","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"i have 64gb of ram and 20 gb of vram and that not enough take the 128 Gb version or you are going to regret. speed is one thing the ability to run is another. and loading the model is cute but what you want is context memory. the ram is soldered so no upgrade possible (maybe you can boost thing a bit with the occulink port or with a m2-&gt;pci express ryser but that quite the expensive path).  \\nqwen3-30b-a3b unsluth ud(q8) qwant with 40960(40k) context length (no flash attention full cpu flash attention and kv cache at q8 slow thing to just under 3 t/s) and firefox with just this page open take 48GB of ram for 4.8 t/s (dual channel ddr4 @ 3600 so about 1/4 of the bandwitch you will get if i am not mistaken)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n27nbov","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;i have 64gb of ram and 20 gb of vram and that not enough take the 128 Gb version or you are going to regret. speed is one thing the ability to run is another. and loading the model is cute but what you want is context memory. the ram is soldered so no upgrade possible (maybe you can boost thing a bit with the occulink port or with a m2-&amp;gt;pci express ryser but that quite the expensive path).&lt;br/&gt;\\nqwen3-30b-a3b unsluth ud(q8) qwant with 40960(40k) context length (no flash attention full cpu flash attention and kv cache at q8 slow thing to just under 3 t/s) and firefox with just this page open take 48GB of ram for 4.8 t/s (dual channel ddr4 @ 3600 so about 1/4 of the bandwitch you will get if i am not mistaken)&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lvh87a/what_modes_can_expect_i_run_on_an_amd_ryzen_ai/n27nbov/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752082204,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lvh87a","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n29eaht","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"fooo12gh","can_mod_post":false,"created_utc":1752099865,"send_replies":true,"parent_id":"t3_1lvh87a","score":2,"author_fullname":"t2_3vvvbmh","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Consider from the point of view, that you'll need quite some RAM for other processes - browser, coding editor/IDE, maybe docker, potentially model for autocomplete in case you'll want some more conveniences when coding (e.g., using \\"continue.dev\\" plugin).","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n29eaht","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Consider from the point of view, that you&amp;#39;ll need quite some RAM for other processes - browser, coding editor/IDE, maybe docker, potentially model for autocomplete in case you&amp;#39;ll want some more conveniences when coding (e.g., using &amp;quot;continue.dev&amp;quot; plugin).&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lvh87a/what_modes_can_expect_i_run_on_an_amd_ryzen_ai/n29eaht/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752099865,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lvh87a","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n27v1os","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"fallingdowndizzyvr","can_mod_post":false,"created_utc":1752084269,"send_replies":true,"parent_id":"t3_1lvh87a","score":2,"author_fullname":"t2_o65i6kx","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"The whole point of this is to get 128GB. There's no point to the 64GB one. You might as well just put together a PC and get a couple of 32GB cards.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n27v1os","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The whole point of this is to get 128GB. There&amp;#39;s no point to the 64GB one. You might as well just put together a PC and get a couple of 32GB cards.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lvh87a/what_modes_can_expect_i_run_on_an_amd_ryzen_ai/n27v1os/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752084269,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lvh87a","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2bw5eo","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Rich_Repeat_22","can_mod_post":false,"created_utc":1752136719,"send_replies":true,"parent_id":"t3_1lvh87a","score":2,"author_fullname":"t2_viufiki6","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"a) Before you get the X2, maybe wait until we see the Thermalright model (same price watercooled so no throttiling) and the Bosman miniPC ($1700 for the 128GB version, -$200 cheaper than the rest). \\n\\nb) LLAMA 3.3 70B  you should be looking for around 5-6tk/s on Vulkan in Windows, maybe get to 7.5 with ROCm and XDNA2/AMD GAIA to use the NPU also. But we still wait for benchmarks on this. \\n\\nFYI 5 times more expensive M3 Ultra studio 512GB is around 8.9 with MLX.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2bw5eo","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;a) Before you get the X2, maybe wait until we see the Thermalright model (same price watercooled so no throttiling) and the Bosman miniPC ($1700 for the 128GB version, -$200 cheaper than the rest). &lt;/p&gt;\\n\\n&lt;p&gt;b) LLAMA 3.3 70B  you should be looking for around 5-6tk/s on Vulkan in Windows, maybe get to 7.5 with ROCm and XDNA2/AMD GAIA to use the NPU also. But we still wait for benchmarks on this. &lt;/p&gt;\\n\\n&lt;p&gt;FYI 5 times more expensive M3 Ultra studio 512GB is around 8.9 with MLX.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lvh87a/what_modes_can_expect_i_run_on_an_amd_ryzen_ai/n2bw5eo/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752136719,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lvh87a","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}}]`),r=()=>e.jsx(l,{data:a});export{r as default};
