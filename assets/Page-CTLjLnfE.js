import{j as e}from"./index-BxgxThME.js";import{R as t}from"./RedditPostRenderer-BL_SOtuv.js";import"./index--Az3yIKM.js";const l=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Sentence Transformers v5.0 was just released, and it introduced sparse embedding models. These are the kind of search models that are often combined with the \\"standard\\" dense embedding models for \\"hybrid search\\". On paper, this can help performance a lot. From the release notes:\\n\\n&gt; A big question is: How do sparse embedding models stack up against the “standard” dense embedding models, and what kind of performance can you expect when combining various?\\n&gt; \\n&gt; For this, I ran a variation of our [hybrid_search.py](https://github.com/UKPLab/sentence-transformers/blob/master/examples/sparse_encoder/applications/retrieve_rerank/hybrid_search.py) evaluation script, with:\\n&gt; \\n&gt; - The [NanoMSMARCO](https://huggingface.co/datasets/zeta-alpha-ai/NanoMSMARCO) dataset (a subset of the MS MARCO eval split)\\n&gt; - [Qwen/Qwen3-Embedding-0.6B](https://huggingface.co/Qwen/Qwen3-Embedding-0.6B) dense embedding model\\n&gt; - [naver/splade-v3-doc](https://huggingface.co/naver/splade-v3-doc) sparse embedding model, inference free for queries\\n&gt; - [Alibaba-NLP/gte-reranker-modernbert-base](https://huggingface.co/Alibaba-NLP/gte-reranker-modernbert-base) reranker\\n&gt; \\n&gt; Which resulted in this evaluation:\\n&gt; \\n&gt; | Dense | Sparse | Reranker | NDCG@10 | MRR@10 | MAP |\\n&gt; | --- | --- | --- | --- | --- | --- |\\n&gt; | x |  |  | 65.33 | 57.56 | 57.97 |\\n&gt; |  | x |  | 67.34 | 59.59 | 59.98 |\\n&gt; | x | x |  | **72.39** | **66.99** | **67.59** |\\n&gt; | x |  | x | 68.37 | 62.76 | 63.56 |\\n&gt; |  | x | x | 69.02 | 63.66 | 64.44 |\\n&gt; | x | x | x | 68.28 | 62.66 | 63.44 |\\n&gt; \\n&gt; Here, the sparse embedding model actually already outperforms the dense one, but the real magic happens when combining the two: hybrid search. In our case, we used Reciprocal Rank Fusion to merge the two rankings. \\n&gt; \\n&gt; Rerankers also help improve the performance of the dense or sparse model here, but hurt the performance of the hybrid search, as its performance is already beyond what the reranker can achieve.\\n\\nSo, on paper you can now get more freedom over the \\"lexical\\" part of your hybrid search pipelines. I'm very excited about it personally.","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Training and Finetuning Sparse Embedding Models with Sentence Transformers v5","link_flair_richtext":[{"e":"text","t":"Tutorial | Guide"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":78,"top_awarded_type":null,"hide_score":false,"name":"t3_1lp2h0e","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.94,"author_flair_background_color":null,"ups":25,"total_awards_received":0,"media_embed":{},"thumbnail_width":140,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_11pki7","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Tutorial | Guide","can_mod_post":false,"score":25,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"https://external-preview.redd.it/Q7oEnpq4LYUPvgpkKMeoddSo-4Wn8UDKMbqnVIBZL8s.png?width=140&amp;height=78&amp;crop=140:78,smart&amp;auto=webp&amp;s=2e36a12650346d2d3907f58452ba204873c4d7ea","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"post_hint":"link","content_categories":null,"is_self":false,"subreddit_type":"public","created":1751378522,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"huggingface.co","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Sentence Transformers v5.0 was just released, and it introduced sparse embedding models. These are the kind of search models that are often combined with the &amp;quot;standard&amp;quot; dense embedding models for &amp;quot;hybrid search&amp;quot;. On paper, this can help performance a lot. From the release notes:&lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;A big question is: How do sparse embedding models stack up against the “standard” dense embedding models, and what kind of performance can you expect when combining various?&lt;/p&gt;\\n\\n&lt;p&gt;For this, I ran a variation of our &lt;a href=\\"https://github.com/UKPLab/sentence-transformers/blob/master/examples/sparse_encoder/applications/retrieve_rerank/hybrid_search.py\\"&gt;hybrid_search.py&lt;/a&gt; evaluation script, with:&lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;The &lt;a href=\\"https://huggingface.co/datasets/zeta-alpha-ai/NanoMSMARCO\\"&gt;NanoMSMARCO&lt;/a&gt; dataset (a subset of the MS MARCO eval split)&lt;/li&gt;\\n&lt;li&gt;&lt;a href=\\"https://huggingface.co/Qwen/Qwen3-Embedding-0.6B\\"&gt;Qwen/Qwen3-Embedding-0.6B&lt;/a&gt; dense embedding model&lt;/li&gt;\\n&lt;li&gt;&lt;a href=\\"https://huggingface.co/naver/splade-v3-doc\\"&gt;naver/splade-v3-doc&lt;/a&gt; sparse embedding model, inference free for queries&lt;/li&gt;\\n&lt;li&gt;&lt;a href=\\"https://huggingface.co/Alibaba-NLP/gte-reranker-modernbert-base\\"&gt;Alibaba-NLP/gte-reranker-modernbert-base&lt;/a&gt; reranker&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;p&gt;Which resulted in this evaluation:&lt;/p&gt;\\n\\n&lt;table&gt;&lt;thead&gt;\\n&lt;tr&gt;\\n&lt;th&gt;Dense&lt;/th&gt;\\n&lt;th&gt;Sparse&lt;/th&gt;\\n&lt;th&gt;Reranker&lt;/th&gt;\\n&lt;th&gt;NDCG@10&lt;/th&gt;\\n&lt;th&gt;MRR@10&lt;/th&gt;\\n&lt;th&gt;MAP&lt;/th&gt;\\n&lt;/tr&gt;\\n&lt;/thead&gt;&lt;tbody&gt;\\n&lt;tr&gt;\\n&lt;td&gt;x&lt;/td&gt;\\n&lt;td&gt;&lt;/td&gt;\\n&lt;td&gt;&lt;/td&gt;\\n&lt;td&gt;65.33&lt;/td&gt;\\n&lt;td&gt;57.56&lt;/td&gt;\\n&lt;td&gt;57.97&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;tr&gt;\\n&lt;td&gt;&lt;/td&gt;\\n&lt;td&gt;x&lt;/td&gt;\\n&lt;td&gt;&lt;/td&gt;\\n&lt;td&gt;67.34&lt;/td&gt;\\n&lt;td&gt;59.59&lt;/td&gt;\\n&lt;td&gt;59.98&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;tr&gt;\\n&lt;td&gt;x&lt;/td&gt;\\n&lt;td&gt;x&lt;/td&gt;\\n&lt;td&gt;&lt;/td&gt;\\n&lt;td&gt;&lt;strong&gt;72.39&lt;/strong&gt;&lt;/td&gt;\\n&lt;td&gt;&lt;strong&gt;66.99&lt;/strong&gt;&lt;/td&gt;\\n&lt;td&gt;&lt;strong&gt;67.59&lt;/strong&gt;&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;tr&gt;\\n&lt;td&gt;x&lt;/td&gt;\\n&lt;td&gt;&lt;/td&gt;\\n&lt;td&gt;x&lt;/td&gt;\\n&lt;td&gt;68.37&lt;/td&gt;\\n&lt;td&gt;62.76&lt;/td&gt;\\n&lt;td&gt;63.56&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;tr&gt;\\n&lt;td&gt;&lt;/td&gt;\\n&lt;td&gt;x&lt;/td&gt;\\n&lt;td&gt;x&lt;/td&gt;\\n&lt;td&gt;69.02&lt;/td&gt;\\n&lt;td&gt;63.66&lt;/td&gt;\\n&lt;td&gt;64.44&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;tr&gt;\\n&lt;td&gt;x&lt;/td&gt;\\n&lt;td&gt;x&lt;/td&gt;\\n&lt;td&gt;x&lt;/td&gt;\\n&lt;td&gt;68.28&lt;/td&gt;\\n&lt;td&gt;62.66&lt;/td&gt;\\n&lt;td&gt;63.44&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;/tbody&gt;&lt;/table&gt;\\n\\n&lt;p&gt;Here, the sparse embedding model actually already outperforms the dense one, but the real magic happens when combining the two: hybrid search. In our case, we used Reciprocal Rank Fusion to merge the two rankings. &lt;/p&gt;\\n\\n&lt;p&gt;Rerankers also help improve the performance of the dense or sparse model here, but hurt the performance of the hybrid search, as its performance is already beyond what the reranker can achieve.&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;So, on paper you can now get more freedom over the &amp;quot;lexical&amp;quot; part of your hybrid search pipelines. I&amp;#39;m very excited about it personally.&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"url_overridden_by_dest":"https://huggingface.co/blog/train-sparse-encoder","view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"preview":{"images":[{"source":{"url":"https://external-preview.redd.it/Q7oEnpq4LYUPvgpkKMeoddSo-4Wn8UDKMbqnVIBZL8s.png?auto=webp&amp;s=0b5a5e1cbfd410f8d953b0bd15f6c12615ee9093","width":1920,"height":1080},"resolutions":[{"url":"https://external-preview.redd.it/Q7oEnpq4LYUPvgpkKMeoddSo-4Wn8UDKMbqnVIBZL8s.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=c6034c885b9c0b0d9df5f31be4cde4f154338168","width":108,"height":60},{"url":"https://external-preview.redd.it/Q7oEnpq4LYUPvgpkKMeoddSo-4Wn8UDKMbqnVIBZL8s.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=cbea5818527a675d4f4ff74268909986b9682f6c","width":216,"height":121},{"url":"https://external-preview.redd.it/Q7oEnpq4LYUPvgpkKMeoddSo-4Wn8UDKMbqnVIBZL8s.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=c2a7e0ac27e6e0d2156ee89c1942e911862b4d07","width":320,"height":180},{"url":"https://external-preview.redd.it/Q7oEnpq4LYUPvgpkKMeoddSo-4Wn8UDKMbqnVIBZL8s.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=2eca6467642c913566d063063339907e970775c0","width":640,"height":360},{"url":"https://external-preview.redd.it/Q7oEnpq4LYUPvgpkKMeoddSo-4Wn8UDKMbqnVIBZL8s.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=81543a48e13686e1562d50da231c3ef26d4e8f9e","width":960,"height":540},{"url":"https://external-preview.redd.it/Q7oEnpq4LYUPvgpkKMeoddSo-4Wn8UDKMbqnVIBZL8s.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=c6ac5e6e7a5f525f9b0bbb027ce873302c27619c","width":1080,"height":607}],"variants":{},"id":"Q7oEnpq4LYUPvgpkKMeoddSo-4Wn8UDKMbqnVIBZL8s"}],"enabled":false},"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"449b05a6-bf8e-11ed-b4bd-66961e47bd50","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"mod_note":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"num_reports":null,"removal_reason":null,"link_flair_background_color":"#0079d3","id":"1lp2h0e","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"-Cubie-","discussion_type":null,"num_comments":4,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lp2h0e/training_and_finetuning_sparse_embedding_models/","stickied":false,"url":"https://huggingface.co/blog/train-sparse-encoder","subreddit_subscribers":493458,"created_utc":1751378522,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0si7il","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Accomplished_Mode170","can_mod_post":false,"created_utc":1751390060,"send_replies":true,"parent_id":"t1_n0rv6ul","score":1,"author_fullname":"t2_4hfmiefj","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"[I built a CLI that does policy validation with BERT x LM\\\\(s\\\\)](https://github.com/rabbidave/LatentSpace.Tools/blob/main/classify.md); should allow you to slot n-models in given ['the model is the product'](https://vintagedata.org/blog/posts/model-is-the-product)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0si7il","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;&lt;a href=\\"https://github.com/rabbidave/LatentSpace.Tools/blob/main/classify.md\\"&gt;I built a CLI that does policy validation with BERT x LM(s)&lt;/a&gt;; should allow you to slot n-models in given &lt;a href=\\"https://vintagedata.org/blog/posts/model-is-the-product\\"&gt;&amp;#39;the model is the product&amp;#39;&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lp2h0e","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp2h0e/training_and_finetuning_sparse_embedding_models/n0si7il/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751390060,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0rv6ul","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Affectionate-Cap-600","can_mod_post":false,"created_utc":1751383686,"send_replies":true,"parent_id":"t3_1lp2h0e","score":1,"author_fullname":"t2_5oltmr5b","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"really interesting!\\n\\nSomeone know if there is any plan to support Colbert-like models other than sparse/dense?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0rv6ul","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;really interesting!&lt;/p&gt;\\n\\n&lt;p&gt;Someone know if there is any plan to support Colbert-like models other than sparse/dense?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp2h0e/training_and_finetuning_sparse_embedding_models/n0rv6ul/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751383686,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lp2h0e","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0shvsi","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Accomplished_Mode170","can_mod_post":false,"created_utc":1751389973,"send_replies":true,"parent_id":"t3_1lp2h0e","score":1,"author_fullname":"t2_4hfmiefj","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Neat, that aligns with emerging evidence that 'the underlying geometries' hold across attention mechanisms too.\\n\\ni.e. [Sparse Attention itself is more expressive](https://www.tilderesearch.com/blog/sparse-attn)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0shvsi","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Neat, that aligns with emerging evidence that &amp;#39;the underlying geometries&amp;#39; hold across attention mechanisms too.&lt;/p&gt;\\n\\n&lt;p&gt;i.e. &lt;a href=\\"https://www.tilderesearch.com/blog/sparse-attn\\"&gt;Sparse Attention itself is more expressive&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp2h0e/training_and_finetuning_sparse_embedding_models/n0shvsi/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751389973,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lp2h0e","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0tm1fg","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MammayKaiseHain","can_mod_post":false,"created_utc":1751401260,"send_replies":true,"parent_id":"t3_1lp2h0e","score":1,"author_fullname":"t2_17owgu9uf5","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Why use a weaker reranker ? There is a Qwen3 reranker same as the embedding model.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0tm1fg","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Why use a weaker reranker ? There is a Qwen3 reranker same as the embedding model.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp2h0e/training_and_finetuning_sparse_embedding_models/n0tm1fg/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751401260,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lp2h0e","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),s=()=>e.jsx(t,{data:l});export{s as default};
