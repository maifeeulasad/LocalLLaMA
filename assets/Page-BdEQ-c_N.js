import{j as e}from"./index-CNyNkRpk.js";import{R as l}from"./RedditPostRenderer-Dza0u9i2.js";import"./index-BUchu_-K.js";const t=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Hi folks!\\n\\nI've recently been learning Triton and CUDA, writing my own kernels and optimizing them using a lot of great tricks Iâ€™ve picked up from blog-posts and docs. However, I currently donâ€™t have access to any local GPUs.\\n\\nRight now, Iâ€™m using Google Colab with T4 GPUs to run my kernels. I collect telemetry and kernel stats using nsight-compute, then download the reports and inspect them locally using the GUI.\\n\\nItâ€™s been workable thus far, but Iâ€™m wondering: how far can I realistically go with this workflow? Iâ€™m also a bit concerned about optimizing against the T4, since itâ€™s now three generations behind the latest architecture and Iâ€™m not sure how transferable performance insights will be.\\n\\nAlso, Iâ€™d love to hear how you are writing and profiling your kernels, especially if you're doing inference-time optimizations. Any tips or suggestions would be much appreciated.\\n\\nThanks in advance!","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Learning triton &amp; cuda: How far can colab + nsight-compute take me?","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lu1z10","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.8,"author_flair_background_color":null,"subreddit_type":"public","ups":3,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_1qr17tf3kl","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":3,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":1751913368,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1751912995,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Hi folks!&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;ve recently been learning Triton and CUDA, writing my own kernels and optimizing them using a lot of great tricks Iâ€™ve picked up from blog-posts and docs. However, I currently donâ€™t have access to any local GPUs.&lt;/p&gt;\\n\\n&lt;p&gt;Right now, Iâ€™m using Google Colab with T4 GPUs to run my kernels. I collect telemetry and kernel stats using nsight-compute, then download the reports and inspect them locally using the GUI.&lt;/p&gt;\\n\\n&lt;p&gt;Itâ€™s been workable thus far, but Iâ€™m wondering: how far can I realistically go with this workflow? Iâ€™m also a bit concerned about optimizing against the T4, since itâ€™s now three generations behind the latest architecture and Iâ€™m not sure how transferable performance insights will be.&lt;/p&gt;\\n\\n&lt;p&gt;Also, Iâ€™d love to hear how you are writing and profiling your kernels, especially if you&amp;#39;re doing inference-time optimizations. Any tips or suggestions would be much appreciated.&lt;/p&gt;\\n\\n&lt;p&gt;Thanks in advance!&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1lu1z10","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"Zealousideal_Elk109","discussion_type":null,"num_comments":3,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lu1z10/learning_triton_cuda_how_far_can_colab/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lu1z10/learning_triton_cuda_how_far_can_colab/","subreddit_subscribers":496034,"created_utc":1751912995,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1uukg0","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Accomplished_Mode170","can_mod_post":false,"created_utc":1751915409,"send_replies":true,"parent_id":"t3_1lu1z10","score":1,"author_fullname":"t2_4hfmiefj","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"As far as evolutionary algorithms and sampling allow? So like, forever I guessâ€¦ until your loss function gets stuck.\\n\\nI.e. build a flow you like that JITs your runtime AGAINST a set corpus, so you have a tool folks can use.\\n\\nOr are you asking if TPUs are gonna get replaced by RISC-V? Note: no idea ðŸ¤· scaling laws are weird ðŸ“Š","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1uukg0","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;As far as evolutionary algorithms and sampling allow? So like, forever I guessâ€¦ until your loss function gets stuck.&lt;/p&gt;\\n\\n&lt;p&gt;I.e. build a flow you like that JITs your runtime AGAINST a set corpus, so you have a tool folks can use.&lt;/p&gt;\\n\\n&lt;p&gt;Or are you asking if TPUs are gonna get replaced by RISC-V? Note: no idea ðŸ¤· scaling laws are weird ðŸ“Š&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lu1z10/learning_triton_cuda_how_far_can_colab/n1uukg0/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751915409,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lu1z10","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1ve7wm","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"FullstackSensei","can_mod_post":false,"created_utc":1751921930,"send_replies":true,"parent_id":"t3_1lu1z10","score":1,"author_fullname":"t2_17n3nqtj56","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"You can get very far with a T4. Focus on honing your skills and don't worry about the hardware. Turing being three generations old doesn't hinder your learning at all. The skill is building a mental model of how an optimization for a given hardware should be done and understanding the hardware characteristics that make the optimization work the way it does. If you do that, you'll be able to optimize for Blackwell in no time after reading about and understanding the architecture. This extends to data types. But if you're blindly trying things to see what sticks, the no amount of hardware will help.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1ve7wm","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You can get very far with a T4. Focus on honing your skills and don&amp;#39;t worry about the hardware. Turing being three generations old doesn&amp;#39;t hinder your learning at all. The skill is building a mental model of how an optimization for a given hardware should be done and understanding the hardware characteristics that make the optimization work the way it does. If you do that, you&amp;#39;ll be able to optimize for Blackwell in no time after reading about and understanding the architecture. This extends to data types. But if you&amp;#39;re blindly trying things to see what sticks, the no amount of hardware will help.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lu1z10/learning_triton_cuda_how_far_can_colab/n1ve7wm/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751921930,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lu1z10","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1ux57f","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"rnosov","can_mod_post":false,"created_utc":1751916258,"send_replies":true,"parent_id":"t3_1lu1z10","score":-1,"author_fullname":"t2_18x6fa","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"FYI modern LLMs are quite good at writing both Triton and Cuda C++ kernels. They often can implement non-trivial algorithms on a first/second try. To compete you'd probably need to hand craft PTX code. I think the ability to debug LLM written kernels will be much more valuable skill than knowing tiny differences between architectures.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1ux57f","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;FYI modern LLMs are quite good at writing both Triton and Cuda C++ kernels. They often can implement non-trivial algorithms on a first/second try. To compete you&amp;#39;d probably need to hand craft PTX code. I think the ability to debug LLM written kernels will be much more valuable skill than knowing tiny differences between architectures.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lu1z10/learning_triton_cuda_how_far_can_colab/n1ux57f/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751916258,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lu1z10","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-1}}],"before":null}}]`),r=()=>e.jsx(l,{data:t});export{r as default};
