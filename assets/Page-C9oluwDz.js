import{j as t}from"./index-CqAPCjw5.js";import{R as e}from"./RedditPostRenderer-4oBDAtGr.js";import"./index-D3Sdy_Op.js";const l=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Anyone know what the difference in tps would be for 64g mini pro vs 64g Studio since the studio has more gpu cores, but is it a meaningful difference for tps.  I'm getting 5.4 tps on 70b on the mini. Curious if it's worth going to the studio ","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"M4 Mini pro Vs M4 Studio","link_flair_richtext":[{"e":"text","t":"Discussion"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lrz52e","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.75,"author_flair_background_color":null,"subreddit_type":"public","ups":4,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_1gzvdilba8","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Discussion","can_mod_post":false,"score":4,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1751681234,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Anyone know what the difference in tps would be for 64g mini pro vs 64g Studio since the studio has more gpu cores, but is it a meaningful difference for tps.  I&amp;#39;m getting 5.4 tps on 70b on the mini. Curious if it&amp;#39;s worth going to the studio &lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"5f921ea4-c7bc-11ed-9c23-3a00622979b4","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#646d73","id":"1lrz52e","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"AlgorithmicMuse","discussion_type":null,"num_comments":14,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lrz52e/m4_mini_pro_vs_m4_studio/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lrz52e/m4_mini_pro_vs_m4_studio/","subreddit_subscribers":494987,"created_utc":1751681234,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1f5hkr","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AlgorithmicMuse","can_mod_post":false,"created_utc":1751689482,"send_replies":true,"parent_id":"t1_n1ept05","score":2,"author_fullname":"t2_1gzvdilba8","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Thanks, that a 100% gain if it scales linearly","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1f5hkr","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thanks, that a 100% gain if it scales linearly&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lrz52e","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrz52e/m4_mini_pro_vs_m4_studio/n1f5hkr/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751689482,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n1ept05","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"techtornado","can_mod_post":false,"created_utc":1751682258,"send_replies":true,"parent_id":"t3_1lrz52e","score":2,"author_fullname":"t2_12koak","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"It's probably about double the TPS between the two processor types \\n\\nYou'll have to scale up from my real-world M1 vs. M1 Pro example, but I can get 15tps on the M1 and 30tps on the M1Pro \\n\\nBoth Macs have 16gb of ram and 10.5gb of VRAM \\n\\nGemma 3 4B was loaded in LM Studio for this test","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1ept05","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It&amp;#39;s probably about double the TPS between the two processor types &lt;/p&gt;\\n\\n&lt;p&gt;You&amp;#39;ll have to scale up from my real-world M1 vs. M1 Pro example, but I can get 15tps on the M1 and 30tps on the M1Pro &lt;/p&gt;\\n\\n&lt;p&gt;Both Macs have 16gb of ram and 10.5gb of VRAM &lt;/p&gt;\\n\\n&lt;p&gt;Gemma 3 4B was loaded in LM Studio for this test&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrz52e/m4_mini_pro_vs_m4_studio/n1ept05/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751682258,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lrz52e","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1f54wd","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AlgorithmicMuse","can_mod_post":false,"created_utc":1751689307,"send_replies":true,"parent_id":"t1_n1ev208","score":1,"author_fullname":"t2_1gzvdilba8","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Two schools of thought on that'. If you need it now get it. If you don't need it now, wait. I'm in the get it now  school right now,  but not if it brings not much to the table. 100% gain is great, 20% gain , not worth it. Thats  the reason for question. No idea what a M5 or M6 would buy in gains in TPS , could wait a year for not much benefit so would not get a M5 or M6  anyway.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1f54wd","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Two schools of thought on that&amp;#39;. If you need it now get it. If you don&amp;#39;t need it now, wait. I&amp;#39;m in the get it now  school right now,  but not if it brings not much to the table. 100% gain is great, 20% gain , not worth it. Thats  the reason for question. No idea what a M5 or M6 would buy in gains in TPS , could wait a year for not much benefit so would not get a M5 or M6  anyway.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lrz52e","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrz52e/m4_mini_pro_vs_m4_studio/n1f54wd/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751689307,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1ev208","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"NNN_Throwaway2","can_mod_post":false,"created_utc":1751684596,"send_replies":true,"parent_id":"t3_1lrz52e","score":1,"author_fullname":"t2_8rrihts9","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"If you already have the Mini, and all you care about is more tps, just wait for M5 and M6 to come out later this year and next year respectively.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1ev208","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;If you already have the Mini, and all you care about is more tps, just wait for M5 and M6 to come out later this year and next year respectively.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrz52e/m4_mini_pro_vs_m4_studio/n1ev208/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751684596,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lrz52e","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1fid6k","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Only-Letterhead-3411","can_mod_post":false,"created_utc":1751696375,"send_replies":true,"parent_id":"t3_1lrz52e","score":1,"author_fullname":"t2_pbfqmgf8","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"You'll get 10 t/s on M4 Max and about 7-8 t/s on M2 Max. Imo it's not worth it. You already have mini and can run the models you want. And power consumption on the studio will be much higher compared to mini.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1fid6k","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You&amp;#39;ll get 10 t/s on M4 Max and about 7-8 t/s on M2 Max. Imo it&amp;#39;s not worth it. You already have mini and can run the models you want. And power consumption on the studio will be much higher compared to mini.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrz52e/m4_mini_pro_vs_m4_studio/n1fid6k/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751696375,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lrz52e","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1lx09r","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Baldur-Norddahl","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1lv9o9","score":1,"author_fullname":"t2_bvqb8ng0","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I am surprised the M3 Ultra is not faster compared to the M4 Max. Based on core count and memory bandwidth, the difference should be much more.","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n1lx09r","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I am surprised the M3 Ultra is not faster compared to the M4 Max. Based on core count and memory bandwidth, the difference should be much more.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lrz52e","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrz52e/m4_mini_pro_vs_m4_studio/n1lx09r/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751793920,"author_flair_text":null,"treatment_tags":[],"created_utc":1751793920,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":6,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1lv9o9","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"whg51","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1ka18a","score":2,"author_fullname":"t2_15tpry","approved_by":null,"mod_note":null,"all_awardings":[],"body":"I asked Mathematica to calculate 64!\\n\\nIn\\\\[3\\\\]:= 64!\\n\\nOut\\\\[3\\\\]= 126886932185884164103433389335161480802865516174545192198801894375214704230400000000000000\\n\\nI don't think that an LLM is able to do this sort of calculation.\\n\\nBut then I also ran your test with my base MacStudio M3 Ultra:  \\nollama run llama3.1:70b --verbose\\n\\n\\\\&gt;&gt;&gt; hi\\n\\nHello! How can I assist you today?\\n\\ntotal duration:       1.960969s\\n\\nload duration:        34.883042ms\\n\\nprompt eval count:    11 token(s)\\n\\nprompt eval duration: 1.195575333s\\n\\nprompt eval rate:     9.20 tokens/s\\n\\neval count:           10 token(s)\\n\\neval duration:        729.635667ms\\n\\neval rate:            13.71 tokens/s\\n\\n\\\\&gt;&gt;&gt; \\n\\n\\\\&gt;&gt;&gt; calculate 64!  , just show the answer \\n\\n1.20892581961462917470617646016850214499758702083380348253418444021176643361.208925819614629174706176460168502144997587020833803482534184440211766433649447624235434718626607895562549594543657425655663244844444044443889998040009447624235434718626607895562549594543657425655663244844444044443889998040000000\\n\\ntotal duration:       5.58363775s\\n\\nload duration:        31.765125ms\\n\\nprompt eval count:    41 token(s)\\n\\nprompt eval duration: 1.235060916s\\n\\nprompt eval rate:     33.20 tokens/s\\n\\neval count:           54 token(s)\\n\\neval duration:        4.316109625s\\n\\neval rate:            12.51 tokens/s\\n\\n\\\\&gt;&gt;&gt;\\n\\n  \\nSo we get these average results:\\n\\nM4 Pro 64GB: 5.5 t/s\\n\\nM4 Max: 11.0 t/s\\n\\nM3 Ultra: 13.1 t/s","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n1lv9o9","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I asked Mathematica to calculate 64!&lt;/p&gt;\\n\\n&lt;p&gt;In[3]:= 64!&lt;/p&gt;\\n\\n&lt;p&gt;Out[3]= 126886932185884164103433389335161480802865516174545192198801894375214704230400000000000000&lt;/p&gt;\\n\\n&lt;p&gt;I don&amp;#39;t think that an LLM is able to do this sort of calculation.&lt;/p&gt;\\n\\n&lt;p&gt;But then I also ran your test with my base MacStudio M3 Ultra:&lt;br/&gt;\\nollama run llama3.1:70b --verbose&lt;/p&gt;\\n\\n&lt;p&gt;&amp;gt;&amp;gt;&amp;gt; hi&lt;/p&gt;\\n\\n&lt;p&gt;Hello! How can I assist you today?&lt;/p&gt;\\n\\n&lt;p&gt;total duration:       1.960969s&lt;/p&gt;\\n\\n&lt;p&gt;load duration:        34.883042ms&lt;/p&gt;\\n\\n&lt;p&gt;prompt eval count:    11 token(s)&lt;/p&gt;\\n\\n&lt;p&gt;prompt eval duration: 1.195575333s&lt;/p&gt;\\n\\n&lt;p&gt;prompt eval rate:     9.20 tokens/s&lt;/p&gt;\\n\\n&lt;p&gt;eval count:           10 token(s)&lt;/p&gt;\\n\\n&lt;p&gt;eval duration:        729.635667ms&lt;/p&gt;\\n\\n&lt;p&gt;eval rate:            13.71 tokens/s&lt;/p&gt;\\n\\n&lt;p&gt;&amp;gt;&amp;gt;&amp;gt; &lt;/p&gt;\\n\\n&lt;p&gt;&amp;gt;&amp;gt;&amp;gt; calculate 64!  , just show the answer &lt;/p&gt;\\n\\n&lt;p&gt;1.20892581961462917470617646016850214499758702083380348253418444021176643361.208925819614629174706176460168502144997587020833803482534184440211766433649447624235434718626607895562549594543657425655663244844444044443889998040009447624235434718626607895562549594543657425655663244844444044443889998040000000&lt;/p&gt;\\n\\n&lt;p&gt;total duration:       5.58363775s&lt;/p&gt;\\n\\n&lt;p&gt;load duration:        31.765125ms&lt;/p&gt;\\n\\n&lt;p&gt;prompt eval count:    41 token(s)&lt;/p&gt;\\n\\n&lt;p&gt;prompt eval duration: 1.235060916s&lt;/p&gt;\\n\\n&lt;p&gt;prompt eval rate:     33.20 tokens/s&lt;/p&gt;\\n\\n&lt;p&gt;eval count:           54 token(s)&lt;/p&gt;\\n\\n&lt;p&gt;eval duration:        4.316109625s&lt;/p&gt;\\n\\n&lt;p&gt;eval rate:            12.51 tokens/s&lt;/p&gt;\\n\\n&lt;p&gt;&amp;gt;&amp;gt;&amp;gt;&lt;/p&gt;\\n\\n&lt;p&gt;So we get these average results:&lt;/p&gt;\\n\\n&lt;p&gt;M4 Pro 64GB: 5.5 t/s&lt;/p&gt;\\n\\n&lt;p&gt;M4 Max: 11.0 t/s&lt;/p&gt;\\n\\n&lt;p&gt;M3 Ultra: 13.1 t/s&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lrz52e","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrz52e/m4_mini_pro_vs_m4_studio/n1lv9o9/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751792841,"author_flair_text":null,"treatment_tags":[],"created_utc":1751792841,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1los66","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AlgorithmicMuse","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1ka18a","score":1,"author_fullname":"t2_1gzvdilba8","approved_by":null,"mod_note":null,"all_awardings":[],"body":"Thanks, you got about double with the max. So that's a good  data point,  I tried running it with cpu only, it was about 5% slower than gpu only. Thought it would have been slower. \\n\\nI read ollama is already using metal but not mlx, but it's being worked on .","edited":1751791115,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n1los66","is_submitter":true,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thanks, you got about double with the max. So that&amp;#39;s a good  data point,  I tried running it with cpu only, it was about 5% slower than gpu only. Thought it would have been slower. &lt;/p&gt;\\n\\n&lt;p&gt;I read ollama is already using metal but not mlx, but it&amp;#39;s being worked on .&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lrz52e","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrz52e/m4_mini_pro_vs_m4_studio/n1los66/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751788918,"author_flair_text":null,"treatment_tags":[],"created_utc":1751788918,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1ka18a","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Baldur-Norddahl","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1k7y6h","score":2,"author_fullname":"t2_bvqb8ng0","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Sure. Please note that I think this could be done faster than ollama. It is not using MLX or Metal.\\n\\n03:01:10 baldur@Mac \\\\~ → ollama run llama3.1:70b --verbose\\n\\n\\\\&gt;&gt;&gt; hi\\n\\nHello! How can I assist you today?\\n\\n\\n\\ntotal duration:       1.424786709s\\n\\nload duration:        21.829209ms\\n\\nprompt eval count:    11 token(s)\\n\\nprompt eval duration: 509.758375ms\\n\\nprompt eval rate:     21.58 tokens/s\\n\\neval count:           10 token(s)\\n\\neval duration:        892.459167ms\\n\\neval rate:            11.20 tokens/s\\n\\n\\\\&gt;&gt;&gt; calculate 64!  , just show the answer\\n\\n1.2089258196146291747061761201961082298123345189576578847639959183685804030574653769794188290179602542226665374197679263997266567309231877570718179964495482774319557102829564970678218451.208925819614629174706176120196108229812334518957657884763995918368580403057465376979418829017960254222666537419767926399726656730923187757071817996449548277431955710282956497067821845424902879609716\\n\\n\\n\\n (note: this is a very large number, and it's not possible to display the full result in a single line. The above answer is truncated for readability)\\n\\n\\n\\nFull answer has 86 digits\\n\\n\\n\\ntotal duration:       11.334491834s\\n\\nload duration:        18.176959ms\\n\\nprompt eval count:    41 token(s)\\n\\nprompt eval duration: 511.931375ms\\n\\nprompt eval rate:     80.09 tokens/s\\n\\neval count:           109 token(s)\\n\\neval duration:        10.803611333s\\n\\neval rate:            10.09 tokens/s","edited":false,"author_flair_css_class":null,"name":"t1_n1ka18a","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Sure. Please note that I think this could be done faster than ollama. It is not using MLX or Metal.&lt;/p&gt;\\n\\n&lt;p&gt;03:01:10 baldur@Mac ~ → ollama run llama3.1:70b --verbose&lt;/p&gt;\\n\\n&lt;p&gt;&amp;gt;&amp;gt;&amp;gt; hi&lt;/p&gt;\\n\\n&lt;p&gt;Hello! How can I assist you today?&lt;/p&gt;\\n\\n&lt;p&gt;total duration:       1.424786709s&lt;/p&gt;\\n\\n&lt;p&gt;load duration:        21.829209ms&lt;/p&gt;\\n\\n&lt;p&gt;prompt eval count:    11 token(s)&lt;/p&gt;\\n\\n&lt;p&gt;prompt eval duration: 509.758375ms&lt;/p&gt;\\n\\n&lt;p&gt;prompt eval rate:     21.58 tokens/s&lt;/p&gt;\\n\\n&lt;p&gt;eval count:           10 token(s)&lt;/p&gt;\\n\\n&lt;p&gt;eval duration:        892.459167ms&lt;/p&gt;\\n\\n&lt;p&gt;eval rate:            11.20 tokens/s&lt;/p&gt;\\n\\n&lt;p&gt;&amp;gt;&amp;gt;&amp;gt; calculate 64!  , just show the answer&lt;/p&gt;\\n\\n&lt;p&gt;1.2089258196146291747061761201961082298123345189576578847639959183685804030574653769794188290179602542226665374197679263997266567309231877570718179964495482774319557102829564970678218451.208925819614629174706176120196108229812334518957657884763995918368580403057465376979418829017960254222666537419767926399726656730923187757071817996449548277431955710282956497067821845424902879609716&lt;/p&gt;\\n\\n&lt;p&gt;(note: this is a very large number, and it&amp;#39;s not possible to display the full result in a single line. The above answer is truncated for readability)&lt;/p&gt;\\n\\n&lt;p&gt;Full answer has 86 digits&lt;/p&gt;\\n\\n&lt;p&gt;total duration:       11.334491834s&lt;/p&gt;\\n\\n&lt;p&gt;load duration:        18.176959ms&lt;/p&gt;\\n\\n&lt;p&gt;prompt eval count:    41 token(s)&lt;/p&gt;\\n\\n&lt;p&gt;prompt eval duration: 511.931375ms&lt;/p&gt;\\n\\n&lt;p&gt;prompt eval rate:     80.09 tokens/s&lt;/p&gt;\\n\\n&lt;p&gt;eval count:           109 token(s)&lt;/p&gt;\\n\\n&lt;p&gt;eval duration:        10.803611333s&lt;/p&gt;\\n\\n&lt;p&gt;eval rate:            10.09 tokens/s&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lrz52e","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrz52e/m4_mini_pro_vs_m4_studio/n1ka18a/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751764314,"author_flair_text":null,"collapsed":false,"created_utc":1751764314,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n1k7y6h","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AlgorithmicMuse","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1got28","score":1,"author_fullname":"t2_1gzvdilba8","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"\\n\\nthanks , I always get in the 5  to 6 TPS , mostly in the low 5's on every 70b model Ive tried.  \\n\\n could you try this  \\n\\njjl \\\\~ $ ollama run llama3.1:70b --verbose\\n\\n\\\\&gt;&gt;&gt; hi\\n\\nHello! How can I assist you today?\\n\\ntotal duration:       3.396920417s\\n\\nload duration:        32.685208ms\\n\\nprompt eval count:    11 token(s)\\n\\nprompt eval duration: 1.672324375s\\n\\nprompt eval rate:     6.58 tokens/s\\n\\neval count:           10 token(s)\\n\\neval duration:        1.69125025s\\n\\neval rate:            5.91 tokens/s\\n\\n\\\\&gt;&gt;&gt; \\n\\n\\\\&gt;&gt;&gt; calculate 64!  , just show the answer \\n\\nHere is the calculation of 64!:\\n\\n1.26886932284691651544955594184144832660631698713246559498528000000000000000000\\n\\nLet me know if you need more!\\n\\n  \\n( calculated as a 64-bit floating-point number, actual result has many more digits )\\n\\ntotal duration:       13.855984833s\\n\\nload duration:        32.039875ms\\n\\nprompt eval count:    204 token(s)\\n\\nprompt eval duration: 1.564407209s\\n\\nprompt eval rate:     130.40 tokens/s\\n\\neval count:           65 token(s)\\n\\neval duration:        12.258783s\\n\\neval rate:            5.30 tokens/s\\n\\n\\n\\n\\\\&gt;&gt;&gt; /bye","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1k7y6h","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;thanks , I always get in the 5  to 6 TPS , mostly in the low 5&amp;#39;s on every 70b model Ive tried.  &lt;/p&gt;\\n\\n&lt;p&gt;could you try this  &lt;/p&gt;\\n\\n&lt;p&gt;jjl ~ $ ollama run llama3.1:70b --verbose&lt;/p&gt;\\n\\n&lt;p&gt;&amp;gt;&amp;gt;&amp;gt; hi&lt;/p&gt;\\n\\n&lt;p&gt;Hello! How can I assist you today?&lt;/p&gt;\\n\\n&lt;p&gt;total duration:       3.396920417s&lt;/p&gt;\\n\\n&lt;p&gt;load duration:        32.685208ms&lt;/p&gt;\\n\\n&lt;p&gt;prompt eval count:    11 token(s)&lt;/p&gt;\\n\\n&lt;p&gt;prompt eval duration: 1.672324375s&lt;/p&gt;\\n\\n&lt;p&gt;prompt eval rate:     6.58 tokens/s&lt;/p&gt;\\n\\n&lt;p&gt;eval count:           10 token(s)&lt;/p&gt;\\n\\n&lt;p&gt;eval duration:        1.69125025s&lt;/p&gt;\\n\\n&lt;p&gt;eval rate:            5.91 tokens/s&lt;/p&gt;\\n\\n&lt;p&gt;&amp;gt;&amp;gt;&amp;gt; &lt;/p&gt;\\n\\n&lt;p&gt;&amp;gt;&amp;gt;&amp;gt; calculate 64!  , just show the answer &lt;/p&gt;\\n\\n&lt;p&gt;Here is the calculation of 64!:&lt;/p&gt;\\n\\n&lt;p&gt;1.26886932284691651544955594184144832660631698713246559498528000000000000000000&lt;/p&gt;\\n\\n&lt;p&gt;Let me know if you need more!&lt;/p&gt;\\n\\n&lt;p&gt;( calculated as a 64-bit floating-point number, actual result has many more digits )&lt;/p&gt;\\n\\n&lt;p&gt;total duration:       13.855984833s&lt;/p&gt;\\n\\n&lt;p&gt;load duration:        32.039875ms&lt;/p&gt;\\n\\n&lt;p&gt;prompt eval count:    204 token(s)&lt;/p&gt;\\n\\n&lt;p&gt;prompt eval duration: 1.564407209s&lt;/p&gt;\\n\\n&lt;p&gt;prompt eval rate:     130.40 tokens/s&lt;/p&gt;\\n\\n&lt;p&gt;eval count:           65 token(s)&lt;/p&gt;\\n\\n&lt;p&gt;eval duration:        12.258783s&lt;/p&gt;\\n\\n&lt;p&gt;eval rate:            5.30 tokens/s&lt;/p&gt;\\n\\n&lt;p&gt;&amp;gt;&amp;gt;&amp;gt; /bye&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lrz52e","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrz52e/m4_mini_pro_vs_m4_studio/n1k7y6h/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751763472,"author_flair_text":null,"treatment_tags":[],"created_utc":1751763472,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1got28","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Baldur-Norddahl","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1ggt2e","score":1,"author_fullname":"t2_bvqb8ng0","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I have a M4 Max MacBook Pro. I can test it for you, but you didn't specify exactly what model you have tested. I need to know model and quant size minimum. The models are not all the same speed even at the same parameters count.\\n\\nLlama 3.3 70b q4 dwq is doing 11.5 t/s on my Macbook.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n1got28","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I have a M4 Max MacBook Pro. I can test it for you, but you didn&amp;#39;t specify exactly what model you have tested. I need to know model and quant size minimum. The models are not all the same speed even at the same parameters count.&lt;/p&gt;\\n\\n&lt;p&gt;Llama 3.3 70b q4 dwq is doing 11.5 t/s on my Macbook.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lrz52e","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrz52e/m4_mini_pro_vs_m4_studio/n1got28/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751720137,"author_flair_text":null,"treatment_tags":[],"created_utc":1751720137,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1ggt2e","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AlgorithmicMuse","can_mod_post":false,"created_utc":1751716530,"send_replies":true,"parent_id":"t1_n1gdfgq","score":1,"author_fullname":"t2_1gzvdilba8","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Only question i asked was did any try it. I already know the specs,  Double gpus, double the mem bw.   70b  model  won't fit into 36g.  It was interesting  why on the mini setting ollama to use  gpu only or cpu only,  the tps were within 10% of each other","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1ggt2e","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Only question i asked was did any try it. I already know the specs,  Double gpus, double the mem bw.   70b  model  won&amp;#39;t fit into 36g.  It was interesting  why on the mini setting ollama to use  gpu only or cpu only,  the tps were within 10% of each other&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lrz52e","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrz52e/m4_mini_pro_vs_m4_studio/n1ggt2e/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751716530,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1gdfgq","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Baldur-Norddahl","can_mod_post":false,"created_utc":1751714852,"send_replies":true,"parent_id":"t3_1lrz52e","score":1,"author_fullname":"t2_bvqb8ng0","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"It is the wrong question. You want the M4 \\\\_MAX\\\\_ to double the memory bandwidth. This will double your t/s.\\n\\nLooks like M4 Max Studio 36 GB is the same price as M4 Pro Mini 64 GB. So you are trading memory for speed. Of course you could spend a little more to get the memory back.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1gdfgq","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It is the wrong question. You want the M4 _MAX_ to double the memory bandwidth. This will double your t/s.&lt;/p&gt;\\n\\n&lt;p&gt;Looks like M4 Max Studio 36 GB is the same price as M4 Pro Mini 64 GB. So you are trading memory for speed. Of course you could spend a little more to get the memory back.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrz52e/m4_mini_pro_vs_m4_studio/n1gdfgq/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751714852,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lrz52e","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1ikdwp","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Cergorach","can_mod_post":false,"created_utc":1751742456,"send_replies":true,"parent_id":"t3_1lrz52e","score":1,"author_fullname":"t2_cs4w88d2l","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"The M4 Max 12 core CPU (important! The M4 Max 10 core has 25% less memory bandwidth) has about double the memory bandwidth compared to the M4 Pro, thus about double the performance. The M3 Ultra has about trice the memory bandwidth of the M4 Pro, thus about 3x the performance of the M4 Pro. The GPU speed (amount of cores) has an impact on the speed to first token.\\n\\nAs for worth, it is significantly more expensive, takes up more space and uses more power. Personally I think that if you go for the Studio, you should go for the higher RAM configurations. Imho the speed of the LLM tens to be not so important, what it can handle more so.\\n\\nYou might be able to get a little bit more performance if your model is optimized for MLX. I get around 6t/s when I use the MLX version of 70b on my Mac Mini M4 Pro (20c) 64GB. Almost 12 seconds to first token (with a single line prompt). But golly! I forgot how wordy the darned thing is, so I can understand a certain level of frustration with it being wordy AND slow. With the M3 Ultra it would probably be up to \\\\~18t/s...","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1ikdwp","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The M4 Max 12 core CPU (important! The M4 Max 10 core has 25% less memory bandwidth) has about double the memory bandwidth compared to the M4 Pro, thus about double the performance. The M3 Ultra has about trice the memory bandwidth of the M4 Pro, thus about 3x the performance of the M4 Pro. The GPU speed (amount of cores) has an impact on the speed to first token.&lt;/p&gt;\\n\\n&lt;p&gt;As for worth, it is significantly more expensive, takes up more space and uses more power. Personally I think that if you go for the Studio, you should go for the higher RAM configurations. Imho the speed of the LLM tens to be not so important, what it can handle more so.&lt;/p&gt;\\n\\n&lt;p&gt;You might be able to get a little bit more performance if your model is optimized for MLX. I get around 6t/s when I use the MLX version of 70b on my Mac Mini M4 Pro (20c) 64GB. Almost 12 seconds to first token (with a single line prompt). But golly! I forgot how wordy the darned thing is, so I can understand a certain level of frustration with it being wordy AND slow. With the M3 Ultra it would probably be up to ~18t/s...&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrz52e/m4_mini_pro_vs_m4_studio/n1ikdwp/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751742456,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lrz52e","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),r=()=>t.jsx(e,{data:l});export{r as default};
