import{j as e}from"./index-BlGsFJYy.js";import{R as l}from"./RedditPostRenderer-B6uvq_Zl.js";import"./index-DDvVtNwD.js";const a=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"For you big rig runners who are fan's of ik_llama.cpp I just released a unique recipe of Kimi-K2-Instruct suitable for running on \\"only\\" ~368GB RAM - or less if you got any of that $weet $weet VRAM!\\n\\nThe perplexity clocks in at \`3.2741 +/- 0.01689\` which is not much higher (worse) than the full massive 1TB \`Q8_0\` baseline score of \`2.9507 +/- 0.01468\` despite being 34% of the full size!\\n\\nThe new \`IQ2_KL\` quant type just came out this week and I couldn't wait to give it a go. It is runs fast on both CUDA and CPU backend and packs in a ton of quality at only 2.69 bpw!\\n\\nWendell over at level1techs just hooked me up with a new remote rig with enough RAM and kioxia flash drives to actually maneuver this barge of a model, so big thanks as usual!\\n\\nI'll be releasing some more sizes soon so feel free to open a discussion on hf if there is a target break point size you'd like to see.\\n\\n*Remember* this quant only runs on ik_llama.cpp, instructions are on the github to download build and run any quants you already have as well as my quants.\\n\\nCheers!","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"IQ2_KL 345.687 GiB (2.892 BPW) Kimi-K2-Instruct GGUF ik exclusive!","link_flair_richtext":[{"e":"text","t":"New Model"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":75,"top_awarded_type":null,"hide_score":false,"name":"t3_1m0uoqo","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.9,"author_flair_background_color":"#bbbdbf","ups":54,"total_awards_received":0,"media_embed":{},"thumbnail_width":140,"author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","is_original_content":false,"author_fullname":"t2_n321yfw5","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"New Model","can_mod_post":false,"score":54,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"https://external-preview.redd.it/BsbQTqxa1yAiO8grxXFQK5H9V6cfqDi96Lmqnx03P4E.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=9409131e1802d5805748fc5e36865d9827100222","edited":false,"author_flair_css_class":null,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"gildings":{},"post_hint":"link","content_categories":null,"is_self":false,"subreddit_type":"public","created":1752615641,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"richtext","domain":"huggingface.co","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;For you big rig runners who are fan&amp;#39;s of ik_llama.cpp I just released a unique recipe of Kimi-K2-Instruct suitable for running on &amp;quot;only&amp;quot; ~368GB RAM - or less if you got any of that $weet $weet VRAM!&lt;/p&gt;\\n\\n&lt;p&gt;The perplexity clocks in at &lt;code&gt;3.2741 +/- 0.01689&lt;/code&gt; which is not much higher (worse) than the full massive 1TB &lt;code&gt;Q8_0&lt;/code&gt; baseline score of &lt;code&gt;2.9507 +/- 0.01468&lt;/code&gt; despite being 34% of the full size!&lt;/p&gt;\\n\\n&lt;p&gt;The new &lt;code&gt;IQ2_KL&lt;/code&gt; quant type just came out this week and I couldn&amp;#39;t wait to give it a go. It is runs fast on both CUDA and CPU backend and packs in a ton of quality at only 2.69 bpw!&lt;/p&gt;\\n\\n&lt;p&gt;Wendell over at level1techs just hooked me up with a new remote rig with enough RAM and kioxia flash drives to actually maneuver this barge of a model, so big thanks as usual!&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;ll be releasing some more sizes soon so feel free to open a discussion on hf if there is a target break point size you&amp;#39;d like to see.&lt;/p&gt;\\n\\n&lt;p&gt;&lt;em&gt;Remember&lt;/em&gt; this quant only runs on ik_llama.cpp, instructions are on the github to download build and run any quants you already have as well as my quants.&lt;/p&gt;\\n\\n&lt;p&gt;Cheers!&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"url_overridden_by_dest":"https://huggingface.co/ubergarm/Kimi-K2-Instruct-GGUF","view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"preview":{"images":[{"source":{"url":"https://external-preview.redd.it/BsbQTqxa1yAiO8grxXFQK5H9V6cfqDi96Lmqnx03P4E.png?auto=webp&amp;s=c9cbc5d289fcb88e7f2a478e46fb59540e618b43","width":1200,"height":648},"resolutions":[{"url":"https://external-preview.redd.it/BsbQTqxa1yAiO8grxXFQK5H9V6cfqDi96Lmqnx03P4E.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=b66a7e69efb7531c88f64412897d24ba07bb4949","width":108,"height":58},{"url":"https://external-preview.redd.it/BsbQTqxa1yAiO8grxXFQK5H9V6cfqDi96Lmqnx03P4E.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=1503aaf6c1d3c756fc94832b2e5690303d632991","width":216,"height":116},{"url":"https://external-preview.redd.it/BsbQTqxa1yAiO8grxXFQK5H9V6cfqDi96Lmqnx03P4E.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=a089811ac299b56e1a0108bf66794197b994c231","width":320,"height":172},{"url":"https://external-preview.redd.it/BsbQTqxa1yAiO8grxXFQK5H9V6cfqDi96Lmqnx03P4E.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=fe89633f63f298fcdd1962f72916307a0c4801ec","width":640,"height":345},{"url":"https://external-preview.redd.it/BsbQTqxa1yAiO8grxXFQK5H9V6cfqDi96Lmqnx03P4E.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=d4ca87e21be46a2fba515643708e937b12a1e622","width":960,"height":518},{"url":"https://external-preview.redd.it/BsbQTqxa1yAiO8grxXFQK5H9V6cfqDi96Lmqnx03P4E.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=065fcaed86fb7a4c21613ab7baf201d7f06089f5","width":1080,"height":583}],"variants":{},"id":"BsbQTqxa1yAiO8grxXFQK5H9V6cfqDi96Lmqnx03P4E"}],"enabled":false},"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"ced98442-f5d3-11ed-b657-66d3b15490c6","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":"llama.cpp","treatment_tags":[],"visited":false,"removed_by":null,"mod_note":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"num_reports":null,"removal_reason":null,"link_flair_background_color":"#ffb000","id":"1m0uoqo","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"VoidAlchemy","discussion_type":null,"num_comments":29,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":"light","permalink":"/r/LocalLLaMA/comments/1m0uoqo/iq2_kl_345687_gib_2892_bpw_kimik2instruct_gguf_ik/","stickied":false,"url":"https://huggingface.co/ubergarm/Kimi-K2-Instruct-GGUF","subreddit_subscribers":499773,"created_utc":1752615641,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3dqg0b","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"VoidAlchemy","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3cy2y9","score":3,"author_fullname":"t2_n321yfw5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"  \\nThe PP is good, thanks for asking!\\n\\nhttps://preview.redd.it/a6xwswl6i5df1.png?width=4176&amp;format=png&amp;auto=webp&amp;s=f83b8f644d3c3abf45934fca6a8e10c180871efb\\n\\nFull command and details [https://github.com/ikawrakow/ik\\\\_llama.cpp/pull/612#issuecomment-3076539817](https://github.com/ikawrakow/ik_llama.cpp/pull/612#issuecomment-3076539817)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3dqg0b","is_submitter":true,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The PP is good, thanks for asking!&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://preview.redd.it/a6xwswl6i5df1.png?width=4176&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f83b8f644d3c3abf45934fca6a8e10c180871efb\\"&gt;https://preview.redd.it/a6xwswl6i5df1.png?width=4176&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=f83b8f644d3c3abf45934fca6a8e10c180871efb&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;Full command and details &lt;a href=\\"https://github.com/ikawrakow/ik_llama.cpp/pull/612#issuecomment-3076539817\\"&gt;https://github.com/ikawrakow/ik_llama.cpp/pull/612#issuecomment-3076539817&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0uoqo","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0uoqo/iq2_kl_345687_gib_2892_bpw_kimik2instruct_gguf_ik/n3dqg0b/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752634516,"media_metadata":{"a6xwswl6i5df1":{"status":"valid","e":"Image","m":"image/png","p":[{"y":57,"x":108,"u":"https://preview.redd.it/a6xwswl6i5df1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=bdb4df2d74cf07c22cc9a74f9948b97b8ed863e8"},{"y":114,"x":216,"u":"https://preview.redd.it/a6xwswl6i5df1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=a3f9e0d0c28be454d0a6c3ee3a2b2db69d009f1f"},{"y":169,"x":320,"u":"https://preview.redd.it/a6xwswl6i5df1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=9abdc7f1dafef03efc8c89a63a63225ecd68d336"},{"y":339,"x":640,"u":"https://preview.redd.it/a6xwswl6i5df1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=7af4ece1c6f4398b7aabedd5fb5fcbf01bf25b69"},{"y":509,"x":960,"u":"https://preview.redd.it/a6xwswl6i5df1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=4493769031aa06ad89a90b3e06271376f1b46708"},{"y":573,"x":1080,"u":"https://preview.redd.it/a6xwswl6i5df1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=20c56d9924fefab33c8436dcf86b9fd0d01e576a"}],"s":{"y":2217,"x":4176,"u":"https://preview.redd.it/a6xwswl6i5df1.png?width=4176&amp;format=png&amp;auto=webp&amp;s=f83b8f644d3c3abf45934fca6a8e10c180871efb"},"id":"a6xwswl6i5df1"}},"author_flair_text":"llama.cpp","treatment_tags":[],"created_utc":1752634516,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n3cy2y9","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Agreeable-Prompt-666","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3cuxtq","score":2,"author_fullname":"t2_1l3z4stvkq","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Hi ubergaram love your work and thank you. Is the aprox ~13/toks from llama-bench/sweep? How's the pp? Thank you","edited":1752624726,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3cy2y9","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Hi ubergaram love your work and thank you. Is the aprox ~13/toks from llama-bench/sweep? How&amp;#39;s the pp? Thank you&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0uoqo","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0uoqo/iq2_kl_345687_gib_2892_bpw_kimik2instruct_gguf_ik/n3cy2y9/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752624475,"author_flair_text":null,"treatment_tags":[],"created_utc":1752624475,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3es6ml","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Minute_Attempt3063","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3cuxtq","score":3,"author_fullname":"t2_t6m6d9my","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"13 tokens a second is good on a CPU, with a model this size. Neat!","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3es6ml","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;13 tokens a second is good on a CPU, with a model this size. Neat!&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0uoqo","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0uoqo/iq2_kl_345687_gib_2892_bpw_kimik2instruct_gguf_ik/n3es6ml/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752652903,"author_flair_text":null,"treatment_tags":[],"created_utc":1752652903,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n3cuxtq","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"VoidAlchemy","can_mod_post":false,"created_utc":1752623391,"send_replies":true,"parent_id":"t1_n3ciz4f","score":4,"author_fullname":"t2_n321yfw5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Anecdotally like 13 tok/sec TG on a newer amd epyc with \\\\~256GB/s RAM bandwidth in a single socket so not bad. It would go faster with some GPUs but this was just a big RAM rig no VRAM. I haven't run it on CUDA yet actually, gotta make a smaller quant xD","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3cuxtq","is_submitter":true,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Anecdotally like 13 tok/sec TG on a newer amd epyc with ~256GB/s RAM bandwidth in a single socket so not bad. It would go faster with some GPUs but this was just a big RAM rig no VRAM. I haven&amp;#39;t run it on CUDA yet actually, gotta make a smaller quant xD&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0uoqo","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0uoqo/iq2_kl_345687_gib_2892_bpw_kimik2instruct_gguf_ik/n3cuxtq/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752623391,"author_flair_text":"llama.cpp","treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}}],"before":null}},"user_reports":[],"saved":false,"id":"n3ciz4f","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"segmond","can_mod_post":false,"created_utc":1752619460,"send_replies":true,"parent_id":"t3_1m0uoqo","score":4,"author_fullname":"t2_ah13x","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"what kind of performance do you see, system specs?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3ciz4f","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;what kind of performance do you see, system specs?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0uoqo/iq2_kl_345687_gib_2892_bpw_kimik2instruct_gguf_ik/n3ciz4f/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752619460,"author_flair_text":"llama.cpp","treatment_tags":[],"link_id":"t3_1m0uoqo","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3ctryr","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"VoidAlchemy","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3cb2e7","score":2,"author_fullname":"t2_n321yfw5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yeah one of my fav combos it iq5\\\\_ks and iq4\\\\_ks... they ks are a little faster but slightly worse PPL. The iq5\\\\_K and iq6\\\\_K are probably the best quality available, but they inference a little slower so its whatever you prefer really for your application and hardware combination.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3ctryr","is_submitter":true,"collapsed":false,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yeah one of my fav combos it iq5_ks and iq4_ks... they ks are a little faster but slightly worse PPL. The iq5_K and iq6_K are probably the best quality available, but they inference a little slower so its whatever you prefer really for your application and hardware combination.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0uoqo","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0uoqo/iq2_kl_345687_gib_2892_bpw_kimik2instruct_gguf_ik/n3ctryr/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752623004,"author_flair_text":"llama.cpp","treatment_tags":[],"created_utc":1752623004,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n3cb2e7","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"erazortt","can_mod_post":false,"created_utc":1752616816,"send_replies":true,"parent_id":"t1_n3c7guh","score":1,"author_fullname":"t2_6z7m9i7r","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Could you also run the higher Qs, like IQ5\\\\_K or IQ6\\\\_K? That would be interesting.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3cb2e7","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Could you also run the higher Qs, like IQ5_K or IQ6_K? That would be interesting.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0uoqo","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0uoqo/iq2_kl_345687_gib_2892_bpw_kimik2instruct_gguf_ik/n3cb2e7/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752616816,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3c7guh","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"VoidAlchemy","can_mod_post":false,"created_utc":1752615716,"send_replies":true,"parent_id":"t3_1m0uoqo","score":5,"author_fullname":"t2_n321yfw5","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I ran a few benchmarks comparing ik's quants to show where the new IQ2\\\\_KL falls. The Kimi-K2-Instruct released combines IQ2\\\\_KL and IQ3\\\\_KS to hit that sweet spot trade-off between size and quality. (And higher quality for attn/shexp/dense layers.) Remember pretty much \\\\*all\\\\* quants are \\"dynamic\\" since the beginning. That is why you have to specify \\\\\`llama-quantize --pure\\\\\` if you don't want that, as quants are \\"dynamic\\" by default!\\n\\nhttps://preview.redd.it/t6oo42i9y3df1.png?width=1782&amp;format=png&amp;auto=webp&amp;s=02fe76b375628b2b8c987d3f7c163b456ca4f549","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3c7guh","is_submitter":true,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I ran a few benchmarks comparing ik&amp;#39;s quants to show where the new IQ2_KL falls. The Kimi-K2-Instruct released combines IQ2_KL and IQ3_KS to hit that sweet spot trade-off between size and quality. (And higher quality for attn/shexp/dense layers.) Remember pretty much *all* quants are &amp;quot;dynamic&amp;quot; since the beginning. That is why you have to specify \`llama-quantize --pure\` if you don&amp;#39;t want that, as quants are &amp;quot;dynamic&amp;quot; by default!&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://preview.redd.it/t6oo42i9y3df1.png?width=1782&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=02fe76b375628b2b8c987d3f7c163b456ca4f549\\"&gt;https://preview.redd.it/t6oo42i9y3df1.png?width=1782&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=02fe76b375628b2b8c987d3f7c163b456ca4f549&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0uoqo/iq2_kl_345687_gib_2892_bpw_kimik2instruct_gguf_ik/n3c7guh/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752615716,"media_metadata":{"t6oo42i9y3df1":{"status":"valid","e":"Image","m":"image/png","p":[{"y":71,"x":108,"u":"https://preview.redd.it/t6oo42i9y3df1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=36789d5d2f2f0b342a1f9531ed99d41cf4b9c2af"},{"y":143,"x":216,"u":"https://preview.redd.it/t6oo42i9y3df1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=4a80051680e25fb3385874ab87aa5f10ff5227fe"},{"y":212,"x":320,"u":"https://preview.redd.it/t6oo42i9y3df1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=e3995c17566779b48fd8e72d1ea1b06fb3c5451d"},{"y":424,"x":640,"u":"https://preview.redd.it/t6oo42i9y3df1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=37ba03600138ad2ecc1e23d3b326bde1e92b044a"},{"y":636,"x":960,"u":"https://preview.redd.it/t6oo42i9y3df1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=d30bcafa2f193a37c26915add637835a6d155c2b"},{"y":715,"x":1080,"u":"https://preview.redd.it/t6oo42i9y3df1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=228fef480a356c83bdde9b740b9d91c09d826370"}],"s":{"y":1181,"x":1782,"u":"https://preview.redd.it/t6oo42i9y3df1.png?width=1782&amp;format=png&amp;auto=webp&amp;s=02fe76b375628b2b8c987d3f7c163b456ca4f549"},"id":"t6oo42i9y3df1"}},"author_flair_text":"llama.cpp","treatment_tags":[],"link_id":"t3_1m0uoqo","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ef488598-491f-11ef-a847-9a3dd315819c","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ef488598-491f-11ef-a847-9a3dd315819c","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3cyb2q","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"segmond","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3cjln2","score":2,"author_fullname":"t2_ah13x","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"thanks, i think i can run it across clusters, but network RPC latency is a killer.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3cyb2q","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;thanks, i think i can run it across clusters, but network RPC latency is a killer.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0uoqo","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0uoqo/iq2_kl_345687_gib_2892_bpw_kimik2instruct_gguf_ik/n3cyb2q/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752624552,"author_flair_text":"llama.cpp","treatment_tags":[],"created_utc":1752624552,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ef488598-491f-11ef-a847-9a3dd315819c","distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ef488598-491f-11ef-a847-9a3dd315819c","distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3dgcv2","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"VoidAlchemy","can_mod_post":false,"created_utc":1752630855,"send_replies":true,"parent_id":"t1_n3d4m6g","score":2,"author_fullname":"t2_n321yfw5","approved_by":null,"mod_note":null,"all_awardings":[],"body":"This is one area on which I'm most unclear given I don't have so many GPUs to try haha.. I know that \\\\\`-amb 512\\\\\` can help that out, but I guess multi-GPUs does something else to blow up the CUDA compute buffers?\\n\\nI just ran my IQ2\\\\_KT TNG-R1T2-Chimera on dual RTX PRO 6000s Blackwell's 198GB VRAM total with \\\\\`-ub 4096\\\\\` and 40k context and got 1200 tok/sec PP and 40 tok/sec TG in short kv-cache depths.\\n\\nPretty wild I saw you run that same model and get great speeds on 5 GPUs too!","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n3dgcv2","is_submitter":true,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;This is one area on which I&amp;#39;m most unclear given I don&amp;#39;t have so many GPUs to try haha.. I know that \`-amb 512\` can help that out, but I guess multi-GPUs does something else to blow up the CUDA compute buffers?&lt;/p&gt;\\n\\n&lt;p&gt;I just ran my IQ2_KT TNG-R1T2-Chimera on dual RTX PRO 6000s Blackwell&amp;#39;s 198GB VRAM total with \`-ub 4096\` and 40k context and got 1200 tok/sec PP and 40 tok/sec TG in short kv-cache depths.&lt;/p&gt;\\n\\n&lt;p&gt;Pretty wild I saw you run that same model and get great speeds on 5 GPUs too!&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1m0uoqo","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0uoqo/iq2_kl_345687_gib_2892_bpw_kimik2instruct_gguf_ik/n3dgcv2/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752630855,"author_flair_text":"llama.cpp","treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":7,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n3d4m6g","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"panchovix","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3d4hc0","score":2,"author_fullname":"t2_j1kqr","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Thanks for the suggestion tho! Anyone with multigpu should do that as compute buffers get HUGE with that value on 4.","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n3d4m6g","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"Llama 405B"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thanks for the suggestion tho! Anyone with multigpu should do that as compute buffers get HUGE with that value on 4.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1m0uoqo","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0uoqo/iq2_kl_345687_gib_2892_bpw_kimik2instruct_gguf_ik/n3d4m6g/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752626749,"author_flair_text":"Llama 405B","treatment_tags":[],"created_utc":1752626749,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":6,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n3d4hc0","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Kooshi_Govno","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3d2xxm","score":2,"author_fullname":"t2_7kg5p","approved_by":null,"mod_note":null,"all_awardings":[],"body":"Damn, alright","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n3d4hc0","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Damn, alright&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1m0uoqo","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0uoqo/iq2_kl_345687_gib_2892_bpw_kimik2instruct_gguf_ik/n3d4hc0/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752626702,"author_flair_text":null,"treatment_tags":[],"created_utc":1752626702,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n3d2xxm","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"panchovix","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3d1d91","score":2,"author_fullname":"t2_j1kqr","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I do (on ik llamacpp) but still has overhead.","edited":false,"author_flair_css_class":null,"name":"t1_n3d2xxm","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"Llama 405B"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I do (on ik llamacpp) but still has overhead.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1m0uoqo","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":"light","treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0uoqo/iq2_kl_345687_gib_2892_bpw_kimik2instruct_gguf_ik/n3d2xxm/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752626170,"author_flair_text":"Llama 405B","collapsed":false,"created_utc":1752626170,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n3d1d91","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Kooshi_Govno","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3cjln2","score":2,"author_fullname":"t2_7kg5p","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Recompile with \`GGML_SCHED_MAX_COPIES=1\` to eliminate the overhead. It defaults to 4, which is a ridiculous waste of VRAM.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3d1d91","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Recompile with &lt;code&gt;GGML_SCHED_MAX_COPIES=1&lt;/code&gt; to eliminate the overhead. It defaults to 4, which is a ridiculous waste of VRAM.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0uoqo","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0uoqo/iq2_kl_345687_gib_2892_bpw_kimik2instruct_gguf_ik/n3d1d91/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752625616,"author_flair_text":null,"treatment_tags":[],"created_utc":1752625616,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n3cjln2","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"panchovix","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3cj62m","score":3,"author_fullname":"t2_j1kqr","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"208GB VRAM + 192GB RAM.\\n\\nIt has MLA so compute buffers are quite small vs other models, but multigpu penalty on VRAM (7 GPUs) is a lot so usable is more like 180GB VRAM or so. Also I can't quite use l the ram on Linux either, about 180GB before starts to use swap.\\n\\nWindows is way worse so it's not considered lol.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3cjln2","is_submitter":false,"collapsed":false,"author_flair_richtext":[{"e":"text","t":"Llama 405B"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;208GB VRAM + 192GB RAM.&lt;/p&gt;\\n\\n&lt;p&gt;It has MLA so compute buffers are quite small vs other models, but multigpu penalty on VRAM (7 GPUs) is a lot so usable is more like 180GB VRAM or so. Also I can&amp;#39;t quite use l the ram on Linux either, about 180GB before starts to use swap.&lt;/p&gt;\\n\\n&lt;p&gt;Windows is way worse so it&amp;#39;s not considered lol.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0uoqo","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0uoqo/iq2_kl_345687_gib_2892_bpw_kimik2instruct_gguf_ik/n3cjln2/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752619663,"author_flair_text":"Llama 405B","treatment_tags":[],"created_utc":1752619663,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n3cj62m","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"segmond","can_mod_post":false,"created_utc":1752619522,"send_replies":true,"parent_id":"t1_n3cfhg4","score":2,"author_fullname":"t2_ah13x","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"400gb total between vram and system or just system?  is it as bad as deepseek was in the beginning? i know deepseek was a monster for compute/kv buffers until they had some optimization.  i couldn't run it before and after they fixed it, it became manageable.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3cj62m","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;400gb total between vram and system or just system?  is it as bad as deepseek was in the beginning? i know deepseek was a monster for compute/kv buffers until they had some optimization.  i couldn&amp;#39;t run it before and after they fixed it, it became manageable.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0uoqo","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0uoqo/iq2_kl_345687_gib_2892_bpw_kimik2instruct_gguf_ik/n3cj62m/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752619522,"author_flair_text":"llama.cpp","treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ef488598-491f-11ef-a847-9a3dd315819c","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ef488598-491f-11ef-a847-9a3dd315819c","distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3fpgk9","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"VoidAlchemy","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3cyygu","score":1,"author_fullname":"t2_n321yfw5","approved_by":null,"mod_note":null,"all_awardings":[],"body":"OMG these big models suck up disk storage so fast!\\n\\nIf you manage to free up some space, currently uploading Kimi-K2-Instruct-IQ2\\\\_KS 286.624 GiB (2.398 BPW). It uses those brand new IQ2\\\\_KL for down and smaller IQ2\\\\_KS for (gate|up).\\n\\nI'll update the model card Perplexity TODO after I get the data. Cheers!","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n3fpgk9","is_submitter":true,"collapsed":false,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;OMG these big models suck up disk storage so fast!&lt;/p&gt;\\n\\n&lt;p&gt;If you manage to free up some space, currently uploading Kimi-K2-Instruct-IQ2_KS 286.624 GiB (2.398 BPW). It uses those brand new IQ2_KL for down and smaller IQ2_KS for (gate|up).&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;ll update the model card Perplexity TODO after I get the data. Cheers!&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1m0uoqo","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0uoqo/iq2_kl_345687_gib_2892_bpw_kimik2instruct_gguf_ik/n3fpgk9/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752669302,"author_flair_text":"llama.cpp","treatment_tags":[],"created_utc":1752669302,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3cyygu","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"panchovix","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3cuoon","score":2,"author_fullname":"t2_j1kqr","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Oh nice! If you manage to make just a bit smaller I could try, but if not no worries, I think I'm quite short in storage space at the moment lol.","edited":false,"author_flair_css_class":null,"name":"t1_n3cyygu","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"Llama 405B"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Oh nice! If you manage to make just a bit smaller I could try, but if not no worries, I think I&amp;#39;m quite short in storage space at the moment lol.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1m0uoqo","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":"light","treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0uoqo/iq2_kl_345687_gib_2892_bpw_kimik2instruct_gguf_ik/n3cyygu/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752624777,"author_flair_text":"Llama 405B","collapsed":false,"created_utc":1752624777,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n3cuoon","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"VoidAlchemy","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3cotu3","score":3,"author_fullname":"t2_n321yfw5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"rumor is that ik is cooking up a new IQ1\\\\_KT 1.75BPW quant just for you! The KT are the trellis/qtip style quants similar to EXL3. They are super fast on CUDA but have a TG penalty due to CPU overhead on RAM. It was stil about 80% TG as similar sized quants on my amd 9950x. See u round and maybe I can get a smaller one for you going down a step to IQ2\\\\_KL ffn\\\\_(down) and IQ2\\\\_KS ffn\\\\_(gate|up) should fit ur rig and not be much worse PPL.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3cuoon","is_submitter":true,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;rumor is that ik is cooking up a new IQ1_KT 1.75BPW quant just for you! The KT are the trellis/qtip style quants similar to EXL3. They are super fast on CUDA but have a TG penalty due to CPU overhead on RAM. It was stil about 80% TG as similar sized quants on my amd 9950x. See u round and maybe I can get a smaller one for you going down a step to IQ2_KL ffn_(down) and IQ2_KS ffn_(gate|up) should fit ur rig and not be much worse PPL.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0uoqo","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0uoqo/iq2_kl_345687_gib_2892_bpw_kimik2instruct_gguf_ik/n3cuoon/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752623305,"author_flair_text":"llama.cpp","treatment_tags":[],"created_utc":1752623305,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3den3x","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"a_beautiful_rhind","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3cotu3","score":1,"author_fullname":"t2_h5utwre7","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I didn't really love the R1.5 that dropped over R1/V3-0324 so I got it in IQ1. Was ok. Not the 160gb version though, so if the UD Q1 is like that, can def see it.\\n\\nSucks about ernie, it was kind of my last hope from the new releases.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3den3x","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I didn&amp;#39;t really love the R1.5 that dropped over R1/V3-0324 so I got it in IQ1. Was ok. Not the 160gb version though, so if the UD Q1 is like that, can def see it.&lt;/p&gt;\\n\\n&lt;p&gt;Sucks about ernie, it was kind of my last hope from the new releases.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0uoqo","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0uoqo/iq2_kl_345687_gib_2892_bpw_kimik2instruct_gguf_ik/n3den3x/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752630265,"author_flair_text":null,"treatment_tags":[],"created_utc":1752630265,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3cotu3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"panchovix","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3cluq5","score":3,"author_fullname":"t2_j1kqr","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"IQ1 drop in quality is prob too much to my taste (at that point deepseek at a similar size is probably better).\\n\\nI tested Ernie 300B at 5 bpw on exl3 and didn't like it tbh, I still feel DeepSeek is noticeably better. Maybe it is more sensitive to quantization.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3cotu3","is_submitter":false,"collapsed":false,"author_flair_richtext":[{"e":"text","t":"Llama 405B"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;IQ1 drop in quality is prob too much to my taste (at that point deepseek at a similar size is probably better).&lt;/p&gt;\\n\\n&lt;p&gt;I tested Ernie 300B at 5 bpw on exl3 and didn&amp;#39;t like it tbh, I still feel DeepSeek is noticeably better. Maybe it is more sensitive to quantization.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0uoqo","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0uoqo/iq2_kl_345687_gib_2892_bpw_kimik2instruct_gguf_ik/n3cotu3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752621375,"author_flair_text":"Llama 405B","treatment_tags":[],"created_utc":1752621375,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n3cluq5","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"a_beautiful_rhind","can_mod_post":false,"created_utc":1752620399,"send_replies":true,"parent_id":"t1_n3cfhg4","score":2,"author_fullname":"t2_h5utwre7","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"The IQ1 are around 250gb. Personally, I am hoping big ERNIEs get some support. This model is kind of like a more refusy deepseek with better code skills.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3cluq5","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The IQ1 are around 250gb. Personally, I am hoping big ERNIEs get some support. This model is kind of like a more refusy deepseek with better code skills.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0uoqo","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0uoqo/iq2_kl_345687_gib_2892_bpw_kimik2instruct_gguf_ik/n3cluq5/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752620399,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n3cfhg4","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"panchovix","can_mod_post":false,"created_utc":1752618310,"send_replies":true,"parent_id":"t3_1m0uoqo","score":2,"author_fullname":"t2_j1kqr","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"As always nice work! Sadly despite having 400gb total memory, multiple GPUs VRAM overheard doesn't let me load the model :( compute buffers and hidden buffers are really big haha.\\n\\nWill have to skip Kimi-K2 this once, will maybe give a try at the end of year I upgrade to a different system tho!\\n\\nAlso amazing you managed to get the PPL of the Q8_0 model, I guess that took a while.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3cfhg4","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"Llama 405B"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;As always nice work! Sadly despite having 400gb total memory, multiple GPUs VRAM overheard doesn&amp;#39;t let me load the model :( compute buffers and hidden buffers are really big haha.&lt;/p&gt;\\n\\n&lt;p&gt;Will have to skip Kimi-K2 this once, will maybe give a try at the end of year I upgrade to a different system tho!&lt;/p&gt;\\n\\n&lt;p&gt;Also amazing you managed to get the PPL of the Q8_0 model, I guess that took a while.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0uoqo/iq2_kl_345687_gib_2892_bpw_kimik2instruct_gguf_ik/n3cfhg4/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752618310,"author_flair_text":"Llama 405B","treatment_tags":[],"link_id":"t3_1m0uoqo","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3djgkl","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"solidhadriel","can_mod_post":false,"created_utc":1752631947,"send_replies":true,"parent_id":"t3_1m0uoqo","score":2,"author_fullname":"t2_almnq","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Awesome. I just might have to give it a go on my 512 gb ram xeon server 🔥. I haven't tried ik_llama in a while though. I've been rocking llamacpp with Deepseek.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3djgkl","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Awesome. I just might have to give it a go on my 512 gb ram xeon server 🔥. I haven&amp;#39;t tried ik_llama in a while though. I&amp;#39;ve been rocking llamacpp with Deepseek.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0uoqo/iq2_kl_345687_gib_2892_bpw_kimik2instruct_gguf_ik/n3djgkl/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752631947,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m0uoqo","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3fpvd6","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"VoidAlchemy","can_mod_post":false,"created_utc":1752669449,"send_replies":true,"parent_id":"t1_n3fcn3o","score":1,"author_fullname":"t2_n321yfw5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"ik has a few improvements going yes. usually PP is faster on ik's fork when any tensors are running on CPU. If you're hitting 20 t/s token generation you're probably pretty maxed out on ram bandwidth which is a limit on either fork.\\n\\nEasy enough to give it a try with your existing quants and see. Also if you have zen5 there is a new experimental avx512 PP boost in the works.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3fpvd6","is_submitter":true,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;ik has a few improvements going yes. usually PP is faster on ik&amp;#39;s fork when any tensors are running on CPU. If you&amp;#39;re hitting 20 t/s token generation you&amp;#39;re probably pretty maxed out on ram bandwidth which is a limit on either fork.&lt;/p&gt;\\n\\n&lt;p&gt;Easy enough to give it a try with your existing quants and see. Also if you have zen5 there is a new experimental avx512 PP boost in the works.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0uoqo","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0uoqo/iq2_kl_345687_gib_2892_bpw_kimik2instruct_gguf_ik/n3fpvd6/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752669449,"author_flair_text":"llama.cpp","treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3fcn3o","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"harrythunder","can_mod_post":false,"created_utc":1752664130,"send_replies":true,"parent_id":"t3_1m0uoqo","score":2,"author_fullname":"t2_9nksf","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I've got this running on mainline llama.cpp, 20 t/s on 2x RX6000. Are we seeing improvements on ik_llama.cpp?\\n\\n  --ctx-size 131072 -ctk q8_0 -fa \\\\\\n  -b 4096 -ub 4096 \\\\\\n  --n-gpu-layers 999 \\\\\\n  --main-gpu 1 \\\\\\n  --tensor-split 0.2,0.8 \\\\\\n  --parallel 2 \\\\\\n  --threads 16 \\\\\\n  -ot \\"\\\\\\\\.([2-9][6-9]|[3-9][0-9]|[0-9][0-9][0-9])\\\\\\\\.ffn_(gate|up|down)_exps\\\\\\\\.=CPU\\" \\\\","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3fcn3o","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;ve got this running on mainline llama.cpp, 20 t/s on 2x RX6000. Are we seeing improvements on ik_llama.cpp?&lt;/p&gt;\\n\\n&lt;p&gt;--ctx-size 131072 -ctk q8&lt;em&gt;0 -fa \\\\\\n  -b 4096 -ub 4096 \\\\\\n  --n-gpu-layers 999 \\\\\\n  --main-gpu 1 \\\\\\n  --tensor-split 0.2,0.8 \\\\\\n  --parallel 2 \\\\\\n  --threads 16 \\\\\\n  -ot &amp;quot;\\\\.([2-9][6-9]|[3-9][0-9]|[0-9][0-9][0-9])\\\\.ffn&lt;/em&gt;(gate|up|down)_exps\\\\.=CPU&amp;quot; \\\\&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0uoqo/iq2_kl_345687_gib_2892_bpw_kimik2instruct_gguf_ik/n3fcn3o/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752664130,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m0uoqo","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3cu823","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"VoidAlchemy","can_mod_post":false,"created_utc":1752623152,"send_replies":true,"parent_id":"t1_n3ccds8","score":1,"author_fullname":"t2_n321yfw5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"u have 5 gpu's psure lmao... i love it, and always great seeing you around here and GH! i've heard anecdotally that the IQ2\\\\_KL gives less rejections than the larger sizes, but ymmv of course Kimi-K2 is so new just landing in mainline today!\\n\\nonly other gossip i've heard is that Kimi-K2 might be more creative but DeepSeek still more logical/coding but i haven't stopped long enough to try it much yet haha","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3cu823","is_submitter":true,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;u have 5 gpu&amp;#39;s psure lmao... i love it, and always great seeing you around here and GH! i&amp;#39;ve heard anecdotally that the IQ2_KL gives less rejections than the larger sizes, but ymmv of course Kimi-K2 is so new just landing in mainline today!&lt;/p&gt;\\n\\n&lt;p&gt;only other gossip i&amp;#39;ve heard is that Kimi-K2 might be more creative but DeepSeek still more logical/coding but i haven&amp;#39;t stopped long enough to try it much yet haha&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0uoqo","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0uoqo/iq2_kl_345687_gib_2892_bpw_kimik2instruct_gguf_ik/n3cu823/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752623152,"author_flair_text":"llama.cpp","treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3ccds8","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"bullerwins","can_mod_post":false,"created_utc":1752617235,"send_replies":true,"parent_id":"t3_1m0uoqo","score":2,"author_fullname":"t2_d3wk5","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Looking forward to test this one to see if it substitudes Deepseek for me. \\nNot looking forward to the trial and error of manual layer offload per gpu though lol","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3ccds8","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Looking forward to test this one to see if it substitudes Deepseek for me. \\nNot looking forward to the trial and error of manual layer offload per gpu though lol&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0uoqo/iq2_kl_345687_gib_2892_bpw_kimik2instruct_gguf_ik/n3ccds8/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752617235,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m0uoqo","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}}]`),s=()=>e.jsx(l,{data:a});export{s as default};
