import{j as e}from"./index-xfnGEtuL.js";import{R as t}from"./RedditPostRenderer-DAZRIZZK.js";import"./index-BaNn5-RR.js";const a=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"How important is the speed and latency of the system ram when you run out of VRAM when running a local LLM?\\n\\nI know that vram is multitudes faster than ram, and I have experienced the difference myself when I exceeded the vram buffer of my PC.\\n\\nBut I wanted to ask what happens if the plan is to exceed the vram and use system ram?\\n\\nIf I had the same system, but one had a gpu and one didn’t, supposing that the gpu didn’t have enough vram, is there still an appreciable difference in llm performance with the two systems?\\n\\nRight now I have a 7900 xt and 32gb of ddr5 6000 cl36 ram. Would getting a kit of faster 96gb kit of ddr5 6400 do more than getting a used gpu like the rx 6800 for 16 more gen of vram?\\n\\nIn the scenarios I am assuming that the model spills out into the ram either way.\\n\\nIf the llm spills out into the ram, is it cpu inference now?","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Ram Speed importance when exceeding VRAM","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lw8lvt","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.81,"author_flair_background_color":null,"subreddit_type":"public","ups":6,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_rn6co7q5m","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":6,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1752140750,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;How important is the speed and latency of the system ram when you run out of VRAM when running a local LLM?&lt;/p&gt;\\n\\n&lt;p&gt;I know that vram is multitudes faster than ram, and I have experienced the difference myself when I exceeded the vram buffer of my PC.&lt;/p&gt;\\n\\n&lt;p&gt;But I wanted to ask what happens if the plan is to exceed the vram and use system ram?&lt;/p&gt;\\n\\n&lt;p&gt;If I had the same system, but one had a gpu and one didn’t, supposing that the gpu didn’t have enough vram, is there still an appreciable difference in llm performance with the two systems?&lt;/p&gt;\\n\\n&lt;p&gt;Right now I have a 7900 xt and 32gb of ddr5 6000 cl36 ram. Would getting a kit of faster 96gb kit of ddr5 6400 do more than getting a used gpu like the rx 6800 for 16 more gen of vram?&lt;/p&gt;\\n\\n&lt;p&gt;In the scenarios I am assuming that the model spills out into the ram either way.&lt;/p&gt;\\n\\n&lt;p&gt;If the llm spills out into the ram, is it cpu inference now?&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1lw8lvt","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"opoot_","discussion_type":null,"num_comments":14,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lw8lvt/ram_speed_importance_when_exceeding_vram/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lw8lvt/ram_speed_importance_when_exceeding_vram/","subreddit_subscribers":497354,"created_utc":1752140750,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2c4o37","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Rich_Repeat_22","can_mod_post":false,"created_utc":1752141568,"send_replies":true,"parent_id":"t3_1lw8lvt","score":5,"author_fullname":"t2_viufiki6","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"a) 96GB is more than 32GB so can load bigger MOE, speed isn't that much different when comes to dual channel CPUs. Now if we are talking about 8channel then yes, there is big difference :)  \\n  \\nb) I would advice to get a second used 7900XT as the first opportunity instead of an ancient 6800 16GB.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2c4o37","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;a) 96GB is more than 32GB so can load bigger MOE, speed isn&amp;#39;t that much different when comes to dual channel CPUs. Now if we are talking about 8channel then yes, there is big difference :)  &lt;/p&gt;\\n\\n&lt;p&gt;b) I would advice to get a second used 7900XT as the first opportunity instead of an ancient 6800 16GB.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lw8lvt/ram_speed_importance_when_exceeding_vram/n2c4o37/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752141568,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lw8lvt","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2ee1s2","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"FullstackSensei","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2czh9z","score":1,"author_fullname":"t2_17n3nqtj56","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"More or less. Always keep in mind real world bandwidth will always be between 70-80% theoretical max, with Intel platforms generally being a bit above AMD. Prompt processing on Strix Halo will be significantly faster because of the 8060S has a lot more compute power. So, you'll need a GPU or two to handle prompt processing.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2ee1s2","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;More or less. Always keep in mind real world bandwidth will always be between 70-80% theoretical max, with Intel platforms generally being a bit above AMD. Prompt processing on Strix Halo will be significantly faster because of the 8060S has a lot more compute power. So, you&amp;#39;ll need a GPU or two to handle prompt processing.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lw8lvt","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lw8lvt/ram_speed_importance_when_exceeding_vram/n2ee1s2/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752168533,"author_flair_text":null,"treatment_tags":[],"created_utc":1752168533,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2czh9z","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"opoot_","can_mod_post":false,"created_utc":1752154139,"send_replies":true,"parent_id":"t1_n2cf01w","score":1,"author_fullname":"t2_rn6co7q5m","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Oh this is very useful, since all I hear is that memory bandwidth is all that matters, does this mean in your example, the epyc with ddr4 with 203gb/s bandwidth can get me performance close to strix halo, which has I think 256gb/s bandwidth?","edited":1752154934,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2czh9z","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Oh this is very useful, since all I hear is that memory bandwidth is all that matters, does this mean in your example, the epyc with ddr4 with 203gb/s bandwidth can get me performance close to strix halo, which has I think 256gb/s bandwidth?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lw8lvt","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lw8lvt/ram_speed_importance_when_exceeding_vram/n2czh9z/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752154139,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2cf01w","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"FullstackSensei","can_mod_post":false,"created_utc":1752146595,"send_replies":true,"parent_id":"t3_1lw8lvt","score":5,"author_fullname":"t2_17n3nqtj56","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"If you're sticking to the same platform, you're splitting hairs. Latency doesn't make a difference because memory access in LLMs is purely sequential and the memory/cache controller/prefetcher will take care of prefetching the next cache lines.\\n\\nWhere you can gain quite a bit of performance when using system RAM is by switching to server platforms. You don't need to break the bank for that. A 2019 Cascade Lake Xeon has six memory channels running at DDR4-2933, which bring ~140GB/s theoretical peak bandwidth. An Epyc Rome from that same year has eight channels running at DDR4-3200, good for ~208GB/s theoretical bandwidth. Them two channels of DDR5-6400 have ~102GB/s theoretical bandwidth. The best part about Xeon and Epyc is that 32GB ECC RDIMM sticks cost a quarter or even less per GB vs desktop DDR5 memory. Of course, you need to do your homework beforehand and understand how those platforms work and what options you have for motherboards and CPUs, but IMO it's not that much work.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2cf01w","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;If you&amp;#39;re sticking to the same platform, you&amp;#39;re splitting hairs. Latency doesn&amp;#39;t make a difference because memory access in LLMs is purely sequential and the memory/cache controller/prefetcher will take care of prefetching the next cache lines.&lt;/p&gt;\\n\\n&lt;p&gt;Where you can gain quite a bit of performance when using system RAM is by switching to server platforms. You don&amp;#39;t need to break the bank for that. A 2019 Cascade Lake Xeon has six memory channels running at DDR4-2933, which bring ~140GB/s theoretical peak bandwidth. An Epyc Rome from that same year has eight channels running at DDR4-3200, good for ~208GB/s theoretical bandwidth. Them two channels of DDR5-6400 have ~102GB/s theoretical bandwidth. The best part about Xeon and Epyc is that 32GB ECC RDIMM sticks cost a quarter or even less per GB vs desktop DDR5 memory. Of course, you need to do your homework beforehand and understand how those platforms work and what options you have for motherboards and CPUs, but IMO it&amp;#39;s not that much work.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lw8lvt/ram_speed_importance_when_exceeding_vram/n2cf01w/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752146595,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lw8lvt","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2cyzi0","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Aphid_red","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2cn9u0","score":2,"author_fullname":"t2_csn2q","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"True that. You probably only notice it if it spills out into swap. (Going from 32GB to 96GB might change that with big models). At most it's a 3.3% improvement.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2cyzi0","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;True that. You probably only notice it if it spills out into swap. (Going from 32GB to 96GB might change that with big models). At most it&amp;#39;s a 3.3% improvement.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lw8lvt","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lw8lvt/ram_speed_importance_when_exceeding_vram/n2cyzi0/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752153978,"author_flair_text":null,"treatment_tags":[],"created_utc":1752153978,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n2cn9u0","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Nepherpitu","can_mod_post":false,"created_utc":1752149930,"send_replies":true,"parent_id":"t1_n2ceysw","score":8,"author_fullname":"t2_plp1w","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"But author asked about DDR5 6000 vs DDR5 6400 in dual channel. It's like 60Gbps vs 62Gbps, almost zero impact on speed.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2cn9u0","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;But author asked about DDR5 6000 vs DDR5 6400 in dual channel. It&amp;#39;s like 60Gbps vs 62Gbps, almost zero impact on speed.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lw8lvt","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lw8lvt/ram_speed_importance_when_exceeding_vram/n2cn9u0/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752149930,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":8}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2ig8b8","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Aphid_red","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2gegj6","score":1,"author_fullname":"t2_csn2q","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Effectively the latter, unfortunately. GPUs have support for accessing system RAM, but it's too slow to be useful. \\n\\nThe PCI-e bus is even slower than RAM speeds. You can do that (in fact, it's the default in the NVidia driver), and you will get maybe o-kay-ish prompt processing speeds (better or worse than CPU will vary from machine to machine), but your generation speed will be terrible. See [https://en.wikipedia.org/wiki/PCI\\\\_Express#Comparison\\\\_table](https://en.wikipedia.org/wiki/PCI_Express#Comparison_table) \\n\\nNote: each stick of DDR5-4800 is about 40 GB/s. DDR5-6000 does 50GB/s/stick. \\n\\nSo this could be changing. A GPU with a 16x PCIe-8 (that's version 8, so 8x faster than what we have now) would be able to keep up with a full stack of server RAM and effectively use it to extend its own memory. So five years from now you might be able to use a modified 4090 with a version 8 link and extend its memory with server RAM. \\n\\nIn other words, not so useful.","edited":false,"author_flair_css_class":null,"name":"t1_n2ig8b8","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Effectively the latter, unfortunately. GPUs have support for accessing system RAM, but it&amp;#39;s too slow to be useful. &lt;/p&gt;\\n\\n&lt;p&gt;The PCI-e bus is even slower than RAM speeds. You can do that (in fact, it&amp;#39;s the default in the NVidia driver), and you will get maybe o-kay-ish prompt processing speeds (better or worse than CPU will vary from machine to machine), but your generation speed will be terrible. See &lt;a href=\\"https://en.wikipedia.org/wiki/PCI_Express#Comparison_table\\"&gt;https://en.wikipedia.org/wiki/PCI_Express#Comparison_table&lt;/a&gt; &lt;/p&gt;\\n\\n&lt;p&gt;Note: each stick of DDR5-4800 is about 40 GB/s. DDR5-6000 does 50GB/s/stick. &lt;/p&gt;\\n\\n&lt;p&gt;So this could be changing. A GPU with a 16x PCIe-8 (that&amp;#39;s version 8, so 8x faster than what we have now) would be able to keep up with a full stack of server RAM and effectively use it to extend its own memory. So five years from now you might be able to use a modified 4090 with a version 8 link and extend its memory with server RAM. &lt;/p&gt;\\n\\n&lt;p&gt;In other words, not so useful.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lw8lvt","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lw8lvt/ram_speed_importance_when_exceeding_vram/n2ig8b8/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752220368,"author_flair_text":null,"collapsed":false,"created_utc":1752220368,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2gegj6","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"opoot_","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2dhab4","score":1,"author_fullname":"t2_rn6co7q5m","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"So can I just add a gpu to handle the processing part? Then have a lot of memory from something like a server to give me bandwidth?\\n\\nOr does it work like the stuff in system ram is always handled by cpu, while only the stuff in vram is handled by gpu?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2gegj6","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;So can I just add a gpu to handle the processing part? Then have a lot of memory from something like a server to give me bandwidth?&lt;/p&gt;\\n\\n&lt;p&gt;Or does it work like the stuff in system ram is always handled by cpu, while only the stuff in vram is handled by gpu?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lw8lvt","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lw8lvt/ram_speed_importance_when_exceeding_vram/n2gegj6/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752189865,"author_flair_text":null,"treatment_tags":[],"created_utc":1752189865,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2dhab4","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Aphid_red","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2d1vxt","score":2,"author_fullname":"t2_csn2q","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Well, it does. \\n\\nWhen you hit the 'generate' button, you feed some stuff into the model (presumably), your prompt. Unlike when you generate 'new' text, this old text is *immutable*. Each token in it does not depend on the previous token, it's a fait accompli. This prompt needs to also be ran through the model, and that can take some time (with CPU setups, hours before it outputs the first token, sometimes!)\\n\\nUsing the relay metaphor, in token generation, there's one baton. In prompt processing, there's however many you want. You're restricted by the size of the racetrack as well as the speed of the competitors. Eh... it doesn't quite work as well here. \\n\\nAnyway, It's possible to process this in parallel. Rather than having to go token by token, your GPU can compute all the tokens at once. This is what the 'BLAS batch size' means. Calculate 512 tokens at the same time rather than 1. This way the GPU can do this, written in simple code rather than the actual complex pytorch, but it should get the idea across of what's happening under the hood: \\n\\n\`for i in 1..layers,\`   \\n\`for j in 1..layer_size/sram_size\`   \\n\`Load part j of layer i into SRAM\`   \\n\`for k in 1...512.\`  \\n\`Calculate token k KV for layer i part j\`  \\n\`[add up all the parts]\`  \\n\`[feed output of each layer into the next]\` \\n\\nNotice how even doing all 512 tokens each parameter only has to be loaded from VRAM into cache once. Now the limiting factor is the speed of the GPU core rather than the memory bandwidth. In token generation, this isn't possible: each token depends on the previous one(s) generated. \\n\\nIt turns out GPUs are pretty slow at moving data from memory to their internals (1TB/s), compared to how many TFLOPs of matrix multiplication they churn out. That ratio can be something on the order of 1:200 or 1:300 (these are figures for the A100 and H100). Depending on what kind of math the model does, (Relu or Silu, what kind of KV caching, MHA or MQA or GQA, etc.) a model ends up with what I like to call a 'compute intensity', or 'how many flops per parameter each pass through does'. \\n\\nThat's usually a pretty low number: The vast, vast majority of the math in a neural net takes each parameter, multiplies it with an intermediate result, then adds that to another intermediate result. That takes... 2 operations. One add, one multiply. \\n\\nThat's why you can have 10,000 tokens throughput on a fast gpu like the 5090 or the H100 for 7B models, but only end up with 100-odd tokens of actual tps. Or why using a provider can be so cheap even though the hardware is very expensive. The provider can 'batch' multiple generations together (as long as they have enough VRAM for all the KV caches between all the users on their whole system). \\n\\nThis is also why Deepseek was such a big change: Deepseek has a much smaller cache than comparable models in quality. So even though it's bigger than a comparable Llama or mistral model (\\\\~250B vs. 670B), it ends up using *less* resources if you have enough users.\\n\\nOn the token generation side of things, rather than prompt processing. CPUs are much slower than GPUs at this stuff, so they're bottlenecked by memory by a factor of 1-10 usually. In fact, the slowest CPUs with the least cores might be limited by flops. GPUs are pretty much without exception bottlenecked by memory.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2dhab4","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Well, it does. &lt;/p&gt;\\n\\n&lt;p&gt;When you hit the &amp;#39;generate&amp;#39; button, you feed some stuff into the model (presumably), your prompt. Unlike when you generate &amp;#39;new&amp;#39; text, this old text is &lt;em&gt;immutable&lt;/em&gt;. Each token in it does not depend on the previous token, it&amp;#39;s a fait accompli. This prompt needs to also be ran through the model, and that can take some time (with CPU setups, hours before it outputs the first token, sometimes!)&lt;/p&gt;\\n\\n&lt;p&gt;Using the relay metaphor, in token generation, there&amp;#39;s one baton. In prompt processing, there&amp;#39;s however many you want. You&amp;#39;re restricted by the size of the racetrack as well as the speed of the competitors. Eh... it doesn&amp;#39;t quite work as well here. &lt;/p&gt;\\n\\n&lt;p&gt;Anyway, It&amp;#39;s possible to process this in parallel. Rather than having to go token by token, your GPU can compute all the tokens at once. This is what the &amp;#39;BLAS batch size&amp;#39; means. Calculate 512 tokens at the same time rather than 1. This way the GPU can do this, written in simple code rather than the actual complex pytorch, but it should get the idea across of what&amp;#39;s happening under the hood: &lt;/p&gt;\\n\\n&lt;p&gt;&lt;code&gt;for i in 1..layers,&lt;/code&gt;&lt;br/&gt;\\n&lt;code&gt;for j in 1..layer_size/sram_size&lt;/code&gt;&lt;br/&gt;\\n&lt;code&gt;Load part j of layer i into SRAM&lt;/code&gt;&lt;br/&gt;\\n&lt;code&gt;for k in 1...512.&lt;/code&gt;&lt;br/&gt;\\n&lt;code&gt;Calculate token k KV for layer i part j&lt;/code&gt;&lt;br/&gt;\\n&lt;code&gt;[add up all the parts]&lt;/code&gt;&lt;br/&gt;\\n&lt;code&gt;[feed output of each layer into the next]&lt;/code&gt; &lt;/p&gt;\\n\\n&lt;p&gt;Notice how even doing all 512 tokens each parameter only has to be loaded from VRAM into cache once. Now the limiting factor is the speed of the GPU core rather than the memory bandwidth. In token generation, this isn&amp;#39;t possible: each token depends on the previous one(s) generated. &lt;/p&gt;\\n\\n&lt;p&gt;It turns out GPUs are pretty slow at moving data from memory to their internals (1TB/s), compared to how many TFLOPs of matrix multiplication they churn out. That ratio can be something on the order of 1:200 or 1:300 (these are figures for the A100 and H100). Depending on what kind of math the model does, (Relu or Silu, what kind of KV caching, MHA or MQA or GQA, etc.) a model ends up with what I like to call a &amp;#39;compute intensity&amp;#39;, or &amp;#39;how many flops per parameter each pass through does&amp;#39;. &lt;/p&gt;\\n\\n&lt;p&gt;That&amp;#39;s usually a pretty low number: The vast, vast majority of the math in a neural net takes each parameter, multiplies it with an intermediate result, then adds that to another intermediate result. That takes... 2 operations. One add, one multiply. &lt;/p&gt;\\n\\n&lt;p&gt;That&amp;#39;s why you can have 10,000 tokens throughput on a fast gpu like the 5090 or the H100 for 7B models, but only end up with 100-odd tokens of actual tps. Or why using a provider can be so cheap even though the hardware is very expensive. The provider can &amp;#39;batch&amp;#39; multiple generations together (as long as they have enough VRAM for all the KV caches between all the users on their whole system). &lt;/p&gt;\\n\\n&lt;p&gt;This is also why Deepseek was such a big change: Deepseek has a much smaller cache than comparable models in quality. So even though it&amp;#39;s bigger than a comparable Llama or mistral model (~250B vs. 670B), it ends up using &lt;em&gt;less&lt;/em&gt; resources if you have enough users.&lt;/p&gt;\\n\\n&lt;p&gt;On the token generation side of things, rather than prompt processing. CPUs are much slower than GPUs at this stuff, so they&amp;#39;re bottlenecked by memory by a factor of 1-10 usually. In fact, the slowest CPUs with the least cores might be limited by flops. GPUs are pretty much without exception bottlenecked by memory.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lw8lvt","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lw8lvt/ram_speed_importance_when_exceeding_vram/n2dhab4/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752159413,"author_flair_text":null,"treatment_tags":[],"created_utc":1752159413,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n2d1vxt","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"opoot_","can_mod_post":false,"created_utc":1752154906,"send_replies":true,"parent_id":"t1_n2ceysw","score":1,"author_fullname":"t2_rn6co7q5m","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Oh that’s an amazing metaphor, that cleared so much stuff up for me.\\n\\nSo just to make sure I understood clearly, the model size is spread amongst all the ram I have, no matter what kind. \\n\\nAnd each portion carries a portion of the model. And the total generation time is the time taken of each portion of the ram added together.\\n\\nSo this is why having even a little bit of my model spill over to be ram increases the generation speed so much, because I have a 20gb vram card that’s like 800+gb/s, if even 1gb of the model is spilled over to system ram that’s like 50gb/s, the tps tanks.\\n\\nBut I’m curious how processing power plays into it. Does processing power truly not matter at all?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2d1vxt","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Oh that’s an amazing metaphor, that cleared so much stuff up for me.&lt;/p&gt;\\n\\n&lt;p&gt;So just to make sure I understood clearly, the model size is spread amongst all the ram I have, no matter what kind. &lt;/p&gt;\\n\\n&lt;p&gt;And each portion carries a portion of the model. And the total generation time is the time taken of each portion of the ram added together.&lt;/p&gt;\\n\\n&lt;p&gt;So this is why having even a little bit of my model spill over to be ram increases the generation speed so much, because I have a 20gb vram card that’s like 800+gb/s, if even 1gb of the model is spilled over to system ram that’s like 50gb/s, the tps tanks.&lt;/p&gt;\\n\\n&lt;p&gt;But I’m curious how processing power plays into it. Does processing power truly not matter at all?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lw8lvt","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lw8lvt/ram_speed_importance_when_exceeding_vram/n2d1vxt/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752154906,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2ceysw","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Aphid_red","can_mod_post":false,"created_utc":1752146580,"send_replies":true,"parent_id":"t3_1lw8lvt","score":8,"author_fullname":"t2_csn2q","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"It depends on what you're doing. \\n\\nAre you just letting NVidia's/AMD's automated fallback do what it does? Then it doesn't matter *at all*, you're bottlenecked by (at most) 16GB/s PCI-e speeds. Are you using koboldcpp/llama.cpp to manually tune how much (layers) to run on the CPU? Then it *does* matter. In fact, it'll dominate your tps. Let me show you how: \\n\\nImagine your CPU and GPU are a turtle and a cheetah working together in a relay race. When the race is complete, one token is generated. The proportion of the model that is in RAM is the part of the race done by the turtle. \\n\\nAdding another slower GPU is like taking a chunk of the race and letting a greyhound do that part of the race. Going for slightly faster RAM is like speeding the turtle's movements up by 5%. \\n\\nTPS is determined by memory speeds. The more fast memory you have, the more tokens per second your machine can crank out. You can divide your model between slower and faster memory, but you always have to wait for the 'smallest pipe' in the whole system to throughput its data.\\n\\nSay you have 50GB/s RAM speed and 1000GB/s VRAM speed. You're running a model that takes 50GB for its params, and can fit 20GB of it  on your 3090. (The other 4GB are for context) The model's other 30GB goes on the CPU. \\n\\nTo generate one token, your CPU will take at least about 0.6 seconds of time (30GB over 50GB/s).   \\nYour GPU will take at least 0.02 seconds. \\n\\nThe total is 0.62 seconds, or 1.61 tps. In reality you'll see about 70% of that theoretical max, or 1.1 tps.\\n\\nNow let's see what happens with 200GB/s RAM speeds (let's say you upgrade from a consumer DDR4 machine to a second hand epyc milan server, the best bang for your buck for RAM speeds). Here, \\n\\nThe CPU will take 0.15s to do its layers.   \\nThe GPU will again take 0.02s. \\n\\nThe total is 0.17 seconds, or 5.9tps. With again about 70% real life performance 4.1tps seems about what you'd see.\\n\\nA similar thing applies when combining slower and faster GPUs. You won't see much of a difference from going from 2x P40 to P40 + 3090, but you *will* notice going from P40 + 3090 to 2x3090.   \\n(You will also notice using tensor parallel once you go for more than one GPU if the model fits in VRAM of both).\\n\\n  \\nNow your question: Which one of your two upgrades does more? The question should be: what model are you running? Then, you can calculate which upgrade will improve your generation speed more. It could be either one: with a very large model, most of it is in RAM and RAM speed matters. With a smaller model, adding another GPU will move a substantial portion of your calculations to GPU, perhaps even all of it. With no poor turtles in the race, the times obviously improve markedly.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2ceysw","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It depends on what you&amp;#39;re doing. &lt;/p&gt;\\n\\n&lt;p&gt;Are you just letting NVidia&amp;#39;s/AMD&amp;#39;s automated fallback do what it does? Then it doesn&amp;#39;t matter &lt;em&gt;at all&lt;/em&gt;, you&amp;#39;re bottlenecked by (at most) 16GB/s PCI-e speeds. Are you using koboldcpp/llama.cpp to manually tune how much (layers) to run on the CPU? Then it &lt;em&gt;does&lt;/em&gt; matter. In fact, it&amp;#39;ll dominate your tps. Let me show you how: &lt;/p&gt;\\n\\n&lt;p&gt;Imagine your CPU and GPU are a turtle and a cheetah working together in a relay race. When the race is complete, one token is generated. The proportion of the model that is in RAM is the part of the race done by the turtle. &lt;/p&gt;\\n\\n&lt;p&gt;Adding another slower GPU is like taking a chunk of the race and letting a greyhound do that part of the race. Going for slightly faster RAM is like speeding the turtle&amp;#39;s movements up by 5%. &lt;/p&gt;\\n\\n&lt;p&gt;TPS is determined by memory speeds. The more fast memory you have, the more tokens per second your machine can crank out. You can divide your model between slower and faster memory, but you always have to wait for the &amp;#39;smallest pipe&amp;#39; in the whole system to throughput its data.&lt;/p&gt;\\n\\n&lt;p&gt;Say you have 50GB/s RAM speed and 1000GB/s VRAM speed. You&amp;#39;re running a model that takes 50GB for its params, and can fit 20GB of it  on your 3090. (The other 4GB are for context) The model&amp;#39;s other 30GB goes on the CPU. &lt;/p&gt;\\n\\n&lt;p&gt;To generate one token, your CPU will take at least about 0.6 seconds of time (30GB over 50GB/s).&lt;br/&gt;\\nYour GPU will take at least 0.02 seconds. &lt;/p&gt;\\n\\n&lt;p&gt;The total is 0.62 seconds, or 1.61 tps. In reality you&amp;#39;ll see about 70% of that theoretical max, or 1.1 tps.&lt;/p&gt;\\n\\n&lt;p&gt;Now let&amp;#39;s see what happens with 200GB/s RAM speeds (let&amp;#39;s say you upgrade from a consumer DDR4 machine to a second hand epyc milan server, the best bang for your buck for RAM speeds). Here, &lt;/p&gt;\\n\\n&lt;p&gt;The CPU will take 0.15s to do its layers.&lt;br/&gt;\\nThe GPU will again take 0.02s. &lt;/p&gt;\\n\\n&lt;p&gt;The total is 0.17 seconds, or 5.9tps. With again about 70% real life performance 4.1tps seems about what you&amp;#39;d see.&lt;/p&gt;\\n\\n&lt;p&gt;A similar thing applies when combining slower and faster GPUs. You won&amp;#39;t see much of a difference from going from 2x P40 to P40 + 3090, but you &lt;em&gt;will&lt;/em&gt; notice going from P40 + 3090 to 2x3090.&lt;br/&gt;\\n(You will also notice using tensor parallel once you go for more than one GPU if the model fits in VRAM of both).&lt;/p&gt;\\n\\n&lt;p&gt;Now your question: Which one of your two upgrades does more? The question should be: what model are you running? Then, you can calculate which upgrade will improve your generation speed more. It could be either one: with a very large model, most of it is in RAM and RAM speed matters. With a smaller model, adding another GPU will move a substantial portion of your calculations to GPU, perhaps even all of it. With no poor turtles in the race, the times obviously improve markedly.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lw8lvt/ram_speed_importance_when_exceeding_vram/n2ceysw/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752146580,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lw8lvt","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":8}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2c3gma","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"reacusn","can_mod_post":false,"created_utc":1752140916,"send_replies":true,"parent_id":"t3_1lw8lvt","score":2,"author_fullname":"t2_1ppg6hcqm8","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I don't think the difference between 6000 and 6400 is going to have much of an impact.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2c3gma","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I don&amp;#39;t think the difference between 6000 and 6400 is going to have much of an impact.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lw8lvt/ram_speed_importance_when_exceeding_vram/n2c3gma/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752140916,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lw8lvt","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2c3ray","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"fizzy1242","can_mod_post":false,"created_utc":1752141077,"send_replies":true,"parent_id":"t3_1lw8lvt","score":2,"author_fullname":"t2_16zcsx","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"definitely helps for moe models for shuffling experts","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2c3ray","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;definitely helps for moe models for shuffling experts&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lw8lvt/ram_speed_importance_when_exceeding_vram/n2c3ray/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752141077,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lw8lvt","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}}]`),r=()=>e.jsx(t,{data:a});export{r as default};
