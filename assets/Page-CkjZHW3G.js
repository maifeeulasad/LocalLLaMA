import{j as e}from"./index-DFOnUtq9.js";import{R as t}from"./RedditPostRenderer-B-dx19nm.js";import"./index-CUOQn61u.js";const a=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"I’ve been thinking about how we manage context when interacting with LLMs, and thought what if we had chat trees instead of linear threads?\\n\\nThe idea is simple, let users branch off from any point in the conversation to explore alternatives or dive deeper, while hiding irrelevant future context. I put together a quick POC to explore this.\\n\\nWould love to hear your thoughts, is this kind of context control useful? What would you change or build on top?","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"ChatTree: A simple way to context engineer","link_flair_richtext":[{"e":"text","t":"Discussion"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":70,"top_awarded_type":null,"hide_score":false,"name":"t3_1lq5d1o","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.95,"author_flair_background_color":null,"ups":18,"total_awards_received":0,"media_embed":{},"thumbnail_width":140,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_13zuwb","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Discussion","can_mod_post":false,"score":18,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"https://external-preview.redd.it/aq3Jk2PhaI6RNOjpL6IJwNVoG1BpVMN4hyuA4sGDNRM.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=f6d7317f0148bd20e0b924ea76edc1b05218e4c2","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"post_hint":"link","content_categories":null,"is_self":false,"subreddit_type":"public","created":1751485506,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"github.com","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;I’ve been thinking about how we manage context when interacting with LLMs, and thought what if we had chat trees instead of linear threads?&lt;/p&gt;\\n\\n&lt;p&gt;The idea is simple, let users branch off from any point in the conversation to explore alternatives or dive deeper, while hiding irrelevant future context. I put together a quick POC to explore this.&lt;/p&gt;\\n\\n&lt;p&gt;Would love to hear your thoughts, is this kind of context control useful? What would you change or build on top?&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"url_overridden_by_dest":"https://github.com/aadityaubhat/ChatTree","view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"preview":{"images":[{"source":{"url":"https://external-preview.redd.it/aq3Jk2PhaI6RNOjpL6IJwNVoG1BpVMN4hyuA4sGDNRM.png?auto=webp&amp;s=2e946eba7e4832bd40d0095ef8816cc9f8a69818","width":1200,"height":600},"resolutions":[{"url":"https://external-preview.redd.it/aq3Jk2PhaI6RNOjpL6IJwNVoG1BpVMN4hyuA4sGDNRM.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=344fc168447205eb001e41942ab7649037277702","width":108,"height":54},{"url":"https://external-preview.redd.it/aq3Jk2PhaI6RNOjpL6IJwNVoG1BpVMN4hyuA4sGDNRM.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=7fb5ad90e01390b75ebcef865b4c8ec3cef8cc64","width":216,"height":108},{"url":"https://external-preview.redd.it/aq3Jk2PhaI6RNOjpL6IJwNVoG1BpVMN4hyuA4sGDNRM.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=74022766aec6fd5e2c0b69dd4435e7ba27b81bfc","width":320,"height":160},{"url":"https://external-preview.redd.it/aq3Jk2PhaI6RNOjpL6IJwNVoG1BpVMN4hyuA4sGDNRM.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=9b00cfbafde10dd16bd1b2925da2c9cbf5a1efec","width":640,"height":320},{"url":"https://external-preview.redd.it/aq3Jk2PhaI6RNOjpL6IJwNVoG1BpVMN4hyuA4sGDNRM.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=0319737917afcf0f39dd3560814bd2bd5fc01266","width":960,"height":480},{"url":"https://external-preview.redd.it/aq3Jk2PhaI6RNOjpL6IJwNVoG1BpVMN4hyuA4sGDNRM.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=677afc7d9ee0c03d95968ca9015ecaaf15861d57","width":1080,"height":540}],"variants":{},"id":"aq3Jk2PhaI6RNOjpL6IJwNVoG1BpVMN4hyuA4sGDNRM"}],"enabled":false},"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"5f921ea4-c7bc-11ed-9c23-3a00622979b4","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"mod_note":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"num_reports":null,"removal_reason":null,"link_flair_background_color":"#646d73","id":"1lq5d1o","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"aadityaubhat","discussion_type":null,"num_comments":4,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lq5d1o/chattree_a_simple_way_to_context_engineer/","stickied":false,"url":"https://github.com/aadityaubhat/ChatTree","subreddit_subscribers":494198,"created_utc":1751485506,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n10hgfy","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Background_Put_4978","can_mod_post":false,"created_utc":1751490635,"send_replies":true,"parent_id":"t3_1lq5d1o","score":2,"author_fullname":"t2_ap0qx6cm","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"It's a cool idea, for sure. For me, there's something about the linear narrative of a conversation with an LLM that I appreciate a lot. There's a sort of narrative arc that I would feel weird about subverting. I had developed a method for individually muting or compressing messages which worked great, but I sort of gave up on context control ideas when I read about this insane QwenLong-CPRS model (https://arxiv.org/html/2505.18092v2) I haven't tried to integrate this yet, but that's where I'm putting my attention.   \\n  \\nStill, very cool project. :)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n10hgfy","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It&amp;#39;s a cool idea, for sure. For me, there&amp;#39;s something about the linear narrative of a conversation with an LLM that I appreciate a lot. There&amp;#39;s a sort of narrative arc that I would feel weird about subverting. I had developed a method for individually muting or compressing messages which worked great, but I sort of gave up on context control ideas when I read about this insane QwenLong-CPRS model (&lt;a href=\\"https://arxiv.org/html/2505.18092v2\\"&gt;https://arxiv.org/html/2505.18092v2&lt;/a&gt;) I haven&amp;#39;t tried to integrate this yet, but that&amp;#39;s where I&amp;#39;m putting my attention.   &lt;/p&gt;\\n\\n&lt;p&gt;Still, very cool project. :)&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lq5d1o/chattree_a_simple_way_to_context_engineer/n10hgfy/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751490635,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lq5d1o","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n10hys1","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DeProgrammer99","can_mod_post":false,"created_utc":1751490788,"send_replies":true,"parent_id":"t3_1lq5d1o","score":1,"author_fullname":"t2_w4j8t","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"LlamaSharp has this. [https://github.com/SciSharp/LLamaSharp/blob/master/LLama.Examples/Examples/BatchedExecutorFork.cs#L121](https://github.com/SciSharp/LLamaSharp/blob/master/LLama.Examples/Examples/BatchedExecutorFork.cs#L121)\\n\\nIt's what I use to share KV cache between conversations during batch execution [in my flash card generator](https://github.com/dpmm99/Faxtract/blob/main/Services/LlamaExecutor.cs#L239).","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n10hys1","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;LlamaSharp has this. &lt;a href=\\"https://github.com/SciSharp/LLamaSharp/blob/master/LLama.Examples/Examples/BatchedExecutorFork.cs#L121\\"&gt;https://github.com/SciSharp/LLamaSharp/blob/master/LLama.Examples/Examples/BatchedExecutorFork.cs#L121&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;It&amp;#39;s what I use to share KV cache between conversations during batch execution &lt;a href=\\"https://github.com/dpmm99/Faxtract/blob/main/Services/LlamaExecutor.cs#L239\\"&gt;in my flash card generator&lt;/a&gt;.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lq5d1o/chattree_a_simple_way_to_context_engineer/n10hys1/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751490788,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lq5d1o","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n12swft","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"ArtfulGenie69","can_mod_post":false,"created_utc":1751520210,"send_replies":true,"parent_id":"t3_1lq5d1o","score":3,"author_fullname":"t2_1d6ghm3sq0","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Deepseek already does this on there site. You can keep each talk going and you can go back and just do a new one but it saves the previous tree so it's effectively exactly what you wanted. Pretty sure openwebui is also able to do this, cool though nice to see it broken out. ","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n12swft","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Deepseek already does this on there site. You can keep each talk going and you can go back and just do a new one but it saves the previous tree so it&amp;#39;s effectively exactly what you wanted. Pretty sure openwebui is also able to do this, cool though nice to see it broken out. &lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lq5d1o/chattree_a_simple_way_to_context_engineer/n12swft/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751520210,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lq5d1o","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n133zn3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Ok_Appearance3584","can_mod_post":false,"created_utc":1751526067,"send_replies":true,"parent_id":"t3_1lq5d1o","score":2,"author_fullname":"t2_oyxj85n1","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I think this is a great way to compress info. Many times I have an initial question which is big, then we go into multiple smaller parts of it. If we could branch out for every small part and come back once we have the answer, the main thread would contain the steps in a way that makes sense and you could solve the bigger problem.\\n\\n\\nThe idea here would be that once you've solved the minor problem, you can throw away the info about how we got there and just have the solution. 10 - 30 messages -&gt; one resolution. Repeat multiple times, then you have ten resolutions and now you can take the step to solve the big problem at once.\\n\\n\\nDoing this in one big conversation, LLM loses track and focus + starts mixing up different parts and solutions of the problem. So a great idea indeed!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n133zn3","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I think this is a great way to compress info. Many times I have an initial question which is big, then we go into multiple smaller parts of it. If we could branch out for every small part and come back once we have the answer, the main thread would contain the steps in a way that makes sense and you could solve the bigger problem.&lt;/p&gt;\\n\\n&lt;p&gt;The idea here would be that once you&amp;#39;ve solved the minor problem, you can throw away the info about how we got there and just have the solution. 10 - 30 messages -&amp;gt; one resolution. Repeat multiple times, then you have ten resolutions and now you can take the step to solve the big problem at once.&lt;/p&gt;\\n\\n&lt;p&gt;Doing this in one big conversation, LLM loses track and focus + starts mixing up different parts and solutions of the problem. So a great idea indeed!&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lq5d1o/chattree_a_simple_way_to_context_engineer/n133zn3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751526067,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lq5d1o","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}}]`),s=()=>e.jsx(t,{data:a});export{s as default};
