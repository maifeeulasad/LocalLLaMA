import{j as e}from"./index-BlGsFJYy.js";import{R as l}from"./RedditPostRenderer-B6uvq_Zl.js";import"./index-DDvVtNwD.js";const t=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"What am I looking at for something that can run DeepSeek R1 Q8 w/ full 128K context window?  \\nI know an Epyc setup can do this, I am not sure about if it can hit 20 tokens/second.\\n\\nI suspect it will need 1024G ram, potentially more?\\n\\nAnyone have a CPU system running full DeepSeek R1 (ideally Q8) at 20+ tokens/second?\\n\\nFrom what I understand, a handful of GPUs won't improve the performance that much?  \\n","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Best setup for ~20 tokens/sec DeepSeek R1 671B Q8 w/ 128K context window","link_flair_richtext":[{"e":"text","t":"Discussion"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lxwodv","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.91,"author_flair_background_color":null,"subreddit_type":"public","ups":24,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_ijzb7","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Discussion","can_mod_post":false,"score":24,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1752313888,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;What am I looking at for something that can run DeepSeek R1 Q8 w/ full 128K context window?&lt;br/&gt;\\nI know an Epyc setup can do this, I am not sure about if it can hit 20 tokens/second.&lt;/p&gt;\\n\\n&lt;p&gt;I suspect it will need 1024G ram, potentially more?&lt;/p&gt;\\n\\n&lt;p&gt;Anyone have a CPU system running full DeepSeek R1 (ideally Q8) at 20+ tokens/second?&lt;/p&gt;\\n\\n&lt;p&gt;From what I understand, a handful of GPUs won&amp;#39;t improve the performance that much?  &lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"5f921ea4-c7bc-11ed-9c23-3a00622979b4","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#646d73","id":"1lxwodv","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"MidnightProgrammer","discussion_type":null,"num_comments":56,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lxwodv/best_setup_for_20_tokenssec_deepseek_r1_671b_q8_w/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lxwodv/best_setup_for_20_tokenssec_deepseek_r1_671b_q8_w/","subreddit_subscribers":498346,"created_utc":1752313888,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2pmg9x","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"No_Afternoon_4260","can_mod_post":false,"created_utc":1752319736,"send_replies":true,"parent_id":"t1_n2pckaa","score":6,"author_fullname":"t2_cj9kap4bx","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Seems like you talking amd epyc zen 6 (iirc 2026)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2pmg9x","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Seems like you talking amd epyc zen 6 (iirc 2026)&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lxwodv","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxwodv/best_setup_for_20_tokenssec_deepseek_r1_671b_q8_w/n2pmg9x/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752319736,"author_flair_text":"llama.cpp","treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2qim4n","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Freonr2","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2pcpeb","score":2,"author_fullname":"t2_8xi6x","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Any GPU should help a lot, whether or not a 6000 Pro is worth it over a 4090/5090 is a good question, though since you're talking an extra ~$7k.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2qim4n","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Any GPU should help a lot, whether or not a 6000 Pro is worth it over a 4090/5090 is a good question, though since you&amp;#39;re talking an extra ~$7k.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lxwodv","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxwodv/best_setup_for_20_tokenssec_deepseek_r1_671b_q8_w/n2qim4n/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752331958,"author_flair_text":null,"treatment_tags":[],"created_utc":1752331958,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ef488598-491f-11ef-a847-9a3dd315819c","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2r4ofd","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"panchovix","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2pcpeb","score":0,"author_fullname":"t2_j1kqr","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"It would make PP t/s way faster yes.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2r4ofd","is_submitter":false,"collapsed":false,"author_flair_richtext":[{"e":"text","t":"Llama 405B"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It would make PP t/s way faster yes.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lxwodv","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxwodv/best_setup_for_20_tokenssec_deepseek_r1_671b_q8_w/n2r4ofd/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752338857,"author_flair_text":"Llama 405B","treatment_tags":[],"created_utc":1752338857,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}},"user_reports":[],"saved":false,"id":"n2pcpeb","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MidnightProgrammer","can_mod_post":false,"created_utc":1752314477,"send_replies":true,"parent_id":"t1_n2pckaa","score":2,"author_fullname":"t2_ijzb7","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Would adding a RTX 6000 Pro help at all or would it not be worth it?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2pcpeb","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Would adding a RTX 6000 Pro help at all or would it not be worth it?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lxwodv","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxwodv/best_setup_for_20_tokenssec_deepseek_r1_671b_q8_w/n2pcpeb/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752314477,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2unkbx","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Caffdy","can_mod_post":false,"created_utc":1752384069,"send_replies":true,"parent_id":"t1_n2pckaa","score":1,"author_fullname":"t2_ql2vu0wz","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Intel Xeon 6962P, 12 channels at 8800MT/s could pull it off, but it's a pretty penny that's for sure","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2unkbx","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Intel Xeon 6962P, 12 channels at 8800MT/s could pull it off, but it&amp;#39;s a pretty penny that&amp;#39;s for sure&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lxwodv","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxwodv/best_setup_for_20_tokenssec_deepseek_r1_671b_q8_w/n2unkbx/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752384069,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2pnb9n","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"No_Afternoon_4260","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2ph62s","score":0,"author_fullname":"t2_cj9kap4bx","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"May be 5 idk shooting in the dark here","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2pnb9n","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;May be 5 idk shooting in the dark here&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lxwodv","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxwodv/best_setup_for_20_tokenssec_deepseek_r1_671b_q8_w/n2pnb9n/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752320149,"author_flair_text":"llama.cpp","treatment_tags":[],"created_utc":1752320149,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2pidlo","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ISHITTEDINYOURPANTS","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2ph62s","score":-1,"author_fullname":"t2_kjyc765o","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"apparently they're all capped at 76.8gb per cpu so it's not going to be really fast for big models, for 7B ones i did test myself and with a single E5 i would get around 25tk/s on quantized 8B models (i have 8 channels populated with a total of 192gb of DDR4)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2pidlo","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;apparently they&amp;#39;re all capped at 76.8gb per cpu so it&amp;#39;s not going to be really fast for big models, for 7B ones i did test myself and with a single E5 i would get around 25tk/s on quantized 8B models (i have 8 channels populated with a total of 192gb of DDR4)&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lxwodv","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxwodv/best_setup_for_20_tokenssec_deepseek_r1_671b_q8_w/n2pidlo/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752317681,"author_flair_text":null,"treatment_tags":[],"created_utc":1752317681,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2ph62s","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MidnightProgrammer","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2pebet","score":2,"author_fullname":"t2_ijzb7","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Do you know what the fastest E5 can get for tokens/second?","edited":1752317412,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2ph62s","is_submitter":true,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Do you know what the fastest E5 can get for tokens/second?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lxwodv","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxwodv/best_setup_for_20_tokenssec_deepseek_r1_671b_q8_w/n2ph62s/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752317034,"author_flair_text":null,"treatment_tags":[],"created_utc":1752317034,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n2pebet","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ISHITTEDINYOURPANTS","can_mod_post":false,"created_utc":1752315420,"send_replies":true,"parent_id":"t1_n2pckaa","score":1,"author_fullname":"t2_kjyc765o","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"the problem is not the amount of populated channels but the maximum bandwith that the cpu is able to reach, with a dual Xeon E5 you would be limited to around 150gb/s of bandwith","edited":1752317620,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2pebet","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;the problem is not the amount of populated channels but the maximum bandwith that the cpu is able to reach, with a dual Xeon E5 you would be limited to around 150gb/s of bandwith&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lxwodv","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxwodv/best_setup_for_20_tokenssec_deepseek_r1_671b_q8_w/n2pebet/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752315420,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2pckaa","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"cybran3","can_mod_post":false,"created_utc":1752314393,"send_replies":true,"parent_id":"t3_1lxwodv","score":25,"author_fullname":"t2_41gmkw5z","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"For q8 you would need ~740 GBs bandwidth to get 20 tokens per second. Iâ€™m not sure if there are any CPUs that can handle that many channels. You would need 16 DDR5 channels at 6000 MT/s to achieve that speed. But you will also have an issue with prompt processing speeds on CPU with a large context like that.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2pckaa","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;For q8 you would need ~740 GBs bandwidth to get 20 tokens per second. Iâ€™m not sure if there are any CPUs that can handle that many channels. You would need 16 DDR5 channels at 6000 MT/s to achieve that speed. But you will also have an issue with prompt processing speeds on CPU with a large context like that.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxwodv/best_setup_for_20_tokenssec_deepseek_r1_671b_q8_w/n2pckaa/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752314393,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lxwodv","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":25}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2pr6ug","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"No_Afternoon_4260","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2pqnhi","score":2,"author_fullname":"t2_cj9kap4bx","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Check my comment, the exact quant i don't remember iirc they are ik quant may be ðŸ¤· sorry\\nAnyway these numbers will evolve as support for that platform and ik_llama are evolving","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2pr6ug","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Check my comment, the exact quant i don&amp;#39;t remember iirc they are ik quant may be ðŸ¤· sorry\\nAnyway these numbers will evolve as support for that platform and ik_llama are evolving&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lxwodv","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxwodv/best_setup_for_20_tokenssec_deepseek_r1_671b_q8_w/n2pr6ug/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752321911,"author_flair_text":"llama.cpp","treatment_tags":[],"created_utc":1752321911,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n2pqnhi","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MidnightProgrammer","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2pmpzh","score":1,"author_fullname":"t2_ijzb7","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"You know what quant size he was running and what the total build was?","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2pqnhi","is_submitter":true,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You know what quant size he was running and what the total build was?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lxwodv","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxwodv/best_setup_for_20_tokenssec_deepseek_r1_671b_q8_w/n2pqnhi/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752321676,"author_flair_text":null,"treatment_tags":[],"created_utc":1752321676,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2pmpzh","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"No_Afternoon_4260","can_mod_post":false,"created_utc":1752319866,"send_replies":true,"parent_id":"t1_n2pgtiv","score":2,"author_fullname":"t2_cj9kap4bx","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Check level1tech he has a video of a 6980p running deepseek, although fast, it doesn't go that fast.  \\nMy comments on the speeds on his video [here](https://www.reddit.com/r/LocalLLaMA/s/kqP1mJgyAv) (also read his follow up comment where he stated it's single socket)","edited":1752320070,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2pmpzh","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Check level1tech he has a video of a 6980p running deepseek, although fast, it doesn&amp;#39;t go that fast.&lt;br/&gt;\\nMy comments on the speeds on his video &lt;a href=\\"https://www.reddit.com/r/LocalLLaMA/s/kqP1mJgyAv\\"&gt;here&lt;/a&gt; (also read his follow up comment where he stated it&amp;#39;s single socket)&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lxwodv","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxwodv/best_setup_for_20_tokenssec_deepseek_r1_671b_q8_w/n2pmpzh/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752319866,"author_flair_text":"llama.cpp","treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n2pgtiv","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"lly0571","can_mod_post":false,"created_utc":1752316843,"send_replies":true,"parent_id":"t3_1lxwodv","score":11,"author_fullname":"t2_70vzcleel","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Your CPU needs to handle the moe parts of the model which has:  7168(hidden\\\\_size)\\\\*2048(moe\\\\_intermediate\\\\_size)\\\\*3(qkv)\\\\*8(experts)\\\\*61(layers)\\\\~21.5B parameters. And you can offload the non moe part(shared experts, embedding layers, gating network) to GPU.\\n\\nAnd those 21.5B moe active parameters needs \\\\~430GB/s of CPU bandwidth. Considering of PCIe lag(spliting the model on both CPUs and GPUs) and bandwidth loss(I think most of the system can only use 70-80% of their theoretical bandwidth). Maybe you can only made it with 1S Xeon 6980P with MRDIMM(840GB/s each socket) or 2S Epyc Turin(576GB/s each socket), which ain't cheap.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2pgtiv","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Your CPU needs to handle the moe parts of the model which has:  7168(hidden_size)*2048(moe_intermediate_size)*3(qkv)*8(experts)*61(layers)~21.5B parameters. And you can offload the non moe part(shared experts, embedding layers, gating network) to GPU.&lt;/p&gt;\\n\\n&lt;p&gt;And those 21.5B moe active parameters needs ~430GB/s of CPU bandwidth. Considering of PCIe lag(spliting the model on both CPUs and GPUs) and bandwidth loss(I think most of the system can only use 70-80% of their theoretical bandwidth). Maybe you can only made it with 1S Xeon 6980P with MRDIMM(840GB/s each socket) or 2S Epyc Turin(576GB/s each socket), which ain&amp;#39;t cheap.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxwodv/best_setup_for_20_tokenssec_deepseek_r1_671b_q8_w/n2pgtiv/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752316843,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lxwodv","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":11}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2qgt9k","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"false79","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2q9ul5","score":4,"author_fullname":"t2_wn888","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I got a dual Epyc setup thinking I would have 2x compute. Learned the hard way there is quite a bit of performance lost in communication alone between the two CPUs.Â \\n\\n\\nBut my original intent was to have insane thread count at the time","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2qgt9k","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I got a dual Epyc setup thinking I would have 2x compute. Learned the hard way there is quite a bit of performance lost in communication alone between the two CPUs.Â &lt;/p&gt;\\n\\n&lt;p&gt;But my original intent was to have insane thread count at the time&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lxwodv","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxwodv/best_setup_for_20_tokenssec_deepseek_r1_671b_q8_w/n2qgt9k/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752331379,"author_flair_text":null,"treatment_tags":[],"created_utc":1752331379,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}}],"before":null}},"user_reports":[],"saved":false,"id":"n2q9ul5","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Conscious_Cut_6144","can_mod_post":false,"created_utc":1752329062,"send_replies":true,"parent_id":"t1_n2pp03k","score":10,"author_fullname":"t2_9hl4ymvj","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Dual cpu inference scales like trash unfortunately.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2q9ul5","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Dual cpu inference scales like trash unfortunately.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lxwodv","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxwodv/best_setup_for_20_tokenssec_deepseek_r1_671b_q8_w/n2q9ul5/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752329062,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":10}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2qadk9","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"__JockY__","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2q7n3u","score":3,"author_fullname":"t2_qf8h7ka8","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Thatâ€™ll work, however if OP is using long prompts then processing time will become arduous.\\n\\nEdit: I guess itâ€™ll be arduous either way.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2qadk9","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thatâ€™ll work, however if OP is using long prompts then processing time will become arduous.&lt;/p&gt;\\n\\n&lt;p&gt;Edit: I guess itâ€™ll be arduous either way.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lxwodv","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxwodv/best_setup_for_20_tokenssec_deepseek_r1_671b_q8_w/n2qadk9/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752329245,"author_flair_text":null,"treatment_tags":[],"created_utc":1752329245,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ef488598-491f-11ef-a847-9a3dd315819c","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ef488598-491f-11ef-a847-9a3dd315819c","distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ef488598-491f-11ef-a847-9a3dd315819c","distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2rnbo4","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"panchovix","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2rn3uo","score":2,"author_fullname":"t2_j1kqr","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I think it will be more feasible to get better models at smaller sizes than getting so much VRAM for cheaper.\\n\\nRTX Quadro 8000 with 48GB price is still absurd just because VRAM and it was released on 2018.","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n2rnbo4","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"Llama 405B"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I think it will be more feasible to get better models at smaller sizes than getting so much VRAM for cheaper.&lt;/p&gt;\\n\\n&lt;p&gt;RTX Quadro 8000 with 48GB price is still absurd just because VRAM and it was released on 2018.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lxwodv","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxwodv/best_setup_for_20_tokenssec_deepseek_r1_671b_q8_w/n2rnbo4/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752344534,"author_flair_text":"Llama 405B","treatment_tags":[],"created_utc":1752344534,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":6,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n2rn3uo","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Forgot_Password_Dude","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2rmlg1","score":1,"author_fullname":"t2_g8xg6sut","approved_by":null,"mod_note":null,"all_awardings":[],"body":"I feel like everything is so expensive but I know it will be super cheap in the future.  It's like paying 200$ for 2 MB of RAM in the 90s but early stage AI","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n2rn3uo","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I feel like everything is so expensive but I know it will be super cheap in the future.  It&amp;#39;s like paying 200$ for 2 MB of RAM in the 90s but early stage AI&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lxwodv","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxwodv/best_setup_for_20_tokenssec_deepseek_r1_671b_q8_w/n2rn3uo/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752344465,"author_flair_text":null,"treatment_tags":[],"created_utc":1752344465,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2rmlg1","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"panchovix","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2rm72y","score":1,"author_fullname":"t2_j1kqr","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"For example H100, or B200, etc\\n\\n16xH100 should be able to server DeepSeek with all the whistles and good speed.\\n\\nOr \\\\~10H200, or 8XB200, etc.","edited":false,"author_flair_css_class":null,"name":"t1_n2rmlg1","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"Llama 405B"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;For example H100, or B200, etc&lt;/p&gt;\\n\\n&lt;p&gt;16xH100 should be able to server DeepSeek with all the whistles and good speed.&lt;/p&gt;\\n\\n&lt;p&gt;Or ~10H200, or 8XB200, etc.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lxwodv","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":"light","treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxwodv/best_setup_for_20_tokenssec_deepseek_r1_671b_q8_w/n2rmlg1/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752344304,"author_flair_text":"Llama 405B","collapsed":false,"created_utc":1752344304,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2rm72y","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Forgot_Password_Dude","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2r4wio","score":1,"author_fullname":"t2_g8xg6sut","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"How are online services hosting it?  H100s?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2rm72y","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;How are online services hosting it?  H100s?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lxwodv","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxwodv/best_setup_for_20_tokenssec_deepseek_r1_671b_q8_w/n2rm72y/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752344177,"author_flair_text":null,"treatment_tags":[],"created_utc":1752344177,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2r4wio","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"panchovix","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2q7n3u","score":2,"author_fullname":"t2_j1kqr","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"At Q8\\\\_0 the PP speed will make it really not worth the wait above 16K ctx or so.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2r4wio","is_submitter":false,"collapsed":false,"author_flair_richtext":[{"e":"text","t":"Llama 405B"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;At Q8_0 the PP speed will make it really not worth the wait above 16K ctx or so.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lxwodv","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxwodv/best_setup_for_20_tokenssec_deepseek_r1_671b_q8_w/n2r4wio/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752338928,"author_flair_text":"Llama 405B","treatment_tags":[],"created_utc":1752338928,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n2q7n3u","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Forgot_Password_Dude","can_mod_post":false,"created_utc":1752328297,"send_replies":true,"parent_id":"t1_n2pp03k","score":2,"author_fullname":"t2_g8xg6sut","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"At 20k might as well get 2x Mac studio 512gb each","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2q7n3u","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;At 20k might as well get 2x Mac studio 512gb each&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lxwodv","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxwodv/best_setup_for_20_tokenssec_deepseek_r1_671b_q8_w/n2q7n3u/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752328297,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2rtl0k","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"__JockY__","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2qt9sp","score":1,"author_fullname":"t2_qf8h7ka8","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"32k available, those numbers are for around 2k tokens used, iirc. I should really try ik_llama at some point. \\n\\nI also failed at building ktransformers once, perhaps I should just concede defeat and try the docker images!","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2rtl0k","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;32k available, those numbers are for around 2k tokens used, iirc. I should really try ik_llama at some point. &lt;/p&gt;\\n\\n&lt;p&gt;I also failed at building ktransformers once, perhaps I should just concede defeat and try the docker images!&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lxwodv","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxwodv/best_setup_for_20_tokenssec_deepseek_r1_671b_q8_w/n2rtl0k/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752346516,"author_flair_text":null,"treatment_tags":[],"created_utc":1752346516,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2qt9sp","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"cantgetthistowork","can_mod_post":false,"created_utc":1752335295,"send_replies":true,"parent_id":"t1_n2pp03k","score":1,"author_fullname":"t2_j1i0o","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Context?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2qt9sp","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Context?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lxwodv","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxwodv/best_setup_for_20_tokenssec_deepseek_r1_671b_q8_w/n2qt9sp/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752335295,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2pp03k","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"__JockY__","can_mod_post":false,"created_utc":1752320934,"send_replies":true,"parent_id":"t3_1lxwodv","score":6,"author_fullname":"t2_qf8h7ka8","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"You didnâ€™t mention budget, so Iâ€™ll assume limitless money is available and suggest you buy 10x Blackwell Pro 6000 GPUs.\\n\\nItâ€™s onlyâ€¦ whatâ€¦ $90k?\\n\\nSeriously though, I have access to an EPYC 9745 DDR5 (running at 5200 MT/s) system and it runs DeepSeek 0528 Q8_0 in regular llama.cpp at 8-9t ok/sec.\\n\\nTwo of those CPUs with similar RAM would get you very close, although youâ€™re now talking about a $20k server.","edited":1752321189,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2pp03k","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You didnâ€™t mention budget, so Iâ€™ll assume limitless money is available and suggest you buy 10x Blackwell Pro 6000 GPUs.&lt;/p&gt;\\n\\n&lt;p&gt;Itâ€™s onlyâ€¦ whatâ€¦ $90k?&lt;/p&gt;\\n\\n&lt;p&gt;Seriously though, I have access to an EPYC 9745 DDR5 (running at 5200 MT/s) system and it runs DeepSeek 0528 Q8_0 in regular llama.cpp at 8-9t ok/sec.&lt;/p&gt;\\n\\n&lt;p&gt;Two of those CPUs with similar RAM would get you very close, although youâ€™re now talking about a $20k server.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxwodv/best_setup_for_20_tokenssec_deepseek_r1_671b_q8_w/n2pp03k/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752320934,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lxwodv","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2qg44t","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ILoveMy2Balls","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2qfsfy","score":2,"author_fullname":"t2_1nisx8ggay","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I was just joking and you're right","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2qg44t","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I was just joking and you&amp;#39;re right&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lxwodv","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxwodv/best_setup_for_20_tokenssec_deepseek_r1_671b_q8_w/n2qg44t/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752331153,"author_flair_text":null,"treatment_tags":[],"created_utc":1752331153,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n2qfsfy","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Willing_Landscape_61","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2pn4vq","score":2,"author_fullname":"t2_8lvrytgw","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I'd love to be wrong but I don't see how to get the desired performance for less than $30k.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2qfsfy","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;d love to be wrong but I don&amp;#39;t see how to get the desired performance for less than $30k.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lxwodv","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxwodv/best_setup_for_20_tokenssec_deepseek_r1_671b_q8_w/n2qfsfy/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752331046,"author_flair_text":null,"treatment_tags":[],"created_utc":1752331046,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n2pn4vq","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ILoveMy2Balls","can_mod_post":false,"created_utc":1752320063,"send_replies":true,"parent_id":"t1_n2pdgpr","score":4,"author_fullname":"t2_1nisx8ggay","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Define reasonable price","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2pn4vq","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Define reasonable price&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lxwodv","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxwodv/best_setup_for_20_tokenssec_deepseek_r1_671b_q8_w/n2pn4vq/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752320063,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}}],"before":null}},"user_reports":[],"saved":false,"id":"n2pdgpr","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Willing_Landscape_61","can_mod_post":false,"created_utc":1752314926,"send_replies":true,"parent_id":"t3_1lxwodv","score":7,"author_fullname":"t2_8lvrytgw","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Not possible at a reasonable price, sorry.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2pdgpr","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Not possible at a reasonable price, sorry.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxwodv/best_setup_for_20_tokenssec_deepseek_r1_671b_q8_w/n2pdgpr/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752314926,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lxwodv","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":7}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2r90be","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"mxmumtuna","can_mod_post":false,"created_utc":1752340173,"send_replies":true,"parent_id":"t1_n2r6can","score":1,"author_fullname":"t2_igbly","approved_by":null,"mod_note":null,"all_awardings":[],"body":"For coding, the upcoming Qwen3 coder could be a good choice, or devstral now. Financial will depend on the actual activities. \\n\\nMaybe start with an RTX Pro 6000 in a regularish AM5 desktop and go from there. You can do a lot of experimentation and can build on your gear once you have a 6000 to play with.\\n\\nRead through [this](https://forum.level1techs.com/t/deepseek-deep-dive-r1-at-home/225826?u=mxmumtuna), it gives a lot of recent data about DeepSeek on varying degrees of accessible PC hardware using GPU/CPU and what to expect. \\n\\n192gb of VRAM get you to where you want to be, with the exception of being at Q8. Whether that works for you though is probably worth testing the target quant before you buy hardware. Lease some GPUs on Runpod maybe.","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n2r90be","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;For coding, the upcoming Qwen3 coder could be a good choice, or devstral now. Financial will depend on the actual activities. &lt;/p&gt;\\n\\n&lt;p&gt;Maybe start with an RTX Pro 6000 in a regularish AM5 desktop and go from there. You can do a lot of experimentation and can build on your gear once you have a 6000 to play with.&lt;/p&gt;\\n\\n&lt;p&gt;Read through &lt;a href=\\"https://forum.level1techs.com/t/deepseek-deep-dive-r1-at-home/225826?u=mxmumtuna\\"&gt;this&lt;/a&gt;, it gives a lot of recent data about DeepSeek on varying degrees of accessible PC hardware using GPU/CPU and what to expect. &lt;/p&gt;\\n\\n&lt;p&gt;192gb of VRAM get you to where you want to be, with the exception of being at Q8. Whether that works for you though is probably worth testing the target quant before you buy hardware. Lease some GPUs on Runpod maybe.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":true,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxwodv/best_setup_for_20_tokenssec_deepseek_r1_671b_q8_w/n2r90be/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752340173,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lxwodv","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":8,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2te48p","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"valdev","can_mod_post":false,"created_utc":1752365523,"send_replies":true,"parent_id":"t1_n2r6can","score":1,"author_fullname":"t2_3oazk","approved_by":null,"mod_note":null,"all_awardings":[],"body":"Financial... You should reconsider that. LLM's are bad with math in general, and adding any level of larger context makes it exponentially worse. The reason some LLM providers seem better than they are is because they use tools to detect when math is going to occur, and send it out to a system that is specific for math.\\n\\nI hope you are not trying to do stuff like \\"Look at this huge bill and find oddities\\".","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n2te48p","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Financial... You should reconsider that. LLM&amp;#39;s are bad with math in general, and adding any level of larger context makes it exponentially worse. The reason some LLM providers seem better than they are is because they use tools to detect when math is going to occur, and send it out to a system that is specific for math.&lt;/p&gt;\\n\\n&lt;p&gt;I hope you are not trying to do stuff like &amp;quot;Look at this huge bill and find oddities&amp;quot;.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxwodv/best_setup_for_20_tokenssec_deepseek_r1_671b_q8_w/n2te48p/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752365523,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lxwodv","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":8,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2r6can","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MidnightProgrammer","can_mod_post":false,"created_utc":1752339364,"send_replies":true,"parent_id":"t1_n2r66nu","score":1,"author_fullname":"t2_ijzb7","approved_by":null,"mod_note":null,"all_awardings":[],"body":"Financial and coding.","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n2r6can","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Financial and coding.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lxwodv","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxwodv/best_setup_for_20_tokenssec_deepseek_r1_671b_q8_w/n2r6can/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752339364,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":7,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2r66nu","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"mxmumtuna","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2r5ca8","score":1,"author_fullname":"t2_igbly","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"What sort of work are you planning to do? Theyâ€™re not necessarily interchangeable. 235B is definitely accessible in a reasonable budget.","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n2r66nu","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;What sort of work are you planning to do? Theyâ€™re not necessarily interchangeable. 235B is definitely accessible in a reasonable budget.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lxwodv","associated_award":null,"stickied":false,"author_premium":true,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxwodv/best_setup_for_20_tokenssec_deepseek_r1_671b_q8_w/n2r66nu/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752339316,"author_flair_text":null,"treatment_tags":[],"created_utc":1752339316,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":6,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2r5ca8","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MidnightProgrammer","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2qkc6s","score":1,"author_fullname":"t2_ijzb7","approved_by":null,"mod_note":null,"all_awardings":[],"body":"Iâ€™d rarely ever use up the context but damn hours is out the question.  Also considering settling for Qwen 235B as an alternative that I potentially. Use GPU for.","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n2r5ca8","is_submitter":true,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Iâ€™d rarely ever use up the context but damn hours is out the question.  Also considering settling for Qwen 235B as an alternative that I potentially. Use GPU for.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lxwodv","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxwodv/best_setup_for_20_tokenssec_deepseek_r1_671b_q8_w/n2r5ca8/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752339063,"author_flair_text":null,"treatment_tags":[],"created_utc":1752339063,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2rszme","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Baldur-Norddahl","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2qkc6s","score":2,"author_fullname":"t2_bvqb8ng0","approved_by":null,"mod_note":null,"all_awardings":[],"body":"According to some 5 month old reddit posts, prompt processing is about 60 tps. So about half an hour to fill up the entire 128k max context size. It is not hours.\\n\\nAlthough this might have been improved with MLX support since. We are also seeing good improvements in prompt caching. Caching means that the large system prompts from agentic coding will not be processed all over constantly.","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n2rszme","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;According to some 5 month old reddit posts, prompt processing is about 60 tps. So about half an hour to fill up the entire 128k max context size. It is not hours.&lt;/p&gt;\\n\\n&lt;p&gt;Although this might have been improved with MLX support since. We are also seeing good improvements in prompt caching. Caching means that the large system prompts from agentic coding will not be processed all over constantly.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lxwodv","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxwodv/best_setup_for_20_tokenssec_deepseek_r1_671b_q8_w/n2rszme/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752346329,"author_flair_text":null,"treatment_tags":[],"created_utc":1752346329,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n2qkc6s","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"mxmumtuna","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2qj0zs","score":2,"author_fullname":"t2_igbly","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yeah the prompt processing is super slow on the Mac due to the limited GPU compute. Youâ€™re probably looking at a couple hours of prompt processing for 128k. \\n\\nRealistically for your requirements of Q8 at 20tps youâ€™re going to have to do that on a lot of GPU, so expect 4 RTX Pro 6000s combined with 24 channel RAM and numa node shenanigans on EPYC. Can probably get it done for something like $40-50k.","edited":false,"author_flair_css_class":null,"name":"t1_n2qkc6s","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yeah the prompt processing is super slow on the Mac due to the limited GPU compute. Youâ€™re probably looking at a couple hours of prompt processing for 128k. &lt;/p&gt;\\n\\n&lt;p&gt;Realistically for your requirements of Q8 at 20tps youâ€™re going to have to do that on a lot of GPU, so expect 4 RTX Pro 6000s combined with 24 channel RAM and numa node shenanigans on EPYC. Can probably get it done for something like $40-50k.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lxwodv","associated_award":null,"stickied":false,"author_premium":true,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxwodv/best_setup_for_20_tokenssec_deepseek_r1_671b_q8_w/n2qkc6s/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752332502,"author_flair_text":null,"collapsed":false,"created_utc":1752332502,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n2qj0zs","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MidnightProgrammer","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2qigfc","score":1,"author_fullname":"t2_ijzb7","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I know it is limited to Q4 due to only 512G, I am seeing 17-18 tokens/sec, is it slower than that?  Is the prompt processing really slow?  I don't expect to use full context all the time, I just would like to do it if it is just a matter of more ram.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2qj0zs","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I know it is limited to Q4 due to only 512G, I am seeing 17-18 tokens/sec, is it slower than that?  Is the prompt processing really slow?  I don&amp;#39;t expect to use full context all the time, I just would like to do it if it is just a matter of more ram.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lxwodv","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxwodv/best_setup_for_20_tokenssec_deepseek_r1_671b_q8_w/n2qj0zs/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752332089,"author_flair_text":null,"treatment_tags":[],"created_utc":1752332089,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2qigfc","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"mxmumtuna","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2prmuw","score":2,"author_fullname":"t2_igbly","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"That Mac wonâ€™t give you 17-18 T/s at full context at Q8. It also will take approximately 5 years to process that context.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2qigfc","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;That Mac wonâ€™t give you 17-18 T/s at full context at Q8. It also will take approximately 5 years to process that context.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":true,"can_gild":false,"link_id":"t3_1lxwodv","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxwodv/best_setup_for_20_tokenssec_deepseek_r1_671b_q8_w/n2qigfc/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752331907,"author_flair_text":null,"treatment_tags":[],"created_utc":1752331907,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2psote","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"a_beautiful_rhind","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2prmuw","score":1,"author_fullname":"t2_h5utwre7","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I only touch 200gb/s and get 11, almost 12 on IQ2. I have the 4 3090s though. Q4 is pretty respectable as long as you're not blowing the budget. You could rent some HW in different configurations before spending bank and see what you're willing to put up with vs asking all of us for hearsay.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2psote","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I only touch 200gb/s and get 11, almost 12 on IQ2. I have the 4 3090s though. Q4 is pretty respectable as long as you&amp;#39;re not blowing the budget. You could rent some HW in different configurations before spending bank and see what you&amp;#39;re willing to put up with vs asking all of us for hearsay.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lxwodv","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxwodv/best_setup_for_20_tokenssec_deepseek_r1_671b_q8_w/n2psote/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752322561,"author_flair_text":null,"treatment_tags":[],"created_utc":1752322561,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2prmuw","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MidnightProgrammer","can_mod_post":false,"created_utc":1752322104,"send_replies":true,"parent_id":"t1_n2pqyul","score":2,"author_fullname":"t2_ijzb7","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I was thinking an epyc system with 1TB ram, but I am not sure what I can expect for token/sec and what size cpu I'd need.  I believe the Epyc 9965 is where you start getting the faster ram but still only 576Gb/s.\\n\\nI'm considering just a mac studio but that will limit me to Q4 but I believe will give around 17-18 tokens/sec which would be usable.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2prmuw","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I was thinking an epyc system with 1TB ram, but I am not sure what I can expect for token/sec and what size cpu I&amp;#39;d need.  I believe the Epyc 9965 is where you start getting the faster ram but still only 576Gb/s.&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;m considering just a mac studio but that will limit me to Q4 but I believe will give around 17-18 tokens/sec which would be usable.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lxwodv","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxwodv/best_setup_for_20_tokenssec_deepseek_r1_671b_q8_w/n2prmuw/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752322104,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2re58t","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"a_beautiful_rhind","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2qt8rg","score":1,"author_fullname":"t2_h5utwre7","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Didn't they add multi-gpu support a while back? ik_llama does all the same stuff with less mess.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2re58t","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Didn&amp;#39;t they add multi-gpu support a while back? ik_llama does all the same stuff with less mess.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lxwodv","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxwodv/best_setup_for_20_tokenssec_deepseek_r1_671b_q8_w/n2re58t/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752341712,"author_flair_text":null,"treatment_tags":[],"created_utc":1752341712,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2qt8rg","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"cantgetthistowork","can_mod_post":false,"created_utc":1752335286,"send_replies":true,"parent_id":"t1_n2pqyul","score":2,"author_fullname":"t2_j1i0o","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Don't think ktransformers scales with more GPUs","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2qt8rg","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Don&amp;#39;t think ktransformers scales with more GPUs&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lxwodv","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxwodv/best_setup_for_20_tokenssec_deepseek_r1_671b_q8_w/n2qt8rg/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752335286,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n2pqyul","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"a_beautiful_rhind","can_mod_post":false,"created_utc":1752321814,"send_replies":true,"parent_id":"t3_1lxwodv","score":3,"author_fullname":"t2_h5utwre7","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Handful of GPUs improve performance a ton. The more I offload, the faster it gets.\\n\\nYou are asking for a lot to have Q8. Maybe you can swing some ES intel chips with DDR5 or the equivalent AMD.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2pqyul","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Handful of GPUs improve performance a ton. The more I offload, the faster it gets.&lt;/p&gt;\\n\\n&lt;p&gt;You are asking for a lot to have Q8. Maybe you can swing some ES intel chips with DDR5 or the equivalent AMD.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxwodv/best_setup_for_20_tokenssec_deepseek_r1_671b_q8_w/n2pqyul/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752321814,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lxwodv","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2sk70s","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Threatening-Silence-","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2shkyk","score":4,"author_fullname":"t2_15wqsifdjf","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"\`-mg\` parameter in llama-cpp flags a specific gpu as your \\"main\\" GPU and all pp will be done on that one.","edited":false,"author_flair_css_class":null,"name":"t1_n2sk70s","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;&lt;code&gt;-mg&lt;/code&gt; parameter in llama-cpp flags a specific gpu as your &amp;quot;main&amp;quot; GPU and all pp will be done on that one.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lxwodv","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxwodv/best_setup_for_20_tokenssec_deepseek_r1_671b_q8_w/n2sk70s/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752355114,"author_flair_text":null,"collapsed":false,"created_utc":1752355114,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}}],"before":null}},"user_reports":[],"saved":false,"id":"n2shkyk","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MidnightProgrammer","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2shelf","score":1,"author_fullname":"t2_ijzb7","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"How do you ensure the prompt processing is done on the 3090 and not one of the mi50?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2shkyk","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;How do you ensure the prompt processing is done on the 3090 and not one of the mi50?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lxwodv","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxwodv/best_setup_for_20_tokenssec_deepseek_r1_671b_q8_w/n2shkyk/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752354247,"author_flair_text":null,"treatment_tags":[],"created_utc":1752354247,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2shelf","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Threatening-Silence-","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2sc1kd","score":1,"author_fullname":"t2_15wqsifdjf","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I'll power limit to 120w or so. Shouldn't be too bad. \\n\\n3090 better for prompt processing.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2shelf","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;ll power limit to 120w or so. Shouldn&amp;#39;t be too bad. &lt;/p&gt;\\n\\n&lt;p&gt;3090 better for prompt processing.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lxwodv","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxwodv/best_setup_for_20_tokenssec_deepseek_r1_671b_q8_w/n2shelf/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752354189,"author_flair_text":null,"treatment_tags":[],"created_utc":1752354189,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2sc1kd","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MidnightProgrammer","can_mod_post":false,"created_utc":1752352474,"send_replies":true,"parent_id":"t1_n2s5ft5","score":1,"author_fullname":"t2_ijzb7","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Why 3090, not another mi50?  \\nI'd love to hear how fast it runs when you finish it.  \\nThat's got to take a lot of power.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2sc1kd","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Why 3090, not another mi50?&lt;br/&gt;\\nI&amp;#39;d love to hear how fast it runs when you finish it.&lt;br/&gt;\\nThat&amp;#39;s got to take a lot of power.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lxwodv","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxwodv/best_setup_for_20_tokenssec_deepseek_r1_671b_q8_w/n2sc1kd/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752352474,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2sh70m","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Threatening-Silence-","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2sf18l","score":2,"author_fullname":"t2_15wqsifdjf","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Vulcan.\\n\\nvLLM has Vulcan support in the works (might be out already) but in any case llama-cpp will do fine. \\n\\nI'm not sure what the state of Vulcan is on ktransformers and ikllama.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2sh70m","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Vulcan.&lt;/p&gt;\\n\\n&lt;p&gt;vLLM has Vulcan support in the works (might be out already) but in any case llama-cpp will do fine. &lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;m not sure what the state of Vulcan is on ktransformers and ikllama.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lxwodv","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxwodv/best_setup_for_20_tokenssec_deepseek_r1_671b_q8_w/n2sh70m/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752354120,"author_flair_text":null,"treatment_tags":[],"created_utc":1752354120,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n2sf18l","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"gingerbeer987654321","can_mod_post":false,"created_utc":1752353428,"send_replies":true,"parent_id":"t1_n2s5ft5","score":1,"author_fullname":"t2_986wl7b","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"What software will you run/ compile to combine the amd and nvidia cards simultaneously?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2sf18l","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;What software will you run/ compile to combine the amd and nvidia cards simultaneously?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lxwodv","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxwodv/best_setup_for_20_tokenssec_deepseek_r1_671b_q8_w/n2sf18l/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752353428,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2s5ft5","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Threatening-Silence-","can_mod_post":false,"created_utc":1752350368,"send_replies":true,"parent_id":"t3_1lxwodv","score":2,"author_fullname":"t2_15wqsifdjf","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I'm in the middle of rebuilding my Frankenstein inferencing box and I've chosen the following components: \\n\\n* Supermicro x11dpi-n mobo (cost Â£430)\\n* Dual Xeon Gold 6240 (Â£160)\\n* 12 x 64GB DDR4 2933 (Â£950)\\n\\nGiving 768GB of RAM with 400-480GB/s system memory bandwidth (12 channels).\\n\\nPaired with:\\n\\n* 11 x AMD mi50 32gb (Â£1600 off Alibaba)\\n* 1 X RTX 3090 24GB (Â£650)\\n\\nGiving 376GB VRAM.\\n\\nFor a total cost of Â£3790.\\n\\nI'm expecting better than 20t/s but we will see.\\n\\nEdit: in this open mining frame\\n\\n https://amzn.eu/d/h66gdwI","edited":1752354296,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2s5ft5","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;m in the middle of rebuilding my Frankenstein inferencing box and I&amp;#39;ve chosen the following components: &lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;Supermicro x11dpi-n mobo (cost Â£430)&lt;/li&gt;\\n&lt;li&gt;Dual Xeon Gold 6240 (Â£160)&lt;/li&gt;\\n&lt;li&gt;12 x 64GB DDR4 2933 (Â£950)&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;p&gt;Giving 768GB of RAM with 400-480GB/s system memory bandwidth (12 channels).&lt;/p&gt;\\n\\n&lt;p&gt;Paired with:&lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;11 x AMD mi50 32gb (Â£1600 off Alibaba)&lt;/li&gt;\\n&lt;li&gt;1 X RTX 3090 24GB (Â£650)&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;p&gt;Giving 376GB VRAM.&lt;/p&gt;\\n\\n&lt;p&gt;For a total cost of Â£3790.&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;m expecting better than 20t/s but we will see.&lt;/p&gt;\\n\\n&lt;p&gt;Edit: in this open mining frame&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://amzn.eu/d/h66gdwI\\"&gt;https://amzn.eu/d/h66gdwI&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxwodv/best_setup_for_20_tokenssec_deepseek_r1_671b_q8_w/n2s5ft5/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752350368,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lxwodv","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"d2642412-d9ce-11ed-ae30-32b11309f5bd","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"d2642412-d9ce-11ed-ae30-32b11309f5bd","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2sl91q","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ortegaalfredo","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2pn0ya","score":1,"author_fullname":"t2_g177e","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"ah yes, correct. Too slow. And a CPU will be worse.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2sl91q","is_submitter":false,"collapsed":false,"author_flair_richtext":[{"e":"text","t":"Alpaca"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;ah yes, correct. Too slow. And a CPU will be worse.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lxwodv","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxwodv/best_setup_for_20_tokenssec_deepseek_r1_671b_q8_w/n2sl91q/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752355466,"author_flair_text":"Alpaca","treatment_tags":[],"created_utc":1752355466,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":"#bd9e9e","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2pn0ya","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"joninco","can_mod_post":false,"created_utc":1752320013,"send_replies":true,"parent_id":"t1_n2pf1r5","score":9,"author_fullname":"t2_8e8y0","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"He said 128K context window, the GPU on a MAC will shit its panties trying to process that.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2pn0ya","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;He said 128K context window, the GPU on a MAC will shit its panties trying to process that.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lxwodv","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxwodv/best_setup_for_20_tokenssec_deepseek_r1_671b_q8_w/n2pn0ya/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752320013,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":9}}],"before":null}},"user_reports":[],"saved":false,"id":"n2pf1r5","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ortegaalfredo","can_mod_post":false,"created_utc":1752315840,"send_replies":true,"parent_id":"t3_1lxwodv","score":3,"author_fullname":"t2_g177e","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Check ik\\\\_llama project, I don't think you can get to 20+ tok/s but 10 is doable. Also you can buy a 512 GB mac and it will get quite close using MLX.","edited":1752355449,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2pf1r5","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"Alpaca"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Check ik_llama project, I don&amp;#39;t think you can get to 20+ tok/s but 10 is doable. Also you can buy a 512 GB mac and it will get quite close using MLX.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxwodv/best_setup_for_20_tokenssec_deepseek_r1_671b_q8_w/n2pf1r5/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752315840,"author_flair_text":"Alpaca","treatment_tags":[],"link_id":"t3_1lxwodv","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bd9e9e","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2upkrq","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MidnightProgrammer","can_mod_post":false,"created_utc":1752385100,"send_replies":true,"parent_id":"t1_n2trbca","score":1,"author_fullname":"t2_ijzb7","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I am not specifically tied to R1, I was just looking at building around it and hope it will allow me to keep up with better than average models.  I find most things under R1 are not worth running for anything serious.\\n\\nKimi looks really interesting, but I haven't looked into what is required to get that running locally without lobotomizing it as it just got announced.  I heard great things about it.\\n\\nIdeally I want a system that just runs a single RTX 6000 Pro, that would be great, but I don't think there is enough VRAM to run anything that could justify not using Claude.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2upkrq","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I am not specifically tied to R1, I was just looking at building around it and hope it will allow me to keep up with better than average models.  I find most things under R1 are not worth running for anything serious.&lt;/p&gt;\\n\\n&lt;p&gt;Kimi looks really interesting, but I haven&amp;#39;t looked into what is required to get that running locally without lobotomizing it as it just got announced.  I heard great things about it.&lt;/p&gt;\\n\\n&lt;p&gt;Ideally I want a system that just runs a single RTX 6000 Pro, that would be great, but I don&amp;#39;t think there is enough VRAM to run anything that could justify not using Claude.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lxwodv","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxwodv/best_setup_for_20_tokenssec_deepseek_r1_671b_q8_w/n2upkrq/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752385100,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2trbca","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"georgejrjrjr","can_mod_post":false,"created_utc":1752370503,"send_replies":true,"parent_id":"t3_1lxwodv","score":1,"author_fullname":"t2_ei8i09kjx","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"OK, gotchas abound here:\\n\\n1. A suitable EPYC (/+GPU) setup can do full 128k context (total), and it can do 20 tokens/s (at short context, while you're memory bandwidth limited), but 20tps at 128k context (when you're compute limited)...probably not. DeepSeek doesn't even serve the full 128k context of their model, third parties mostly don't either, and this is why. Much though I love DeepSeek, their advertised 128K is kind-of fake given the compute needed with their MLA configuration --and the May release of their model is absurdly yappy in thinking, which exacerbates this problem.\\n\\n2. Unless you have a compelling need for R1 in particular, you're probably better off making room for Kimi K2 if you care about context capacity and speed at high context. You need a bit more RAM, but it's a stronger model that is MUCH faster: more concise (many fewer tokens and shorter context for the same work), faster at long context (because less attention), with fewer total active parameters.\\n\\nIf you do need R1 in particular, maybe consider the Chimera model (/merge) from TNG-Tech, which reduces the yap factor somewhat.\\n\\n3. AMD blatantly lies about their memory bandwidth specs, so configuring a \\\\*\\\\*suitable\\\\*\\\\* EPYC + GPU configuration is non-obvious. Specifically, AMD advertises 12 memory channels per chip (/24 for a dual processor setup), but the number you can actually make use of depends on the number of CCDs on the particular SKUs you buy.  \\n  \\nSo what keeps happening is people look at the specs, buy a processor or two that looks suitable (but has too few CCDs), and then wonder why their LLM is so slow. The Level1Tech build mentioned here was one such case, iirc. So do some research + find some MBW benchmarks before buying chips.\\n\\n4. Dual processor might be a good idea: the board is a bit more expensive, but price per core, price per MBW, and price per GB ram all goes down...and ofc maximum performance goes up if you've got budget for it (and it sounds like you do).  \\n  \\n5. You want all your common parameters (attention and shared experts) and ideally also your KV cache on a GPU. This takes a ton of burden off the MBW of your CPU memory because you're only hosting the non-shared experts. The RTX 6000 Pro you mentioned elsewhere would work, but idk if it's optimal. Your GPU(s) are going to be doing most of the processing (at long context when it makes a difference), so you may end up caring more about FLOPs per dollar (at FP8!) than VRAM per dollar provided you have enough ram for all the shared parameters and KV cache.\\n\\n6. ktransformers is inference framework that is set up specifically for these CPU+GPU offloading scenarios.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2trbca","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;OK, gotchas abound here:&lt;/p&gt;\\n\\n&lt;ol&gt;\\n&lt;li&gt;&lt;p&gt;A suitable EPYC (/+GPU) setup can do full 128k context (total), and it can do 20 tokens/s (at short context, while you&amp;#39;re memory bandwidth limited), but 20tps at 128k context (when you&amp;#39;re compute limited)...probably not. DeepSeek doesn&amp;#39;t even serve the full 128k context of their model, third parties mostly don&amp;#39;t either, and this is why. Much though I love DeepSeek, their advertised 128K is kind-of fake given the compute needed with their MLA configuration --and the May release of their model is absurdly yappy in thinking, which exacerbates this problem.&lt;/p&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;p&gt;Unless you have a compelling need for R1 in particular, you&amp;#39;re probably better off making room for Kimi K2 if you care about context capacity and speed at high context. You need a bit more RAM, but it&amp;#39;s a stronger model that is MUCH faster: more concise (many fewer tokens and shorter context for the same work), faster at long context (because less attention), with fewer total active parameters.&lt;/p&gt;&lt;/li&gt;\\n&lt;/ol&gt;\\n\\n&lt;p&gt;If you do need R1 in particular, maybe consider the Chimera model (/merge) from TNG-Tech, which reduces the yap factor somewhat.&lt;/p&gt;\\n\\n&lt;ol&gt;\\n&lt;li&gt;AMD blatantly lies about their memory bandwidth specs, so configuring a **suitable** EPYC + GPU configuration is non-obvious. Specifically, AMD advertises 12 memory channels per chip (/24 for a dual processor setup), but the number you can actually make use of depends on the number of CCDs on the particular SKUs you buy.&lt;br/&gt;&lt;/li&gt;\\n&lt;/ol&gt;\\n\\n&lt;p&gt;So what keeps happening is people look at the specs, buy a processor or two that looks suitable (but has too few CCDs), and then wonder why their LLM is so slow. The Level1Tech build mentioned here was one such case, iirc. So do some research + find some MBW benchmarks before buying chips.&lt;/p&gt;\\n\\n&lt;ol&gt;\\n&lt;li&gt;&lt;p&gt;Dual processor might be a good idea: the board is a bit more expensive, but price per core, price per MBW, and price per GB ram all goes down...and ofc maximum performance goes up if you&amp;#39;ve got budget for it (and it sounds like you do).  &lt;/p&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;p&gt;You want all your common parameters (attention and shared experts) and ideally also your KV cache on a GPU. This takes a ton of burden off the MBW of your CPU memory because you&amp;#39;re only hosting the non-shared experts. The RTX 6000 Pro you mentioned elsewhere would work, but idk if it&amp;#39;s optimal. Your GPU(s) are going to be doing most of the processing (at long context when it makes a difference), so you may end up caring more about FLOPs per dollar (at FP8!) than VRAM per dollar provided you have enough ram for all the shared parameters and KV cache.&lt;/p&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;p&gt;ktransformers is inference framework that is set up specifically for these CPU+GPU offloading scenarios.&lt;/p&gt;&lt;/li&gt;\\n&lt;/ol&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxwodv/best_setup_for_20_tokenssec_deepseek_r1_671b_q8_w/n2trbca/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752370503,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lxwodv","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),n=()=>e.jsx(l,{data:t});export{n as default};
