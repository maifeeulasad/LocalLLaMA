import{j as e}from"./index-cvG704yx.js";import{R as l}from"./RedditPostRenderer-CBthLTAH.js";import"./index-D-GavSZU.js";const t=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"As a partner with Moonshot AI, we present you the q4km version of Kimi K2 and the instructions to run it with KTransformers.\\n\\n[KVCache-ai/Kimi-K2-Instruct-GGUF · Hugging Face](https://huggingface.co/KVCache-ai/Kimi-K2-Instruct-GGUF) \\n\\n[ktransformers/doc/en/Kimi-K2.md at main · kvcache-ai/ktransformers](https://github.com/kvcache-ai/ktransformers/blob/main/doc/en/Kimi-K2.md)\\n\\n10tps for single-socket CPU and one 4090, 14tps if you have two.\\n\\nBe careful of the DRAM OOM.\\n\\nIt is a Big Beautiful Model.  \\nEnjoy it\\n\\n ","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Kimi K2 q4km is here and also the instructions to run it locally with KTransformers 10-14tps","link_flair_richtext":[{"e":"text","t":"Resources"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":75,"top_awarded_type":null,"hide_score":false,"name":"t3_1lxr5s3","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.96,"author_flair_background_color":null,"ups":241,"total_awards_received":0,"media_embed":{},"thumbnail_width":140,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_k4k4yxry","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Resources","can_mod_post":false,"score":241,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"https://external-preview.redd.it/7um7XAkvHQRx2MF4TRo122daGoOYixRc6uShLTRN9Tw.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=6bc4df21524fcb111aa453496f33bdcbd743562e","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"post_hint":"link","content_categories":null,"is_self":false,"subreddit_type":"public","created":1752293183,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"huggingface.co","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;As a partner with Moonshot AI, we present you the q4km version of Kimi K2 and the instructions to run it with KTransformers.&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://huggingface.co/KVCache-ai/Kimi-K2-Instruct-GGUF\\"&gt;KVCache-ai/Kimi-K2-Instruct-GGUF · Hugging Face&lt;/a&gt; &lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://github.com/kvcache-ai/ktransformers/blob/main/doc/en/Kimi-K2.md\\"&gt;ktransformers/doc/en/Kimi-K2.md at main · kvcache-ai/ktransformers&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;10tps for single-socket CPU and one 4090, 14tps if you have two.&lt;/p&gt;\\n\\n&lt;p&gt;Be careful of the DRAM OOM.&lt;/p&gt;\\n\\n&lt;p&gt;It is a Big Beautiful Model.&lt;br/&gt;\\nEnjoy it&lt;/p&gt;\\n\\n&lt;p&gt; &lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"url_overridden_by_dest":"https://huggingface.co/KVCache-ai/Kimi-K2-Instruct-GGUF","view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"preview":{"images":[{"source":{"url":"https://external-preview.redd.it/7um7XAkvHQRx2MF4TRo122daGoOYixRc6uShLTRN9Tw.png?auto=webp&amp;s=1ea460e4680601c46cf15192ac5a20bb3e17a787","width":1200,"height":648},"resolutions":[{"url":"https://external-preview.redd.it/7um7XAkvHQRx2MF4TRo122daGoOYixRc6uShLTRN9Tw.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=7e95d53ecb2c94b00d53ef66bf67cdceb012ec71","width":108,"height":58},{"url":"https://external-preview.redd.it/7um7XAkvHQRx2MF4TRo122daGoOYixRc6uShLTRN9Tw.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=b33ac8b11315e69d9788c830db632013347f52e0","width":216,"height":116},{"url":"https://external-preview.redd.it/7um7XAkvHQRx2MF4TRo122daGoOYixRc6uShLTRN9Tw.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=e65c3d6165edd1aea775e557547c21dc4566681a","width":320,"height":172},{"url":"https://external-preview.redd.it/7um7XAkvHQRx2MF4TRo122daGoOYixRc6uShLTRN9Tw.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=2cc494cf2008a19ce100d156817257c3630b664e","width":640,"height":345},{"url":"https://external-preview.redd.it/7um7XAkvHQRx2MF4TRo122daGoOYixRc6uShLTRN9Tw.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=9ca02a0ef14101d0b6155c64607e453bb33f317f","width":960,"height":518},{"url":"https://external-preview.redd.it/7um7XAkvHQRx2MF4TRo122daGoOYixRc6uShLTRN9Tw.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=c481b4435d60f9f20061fcf0fcc9a69b53bb8aec","width":1080,"height":583}],"variants":{},"id":"7um7XAkvHQRx2MF4TRo122daGoOYixRc6uShLTRN9Tw"}],"enabled":false},"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"mod_note":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"num_reports":null,"removal_reason":null,"link_flair_background_color":"#ccac2b","id":"1lxr5s3","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"CombinationNo780","discussion_type":null,"num_comments":42,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lxr5s3/kimi_k2_q4km_is_here_and_also_the_instructions_to/","stickied":false,"url":"https://huggingface.co/KVCache-ai/Kimi-K2-Instruct-GGUF","subreddit_subscribers":498346,"created_utc":1752293183,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2py0r9","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"rorowhat","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2pf31h","score":13,"author_fullname":"t2_yq51a","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Not only is it by token, but I think it's also by layer of the model. You need to load the whole thing in case it picks another expert along the way.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2py0r9","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Not only is it by token, but I think it&amp;#39;s also by layer of the model. You need to load the whole thing in case it picks another expert along the way.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lxr5s3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxr5s3/kimi_k2_q4km_is_here_and_also_the_instructions_to/n2py0r9/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752324728,"author_flair_text":null,"treatment_tags":[],"created_utc":1752324728,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":13}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2pvrry","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"mearyu_","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2pf31h","score":4,"author_fullname":"t2_4vjcpsi7h","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"EAddario does quants like that taking out lesser used/important experts [https://huggingface.co/eaddario/Qwen3-30B-A3B-pruned-GGUF](https://huggingface.co/eaddario/Qwen3-30B-A3B-pruned-GGUF)   \\nBased on these statistics [https://github.com/ggml-org/llama.cpp/pull/12718](https://github.com/ggml-org/llama.cpp/pull/12718)","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2pvrry","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;EAddario does quants like that taking out lesser used/important experts &lt;a href=\\"https://huggingface.co/eaddario/Qwen3-30B-A3B-pruned-GGUF\\"&gt;https://huggingface.co/eaddario/Qwen3-30B-A3B-pruned-GGUF&lt;/a&gt;&lt;br/&gt;\\nBased on these statistics &lt;a href=\\"https://github.com/ggml-org/llama.cpp/pull/12718\\"&gt;https://github.com/ggml-org/llama.cpp/pull/12718&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lxr5s3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxr5s3/kimi_k2_q4km_is_here_and_also_the_instructions_to/n2pvrry/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752323838,"author_flair_text":null,"treatment_tags":[],"created_utc":1752323838,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2sblgw","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"TheRealMasonMac","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2ppmo5","score":5,"author_fullname":"t2_101haj","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"It sounds like branch prediction, but you're paying a heftier cost with respect to throughput. It might be usable for single-user deployments though.","edited":false,"author_flair_css_class":null,"name":"t1_n2sblgw","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It sounds like branch prediction, but you&amp;#39;re paying a heftier cost with respect to throughput. It might be usable for single-user deployments though.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lxr5s3","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxr5s3/kimi_k2_q4km_is_here_and_also_the_instructions_to/n2sblgw/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752352330,"author_flair_text":null,"collapsed":false,"created_utc":1752352330,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}}],"before":null}},"user_reports":[],"saved":false,"id":"n2ppmo5","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"xmBQWugdxjaA","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2pgm75","score":6,"author_fullname":"t2_nyyscwdgr","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"https://github.com/ggml-org/llama.cpp/issues/11532\\n\\nhttps://www.reddit.com/r/LocalLLaMA/comments/1kry8m8/dynamically_loading_experts_in_moe_models/\\n\\nThe hard part is that if you can't predict perfectly then you have to read from disk and it will be very slow.\\n\\nSo it's a trade-off against how many you can load, it could be worth investigating though. As Kimi K2 claims \\"only\\" 32B are activated out of the 1T total parameters of all expert layers: https://huggingface.co/moonshotai/Kimi-K2-Instruct\\n\\nThe issue is that set of 32B changes every token then it's still not practical to cut it down.\\n\\nAnd even 32B is a lot of parameters for consumer GPUs :(","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2ppmo5","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;&lt;a href=\\"https://github.com/ggml-org/llama.cpp/issues/11532\\"&gt;https://github.com/ggml-org/llama.cpp/issues/11532&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://www.reddit.com/r/LocalLLaMA/comments/1kry8m8/dynamically_loading_experts_in_moe_models/\\"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1kry8m8/dynamically_loading_experts_in_moe_models/&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;The hard part is that if you can&amp;#39;t predict perfectly then you have to read from disk and it will be very slow.&lt;/p&gt;\\n\\n&lt;p&gt;So it&amp;#39;s a trade-off against how many you can load, it could be worth investigating though. As Kimi K2 claims &amp;quot;only&amp;quot; 32B are activated out of the 1T total parameters of all expert layers: &lt;a href=\\"https://huggingface.co/moonshotai/Kimi-K2-Instruct\\"&gt;https://huggingface.co/moonshotai/Kimi-K2-Instruct&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;The issue is that set of 32B changes every token then it&amp;#39;s still not practical to cut it down.&lt;/p&gt;\\n\\n&lt;p&gt;And even 32B is a lot of parameters for consumer GPUs :(&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lxr5s3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxr5s3/kimi_k2_q4km_is_here_and_also_the_instructions_to/n2ppmo5/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752321220,"author_flair_text":null,"treatment_tags":[],"created_utc":1752321220,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}}],"before":null}},"user_reports":[],"saved":false,"id":"n2pgm75","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"JohnnyLiverman","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2pf31h","score":6,"author_fullname":"t2_82gdsryl","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"there must be some way you could use the router for this right? This actually sounds like a solid idea (I have barely any idea how MOE works lmao)","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2pgm75","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;there must be some way you could use the router for this right? This actually sounds like a solid idea (I have barely any idea how MOE works lmao)&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lxr5s3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxr5s3/kimi_k2_q4km_is_here_and_also_the_instructions_to/n2pgm75/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752316728,"author_flair_text":null,"treatment_tags":[],"created_utc":1752316728,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2umh3r","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"martinus","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2pf31h","score":2,"author_fullname":"t2_1oc8","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Doesn't llama just mmap everything and let the os figure out the rest?","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2umh3r","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Doesn&amp;#39;t llama just mmap everything and let the os figure out the rest?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lxr5s3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxr5s3/kimi_k2_q4km_is_here_and_also_the_instructions_to/n2umh3r/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752383533,"author_flair_text":null,"treatment_tags":[],"created_utc":1752383533,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n2pf31h","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"xmBQWugdxjaA","can_mod_post":false,"created_utc":1752315861,"send_replies":true,"parent_id":"t1_n2ocej3","score":25,"author_fullname":"t2_nyyscwdgr","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Kimi K2 is a huge MoE model though - it'd be great if llama.cpp could only load the specific MoE layers that are actually used at inference time, although it's complicated since it can vary so much by token.\\n\\nI wonder if you could train another model to take a set of tokens and predict which set of experts will actually be used, and then load only those for each prompt.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2pf31h","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Kimi K2 is a huge MoE model though - it&amp;#39;d be great if llama.cpp could only load the specific MoE layers that are actually used at inference time, although it&amp;#39;s complicated since it can vary so much by token.&lt;/p&gt;\\n\\n&lt;p&gt;I wonder if you could train another model to take a set of tokens and predict which set of experts will actually be used, and then load only those for each prompt.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lxr5s3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxr5s3/kimi_k2_q4km_is_here_and_also_the_instructions_to/n2pf31h/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752315861,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":25}}],"before":null}},"user_reports":[],"saved":false,"id":"n2ocej3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Starman-Paradox","can_mod_post":false,"created_utc":1752294320,"send_replies":true,"parent_id":"t3_1lxr5s3","score":59,"author_fullname":"t2_trskein","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"llama.cpp can run models directly from SSD. *Slowly,* but it can...","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2ocej3","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;llama.cpp can run models directly from SSD. &lt;em&gt;Slowly,&lt;/em&gt; but it can...&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxr5s3/kimi_k2_q4km_is_here_and_also_the_instructions_to/n2ocej3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752294320,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lxr5s3","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":59}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ef488598-491f-11ef-a847-9a3dd315819c","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2qbloj","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"CockBrother","can_mod_post":false,"created_utc":1752329658,"send_replies":true,"parent_id":"t1_n2oact9","score":6,"author_fullname":"t2_o04mu","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Those requirements appear to be using it with fp16. The first thing they described doing was converting the fp8 to fp16 which would make sense for the 2tb requirement. This q4 quant should easily fit into 768GB machine. Looks like 512GB is out which also means my 1TB machine is out for the full precision.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2qbloj","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Those requirements appear to be using it with fp16. The first thing they described doing was converting the fp8 to fp16 which would make sense for the 2tb requirement. This q4 quant should easily fit into 768GB machine. Looks like 512GB is out which also means my 1TB machine is out for the full precision.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lxr5s3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxr5s3/kimi_k2_q4km_is_here_and_also_the_instructions_to/n2qbloj/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752329658,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"a8acc9bc-4792-11ee-b77d-c61a47557e59","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2tlhs0","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"henk717","can_mod_post":false,"created_utc":1752368292,"send_replies":true,"parent_id":"t1_n2oact9","score":2,"author_fullname":"t2_bx8b9","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Keep in mind this one is only usable with KTransformers. Don't waste your bandwith if you want to use something llamacpp based, wait for the usual quanters once llamacpp has their converter ready.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2tlhs0","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"KoboldAI"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Keep in mind this one is only usable with KTransformers. Don&amp;#39;t waste your bandwith if you want to use something llamacpp based, wait for the usual quanters once llamacpp has their converter ready.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lxr5s3","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxr5s3/kimi_k2_q4km_is_here_and_also_the_instructions_to/n2tlhs0/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752368292,"author_flair_text":"KoboldAI","treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":"#5a74cc","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n2oact9","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"panchovix","can_mod_post":false,"created_utc":1752293419,"send_replies":true,"parent_id":"t3_1lxr5s3","score":57,"author_fullname":"t2_j1kqr","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"&gt;The model running with 384 Experts requires approximately 2 TB of memory and 14 GB of GPU memory.\\n\\nOof, I'm out of luck. But thanks for the first GGUF quant!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2oact9","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"Llama 405B"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;The model running with 384 Experts requires approximately 2 TB of memory and 14 GB of GPU memory.&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;Oof, I&amp;#39;m out of luck. But thanks for the first GGUF quant!&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxr5s3/kimi_k2_q4km_is_here_and_also_the_instructions_to/n2oact9/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752293419,"author_flair_text":"Llama 405B","treatment_tags":[],"link_id":"t3_1lxr5s3","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":57}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2uoivr","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Caffdy","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2p1xvw","score":2,"author_fullname":"t2_ql2vu0wz","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"practically anything, R1 needs around 400GB at Q4","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2uoivr","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;practically anything, R1 needs around 400GB at Q4&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lxr5s3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxr5s3/kimi_k2_q4km_is_here_and_also_the_instructions_to/n2uoivr/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752384558,"author_flair_text":null,"treatment_tags":[],"created_utc":1752384558,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n2p1xvw","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"eatmypekpek","can_mod_post":false,"created_utc":1752307936,"send_replies":true,"parent_id":"t1_n2ol61h","score":6,"author_fullname":"t2_czsr17k","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Kinda going off-topic, but what large models and quants are you able to run with your set up? I got 512gb RAM too (but dual 3090s).","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2p1xvw","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Kinda going off-topic, but what large models and quants are you able to run with your set up? I got 512gb RAM too (but dual 3090s).&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lxr5s3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxr5s3/kimi_k2_q4km_is_here_and_also_the_instructions_to/n2p1xvw/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752307936,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2pusxa","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Spectrum1523","can_mod_post":false,"created_utc":1752323445,"send_replies":true,"parent_id":"t1_n2ol61h","score":1,"author_fullname":"t2_w42gfoi19","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"i think a 2bpw quant would let you pull it off","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2pusxa","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;i think a 2bpw quant would let you pull it off&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lxr5s3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxr5s3/kimi_k2_q4km_is_here_and_also_the_instructions_to/n2pusxa/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752323445,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2ol61h","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"mnt_brain","can_mod_post":false,"created_utc":1752298552,"send_replies":true,"parent_id":"t3_1lxr5s3","score":15,"author_fullname":"t2_1mtt9dytfn","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Hmm I’ve got 512gb of RAM so I’m gonna have to figure something out. I do have dual 4090s though.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2ol61h","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Hmm I’ve got 512gb of RAM so I’m gonna have to figure something out. I do have dual 4090s though.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxr5s3/kimi_k2_q4km_is_here_and_also_the_instructions_to/n2ol61h/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752298552,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lxr5s3","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":15}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"d2642412-d9ce-11ed-ae30-32b11309f5bd","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2pyryu","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"ForsookComparison","can_mod_post":false,"created_utc":1752325016,"send_replies":true,"parent_id":"t1_n2pa29u","score":14,"author_fullname":"t2_on5es7pe3","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"At the point you can run this thing (not on SSD) I start considering your machine prosumer or enthusiast","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2pyryu","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;At the point you can run this thing (not on SSD) I start considering your machine prosumer or enthusiast&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lxr5s3","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxr5s3/kimi_k2_q4km_is_here_and_also_the_instructions_to/n2pyryu/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752325016,"author_flair_text":"llama.cpp","treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":14}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2vo5wc","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"BalorNG","can_mod_post":false,"created_utc":1752404730,"send_replies":true,"parent_id":"t1_n2pa29u","score":1,"author_fullname":"t2_b6gw9q","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I doubt that, it will remain server-grade hardware, just way more affordable. Half TB of ram is massive overkill for a typical 'consumer', even someone like a graphic designer...\\n\\n Yea, you can buy that as a consumer, but than you can buy a CNC router or a laser sintering 3d printer for your personal hobby if you are rich, that's not a tank or MRLS.\\n\\nUnless you mean high-end workstation with some sort of SSD raid + future moes that are even more fine-grained and use memory-bandwith-saving tricks like replacing number/size of executed experts with recursive/batched inference of every 'layer', which \\tmostly preserves the quality while drastically reducing memory io from the main model file, but still allow plenty of compute thrown a each token - according to recent papers.\\n\\nI bet there are more low-hanging fruits within this paradigm, like using first iterations to predictively pull possible next experts into faster storage while subsequent iterations are being executed... This way you can get ram or even vram speeds, provided you have enough vram for at least two sets of model active expert being executed (that's where having dual gpu setup will be a *massive* boost, if you think about it) *regardless* of model size - provided that your ssd raid/ram io is x-1 times slower, where X is number of recursive executions of each expert.\\n\\nNot sure about kv cache, I presume it will need to be kept in vram so will likely become a bottleneck fast. That's where hybrid ssms might shine tho.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2vo5wc","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I doubt that, it will remain server-grade hardware, just way more affordable. Half TB of ram is massive overkill for a typical &amp;#39;consumer&amp;#39;, even someone like a graphic designer...&lt;/p&gt;\\n\\n&lt;p&gt;Yea, you can buy that as a consumer, but than you can buy a CNC router or a laser sintering 3d printer for your personal hobby if you are rich, that&amp;#39;s not a tank or MRLS.&lt;/p&gt;\\n\\n&lt;p&gt;Unless you mean high-end workstation with some sort of SSD raid + future moes that are even more fine-grained and use memory-bandwith-saving tricks like replacing number/size of executed experts with recursive/batched inference of every &amp;#39;layer&amp;#39;, which     mostly preserves the quality while drastically reducing memory io from the main model file, but still allow plenty of compute thrown a each token - according to recent papers.&lt;/p&gt;\\n\\n&lt;p&gt;I bet there are more low-hanging fruits within this paradigm, like using first iterations to predictively pull possible next experts into faster storage while subsequent iterations are being executed... This way you can get ram or even vram speeds, provided you have enough vram for at least two sets of model active expert being executed (that&amp;#39;s where having dual gpu setup will be a &lt;em&gt;massive&lt;/em&gt; boost, if you think about it) &lt;em&gt;regardless&lt;/em&gt; of model size - provided that your ssd raid/ram io is x-1 times slower, where X is number of recursive executions of each expert.&lt;/p&gt;\\n\\n&lt;p&gt;Not sure about kv cache, I presume it will need to be kept in vram so will likely become a bottleneck fast. That&amp;#39;s where hybrid ssms might shine tho.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lxr5s3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxr5s3/kimi_k2_q4km_is_here_and_also_the_instructions_to/n2vo5wc/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752404730,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2pa29u","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"ortegaalfredo","can_mod_post":false,"created_utc":1752312886,"send_replies":true,"parent_id":"t3_1lxr5s3","score":18,"author_fullname":"t2_g177e","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Incredible that in 2 years we can run 1 \\\\*\\\\*trillion\\\\*\\\\* parameter LLM at usable speed on high-end consumer workstations.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2pa29u","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"Alpaca"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Incredible that in 2 years we can run 1 **trillion** parameter LLM at usable speed on high-end consumer workstations.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxr5s3/kimi_k2_q4km_is_here_and_also_the_instructions_to/n2pa29u/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752312886,"author_flair_text":"Alpaca","treatment_tags":[],"link_id":"t3_1lxr5s3","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bd9e9e","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":18}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2rvbue","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Freonr2","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2rstjr","score":1,"author_fullname":"t2_8xi6x","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Thanks for the tips!","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n2rvbue","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thanks for the tips!&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lxr5s3","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxr5s3/kimi_k2_q4km_is_here_and_also_the_instructions_to/n2rvbue/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752347074,"author_flair_text":null,"treatment_tags":[],"created_utc":1752347074,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":6,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2rstjr","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"FullstackSensei","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2rde63","score":4,"author_fullname":"t2_17n3nqtj56","approved_by":null,"mod_note":null,"all_awardings":[],"body":"Anything Milan with a letter has bad compatibility with motherboards. Do your homework beforehand to make sure you don't end up with an expensive paperweight.\\n\\nMilan in general doesn't bring any benefits for LLM inference over Rome. Even at 48 cores (7642) the cores can handle more than the memory controller can provide. Prompt processing will not be great on either platform anyways. That's why I stuck with Rome and got said 7642s.\\n\\nOnce you get to DDR5, Xeon Scalable 4 Engineering Samples (8480 ES, ex: QYFS, QYFX) are a much better bang for the buck IMO. EPYC 9004 might have more memory bandwidth, but Xeon 4 has AMX, which improves matrix multiplication performance substantially, especially in prompt processing. Motherboards cost about the same between the two platforms.","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n2rstjr","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Anything Milan with a letter has bad compatibility with motherboards. Do your homework beforehand to make sure you don&amp;#39;t end up with an expensive paperweight.&lt;/p&gt;\\n\\n&lt;p&gt;Milan in general doesn&amp;#39;t bring any benefits for LLM inference over Rome. Even at 48 cores (7642) the cores can handle more than the memory controller can provide. Prompt processing will not be great on either platform anyways. That&amp;#39;s why I stuck with Rome and got said 7642s.&lt;/p&gt;\\n\\n&lt;p&gt;Once you get to DDR5, Xeon Scalable 4 Engineering Samples (8480 ES, ex: QYFS, QYFX) are a much better bang for the buck IMO. EPYC 9004 might have more memory bandwidth, but Xeon 4 has AMX, which improves matrix multiplication performance substantially, especially in prompt processing. Motherboards cost about the same between the two platforms.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lxr5s3","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxr5s3/kimi_k2_q4km_is_here_and_also_the_instructions_to/n2rstjr/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752346276,"author_flair_text":null,"treatment_tags":[],"created_utc":1752346276,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}}],"before":null}},"user_reports":[],"saved":false,"id":"n2rde63","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Freonr2","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2oqhee","score":3,"author_fullname":"t2_8xi6x","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Epyc (Milan) 7C13 in particular looks fairly attractive and they're not terribly expensive.  It appears to be a 7713 (64c 8ccd) equivalent oem sku.\\n\\nIndeed it seems TR Pro is just not priced well right now compared to Epyc Rome/Milan.\\n\\n9004 would be nice to jump to 12ch DDR5 but the relevant CPUs are all crazy expensive.  :(","edited":false,"author_flair_css_class":null,"name":"t1_n2rde63","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Epyc (Milan) 7C13 in particular looks fairly attractive and they&amp;#39;re not terribly expensive.  It appears to be a 7713 (64c 8ccd) equivalent oem sku.&lt;/p&gt;\\n\\n&lt;p&gt;Indeed it seems TR Pro is just not priced well right now compared to Epyc Rome/Milan.&lt;/p&gt;\\n\\n&lt;p&gt;9004 would be nice to jump to 12ch DDR5 but the relevant CPUs are all crazy expensive.  :(&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lxr5s3","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxr5s3/kimi_k2_q4km_is_here_and_also_the_instructions_to/n2rde63/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752341489,"author_flair_text":null,"collapsed":false,"created_utc":1752341489,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2t22po","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"FullstackSensei","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2szw88","score":0,"author_fullname":"t2_17n3nqtj56","approved_by":null,"mod_note":null,"all_awardings":[],"body":"I don't know. It depends on where you live, how savvy you are in searching, how good your negotiating skills are, how much effort and time you're willing to put into this, and the motherboard/server/platform you can put them into.","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n2t22po","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I don&amp;#39;t know. It depends on where you live, how savvy you are in searching, how good your negotiating skills are, how much effort and time you&amp;#39;re willing to put into this, and the motherboard/server/platform you can put them into.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lxr5s3","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxr5s3/kimi_k2_q4km_is_here_and_also_the_instructions_to/n2t22po/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752361245,"author_flair_text":null,"treatment_tags":[],"created_utc":1752361245,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}},"user_reports":[],"saved":false,"id":"n2szw88","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Highwaytothebeach","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2oqhee","score":1,"author_fullname":"t2_1qychuraq9","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"OK. How much these days it would be 512 - 768 GB ECC RDIMM DDR4-3200  and 48-64 core Epyc ?","edited":false,"author_flair_css_class":null,"name":"t1_n2szw88","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;OK. How much these days it would be 512 - 768 GB ECC RDIMM DDR4-3200  and 48-64 core Epyc ?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lxr5s3","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxr5s3/kimi_k2_q4km_is_here_and_also_the_instructions_to/n2szw88/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752360487,"author_flair_text":null,"collapsed":false,"created_utc":1752360487,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2oqhee","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"FullstackSensei","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2ofths","score":10,"author_fullname":"t2_17n3nqtj56","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I wouldn't say cheap TR. Desktop DDR4 is still somewhat expensive and you'll need a high core count TR to get anywhere near decent performance. Zen 2 based Epyc Rome, OTOH, will give you the same performance at a cheaper price. ECC RDIMM DDR4-3200 is about half the price as unbufffered memory and 48-64 core Epyc cost less than the equivalent TR. You really need the CPU to have 256MB L3 cache to have all 8 CCDs populated in order to get maximum memory bandwidth.","edited":1752346294,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2oqhee","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I wouldn&amp;#39;t say cheap TR. Desktop DDR4 is still somewhat expensive and you&amp;#39;ll need a high core count TR to get anywhere near decent performance. Zen 2 based Epyc Rome, OTOH, will give you the same performance at a cheaper price. ECC RDIMM DDR4-3200 is about half the price as unbufffered memory and 48-64 core Epyc cost less than the equivalent TR. You really need the CPU to have 256MB L3 cache to have all 8 CCDs populated in order to get maximum memory bandwidth.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lxr5s3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxr5s3/kimi_k2_q4km_is_here_and_also_the_instructions_to/n2oqhee/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752301382,"author_flair_text":null,"treatment_tags":[],"created_utc":1752301382,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":10}}],"before":null}},"user_reports":[],"saved":false,"id":"n2ofths","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"reacusn","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2oeoll","score":6,"author_fullname":"t2_1ppg6hcqm8","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Oh, okay, so 8 channels of ddr5 at about 4000mhz?\\nI guess a cheap zen 2 threadripper pro system with 3200 ddr4 and a used 3090 could probably do a bit more than 5tps.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2ofths","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Oh, okay, so 8 channels of ddr5 at about 4000mhz?\\nI guess a cheap zen 2 threadripper pro system with 3200 ddr4 and a used 3090 could probably do a bit more than 5tps.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lxr5s3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxr5s3/kimi_k2_q4km_is_here_and_also_the_instructions_to/n2ofths/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752295891,"author_flair_text":null,"treatment_tags":[],"created_utc":1752295891,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}}],"before":null}},"user_reports":[],"saved":false,"id":"n2oeoll","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"CombinationNo780","can_mod_post":false,"created_utc":1752295358,"send_replies":true,"parent_id":"t1_n2od184","score":30,"author_fullname":"t2_k4k4yxry","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Sorry for typo. It is 600GB DRAM (Xeon 4) and abut 14GB VRAM (4090)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2oeoll","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Sorry for typo. It is 600GB DRAM (Xeon 4) and abut 14GB VRAM (4090)&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lxr5s3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxr5s3/kimi_k2_q4km_is_here_and_also_the_instructions_to/n2oeoll/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752295358,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":30}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2og3w5","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"reacusn","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2ofqvd","score":1,"author_fullname":"t2_1ppg6hcqm8","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"https://github.com/kvcache-ai/ktransformers/blob/main/doc/en/DeepseekR1_V3_tutorial.md  \\nThis one here?","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2og3w5","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;&lt;a href=\\"https://github.com/kvcache-ai/ktransformers/blob/main/doc/en/DeepseekR1_V3_tutorial.md\\"&gt;https://github.com/kvcache-ai/ktransformers/blob/main/doc/en/DeepseekR1_V3_tutorial.md&lt;/a&gt;&lt;br/&gt;\\nThis one here?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lxr5s3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxr5s3/kimi_k2_q4km_is_here_and_also_the_instructions_to/n2og3w5/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752296026,"author_flair_text":null,"treatment_tags":[],"created_utc":1752296026,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2ofqvd","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"eloquentemu","can_mod_post":false,"created_utc":1752295856,"send_replies":true,"parent_id":"t1_n2od184","score":6,"author_fullname":"t2_lpdsy","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"While a good question, their Deepseek docs lists:\\n\\n&gt; CPU: Intel (R) Xeon (R) Gold 6454S 1T DRAM (2 NUMA nodes)\\n&gt; GPU: 4090D 24G VRAM\\n&gt; Memory: standard DDR5-4800 server DRAM (1 TB), each socket with 8×DDR5-4800\\n\\nSo probably that and the numbers check out.  With 32B active parameters vs Deepseek's 37B, you can expect it to be slightly faster than Deepseek in TG, if you've tested that before.  It does have half the attention heads, so the context might use less memory and the required compute should be less (important for PP at least) though IDK how significant those effects will be.","edited":1752297620,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2ofqvd","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;While a good question, their Deepseek docs lists:&lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;CPU: Intel (R) Xeon (R) Gold 6454S 1T DRAM (2 NUMA nodes)\\nGPU: 4090D 24G VRAM\\nMemory: standard DDR5-4800 server DRAM (1 TB), each socket with 8×DDR5-4800&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;So probably that and the numbers check out.  With 32B active parameters vs Deepseek&amp;#39;s 37B, you can expect it to be slightly faster than Deepseek in TG, if you&amp;#39;ve tested that before.  It does have half the attention heads, so the context might use less memory and the required compute should be less (important for PP at least) though IDK how significant those effects will be.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lxr5s3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxr5s3/kimi_k2_q4km_is_here_and_also_the_instructions_to/n2ofqvd/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752295856,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"d2642412-d9ce-11ed-ae30-32b11309f5bd","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2u5utn","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"bene_42069","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2sk6jg","score":2,"author_fullname":"t2_9yo3ah1u","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"512","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2u5utn","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;512&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lxr5s3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxr5s3/kimi_k2_q4km_is_here_and_also_the_instructions_to/n2u5utn/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752376133,"author_flair_text":null,"treatment_tags":[],"created_utc":1752376133,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n2sk6jg","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ortegaalfredo","can_mod_post":false,"created_utc":1752355109,"send_replies":true,"parent_id":"t1_n2od184","score":1,"author_fullname":"t2_g177e","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"\\\\&gt;  What consumer-grade gpu has 600gb of vram?\\n\\nMac studio","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2sk6jg","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"Alpaca"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;&amp;gt;  What consumer-grade gpu has 600gb of vram?&lt;/p&gt;\\n\\n&lt;p&gt;Mac studio&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lxr5s3","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxr5s3/kimi_k2_q4km_is_here_and_also_the_instructions_to/n2sk6jg/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752355109,"author_flair_text":"Alpaca","treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":"#bd9e9e","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2od184","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"reacusn","can_mod_post":false,"created_utc":1752294602,"send_replies":true,"parent_id":"t3_1lxr5s3","score":21,"author_fullname":"t2_1ppg6hcqm8","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"&gt;We are very pleased to announce that Ktransformers now supports Kimi-K2.\\n\\n&gt;On a single-socket CPU with one consumer-grade GPU, running the Q4_K_M model yields roughly 10 TPS and requires about 600 GB of VRAM.\\n&gt;With a dual-socket CPU and sufficient system memory, enabling NUMA optimizations increases performance to about 14 TPS.\\n\\n... What cpu? What gpu? What consumer-grade gpu has 600gb of vram? Do they mean just memory in general?\\n\\nFor example, are these speeds achievable natty on a xeon 3204 with 2133mhz ram?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2od184","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;We are very pleased to announce that Ktransformers now supports Kimi-K2.&lt;/p&gt;\\n\\n&lt;p&gt;On a single-socket CPU with one consumer-grade GPU, running the Q4_K_M model yields roughly 10 TPS and requires about 600 GB of VRAM.\\nWith a dual-socket CPU and sufficient system memory, enabling NUMA optimizations increases performance to about 14 TPS.&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;... What cpu? What gpu? What consumer-grade gpu has 600gb of vram? Do they mean just memory in general?&lt;/p&gt;\\n\\n&lt;p&gt;For example, are these speeds achievable natty on a xeon 3204 with 2133mhz ram?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxr5s3/kimi_k2_q4km_is_here_and_also_the_instructions_to/n2od184/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752294602,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lxr5s3","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":21}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2or6f0","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Glittering-Call8746","can_mod_post":false,"created_utc":1752301768,"send_replies":true,"parent_id":"t3_1lxr5s3","score":4,"author_fullname":"t2_tqwl6sawb","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Anyone has it working on ddr4 512gb ram. Update this thread","edited":1752380415,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2or6f0","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Anyone has it working on ddr4 512gb ram. Update this thread&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxr5s3/kimi_k2_q4km_is_here_and_also_the_instructions_to/n2or6f0/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752301768,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lxr5s3","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2p7s00","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Baldur-Norddahl","can_mod_post":false,"created_utc":1752311471,"send_replies":true,"parent_id":"t3_1lxr5s3","score":5,"author_fullname":"t2_bvqb8ng0","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"\\\\&gt; 10tps for single-socket CPU and one 4090, 14tps if you have two.\\n\\nWhat CPU exactly is that? Are we maxing out memory bandwidth here?\\n\\nAMD EPYC 9175F has an advertised memory bandwidth of 576 GB/s. Theoretical max at q4 would be 36 tps. More if you have two.\\n\\nWhile not exactly a consumer CPU, it could be very interesting if it was possible to build a 10k USD server that could deliver tps in that range.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2p7s00","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;&amp;gt; 10tps for single-socket CPU and one 4090, 14tps if you have two.&lt;/p&gt;\\n\\n&lt;p&gt;What CPU exactly is that? Are we maxing out memory bandwidth here?&lt;/p&gt;\\n\\n&lt;p&gt;AMD EPYC 9175F has an advertised memory bandwidth of 576 GB/s. Theoretical max at q4 would be 36 tps. More if you have two.&lt;/p&gt;\\n\\n&lt;p&gt;While not exactly a consumer CPU, it could be very interesting if it was possible to build a 10k USD server that could deliver tps in that range.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxr5s3/kimi_k2_q4km_is_here_and_also_the_instructions_to/n2p7s00/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752311471,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lxr5s3","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2ppz9o","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"a_beautiful_rhind","can_mod_post":false,"created_utc":1752321377,"send_replies":true,"parent_id":"t3_1lxr5s3","score":2,"author_fullname":"t2_h5utwre7","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"10-14 if you have the latest *intel* CPUs.. I probably get 6-9 at best and have to run Q1 or Q2.  \\n\\nThey should give us a week of it on openrouter.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2ppz9o","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;10-14 if you have the latest &lt;em&gt;intel&lt;/em&gt; CPUs.. I probably get 6-9 at best and have to run Q1 or Q2.  &lt;/p&gt;\\n\\n&lt;p&gt;They should give us a week of it on openrouter.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxr5s3/kimi_k2_q4km_is_here_and_also_the_instructions_to/n2ppz9o/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752321377,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lxr5s3","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"2b12e2b8-fdc0-11ee-9a03-6e2f48afd456","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2ufymo","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Glittering-Call8746","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2twavm","score":1,"author_fullname":"t2_tqwl6sawb","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"It's the bandwith.. consumer motherboard is dual channel only","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2ufymo","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It&amp;#39;s the bandwith.. consumer motherboard is dual channel only&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lxr5s3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxr5s3/kimi_k2_q4km_is_here_and_also_the_instructions_to/n2ufymo/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752380468,"author_flair_text":null,"treatment_tags":[],"created_utc":1752380468,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2twavm","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"xXWarMachineRoXx","can_mod_post":false,"created_utc":1752372353,"send_replies":true,"parent_id":"t1_n2p7ubg","score":1,"author_fullname":"t2_6zhl6n94","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Is xeon better than like 14900kf ?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2twavm","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"Llama 3"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Is xeon better than like 14900kf ?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lxr5s3","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxr5s3/kimi_k2_q4km_is_here_and_also_the_instructions_to/n2twavm/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752372353,"author_flair_text":"Llama 3","treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":"#c7b594","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2p7ubg","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Glittering-Call8746","can_mod_post":false,"created_utc":1752311511,"send_replies":true,"parent_id":"t3_1lxr5s3","score":1,"author_fullname":"t2_tqwl6sawb","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"They using xeon 4 if I'm not wrong","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2p7ubg","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;They using xeon 4 if I&amp;#39;m not wrong&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxr5s3/kimi_k2_q4km_is_here_and_also_the_instructions_to/n2p7ubg/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752311511,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lxr5s3","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2qmzlz","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"pigeon57434","can_mod_post":false,"created_utc":1752333339,"send_replies":true,"parent_id":"t3_1lxr5s3","score":1,"author_fullname":"t2_8j5t7yjq","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"someone should make a quant of it using that quant method that Reka published a few days ago they say Q3 with 0 quality loss","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2qmzlz","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;someone should make a quant of it using that quant method that Reka published a few days ago they say Q3 with 0 quality loss&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxr5s3/kimi_k2_q4km_is_here_and_also_the_instructions_to/n2qmzlz/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752333339,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lxr5s3","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2s6td8","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Sorry_Ad191","can_mod_post":false,"created_utc":1752350810,"send_replies":true,"parent_id":"t3_1lxr5s3","score":1,"author_fullname":"t2_1rig07ocmc","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Does Ktranformers work with 4 node Xeon v4? Like a HPE DL 580 gen9? How would I compile and run it together with various gpus in the mix too?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2s6td8","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Does Ktranformers work with 4 node Xeon v4? Like a HPE DL 580 gen9? How would I compile and run it together with various gpus in the mix too?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxr5s3/kimi_k2_q4km_is_here_and_also_the_instructions_to/n2s6td8/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752350810,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lxr5s3","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2vkskw","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Voxandr","can_mod_post":false,"created_utc":1752402934,"send_replies":true,"parent_id":"t3_1lxr5s3","score":2,"author_fullname":"t2_86dk0gye","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Just 600GB Ram......","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2vkskw","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Just 600GB Ram......&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxr5s3/kimi_k2_q4km_is_here_and_also_the_instructions_to/n2vkskw/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752402934,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lxr5s3","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2vlsci","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Glittering-Call8746","can_mod_post":false,"created_utc":1752403484,"send_replies":true,"parent_id":"t3_1lxr5s3","score":1,"author_fullname":"t2_tqwl6sawb","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Pity 600gb such at wierd number with 64gb dimms. 9.375 slots..","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2vlsci","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Pity 600gb such at wierd number with 64gb dimms. 9.375 slots..&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxr5s3/kimi_k2_q4km_is_here_and_also_the_instructions_to/n2vlsci/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752403484,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lxr5s3","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2vswb2","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Few-Yam9901","can_mod_post":false,"created_utc":1752407097,"send_replies":true,"parent_id":"t3_1lxr5s3","score":1,"author_fullname":"t2_1rhlf3bcfk","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I don’t understand how to install it with 4 CPUs and 128gb on each cpu? or 256gb on each cpu is also possible for total tb. The instructions only have 1 or 2 cpu? For those who have two cpu and 1T RAM:","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2vswb2","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I don’t understand how to install it with 4 CPUs and 128gb on each cpu? or 256gb on each cpu is also possible for total tb. The instructions only have 1 or 2 cpu? For those who have two cpu and 1T RAM:&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxr5s3/kimi_k2_q4km_is_here_and_also_the_instructions_to/n2vswb2/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752407097,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lxr5s3","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),s=()=>e.jsx(l,{data:t});export{s as default};
