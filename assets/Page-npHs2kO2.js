import{j as e}from"./index-Cs1fA_b_.js";import{R as t}from"./RedditPostRenderer-CcGVkzgc.js";import"./index-DCw-JEtz.js";const l=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"I want to work on banking related data like customer phone call conversations , emails, chat conversations etc., to build a banking product. But these are generally not available due to privacy and security issues. Now, I want to generate these type of real world text data from some structured finance related datasets using AWS Bedrock. \\n\\nAny previous experience or suggestions to consider while generating this using LLMs!!","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Generating real world type conversations from structured data","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lm04jn","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.67,"author_flair_background_color":null,"subreddit_type":"public","ups":1,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_12ylk3ot2j","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":1,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1751046081,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;I want to work on banking related data like customer phone call conversations , emails, chat conversations etc., to build a banking product. But these are generally not available due to privacy and security issues. Now, I want to generate these type of real world text data from some structured finance related datasets using AWS Bedrock. &lt;/p&gt;\\n\\n&lt;p&gt;Any previous experience or suggestions to consider while generating this using LLMs!!&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":true,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1lm04jn","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"ThomasSparrow0511","discussion_type":null,"num_comments":3,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lm04jn/generating_real_world_type_conversations_from/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lm04jn/generating_real_world_type_conversations_from/","subreddit_subscribers":492234,"created_utc":1751046081,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n06sqcn","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ThomasSparrow0511","can_mod_post":false,"created_utc":1751084223,"send_replies":true,"parent_id":"t1_n04b4lb","score":1,"author_fullname":"t2_12ylk3ot2j","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Noted. Thank you!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n06sqcn","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Noted. Thank you!&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lm04jn","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lm04jn/generating_real_world_type_conversations_from/n06sqcn/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751084223,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n04b4lb","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"j0holo","can_mod_post":false,"created_utc":1751052417,"send_replies":true,"parent_id":"t3_1lm04jn","score":1,"author_fullname":"t2_hodrk","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I created product data via ollama. From experimenting with models, quants and prompts I had the best result with Qwen2.5 Q4 (thinking models really don't help you here) and generating 20 products at a time.\\n\\nOllama has the option to give Pydantic model to the LLM which helps with structured output. Be specific in you prompt. For example you say phone calls, emails, chats. Those are all different prompts or at least types of conversations.\\n\\nI also faced the issue that Ollama struggles with lots of calls (yet one at a time) which forced me to create a Ollama client in python after every call and a fair amount of try/except code.\\n\\nvLLM will work better and may also perform better because you can batch requests so the GPU works on multiple requests at a time making use of all resources.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n04b4lb","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I created product data via ollama. From experimenting with models, quants and prompts I had the best result with Qwen2.5 Q4 (thinking models really don&amp;#39;t help you here) and generating 20 products at a time.&lt;/p&gt;\\n\\n&lt;p&gt;Ollama has the option to give Pydantic model to the LLM which helps with structured output. Be specific in you prompt. For example you say phone calls, emails, chats. Those are all different prompts or at least types of conversations.&lt;/p&gt;\\n\\n&lt;p&gt;I also faced the issue that Ollama struggles with lots of calls (yet one at a time) which forced me to create a Ollama client in python after every call and a fair amount of try/except code.&lt;/p&gt;\\n\\n&lt;p&gt;vLLM will work better and may also perform better because you can batch requests so the GPU works on multiple requests at a time making use of all resources.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lm04jn/generating_real_world_type_conversations_from/n04b4lb/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751052417,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lm04jn","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),s=()=>e.jsx(t,{data:l});export{s as default};
