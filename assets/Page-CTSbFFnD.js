import{j as e}from"./index-CNyNkRpk.js";import{R as l}from"./RedditPostRenderer-Dza0u9i2.js";import"./index-BUchu_-K.js";const a=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Hey everyone! Just wanted to share some thoughts on my experience with the new Kimi K2 model.\\n\\nEver since Unsloth released their quantized version of Kimi K2 yesterday, I‚Äôve been giving it a real workout. I‚Äôve mostly been pairing it with Roo Code, and honestly‚Ä¶ I‚Äôm blown away.\\n\\nBack in March, I built myself a server mainly for coding experiments and to mess around with all sorts of models and setups (definitely not to save money‚Äîlet‚Äôs be real, using the Claude API probably would have been cheaper). But this became a hobby, and I wanted to really get into it.\\n\\nUp until now, I‚Äôve tried DeepSeek V3, R1, R1 0528‚Äîyou name it. Nothing comes close to what I‚Äôm seeing with Kimi K2 today. Usually, my server was just for quick bug fixes that didn‚Äôt need much context. For anything big or complex, I‚Äôd have to use Claude.\\n\\nBut now that‚Äôs changed. Kimi K2 is handling everything I throw at it, even big, complicated tasks. For example, it‚Äôs making changes to a C++ firmware project‚Äî*deep* into a 90,000-token context‚Äîand it‚Äôs nailing the search and replace stuff in Roo Code without getting lost or mixing things up.\\n\\nJust wanted to share my excitement! Huge thanks to the folks at Moonshot AI for releasing this, and big shoutout to Unsloth and Ik\\\\_llama. Seriously, none of this would be possible without you all. You‚Äôre the real MVPs.\\n\\nIf you‚Äôre curious about my setup: I‚Äôm running this on a dual EPYC 7532 server, 512GB of DDR4 RAM (overclocked a bit), and three RTX 3090s.","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Kimi has impressive coding performance! Even deep into context usage.","link_flair_richtext":[{"e":"text","t":"Discussion"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1m0lyjn","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.95,"author_flair_background_color":null,"subreddit_type":"public","ups":133,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_1c88dc0z","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Discussion","can_mod_post":false,"score":133,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1752595918,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Hey everyone! Just wanted to share some thoughts on my experience with the new Kimi K2 model.&lt;/p&gt;\\n\\n&lt;p&gt;Ever since Unsloth released their quantized version of Kimi K2 yesterday, I‚Äôve been giving it a real workout. I‚Äôve mostly been pairing it with Roo Code, and honestly‚Ä¶ I‚Äôm blown away.&lt;/p&gt;\\n\\n&lt;p&gt;Back in March, I built myself a server mainly for coding experiments and to mess around with all sorts of models and setups (definitely not to save money‚Äîlet‚Äôs be real, using the Claude API probably would have been cheaper). But this became a hobby, and I wanted to really get into it.&lt;/p&gt;\\n\\n&lt;p&gt;Up until now, I‚Äôve tried DeepSeek V3, R1, R1 0528‚Äîyou name it. Nothing comes close to what I‚Äôm seeing with Kimi K2 today. Usually, my server was just for quick bug fixes that didn‚Äôt need much context. For anything big or complex, I‚Äôd have to use Claude.&lt;/p&gt;\\n\\n&lt;p&gt;But now that‚Äôs changed. Kimi K2 is handling everything I throw at it, even big, complicated tasks. For example, it‚Äôs making changes to a C++ firmware project‚Äî&lt;em&gt;deep&lt;/em&gt; into a 90,000-token context‚Äîand it‚Äôs nailing the search and replace stuff in Roo Code without getting lost or mixing things up.&lt;/p&gt;\\n\\n&lt;p&gt;Just wanted to share my excitement! Huge thanks to the folks at Moonshot AI for releasing this, and big shoutout to Unsloth and Ik_llama. Seriously, none of this would be possible without you all. You‚Äôre the real MVPs.&lt;/p&gt;\\n\\n&lt;p&gt;If you‚Äôre curious about my setup: I‚Äôm running this on a dual EPYC 7532 server, 512GB of DDR4 RAM (overclocked a bit), and three RTX 3090s.&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"5f921ea4-c7bc-11ed-9c23-3a00622979b4","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#646d73","id":"1m0lyjn","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"mattescala","discussion_type":null,"num_comments":51,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1m0lyjn/kimi_has_impressive_coding_performance_even_deep/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1m0lyjn/kimi_has_impressive_coding_performance_even_deep/","subreddit_subscribers":499773,"created_utc":1752595918,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3adn7y","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"plankalkul-z1","can_mod_post":false,"created_utc":1752597363,"send_replies":true,"parent_id":"t1_n3a8pw7","score":11,"author_fullname":"t2_w73n3yrsx","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"&gt; these are my ik_llama parameters:\\n\\n\\nThank you for the write-up, and for all the details that you provided.\\n\\n\\n\\nOne other thing I'd like to know is what tps are you getting, *especially* as your (pretty massive) context window fills up?\\n\\n\\nEDIT: I see that you already answered it in another message while I was typing this... So, never mind...","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3adn7y","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;these are my ik_llama parameters:&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;Thank you for the write-up, and for all the details that you provided.&lt;/p&gt;\\n\\n&lt;p&gt;One other thing I&amp;#39;d like to know is what tps are you getting, &lt;em&gt;especially&lt;/em&gt; as your (pretty massive) context window fills up?&lt;/p&gt;\\n\\n&lt;p&gt;EDIT: I see that you already answered it in another message while I was typing this... So, never mind...&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0lyjn","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0lyjn/kimi_has_impressive_coding_performance_even_deep/n3adn7y/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752597363,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":11}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3amp86","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"cantgetthistowork","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3aja6k","score":2,"author_fullname":"t2_j1i0o","approved_by":null,"mod_note":null,"all_awardings":[],"body":"Our setups are extremely similar then. How did you pick 3 and 4 as the ones to offload? Asking because I have a couple more 3090s and would like to know how to decide what else to offload","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n3amp86","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Our setups are extremely similar then. How did you pick 3 and 4 as the ones to offload? Asking because I have a couple more 3090s and would like to know how to decide what else to offload&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1m0lyjn","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0lyjn/kimi_has_impressive_coding_performance_even_deep/n3amp86/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752599804,"author_flair_text":null,"treatment_tags":[],"created_utc":1752599804,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n3aja6k","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"mattescala","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3aiv30","score":4,"author_fullname":"t2_1c88dc0z","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Numa settings are there in the ik_llama command, with dual socket its important to interleave the memory and distribute the load. Thats basically it. Cant speak for Intel processors but Amd makes it quite painless to handle numa. Bare in consideration that i run the process inside an lxc in proxmox with full numa passthrough.","edited":false,"author_flair_css_class":null,"name":"t1_n3aja6k","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Numa settings are there in the ik_llama command, with dual socket its important to interleave the memory and distribute the load. Thats basically it. Cant speak for Intel processors but Amd makes it quite painless to handle numa. Bare in consideration that i run the process inside an lxc in proxmox with full numa passthrough.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1m0lyjn","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0lyjn/kimi_has_impressive_coding_performance_even_deep/n3aja6k/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752598883,"author_flair_text":null,"collapsed":false,"created_utc":1752598883,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}}],"before":null}},"user_reports":[],"saved":false,"id":"n3aiv30","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"cantgetthistowork","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3ahgxf","score":1,"author_fullname":"t2_j1i0o","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"How did you pick which layers to offload to GPU? What about NUMA settings? Asking because my dual 7282s are terrible the moment I do CPU offload","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3aiv30","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;How did you pick which layers to offload to GPU? What about NUMA settings? Asking because my dual 7282s are terrible the moment I do CPU offload&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0lyjn","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0lyjn/kimi_has_impressive_coding_performance_even_deep/n3aiv30/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752598771,"author_flair_text":null,"treatment_tags":[],"created_utc":1752598771,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3enua0","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"cantgetthistowork","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3ahgxf","score":1,"author_fullname":"t2_j1i0o","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I tried your settings and it attempts to load 130GB onto CUDA0 for some reason","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3enua0","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I tried your settings and it attempts to load 130GB onto CUDA0 for some reason&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0lyjn","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0lyjn/kimi_has_impressive_coding_performance_even_deep/n3enua0/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752650431,"author_flair_text":null,"treatment_tags":[],"created_utc":1752650431,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3ahgxf","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"mattescala","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3ah2pj","score":8,"author_fullname":"t2_1c88dc0z","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Because cuda 0 gets fully mostly with kvcache and the massive -u -ub size.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3ahgxf","is_submitter":true,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Because cuda 0 gets fully mostly with kvcache and the massive -u -ub size.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0lyjn","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0lyjn/kimi_has_impressive_coding_performance_even_deep/n3ahgxf/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752598400,"author_flair_text":null,"treatment_tags":[],"created_utc":1752598400,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":8}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3aisga","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"mattescala","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3ah2pj","score":2,"author_fullname":"t2_1c88dc0z","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Regarding the numa. I tried the famous nps0 setting, but to be honest, i dont get it. Its much slower and much less stable. One single numa node per socket with all the numa optimizations is the way to go imo.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3aisga","is_submitter":true,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Regarding the numa. I tried the famous nps0 setting, but to be honest, i dont get it. Its much slower and much less stable. One single numa node per socket with all the numa optimizations is the way to go imo.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0lyjn","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0lyjn/kimi_has_impressive_coding_performance_even_deep/n3aisga/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752598752,"author_flair_text":null,"treatment_tags":[],"created_utc":1752598752,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n3ah2pj","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"cantgetthistowork","can_mod_post":false,"created_utc":1752598295,"send_replies":true,"parent_id":"t1_n3a8pw7","score":3,"author_fullname":"t2_j1i0o","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Can I ask why you're only offloading to CUDA1 and CUDA2 when you have 3x3090s?\\n\\nAlso do you have any other settings BIOS/OS to handle the NUMA penalty?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3ah2pj","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Can I ask why you&amp;#39;re only offloading to CUDA1 and CUDA2 when you have 3x3090s?&lt;/p&gt;\\n\\n&lt;p&gt;Also do you have any other settings BIOS/OS to handle the NUMA penalty?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0lyjn","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0lyjn/kimi_has_impressive_coding_performance_even_deep/n3ah2pj/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752598295,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"d40ca12a-0e73-11ee-8563-f216e082168e","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3eb9px","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"yoracale","can_mod_post":false,"created_utc":1752643726,"send_replies":true,"parent_id":"t1_n3a8pw7","score":1,"author_fullname":"t2_1162lx9rgr","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Glad this was useful u/mattescala! ü•≥üôè","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3eb9px","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"Llama 2"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Glad this was useful &lt;a href=\\"/u/mattescala\\"&gt;u/mattescala&lt;/a&gt;! ü•≥üôè&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0lyjn","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0lyjn/kimi_has_impressive_coding_performance_even_deep/n3eb9px/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752643726,"author_flair_text":"Llama 2","treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":"#ab96c2","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3a8pw7","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"mattescala","can_mod_post":false,"created_utc":1752595998,"send_replies":true,"parent_id":"t3_1m0lyjn","score":39,"author_fullname":"t2_1c88dc0z","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"For anyone wondering these are my ik\\\\_llama parameters:\\n\\n    numactl --interleave=all ~/ik_llama.cpp/build/bin/llama-server \\\\\\n        --model ~/models/unsloth/Kimi-K2-Instruct-GGUF-UD-Q2_K_XL/Kimi-K2-Instruct-UD-Q2_K_XL-00001-of-00008.gguf \\\\\\n        --numa distribute \\\\\\n        --alias Kimi-K2-1T \\\\\\n        --threads 86 \\\\\\n        --cache-type-k q8_0 \\\\\\n        --cache-type-v q8_0 \\\\\\n        --temp 0.6 \\\\\\n        --ctx-size 131072 \\\\\\n        --prompt-cache \\\\\\n        --parallel=3 \\\\\\n        --metrics \\\\\\n        --n-gpu-layers 99 \\\\\\n        -ot \\"blk.(3).ffn.=CUDA1\\" \\\\\\n        -ot \\"blk.(4).ffn.=CUDA2\\" \\\\\\n        -ot \\".ffn_.*_exps.=CPU\\" \\\\\\n        -mla 3 -fa -fmoe \\\\\\n        -ub 10240 -b 10240 \\\\\\n        -amb 512 \\\\\\n        --host 0.0.0.0 \\\\\\n        --port 8080 \\\\\\n        -cb \\\\\\n        -v","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3a8pw7","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;For anyone wondering these are my ik_llama parameters:&lt;/p&gt;\\n\\n&lt;pre&gt;&lt;code&gt;numactl --interleave=all ~/ik_llama.cpp/build/bin/llama-server \\\\\\n    --model ~/models/unsloth/Kimi-K2-Instruct-GGUF-UD-Q2_K_XL/Kimi-K2-Instruct-UD-Q2_K_XL-00001-of-00008.gguf \\\\\\n    --numa distribute \\\\\\n    --alias Kimi-K2-1T \\\\\\n    --threads 86 \\\\\\n    --cache-type-k q8_0 \\\\\\n    --cache-type-v q8_0 \\\\\\n    --temp 0.6 \\\\\\n    --ctx-size 131072 \\\\\\n    --prompt-cache \\\\\\n    --parallel=3 \\\\\\n    --metrics \\\\\\n    --n-gpu-layers 99 \\\\\\n    -ot &amp;quot;blk.(3).ffn.=CUDA1&amp;quot; \\\\\\n    -ot &amp;quot;blk.(4).ffn.=CUDA2&amp;quot; \\\\\\n    -ot &amp;quot;.ffn_.*_exps.=CPU&amp;quot; \\\\\\n    -mla 3 -fa -fmoe \\\\\\n    -ub 10240 -b 10240 \\\\\\n    -amb 512 \\\\\\n    --host 0.0.0.0 \\\\\\n    --port 8080 \\\\\\n    -cb \\\\\\n    -v\\n&lt;/code&gt;&lt;/pre&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0lyjn/kimi_has_impressive_coding_performance_even_deep/n3a8pw7/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752595998,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m0lyjn","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":39}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3ah3bt","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"tomz17","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3acdbc","score":13,"author_fullname":"t2_1mhx5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"For comparison on a 9684x DDR5 12/channel @ 4800 + 1x 3090  (out of two in the system) I was getting around 18t/s generation on the same model in llama.cpp.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3ah3bt","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;For comparison on a 9684x DDR5 12/channel @ 4800 + 1x 3090  (out of two in the system) I was getting around 18t/s generation on the same model in llama.cpp.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0lyjn","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0lyjn/kimi_has_impressive_coding_performance_even_deep/n3ah3bt/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752598300,"author_flair_text":null,"treatment_tags":[],"created_utc":1752598300,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":13}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3acxtd","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"daaain","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3acdbc","score":3,"author_fullname":"t2_47j85","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Right, so context engineering is pretty important if you don't want to wait hours!","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3acxtd","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Right, so context engineering is pretty important if you don&amp;#39;t want to wait hours!&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0lyjn","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0lyjn/kimi_has_impressive_coding_performance_even_deep/n3acxtd/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752597167,"author_flair_text":null,"treatment_tags":[],"created_utc":1752597167,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n3acdbc","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"mattescala","can_mod_post":false,"created_utc":1752597008,"send_replies":true,"parent_id":"t1_n3abm43","score":14,"author_fullname":"t2_1c88dc0z","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Its something i would have to test for different context sizes. For 128k i get 7tks in generation and 144 in processing.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3acdbc","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Its something i would have to test for different context sizes. For 128k i get 7tks in generation and 144 in processing.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0lyjn","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0lyjn/kimi_has_impressive_coding_performance_even_deep/n3acdbc/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752597008,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":14}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3e5s09","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"lordpuddingcup","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3brite","score":1,"author_fullname":"t2_vc4z2","approved_by":null,"mod_note":null,"all_awardings":[],"body":"yep","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n3e5s09","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;yep&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1m0lyjn","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0lyjn/kimi_has_impressive_coding_performance_even_deep/n3e5s09/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752640869,"author_flair_text":null,"treatment_tags":[],"created_utc":1752640869,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3brite","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"segmond","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3aemrg","score":1,"author_fullname":"t2_ah13x","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"what does it take to run at 8 channel?  do you have to max out all the ram slots?","edited":false,"author_flair_css_class":null,"name":"t1_n3brite","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;what does it take to run at 8 channel?  do you have to max out all the ram slots?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1m0lyjn","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":"light","treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0lyjn/kimi_has_impressive_coding_performance_even_deep/n3brite/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752611156,"author_flair_text":"llama.cpp","collapsed":false,"created_utc":1752611156,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3aemrg","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"mattescala","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3acjgn","score":8,"author_fullname":"t2_1c88dc0z","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Its mostly due to the fact that im running at quad channel instead of eight channel. But I‚Äôve already ordered another 512gb. Ill keep you posted ;)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3aemrg","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Its mostly due to the fact that im running at quad channel instead of eight channel. But I‚Äôve already ordered another 512gb. Ill keep you posted ;)&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0lyjn","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0lyjn/kimi_has_impressive_coding_performance_even_deep/n3aemrg/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752597635,"author_flair_text":null,"treatment_tags":[],"created_utc":1752597635,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":8}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3bofb5","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Forgot_Password_Dude","can_mod_post":false,"created_utc":1752610296,"send_replies":true,"parent_id":"t1_n3bhdze","score":1,"author_fullname":"t2_g8xg6sut","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Dang it I'm 55GB RAM short for the 1.8, so it will be slow üêå.   I'll test lower, and if it's acceptable maybe I'll upgrade my RAM","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3bofb5","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Dang it I&amp;#39;m 55GB RAM short for the 1.8, so it will be slow üêå.   I&amp;#39;ll test lower, and if it&amp;#39;s acceptable maybe I&amp;#39;ll upgrade my RAM&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0lyjn/kimi_has_impressive_coding_performance_even_deep/n3bofb5/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752610296,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m0lyjn","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":9,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3bhdze","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"daaain","can_mod_post":false,"created_utc":1752608313,"send_replies":true,"parent_id":"t1_n3b04pk","score":1,"author_fullname":"t2_47j85","approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yeah, you need the 1.8bit","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n3bhdze","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yeah, you need the 1.8bit&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0lyjn/kimi_has_impressive_coding_performance_even_deep/n3bhdze/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752608313,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m0lyjn","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":8,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3b04pk","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Forgot_Password_Dude","can_mod_post":false,"created_utc":1752603436,"send_replies":true,"parent_id":"t1_n3awxdf","score":5,"author_fullname":"t2_g8xg6sut","approved_by":null,"mod_note":null,"all_awardings":[],"body":"lol the 48 gb files are 1 of 12","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n3b04pk","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;lol the 48 gb files are 1 of 12&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1m0lyjn","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0lyjn/kimi_has_impressive_coding_performance_even_deep/n3b04pk/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752603436,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":7,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}}],"before":null}},"user_reports":[],"saved":false,"id":"n3awxdf","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Forgot_Password_Dude","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3aflre","score":1,"author_fullname":"t2_g8xg6sut","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"its a bit confusing, all of them are under 50GB, i think i can fit any of them, but i'm downloading the 2B quant one now, any question you want me to ask it?  I'll try 4B as well later if 2B is acceptable","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n3awxdf","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;its a bit confusing, all of them are under 50GB, i think i can fit any of them, but i&amp;#39;m downloading the 2B quant one now, any question you want me to ask it?  I&amp;#39;ll try 4B as well later if 2B is acceptable&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1m0lyjn","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0lyjn/kimi_has_impressive_coding_performance_even_deep/n3awxdf/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752602544,"author_flair_text":null,"treatment_tags":[],"created_utc":1752602544,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":6,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3aflre","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"daaain","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3af22s","score":2,"author_fullname":"t2_47j85","approved_by":null,"mod_note":null,"all_awardings":[],"body":"You could try the Unsloth 1.8 that should just about squeak in 256GB: [https://www.reddit.com/r/LocalLLaMA/comments/1lzps3b/kimi\\\\_k2\\\\_18bit\\\\_unsloth\\\\_dynamic\\\\_ggufs/](https://www.reddit.com/r/LocalLLaMA/comments/1lzps3b/kimi_k2_18bit_unsloth_dynamic_ggufs/)","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n3aflre","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You could try the Unsloth 1.8 that should just about squeak in 256GB: &lt;a href=\\"https://www.reddit.com/r/LocalLLaMA/comments/1lzps3b/kimi_k2_18bit_unsloth_dynamic_ggufs/\\"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1lzps3b/kimi_k2_18bit_unsloth_dynamic_ggufs/&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1m0lyjn","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0lyjn/kimi_has_impressive_coding_performance_even_deep/n3aflre/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752597901,"author_flair_text":null,"treatment_tags":[],"created_utc":1752597901,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n3af22s","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Forgot_Password_Dude","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3ae5bn","score":2,"author_fullname":"t2_g8xg6sut","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Nm not enough regular RAM, only 256GB so won't be to run Q2.   If the tok/s is usable (around 15-20), I'll upgrade mt RAM, let's see OP to response","edited":false,"author_flair_css_class":null,"name":"t1_n3af22s","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Nm not enough regular RAM, only 256GB so won&amp;#39;t be to run Q2.   If the tok/s is usable (around 15-20), I&amp;#39;ll upgrade mt RAM, let&amp;#39;s see OP to response&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1m0lyjn","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0lyjn/kimi_has_impressive_coding_performance_even_deep/n3af22s/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752597752,"author_flair_text":null,"collapsed":false,"created_utc":1752597752,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n3ae5bn","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Forgot_Password_Dude","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3acjgn","score":3,"author_fullname":"t2_g8xg6sut","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I have the similar setup with 70 gbvram and 64 cores, I'll download and try it now","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3ae5bn","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I have the similar setup with 70 gbvram and 64 cores, I&amp;#39;ll download and try it now&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0lyjn","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0lyjn/kimi_has_impressive_coding_performance_even_deep/n3ae5bn/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752597502,"author_flair_text":null,"treatment_tags":[],"created_utc":1752597502,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n3acjgn","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"daaain","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3ac73a","score":3,"author_fullname":"t2_47j85","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I was expecting a bit higher with that beefy setup üòÖ is that with a huge context though?\\n\\nEdit: ah, you're not OP just opining","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3acjgn","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I was expecting a bit higher with that beefy setup üòÖ is that with a huge context though?&lt;/p&gt;\\n\\n&lt;p&gt;Edit: ah, you&amp;#39;re not OP just opining&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0lyjn","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0lyjn/kimi_has_impressive_coding_performance_even_deep/n3acjgn/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752597056,"author_flair_text":null,"treatment_tags":[],"created_utc":1752597056,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n3ac73a","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Forgot_Password_Dude","can_mod_post":false,"created_utc":1752596961,"send_replies":true,"parent_id":"t1_n3abm43","score":2,"author_fullname":"t2_g8xg6sut","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Probably 5 tok/s","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3ac73a","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Probably 5 tok/s&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0lyjn","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0lyjn/kimi_has_impressive_coding_performance_even_deep/n3ac73a/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752596961,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n3abm43","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"daaain","can_mod_post":false,"created_utc":1752596801,"send_replies":true,"parent_id":"t3_1m0lyjn","score":8,"author_fullname":"t2_47j85","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"What kind of PP / TP speeds are you getting with different context sizes?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3abm43","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;What kind of PP / TP speeds are you getting with different context sizes?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0lyjn/kimi_has_impressive_coding_performance_even_deep/n3abm43/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752596801,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m0lyjn","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":8}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3b6xs4","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"mattescala","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3b55mj","score":2,"author_fullname":"t2_1c88dc0z","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Its not oc in the common sense. I just set the memory speed to 2666 and it trained no problem! Therefore i kept it. Its definitely #freerealestate lol.\\n\\nRegarding numa, i did all sorts of trials and errors but in the end, when i kept it simple, it gave me the best results. I tried pinning memory to one proc, to psycpubind to specific cores etc etc etc.\\n\\nBtw the motherboard is the famous rome2d-16T, good one id say.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3b6xs4","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Its not oc in the common sense. I just set the memory speed to 2666 and it trained no problem! Therefore i kept it. Its definitely #freerealestate lol.&lt;/p&gt;\\n\\n&lt;p&gt;Regarding numa, i did all sorts of trials and errors but in the end, when i kept it simple, it gave me the best results. I tried pinning memory to one proc, to psycpubind to specific cores etc etc etc.&lt;/p&gt;\\n\\n&lt;p&gt;Btw the motherboard is the famous rome2d-16T, good one id say.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0lyjn","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0lyjn/kimi_has_impressive_coding_performance_even_deep/n3b6xs4/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752605350,"author_flair_text":null,"treatment_tags":[],"created_utc":1752605350,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n3b55mj","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"FullstackSensei","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3b22sn","score":1,"author_fullname":"t2_17n3nqtj56","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Which motherboard are you using that allows you to OC the memory?\\nAbdout the threads, you have 64 cores total, so anything beyond 64 threads means you're using hyoerthreading, which in my experience slows things down.\\n\\nFor numactl, try this:\\nnumactl --physcpubind=$(seq -s, 1 2 XXX)\\nwhere XXX is the number of hyoerthreading cores minus one. In your case should be 127. This binds each thread to the odd numbered cores. You can also do even numbered if you start from zero, but then you should do total cores minus two. I find physcpubind gives me the fastest performance in both single and dual CPU systems. It makes sure each physical core gets a single thread, maximizing execution resources and minimizing cache contention.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3b55mj","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Which motherboard are you using that allows you to OC the memory?\\nAbdout the threads, you have 64 cores total, so anything beyond 64 threads means you&amp;#39;re using hyoerthreading, which in my experience slows things down.&lt;/p&gt;\\n\\n&lt;p&gt;For numactl, try this:\\nnumactl --physcpubind=$(seq -s, 1 2 XXX)\\nwhere XXX is the number of hyoerthreading cores minus one. In your case should be 127. This binds each thread to the odd numbered cores. You can also do even numbered if you start from zero, but then you should do total cores minus two. I find physcpubind gives me the fastest performance in both single and dual CPU systems. It makes sure each physical core gets a single thread, maximizing execution resources and minimizing cache contention.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0lyjn","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0lyjn/kimi_has_impressive_coding_performance_even_deep/n3b55mj/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752604849,"author_flair_text":null,"treatment_tags":[],"created_utc":1752604849,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3b22sn","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"mattescala","can_mod_post":false,"created_utc":1752603982,"send_replies":true,"parent_id":"t1_n3axo6f","score":3,"author_fullname":"t2_1c88dc0z","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Hello there! The memory is currently running at 2666 despite being rated only for 2400. By the end of the week ill get additional 8 modules to run eight channels. Threads are limited because of 2 reasons, first this is running in an lxc in proxmox, so im sharing resources with a few other machines, second im limiting in this way tdp, and since i did not install the second psu yet i want to be on the safe side ;)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3b22sn","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Hello there! The memory is currently running at 2666 despite being rated only for 2400. By the end of the week ill get additional 8 modules to run eight channels. Threads are limited because of 2 reasons, first this is running in an lxc in proxmox, so im sharing resources with a few other machines, second im limiting in this way tdp, and since i did not install the second psu yet i want to be on the safe side ;)&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0lyjn","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0lyjn/kimi_has_impressive_coding_performance_even_deep/n3b22sn/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752603982,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n3axo6f","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"FullstackSensei","can_mod_post":false,"created_utc":1752602750,"send_replies":true,"parent_id":"t3_1m0lyjn","score":7,"author_fullname":"t2_17n3nqtj56","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"7tk/s is quite impressive given your CPUs are running with half the channels only!\\nDo you mind sharing what memory speed are you running at? How did you overclock the memory? And why threads are 86 when you have 2x32 cores?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3axo6f","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;7tk/s is quite impressive given your CPUs are running with half the channels only!\\nDo you mind sharing what memory speed are you running at? How did you overclock the memory? And why threads are 86 when you have 2x32 cores?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0lyjn/kimi_has_impressive_coding_performance_even_deep/n3axo6f/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752602750,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m0lyjn","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":7}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3dq2fe","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"SashaUsesReddit","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3d35vh","score":1,"author_fullname":"t2_57wafqev","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Why am I getting downvoted?\\n\\nI said the model is a lug.. as in its large and difficult to run optimally\\n\\nIm offering millions of dollars of hardware to the community to eval this model and see what we can do. How am I the bad guy?\\n\\nLug is not a negative about the model. Its a reality about serving 1T params in vram in native weights.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3dq2fe","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Why am I getting downvoted?&lt;/p&gt;\\n\\n&lt;p&gt;I said the model is a lug.. as in its large and difficult to run optimally&lt;/p&gt;\\n\\n&lt;p&gt;Im offering millions of dollars of hardware to the community to eval this model and see what we can do. How am I the bad guy?&lt;/p&gt;\\n\\n&lt;p&gt;Lug is not a negative about the model. Its a reality about serving 1T params in vram in native weights.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0lyjn","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0lyjn/kimi_has_impressive_coding_performance_even_deep/n3dq2fe/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752634375,"author_flair_text":null,"treatment_tags":[],"created_utc":1752634375,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3d35vh","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"SashaUsesReddit","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3d288v","score":0,"author_fullname":"t2_57wafqev","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"This model is a lug. Takes me 8x B200 to run it with reduced max context length.\\n\\nI can run it native with full 128k on 8x Mi325, but it feels like a waste haha\\n\\nProb going to leave it on a node of 8x H200 with 40k context for testing","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3d35vh","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;This model is a lug. Takes me 8x B200 to run it with reduced max context length.&lt;/p&gt;\\n\\n&lt;p&gt;I can run it native with full 128k on 8x Mi325, but it feels like a waste haha&lt;/p&gt;\\n\\n&lt;p&gt;Prob going to leave it on a node of 8x H200 with 40k context for testing&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0lyjn","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0lyjn/kimi_has_impressive_coding_performance_even_deep/n3d35vh/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752626247,"author_flair_text":null,"treatment_tags":[],"created_utc":1752626247,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}},"user_reports":[],"saved":false,"id":"n3d288v","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"mattescala","can_mod_post":false,"created_utc":1752625921,"send_replies":true,"parent_id":"t1_n3d18io","score":3,"author_fullname":"t2_1c88dc0z","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yes absolutely, it is also quite interesting to me, i did not know much about ai when i set to build this machine but knowing it better now, i would definitely set on some 4090 for the native FP8 support.\\n\\nRegarding quants, you‚Äôll be surprised, unsloth dynamic quants manage incredible accuracy even at very low quantization. Furthermore, if the quant is an IQuant (done with Imatrix), it can manage to match or even beat higher quality quants accuracy.","edited":1752626520,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3d288v","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yes absolutely, it is also quite interesting to me, i did not know much about ai when i set to build this machine but knowing it better now, i would definitely set on some 4090 for the native FP8 support.&lt;/p&gt;\\n\\n&lt;p&gt;Regarding quants, you‚Äôll be surprised, unsloth dynamic quants manage incredible accuracy even at very low quantization. Furthermore, if the quant is an IQuant (done with Imatrix), it can manage to match or even beat higher quality quants accuracy.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0lyjn","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0lyjn/kimi_has_impressive_coding_performance_even_deep/n3d288v/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752625921,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n3d18io","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"SashaUsesReddit","can_mod_post":false,"created_utc":1752625568,"send_replies":true,"parent_id":"t3_1m0lyjn","score":2,"author_fullname":"t2_57wafqev","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Im really interested in the difference between native FP8 and these quants. Would you be interested in hitting an endpoint of the FP8 on one of my B200 systems and do some comparisons with me?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3d18io","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Im really interested in the difference between native FP8 and these quants. Would you be interested in hitting an endpoint of the FP8 on one of my B200 systems and do some comparisons with me?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0lyjn/kimi_has_impressive_coding_performance_even_deep/n3d18io/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752625568,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m0lyjn","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3cylb6","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"easyrider99","can_mod_post":false,"created_utc":1752624650,"send_replies":true,"parent_id":"t1_n3bwotl","score":1,"author_fullname":"t2_5rfuw15r","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"about to embark on a ik\\\\_llama deep dive. Can you flesh out the commands you use and what your system specs are?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3cylb6","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;about to embark on a ik_llama deep dive. Can you flesh out the commands you use and what your system specs are?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0lyjn","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0lyjn/kimi_has_impressive_coding_performance_even_deep/n3cylb6/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752624650,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3bwotl","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Key-Boat-7519","can_mod_post":false,"created_utc":1752612589,"send_replies":true,"parent_id":"t3_1m0lyjn","score":4,"author_fullname":"t2_uf3n9chep","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Kimi K2 absolutely feels like the first open model that can stand in for Claude on monster codebases. I switched my microservices repo (200k+ tokens once docs are inlined) over last night and it kept track of file relationships without me spoon-feeding path hints. Key was running Unsloth‚Äôs 5-bit weight merging and passing --new-rope 120k to keep the positional heads calm; without that it drifted after \\\\~65k tokens. Swap space matters too: keep CUDALAUNCHBLOCKING off and let vram spill to CPU, but pin the KV cache to hugepages or the 3090s choke. For speed, vLLM‚Äôs paged\\\\_attention outpaced text-generation-webui by about 35 %. I pull snippets via Ripgrep and stream them in chunks so the model sees only edited diffs, which cuts token cost by half. Side note: I‚Äôve tried vLLM and Ollama for routing, but [APIWrapper.ai](http://APIWrapper.ai) is what finally let me share a single long-context endpoint across my whole team‚Äôs CI without extra glue code. Bottom line: K2 is finally the workstation-friendly Claude alternative we wanted.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3bwotl","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Kimi K2 absolutely feels like the first open model that can stand in for Claude on monster codebases. I switched my microservices repo (200k+ tokens once docs are inlined) over last night and it kept track of file relationships without me spoon-feeding path hints. Key was running Unsloth‚Äôs 5-bit weight merging and passing --new-rope 120k to keep the positional heads calm; without that it drifted after ~65k tokens. Swap space matters too: keep CUDALAUNCHBLOCKING off and let vram spill to CPU, but pin the KV cache to hugepages or the 3090s choke. For speed, vLLM‚Äôs paged_attention outpaced text-generation-webui by about 35 %. I pull snippets via Ripgrep and stream them in chunks so the model sees only edited diffs, which cuts token cost by half. Side note: I‚Äôve tried vLLM and Ollama for routing, but &lt;a href=\\"http://APIWrapper.ai\\"&gt;APIWrapper.ai&lt;/a&gt; is what finally let me share a single long-context endpoint across my whole team‚Äôs CI without extra glue code. Bottom line: K2 is finally the workstation-friendly Claude alternative we wanted.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0lyjn/kimi_has_impressive_coding_performance_even_deep/n3bwotl/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752612589,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m0lyjn","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3b291y","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Imunoglobulin","can_mod_post":false,"created_utc":1752604030,"send_replies":true,"parent_id":"t3_1m0lyjn","score":3,"author_fullname":"t2_9irlbfyb","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I join in thanking the author of the post. Moonshot AI and Unsloth - it's good that you are here!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3b291y","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I join in thanking the author of the post. Moonshot AI and Unsloth - it&amp;#39;s good that you are here!&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0lyjn/kimi_has_impressive_coding_performance_even_deep/n3b291y/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752604030,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m0lyjn","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3dehdq","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"FullstackSensei","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3d2vmu","score":1,"author_fullname":"t2_17n3nqtj56","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"It's not that Epyc is sensitive. Memory channels are interleaved even on Intel, and others. Check my reply to segmond.","edited":false,"author_flair_css_class":null,"name":"t1_n3dehdq","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It&amp;#39;s not that Epyc is sensitive. Memory channels are interleaved even on Intel, and others. Check my reply to segmond.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1m0lyjn","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0lyjn/kimi_has_impressive_coding_performance_even_deep/n3dehdq/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752630210,"author_flair_text":null,"collapsed":false,"created_utc":1752630210,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3d2vmu","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"mattescala","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3bxci3","score":2,"author_fullname":"t2_1c88dc0z","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Do NOT do this. I dont talk out of theory but out of experience. Epyc is highly sensitive on having all memory to be the same. The contrary can cause random crashes under load especially on well optimised numa configurations.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3d2vmu","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Do NOT do this. I dont talk out of theory but out of experience. Epyc is highly sensitive on having all memory to be the same. The contrary can cause random crashes under load especially on well optimised numa configurations.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0lyjn","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0lyjn/kimi_has_impressive_coding_performance_even_deep/n3d2vmu/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752626149,"author_flair_text":null,"treatment_tags":[],"created_utc":1752626149,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3doqwb","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"segmond","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3de37n","score":1,"author_fullname":"t2_ah13x","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"but why won't they be all octa-channel?  My plan is to get 8\\\\*64gb modules and 8\\\\*64gb and max out everything.  All modules will be same speed.","edited":false,"author_flair_css_class":null,"name":"t1_n3doqwb","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;but why won&amp;#39;t they be all octa-channel?  My plan is to get 8*64gb modules and 8*64gb and max out everything.  All modules will be same speed.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1m0lyjn","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":"light","treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0lyjn/kimi_has_impressive_coding_performance_even_deep/n3doqwb/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752633879,"author_flair_text":"llama.cpp","collapsed":false,"created_utc":1752633879,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3de37n","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"FullstackSensei","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3bxci3","score":1,"author_fullname":"t2_17n3nqtj56","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Don't mix. Channels are interleaved. If sizes aren't matched, any spillover above the smaller DIMMs will be interleaved only across the larger ones. Ex: say you get four 16GB and four 32GB DIMMs. You end up with 192GB RAM, but only the first 128GB are octa-channel. The other 64GB are quad channel only. You can't control what goes where and with the way OS' handle memory allocation and freeing, even if you are using less than 128GB, you could very well end up using a substantial chunk of those quad channel 64GBs.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3de37n","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Don&amp;#39;t mix. Channels are interleaved. If sizes aren&amp;#39;t matched, any spillover above the smaller DIMMs will be interleaved only across the larger ones. Ex: say you get four 16GB and four 32GB DIMMs. You end up with 192GB RAM, but only the first 128GB are octa-channel. The other 64GB are quad channel only. You can&amp;#39;t control what goes where and with the way OS&amp;#39; handle memory allocation and freeing, even if you are using less than 128GB, you could very well end up using a substantial chunk of those quad channel 64GBs.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0lyjn","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0lyjn/kimi_has_impressive_coding_performance_even_deep/n3de37n/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752630075,"author_flair_text":null,"treatment_tags":[],"created_utc":1752630075,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3bxci3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"segmond","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3bv2ax","score":1,"author_fullname":"t2_ah13x","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"do you still get max channel if you mix different ram size or do they all need to be the size?  can I mix 32gb and 64gb pairs?","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3bxci3","is_submitter":false,"collapsed":false,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;do you still get max channel if you mix different ram size or do they all need to be the size?  can I mix 32gb and 64gb pairs?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0lyjn","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0lyjn/kimi_has_impressive_coding_performance_even_deep/n3bxci3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752612770,"author_flair_text":"llama.cpp","treatment_tags":[],"created_utc":1752612770,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3ehjyj","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"mattescala","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3df6qp","score":1,"author_fullname":"t2_1c88dc0z","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"You‚Äôre right that L3 cache doesn‚Äôt help much for large LLMs, they blow past it anyway. But saying Milan(-X) is no better than Rome ignores a few important architectural upgrades. Milan has a unified 8-core CCX vs. Rome‚Äôs dual 4-core design, which reduces inter-core latency and improves memory access patterns. Even without more channels, it handles memory traffic more efficiently thanks to better prefetching and higher achievable FCLK. That translates to lower real-world memory latency something that does help with large model inference, especially when you‚Äôre NUMA-aware. So no, it‚Äôs not just about cache or core count Milan‚Äôs better silicon matters.\\n\\nAnd most importantly 3 &gt; 2 so better obv. Lmao","edited":false,"author_flair_css_class":null,"name":"t1_n3ehjyj","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You‚Äôre right that L3 cache doesn‚Äôt help much for large LLMs, they blow past it anyway. But saying Milan(-X) is no better than Rome ignores a few important architectural upgrades. Milan has a unified 8-core CCX vs. Rome‚Äôs dual 4-core design, which reduces inter-core latency and improves memory access patterns. Even without more channels, it handles memory traffic more efficiently thanks to better prefetching and higher achievable FCLK. That translates to lower real-world memory latency something that does help with large model inference, especially when you‚Äôre NUMA-aware. So no, it‚Äôs not just about cache or core count Milan‚Äôs better silicon matters.&lt;/p&gt;\\n\\n&lt;p&gt;And most importantly 3 &amp;gt; 2 so better obv. Lmao&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1m0lyjn","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0lyjn/kimi_has_impressive_coding_performance_even_deep/n3ehjyj/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752647014,"author_flair_text":null,"collapsed":false,"created_utc":1752647014,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3df6qp","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"FullstackSensei","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3d1wt2","score":2,"author_fullname":"t2_17n3nqtj56","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"But why? The extra cache and extra TDP are useless for LLMs. Milan-X doesn't have a faster memory controller or more memory channels, and the extra cache doesn't fit anything useful for inference anyway. You'd get practically the same performance from a 7642 which costs 400 or even less. I have several 7642s, including a dual system, and I've yet to load all 96 cores.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3df6qp","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;But why? The extra cache and extra TDP are useless for LLMs. Milan-X doesn&amp;#39;t have a faster memory controller or more memory channels, and the extra cache doesn&amp;#39;t fit anything useful for inference anyway. You&amp;#39;d get practically the same performance from a 7642 which costs 400 or even less. I have several 7642s, including a dual system, and I&amp;#39;ve yet to load all 96 cores.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0lyjn","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0lyjn/kimi_has_impressive_coding_performance_even_deep/n3df6qp/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752630452,"author_flair_text":null,"treatment_tags":[],"created_utc":1752630452,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n3d1wt2","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"mattescala","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3bv2ax","score":1,"author_fullname":"t2_1c88dc0z","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Dont worry i just dropped 2k on a couple of 7763x. Damn my poor bank account","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3d1wt2","is_submitter":true,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Dont worry i just dropped 2k on a couple of 7763x. Damn my poor bank account&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0lyjn","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0lyjn/kimi_has_impressive_coding_performance_even_deep/n3d1wt2/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752625810,"author_flair_text":null,"treatment_tags":[],"created_utc":1752625810,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3bv2ax","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"FullstackSensei","can_mod_post":false,"created_utc":1752612139,"send_replies":true,"parent_id":"t1_n3bsaag","score":2,"author_fullname":"t2_17n3nqtj56","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"It does. OP is in for an unpleasant surprise when he gets the remaining memory modules to populate the remaining channels. Epyc memory bandwidth is very dependent on the number of CCDs the CPU has. If you want to get anywhere near maximum memory bandwidth (75-80% of theoretical maximum), you need a 8 CCD model. Those can be recognized by having 256MB L3 cache. You'll need at least 32 cores to handle the number crunching. Between these two criteria, there aren't that many models you can chose from.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3bv2ax","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It does. OP is in for an unpleasant surprise when he gets the remaining memory modules to populate the remaining channels. Epyc memory bandwidth is very dependent on the number of CCDs the CPU has. If you want to get anywhere near maximum memory bandwidth (75-80% of theoretical maximum), you need a 8 CCD model. Those can be recognized by having 256MB L3 cache. You&amp;#39;ll need at least 32 cores to handle the number crunching. Between these two criteria, there aren&amp;#39;t that many models you can chose from.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m0lyjn","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0lyjn/kimi_has_impressive_coding_performance_even_deep/n3bv2ax/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752612139,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n3bsaag","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"segmond","can_mod_post":false,"created_utc":1752611368,"send_replies":true,"parent_id":"t3_1m0lyjn","score":2,"author_fullname":"t2_ah13x","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Thanks for sharing this.  I'm going to be buying an epyc server tonight.   Do you think the CPU makes much of a difference?  I'm trying to figure out if I should go for faster cpu or faster memory if I can only do one.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3bsaag","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thanks for sharing this.  I&amp;#39;m going to be buying an epyc server tonight.   Do you think the CPU makes much of a difference?  I&amp;#39;m trying to figure out if I should go for faster cpu or faster memory if I can only do one.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0lyjn/kimi_has_impressive_coding_performance_even_deep/n3bsaag/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752611368,"author_flair_text":"llama.cpp","treatment_tags":[],"link_id":"t3_1m0lyjn","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3badcy","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Alternative_Quote246","can_mod_post":false,"created_utc":1752606309,"send_replies":true,"parent_id":"t3_1m0lyjn","score":2,"author_fullname":"t2_4sfm1yax","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"impressive that the 2-bit quant can do such an amazing job!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3badcy","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;impressive that the 2-bit quant can do such an amazing job!&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0lyjn/kimi_has_impressive_coding_performance_even_deep/n3badcy/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752606309,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m0lyjn","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3bh1b4","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"synn89","can_mod_post":false,"created_utc":1752608213,"send_replies":true,"parent_id":"t3_1m0lyjn","score":1,"author_fullname":"t2_3jm4t","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Never played with AI coding since Aider CLI and thought I'd it a try again and wow, Roo Code + Kimi on Groq is really nice. Very easy to setup and very easy to use. Been while since I've used Groq as well and it's nice to see they're onto paid plans and have HIPAA/SOC 2 compliance.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3bh1b4","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Never played with AI coding since Aider CLI and thought I&amp;#39;d it a try again and wow, Roo Code + Kimi on Groq is really nice. Very easy to setup and very easy to use. Been while since I&amp;#39;ve used Groq as well and it&amp;#39;s nice to see they&amp;#39;re onto paid plans and have HIPAA/SOC 2 compliance.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0lyjn/kimi_has_impressive_coding_performance_even_deep/n3bh1b4/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752608213,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m0lyjn","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3ep307","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"mattescala","can_mod_post":false,"created_utc":1752651129,"send_replies":true,"parent_id":"t3_1m0lyjn","score":1,"author_fullname":"t2_1c88dc0z","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I think its a problem in the build flags i‚Äôll post my build flags later ;)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3ep307","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I think its a problem in the build flags i‚Äôll post my build flags later ;)&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0lyjn/kimi_has_impressive_coding_performance_even_deep/n3ep307/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752651129,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m0lyjn","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),o=()=>e.jsx(l,{data:a});export{o as default};
