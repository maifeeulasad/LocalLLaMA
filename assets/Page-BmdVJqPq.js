import{j as e}from"./index-DFOnUtq9.js";import{R as l}from"./RedditPostRenderer-B-dx19nm.js";import"./index-CUOQn61u.js";const a=JSON.parse('[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Hey everyone,  \\nI\'m interested in learning how to run and work with local LLaMA models (especially for personal or offline use). Are there any good beginner-to-advanced courses or tutorials you\'d recommend?  \\nI\'m open to paid or free options — just want something practical that covers setup, usage, and maybe fine-tuning or integrating with projects.  \\nThanks in advance!","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Good Courses to Learn and Use Local LLaMA Models?","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lmjwtu","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.67,"author_flair_background_color":null,"subreddit_type":"public","ups":3,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_8t5dutk8","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":3,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1751107717,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Hey everyone,&lt;br/&gt;\\nI&amp;#39;m interested in learning how to run and work with local LLaMA models (especially for personal or offline use). Are there any good beginner-to-advanced courses or tutorials you&amp;#39;d recommend?&lt;br/&gt;\\nI&amp;#39;m open to paid or free options — just want something practical that covers setup, usage, and maybe fine-tuning or integrating with projects.&lt;br/&gt;\\nThanks in advance!&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1lmjwtu","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"Blackverb","discussion_type":null,"num_comments":4,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lmjwtu/good_courses_to_learn_and_use_local_llama_models/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lmjwtu/good_courses_to_learn_and_use_local_llama_models/","subreddit_subscribers":492626,"created_utc":1751107717,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0bl6vy","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Blackverb","can_mod_post":false,"created_utc":1751152736,"send_replies":true,"parent_id":"t1_n084lok","score":1,"author_fullname":"t2_8t5dutk8","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"thank you","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0bl6vy","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;thank you&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lmjwtu","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmjwtu/good_courses_to_learn_and_use_local_llama_models/n0bl6vy/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751152736,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n084lok","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Superb_School_8907","can_mod_post":false,"created_utc":1751110414,"send_replies":true,"parent_id":"t3_1lmjwtu","score":2,"author_fullname":"t2_1rybxxmldn","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I found this tutorial helpful using the ollama tooling to running LLMs locally. \\n\\nhttps://www.machinelearningplus.com/gen-ai/ollama-tutorial-your-guide-to-running-llms-locally/","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n084lok","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I found this tutorial helpful using the ollama tooling to running LLMs locally. &lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://www.machinelearningplus.com/gen-ai/ollama-tutorial-your-guide-to-running-llms-locally/\\"&gt;https://www.machinelearningplus.com/gen-ai/ollama-tutorial-your-guide-to-running-llms-locally/&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmjwtu/good_courses_to_learn_and_use_local_llama_models/n084lok/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751110414,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lmjwtu","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0ajgbh","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"No_Afternoon_4260","can_mod_post":false,"created_utc":1751139875,"send_replies":true,"parent_id":"t3_1lmjwtu","score":1,"author_fullname":"t2_cj9kap4bx","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Just not that:\\nWhat you call local llama models are \\"open weights\\" \\"ai\\" models that you can download on your computer.  \\nThey are mostly transformers for llm and diffusion for image gen.  \\n\\nTo run a local llm you need an inference engine (some people call it the backend) i recommend llama.cpp as it is the historic one for this sub. But you could use vllm, shlang, mistral.rs..\\nI also recommend llama.cpp because for me it is the best ecosystem to really understand what is it to run such model, what are quants, etc..  \\nAll these inference engine run an api where you can request generation by passing it your prompt(prompt history..).  \\nSo they all need a UI on top.  \\nGood UI are openwebui, sillytavern etc..\\nBut you find a lot of aimple UI, for exemple running a llama.cpp server comes with a very good minimalistic UI.  \\nBut you can also plus your llama.cpp api to something like roo code in vscode, this allows the models to have function it can call to see your current project, read/write files and so on.\\n\\nYou\'ll find most of the models on hugging face.\\n\\nLaunching a llama.cpp instance is as easy as ``` llama-server -m {path for ur model} -ngl {nb of layer on gpu} -ts {how to slipt it in a multi gpu setup for exemple 50,50) -p {on which port to serve the api)\\n\\nThen you can access the UI on that local machine and port you\'ve set, or use it in another app like roo code\\n\\nHave fun!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0ajgbh","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Just not that:\\nWhat you call local llama models are &amp;quot;open weights&amp;quot; &amp;quot;ai&amp;quot; models that you can download on your computer.&lt;br/&gt;\\nThey are mostly transformers for llm and diffusion for image gen.  &lt;/p&gt;\\n\\n&lt;p&gt;To run a local llm you need an inference engine (some people call it the backend) i recommend llama.cpp as it is the historic one for this sub. But you could use vllm, shlang, mistral.rs..\\nI also recommend llama.cpp because for me it is the best ecosystem to really understand what is it to run such model, what are quants, etc..&lt;br/&gt;\\nAll these inference engine run an api where you can request generation by passing it your prompt(prompt history..).&lt;br/&gt;\\nSo they all need a UI on top.&lt;br/&gt;\\nGood UI are openwebui, sillytavern etc..\\nBut you find a lot of aimple UI, for exemple running a llama.cpp server comes with a very good minimalistic UI.&lt;br/&gt;\\nBut you can also plus your llama.cpp api to something like roo code in vscode, this allows the models to have function it can call to see your current project, read/write files and so on.&lt;/p&gt;\\n\\n&lt;p&gt;You&amp;#39;ll find most of the models on hugging face.&lt;/p&gt;\\n\\n&lt;p&gt;Launching a llama.cpp instance is as easy as ``` llama-server -m {path for ur model} -ngl {nb of layer on gpu} -ts {how to slipt it in a multi gpu setup for exemple 50,50) -p {on which port to serve the api)&lt;/p&gt;\\n\\n&lt;p&gt;Then you can access the UI on that local machine and port you&amp;#39;ve set, or use it in another app like roo code&lt;/p&gt;\\n\\n&lt;p&gt;Have fun!&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lmjwtu/good_courses_to_learn_and_use_local_llama_models/n0ajgbh/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751139875,"author_flair_text":"llama.cpp","treatment_tags":[],"link_id":"t3_1lmjwtu","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]'),r=()=>e.jsx(l,{data:a});export{r as default};
