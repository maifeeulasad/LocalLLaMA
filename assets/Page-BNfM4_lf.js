import{j as e}from"./index-DACS7Nh6.js";import{R as t}from"./RedditPostRenderer-Dqa1NZuX.js";import"./index-DiMIVQx4.js";const l=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"I recently inherited a GPU workstation at work from a project that got shut down. It's an older Vector Lambda with 4x RTX a5000, so I decided to set it up running either one full instance of the new devstral model or some quantized versions. The problem I'm running into is I'm just getting \\\\*terrible\\\\* performance out of it. I've got a simple test script I run that tosses random chunks of \\\\~2k tokens at it and asks it to summarize them, running 5x requests in parallel. With that, on the server with the bf16 unquantized model I get 13-15 tokens/second. To test it, I spun up an instance on [vast.ai](http://vast.ai) that also has 4x a5000, and it's getting well over 100 tokens/second, using the exact same invocation command (the one on the Devstral Huggingface).\\n\\nI've spent the past day off and on trying to debug this and can't figure it out. My server is running a default ubuntu install with updated nvidia drivers and nothing else. I've verified flashinfer/flash-attn are built and appear to be loading, I've verified all sorts of load seems fine. I've verified they're on PCIe 4.0x16 lanes. The only things I can think of that could be causing it:\\n\\n* My server is connected with nvlink, linking gpus 0 and 3 as well as 1 and 2 together. The rental one just has them on the PCIe bus, but if anything that means this server should be going slightly faster, not an order of magnitude slower.\\n* If I pull up nvidia-smi, the gpus always seem to be in the P2 power state, and relatively low draw (\\\\~80W). As I understand it, that should be fine: they should be able to spike to higher draw when under load, so it's possible something is misconfigured and causing them to stay in a lower power state.\\n* What I've seen it looks like it's fine, but under load on the server I have a python process at 100% CPU. My best guess here might be something misconfigured and somehow blocking on the CPU processing data, but I don't understand what that might be (and ps just lists it a python process spawning something for multiprocessing).\\n\\nAny thoughts on how to go about troubleshooting would be appreciated. My next steps at this point are probably disabling nvlink, but as far as I can tell that will require hands on the hardware and it's unfortunately at an office \\\\~50 miles away. I can SSH in without issue, but can't physically touch it until Wednesday.\\n\\n\\\\----- EDIT ------\\n\\nManaged to find someone still in the office who could pull the nvlink bridges. That definitely was \\\\*a\\\\* problem, and it went from that \\\\~14 tokens/second up to \\\\~25 token/second. Better, and good enough to use, but still 1/4 what I'm getting on similar hardware on a rental machine.","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Looking for help with terrible vLLM performance","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1m3cfy9","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":1,"author_flair_background_color":null,"subreddit_type":"public","ups":6,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_1mzgsryavd","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":6,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":1752871963,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"post_hint":"self","content_categories":null,"is_self":true,"mod_note":null,"created":1752868982,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;I recently inherited a GPU workstation at work from a project that got shut down. It&amp;#39;s an older Vector Lambda with 4x RTX a5000, so I decided to set it up running either one full instance of the new devstral model or some quantized versions. The problem I&amp;#39;m running into is I&amp;#39;m just getting *terrible* performance out of it. I&amp;#39;ve got a simple test script I run that tosses random chunks of ~2k tokens at it and asks it to summarize them, running 5x requests in parallel. With that, on the server with the bf16 unquantized model I get 13-15 tokens/second. To test it, I spun up an instance on &lt;a href=\\"http://vast.ai\\"&gt;vast.ai&lt;/a&gt; that also has 4x a5000, and it&amp;#39;s getting well over 100 tokens/second, using the exact same invocation command (the one on the Devstral Huggingface).&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;ve spent the past day off and on trying to debug this and can&amp;#39;t figure it out. My server is running a default ubuntu install with updated nvidia drivers and nothing else. I&amp;#39;ve verified flashinfer/flash-attn are built and appear to be loading, I&amp;#39;ve verified all sorts of load seems fine. I&amp;#39;ve verified they&amp;#39;re on PCIe 4.0x16 lanes. The only things I can think of that could be causing it:&lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;My server is connected with nvlink, linking gpus 0 and 3 as well as 1 and 2 together. The rental one just has them on the PCIe bus, but if anything that means this server should be going slightly faster, not an order of magnitude slower.&lt;/li&gt;\\n&lt;li&gt;If I pull up nvidia-smi, the gpus always seem to be in the P2 power state, and relatively low draw (~80W). As I understand it, that should be fine: they should be able to spike to higher draw when under load, so it&amp;#39;s possible something is misconfigured and causing them to stay in a lower power state.&lt;/li&gt;\\n&lt;li&gt;What I&amp;#39;ve seen it looks like it&amp;#39;s fine, but under load on the server I have a python process at 100% CPU. My best guess here might be something misconfigured and somehow blocking on the CPU processing data, but I don&amp;#39;t understand what that might be (and ps just lists it a python process spawning something for multiprocessing).&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;p&gt;Any thoughts on how to go about troubleshooting would be appreciated. My next steps at this point are probably disabling nvlink, but as far as I can tell that will require hands on the hardware and it&amp;#39;s unfortunately at an office ~50 miles away. I can SSH in without issue, but can&amp;#39;t physically touch it until Wednesday.&lt;/p&gt;\\n\\n&lt;p&gt;----- EDIT ------&lt;/p&gt;\\n\\n&lt;p&gt;Managed to find someone still in the office who could pull the nvlink bridges. That definitely was *a* problem, and it went from that ~14 tokens/second up to ~25 token/second. Better, and good enough to use, but still 1/4 what I&amp;#39;m getting on similar hardware on a rental machine.&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":true,"is_crosspostable":false,"pinned":false,"over_18":false,"preview":{"images":[{"source":{"url":"https://external-preview.redd.it/MV8WQnKGiSSypEI5QKJe4g08BAeccM6KobeueLMOJdY.png?auto=webp&amp;s=c5b1db2b11bd21a955cbe1e863cde94ef57607f4","width":4000,"height":2250},"resolutions":[{"url":"https://external-preview.redd.it/MV8WQnKGiSSypEI5QKJe4g08BAeccM6KobeueLMOJdY.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=a08158a2ec290c8157b492f314bfb148408be1fc","width":108,"height":60},{"url":"https://external-preview.redd.it/MV8WQnKGiSSypEI5QKJe4g08BAeccM6KobeueLMOJdY.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=5d4693d9fc011431e9348152136fa7a13c95504b","width":216,"height":121},{"url":"https://external-preview.redd.it/MV8WQnKGiSSypEI5QKJe4g08BAeccM6KobeueLMOJdY.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=93ef867725a538dad3a6209e5062d3d1de60aeaa","width":320,"height":180},{"url":"https://external-preview.redd.it/MV8WQnKGiSSypEI5QKJe4g08BAeccM6KobeueLMOJdY.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=fc186b216811c20876ecdaf0e913cc0b59498d7a","width":640,"height":360},{"url":"https://external-preview.redd.it/MV8WQnKGiSSypEI5QKJe4g08BAeccM6KobeueLMOJdY.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=67812638cc7d2b930cd8bebf733409c3b2d92397","width":960,"height":540},{"url":"https://external-preview.redd.it/MV8WQnKGiSSypEI5QKJe4g08BAeccM6KobeueLMOJdY.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=bc092f31a95e3a3df682dc8f7222b0fb1363a5df","width":1080,"height":607}],"variants":{},"id":"MV8WQnKGiSSypEI5QKJe4g08BAeccM6KobeueLMOJdY"}],"enabled":false},"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1m3cfy9","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"Render_Arcana","discussion_type":null,"num_comments":27,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1m3cfy9/looking_for_help_with_terrible_vllm_performance/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1m3cfy9/looking_for_help_with_terrible_vllm_performance/","subreddit_subscribers":501527,"created_utc":1752868982,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3xkof2","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"__JockY__","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3vnypj","score":2,"author_fullname":"t2_qf8h7ka8","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Looks like you’re running the full BF16 weights here. Are you sure the cloud service against which you’re comparing is also running non-quantized versions?\\n\\nI bet if you tried the FP4 or INT4 GPTQ quants you’d see speeds closer to the cloud.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3xkof2","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Looks like you’re running the full BF16 weights here. Are you sure the cloud service against which you’re comparing is also running non-quantized versions?&lt;/p&gt;\\n\\n&lt;p&gt;I bet if you tried the FP4 or INT4 GPTQ quants you’d see speeds closer to the cloud.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m3cfy9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3cfy9/looking_for_help_with_terrible_vllm_performance/n3xkof2/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752893775,"author_flair_text":null,"treatment_tags":[],"created_utc":1752893775,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3xomcy","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"fp4guru","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3xhsqc","score":1,"author_fullname":"t2_1tp8zldw5g","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"nvidia-smi topo -m","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n3xomcy","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;nvidia-smi topo -m&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1m3cfy9","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3cfy9/looking_for_help_with_terrible_vllm_performance/n3xomcy/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752895417,"author_flair_text":null,"treatment_tags":[],"created_utc":1752895417,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":6,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3xhsqc","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Render_Arcana","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3x1mt9","score":1,"author_fullname":"t2_1mzgsryavd","approved_by":null,"mod_note":null,"all_awardings":[],"body":"Something is certainly screwy, but I have no idea what. Testing with the opt-125m model vllm defaults to with no model defined, I get a little over 2k token/s on gpu 0, but only 1k/s on gpu 1 and \\\\~700 each on gpus 2 and 3.","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n3xhsqc","is_submitter":true,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Something is certainly screwy, but I have no idea what. Testing with the opt-125m model vllm defaults to with no model defined, I get a little over 2k token/s on gpu 0, but only 1k/s on gpu 1 and ~700 each on gpus 2 and 3.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1m3cfy9","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3cfy9/looking_for_help_with_terrible_vllm_performance/n3xhsqc/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752892608,"author_flair_text":null,"treatment_tags":[],"created_utc":1752892608,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3x1mt9","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"fp4guru","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3vskba","score":1,"author_fullname":"t2_1tp8zldw5g","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Try only use one or two GPUs and test for speed so that we can find the bad ones.","edited":false,"author_flair_css_class":null,"name":"t1_n3x1mt9","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Try only use one or two GPUs and test for speed so that we can find the bad ones.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1m3cfy9","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3cfy9/looking_for_help_with_terrible_vllm_performance/n3x1mt9/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752886445,"author_flair_text":null,"collapsed":false,"created_utc":1752886445,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3vskba","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Render_Arcana","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3vox1r","score":2,"author_fullname":"t2_1mzgsryavd","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Persistence mode is already Enabled for GPU 00000000:01:00.0.\\n\\nPersistence mode is already Enabled for GPU 00000000:2E:00.0.\\n\\nPersistence mode is already Enabled for GPU 00000000:41:00.0.\\n\\nPersistence mode is already Enabled for GPU 00000000:61:00.0.\\n\\nAll done.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3vskba","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Persistence mode is already Enabled for GPU 00000000:01:00.0.&lt;/p&gt;\\n\\n&lt;p&gt;Persistence mode is already Enabled for GPU 00000000:2E:00.0.&lt;/p&gt;\\n\\n&lt;p&gt;Persistence mode is already Enabled for GPU 00000000:41:00.0.&lt;/p&gt;\\n\\n&lt;p&gt;Persistence mode is already Enabled for GPU 00000000:61:00.0.&lt;/p&gt;\\n\\n&lt;p&gt;All done.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m3cfy9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3cfy9/looking_for_help_with_terrible_vllm_performance/n3vskba/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752871289,"author_flair_text":null,"treatment_tags":[],"created_utc":1752871289,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n3vox1r","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"fp4guru","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3vnypj","score":1,"author_fullname":"t2_1tp8zldw5g","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"nvidia-smi -pm 1\\nCan you try this ?","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3vox1r","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;nvidia-smi -pm 1\\nCan you try this ?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m3cfy9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3cfy9/looking_for_help_with_terrible_vllm_performance/n3vox1r/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752870221,"author_flair_text":null,"treatment_tags":[],"created_utc":1752870221,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3vv2ss","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Render_Arcana","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3vru92","score":1,"author_fullname":"t2_1mzgsryavd","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"The tests I'm running for benchmarking are relatively small (\\\\~2k tokens each), and that's what's generating the tiny abysmal throughput.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3vv2ss","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The tests I&amp;#39;m running for benchmarking are relatively small (~2k tokens each), and that&amp;#39;s what&amp;#39;s generating the tiny abysmal throughput.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m3cfy9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3cfy9/looking_for_help_with_terrible_vllm_performance/n3vv2ss/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752872030,"author_flair_text":null,"treatment_tags":[],"created_utc":1752872030,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3vru92","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DinoAmino","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3vnypj","score":1,"author_fullname":"t2_j1v7f","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Maybe try setting a low context length. Like just 8k and see if that changes things.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3vru92","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Maybe try setting a low context length. Like just 8k and see if that changes things.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m3cfy9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3cfy9/looking_for_help_with_terrible_vllm_performance/n3vru92/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752871078,"author_flair_text":null,"treatment_tags":[],"created_utc":1752871078,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3vnypj","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Render_Arcana","can_mod_post":false,"created_utc":1752869943,"send_replies":true,"parent_id":"t1_n3vmru5","score":1,"author_fullname":"t2_1mzgsryavd","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"vllm serve mistralai/Devstral-Small-2507 --tokenizer\\\\_mode mistral --config\\\\_format mistral --load\\\\_format mistral --tool-call-parser mistral --enable-auto-tool-choice --tensor-parallel-size 4 --served-model-name devstral.\\n\\nvLLM is latest for both, so 0.9.2. Both instances are being installed via a \\\\\`uv pip install vllm --torch-backend=auto\\\\\`.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3vnypj","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;vllm serve mistralai/Devstral-Small-2507 --tokenizer_mode mistral --config_format mistral --load_format mistral --tool-call-parser mistral --enable-auto-tool-choice --tensor-parallel-size 4 --served-model-name devstral.&lt;/p&gt;\\n\\n&lt;p&gt;vLLM is latest for both, so 0.9.2. Both instances are being installed via a \`uv pip install vllm --torch-backend=auto\`.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m3cfy9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3cfy9/looking_for_help_with_terrible_vllm_performance/n3vnypj/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752869943,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3vmru5","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"fp4guru","can_mod_post":false,"created_utc":1752869592,"send_replies":true,"parent_id":"t3_1m3cfy9","score":3,"author_fullname":"t2_1tp8zldw5g","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Please share the commmadline for 13-15 tokens per second starting with vllm serve. Also vllm version.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3vmru5","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Please share the commmadline for 13-15 tokens per second starting with vllm serve. Also vllm version.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3cfy9/looking_for_help_with_terrible_vllm_performance/n3vmru5/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752869592,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m3cfy9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3vvn8m","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Render_Arcana","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3vso6p","score":1,"author_fullname":"t2_1mzgsryavd","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Hah, I got over my head trying to debug some of it when flashinfer wouldn't build out of the box, so that's what I've been doing all day (shipping logs to chatgpt to see if it can help).\\n\\nI can't try the -dp 2 -tp 2 because the model won' \\\\*quite\\\\* fit without quantization, even with no context window. It's like 45gb total unquantized.\\n\\nI also don't understand numa to well, but the nvidia-smi topo -m seems to say I'm in good shape there (and chatgpt agrees):\\n\\nGPU0    GPU1    GPU2    GPU3    CPU Affinity    NUMA Affinity   GPU NUMA ID\\n\\nGPU0     X      NODE    NODE    NODE    0-63    0               N/A\\n\\nGPU1    NODE     X      NODE    NODE    0-63    0               N/A\\n\\nGPU2    NODE    NODE     X      NODE    0-63    0               N/A\\n\\nGPU3    NODE    NODE    NODE     X      0-63    0               N/A\\n\\n\\n\\nLegend:\\n\\n\\n\\n  X    = Self\\n\\n  SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)\\n\\n  NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node\\n\\n  PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)\\n\\n  PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)\\n\\n  PIX  = Connection traversing at most a single PCIe bridge\\n\\n  NV#  = Connection traversing a bonded set of # NVLinks","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3vvn8m","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Hah, I got over my head trying to debug some of it when flashinfer wouldn&amp;#39;t build out of the box, so that&amp;#39;s what I&amp;#39;ve been doing all day (shipping logs to chatgpt to see if it can help).&lt;/p&gt;\\n\\n&lt;p&gt;I can&amp;#39;t try the -dp 2 -tp 2 because the model won&amp;#39; *quite* fit without quantization, even with no context window. It&amp;#39;s like 45gb total unquantized.&lt;/p&gt;\\n\\n&lt;p&gt;I also don&amp;#39;t understand numa to well, but the nvidia-smi topo -m seems to say I&amp;#39;m in good shape there (and chatgpt agrees):&lt;/p&gt;\\n\\n&lt;p&gt;GPU0    GPU1    GPU2    GPU3    CPU Affinity    NUMA Affinity   GPU NUMA ID&lt;/p&gt;\\n\\n&lt;p&gt;GPU0     X      NODE    NODE    NODE    0-63    0               N/A&lt;/p&gt;\\n\\n&lt;p&gt;GPU1    NODE     X      NODE    NODE    0-63    0               N/A&lt;/p&gt;\\n\\n&lt;p&gt;GPU2    NODE    NODE     X      NODE    0-63    0               N/A&lt;/p&gt;\\n\\n&lt;p&gt;GPU3    NODE    NODE    NODE     X      0-63    0               N/A&lt;/p&gt;\\n\\n&lt;p&gt;Legend:&lt;/p&gt;\\n\\n&lt;p&gt;X    = Self&lt;/p&gt;\\n\\n&lt;p&gt;SYS  = Connection traversing PCIe as well as the SMP interconnect between NUMA nodes (e.g., QPI/UPI)&lt;/p&gt;\\n\\n&lt;p&gt;NODE = Connection traversing PCIe as well as the interconnect between PCIe Host Bridges within a NUMA node&lt;/p&gt;\\n\\n&lt;p&gt;PHB  = Connection traversing PCIe as well as a PCIe Host Bridge (typically the CPU)&lt;/p&gt;\\n\\n&lt;p&gt;PXB  = Connection traversing multiple PCIe bridges (without traversing the PCIe Host Bridge)&lt;/p&gt;\\n\\n&lt;p&gt;PIX  = Connection traversing at most a single PCIe bridge&lt;/p&gt;\\n\\n&lt;p&gt;NV#  = Connection traversing a bonded set of # NVLinks&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m3cfy9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3cfy9/looking_for_help_with_terrible_vllm_performance/n3vvn8m/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752872203,"author_flair_text":null,"treatment_tags":[],"created_utc":1752872203,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3vso6p","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Candid_Payment_4094","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3volna","score":1,"author_fullname":"t2_anbsqj0od","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Sounds maybe cheesy, but print out all the debug statements, Nvidia-SMI, etc and throw it into Gemini 2.5 Pro, along with debug/trace statements of vLLM. It might catch something that is deeply hidden.\\n\\nAlso check if your workstation has two CPU sockets by any chance. There might be some issues with NUMA. I don't have experiences with this myself, so you need to look up how to bind processes with each socket\\n\\nAlso try -dp 2 and -tp 2 with Ray","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3vso6p","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Sounds maybe cheesy, but print out all the debug statements, Nvidia-SMI, etc and throw it into Gemini 2.5 Pro, along with debug/trace statements of vLLM. It might catch something that is deeply hidden.&lt;/p&gt;\\n\\n&lt;p&gt;Also check if your workstation has two CPU sockets by any chance. There might be some issues with NUMA. I don&amp;#39;t have experiences with this myself, so you need to look up how to bind processes with each socket&lt;/p&gt;\\n\\n&lt;p&gt;Also try -dp 2 and -tp 2 with Ray&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m3cfy9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3cfy9/looking_for_help_with_terrible_vllm_performance/n3vso6p/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752871321,"author_flair_text":null,"treatment_tags":[],"created_utc":1752871321,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n41dnij","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"kmouratidis","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3volna","score":1,"author_fullname":"t2_k6u7rfxb","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"&gt; No to --enforce-eager, I can try that in a bit.\\n\\nEnforce eager will almost always be slower.\\n\\nCan you share the logs from the time you press \\"enter\\" until the request is finished? And please use \\\\\`\\\\\`\\\\\` around it so we can actually read it :D","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n41dnij","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;No to --enforce-eager, I can try that in a bit.&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;Enforce eager will almost always be slower.&lt;/p&gt;\\n\\n&lt;p&gt;Can you share the logs from the time you press &amp;quot;enter&amp;quot; until the request is finished? And please use \`\`\` around it so we can actually read it :D&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m3cfy9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3cfy9/looking_for_help_with_terrible_vllm_performance/n41dnij/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752950257,"author_flair_text":null,"treatment_tags":[],"created_utc":1752950257,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3volna","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Render_Arcana","can_mod_post":false,"created_utc":1752870129,"send_replies":true,"parent_id":"t1_n3vnbez","score":1,"author_fullname":"t2_1mzgsryavd","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I've tried the P2P disable, and it did not seem to have meaningful performance changes (I hadn't come across the second environment variable, so I haven't tried that).\\n\\nI've tried running in docker and not, without drastic difference.\\n\\nI'm already running on latest nvidia drivers (at least, latest as of earlier this week).\\n\\nI haven't tried SGLang, but that might be next.\\n\\nNo to --enforce-eager, I can try that in a bit.\\n\\nYes, I'm using -tp 4.\\n\\nI've tried different debugs, and none of the warnings jump out as being particularly important (most seem related to the specifics of the mistral config, and are identical between the workstation and the rented server).","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3volna","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;ve tried the P2P disable, and it did not seem to have meaningful performance changes (I hadn&amp;#39;t come across the second environment variable, so I haven&amp;#39;t tried that).&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;ve tried running in docker and not, without drastic difference.&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;m already running on latest nvidia drivers (at least, latest as of earlier this week).&lt;/p&gt;\\n\\n&lt;p&gt;I haven&amp;#39;t tried SGLang, but that might be next.&lt;/p&gt;\\n\\n&lt;p&gt;No to --enforce-eager, I can try that in a bit.&lt;/p&gt;\\n\\n&lt;p&gt;Yes, I&amp;#39;m using -tp 4.&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;ve tried different debugs, and none of the warnings jump out as being particularly important (most seem related to the specifics of the mistral config, and are identical between the workstation and the rented server).&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m3cfy9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3cfy9/looking_for_help_with_terrible_vllm_performance/n3volna/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752870129,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3vnbez","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Candid_Payment_4094","can_mod_post":false,"created_utc":1752869751,"send_replies":true,"parent_id":"t3_1m3cfy9","score":3,"author_fullname":"t2_anbsqj0od","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"you can *logically* disable P2P/NVLink without touching hardware  \\nexport NCCL\\\\_P2P\\\\_DISABLE=1  \\nexport NCCL\\\\_P2P\\\\_LEVEL=SYS (or something along those lines)\\n\\nAre you running it in a Docker container?  \\nHave you updated your Nvidia drivers?  \\nHave you tested this with another inference server? For example SGLang (another high performant inference server)  \\nHave you tried it with --enforce-eager  \\nDo you use \`--tensor-parallel-size\` &lt;number of GPus&gt; ?  \\nHave you tried it with all debug on? (NVidia debug, vLLM debug). Does it warn you about anything?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3vnbez","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;you can &lt;em&gt;logically&lt;/em&gt; disable P2P/NVLink without touching hardware&lt;br/&gt;\\nexport NCCL_P2P_DISABLE=1&lt;br/&gt;\\nexport NCCL_P2P_LEVEL=SYS (or something along those lines)&lt;/p&gt;\\n\\n&lt;p&gt;Are you running it in a Docker container?&lt;br/&gt;\\nHave you updated your Nvidia drivers?&lt;br/&gt;\\nHave you tested this with another inference server? For example SGLang (another high performant inference server)&lt;br/&gt;\\nHave you tried it with --enforce-eager&lt;br/&gt;\\nDo you use &lt;code&gt;--tensor-parallel-size&lt;/code&gt; &amp;lt;number of GPus&amp;gt; ?&lt;br/&gt;\\nHave you tried it with all debug on? (NVidia debug, vLLM debug). Does it warn you about anything?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3cfy9/looking_for_help_with_terrible_vllm_performance/n3vnbez/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752869751,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m3cfy9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3xi8uj","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Render_Arcana","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3wa4cr","score":1,"author_fullname":"t2_1mzgsryavd","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"It reports as a AMD Ryzen Threadripper PRO 3975WX 32-Cores, and the ram says it's operating at 3200 MT/s. Neither are current top-of-the-line, but I wouldn't expect either to be so slow as to be the bottleneck without something misconfigured.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3xi8uj","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It reports as a AMD Ryzen Threadripper PRO 3975WX 32-Cores, and the ram says it&amp;#39;s operating at 3200 MT/s. Neither are current top-of-the-line, but I wouldn&amp;#39;t expect either to be so slow as to be the bottleneck without something misconfigured.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m3cfy9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3cfy9/looking_for_help_with_terrible_vllm_performance/n3xi8uj/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752892786,"author_flair_text":null,"treatment_tags":[],"created_utc":1752892786,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3wa4cr","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DeltaSqueezer","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3vybqp","score":1,"author_fullname":"t2_8jqx3m14","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"This seems OK. What CPU do you have in there? I wonder if you could be bottlenecked by the CPU e.g. in the sampler stage. What samplers are you using? Maybe you can test the GPUs with diffusion models to see if you get higher utilization from the GPUs.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3wa4cr","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;This seems OK. What CPU do you have in there? I wonder if you could be bottlenecked by the CPU e.g. in the sampler stage. What samplers are you using? Maybe you can test the GPUs with diffusion models to see if you get higher utilization from the GPUs.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m3cfy9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3cfy9/looking_for_help_with_terrible_vllm_performance/n3wa4cr/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752876796,"author_flair_text":null,"treatment_tags":[],"created_utc":1752876796,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3vybqp","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Render_Arcana","can_mod_post":false,"created_utc":1752873022,"send_replies":true,"parent_id":"t1_n3vvuh1","score":1,"author_fullname":"t2_1mzgsryavd","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Output while under load:\\n\\nFri Jul 18 17:09:10 2025\\n\\n\\\\+-----------------------------------------------------------------------------------------+\\n\\n| NVIDIA-SMI 575.57.08              Driver Version: 575.57.08      CUDA Version: 12.9     |\\n\\n|-----------------------------------------+------------------------+----------------------+\\n\\n| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\\n\\n| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\\n\\n|                                         |                        |               MIG M. |\\n\\n|=========================================+========================+======================|\\n\\n|   0  NVIDIA RTX A5000               On  |   00000000:01:00.0 Off |                  Off |\\n\\n| 30%   39C    P2             85W /  230W |   22939MiB /  24564MiB |    100%      Default |\\n\\n|                                         |                        |                  N/A |\\n\\n\\\\+-----------------------------------------+------------------------+----------------------+\\n\\n|   1  NVIDIA RTX A5000               On  |   00000000:2E:00.0 Off |                  Off |\\n\\n| 30%   55C    P2             94W /  230W |   22939MiB /  24564MiB |     98%      Default |\\n\\n|                                         |                        |                  N/A |\\n\\n\\\\+-----------------------------------------+------------------------+----------------------+\\n\\n|   2  NVIDIA RTX A5000               On  |   00000000:41:00.0 Off |                  Off |\\n\\n| 30%   55C    P2             82W /  230W |   22939MiB /  24564MiB |     99%      Default |\\n\\n|                                         |                        |                  N/A |\\n\\n\\\\+-----------------------------------------+------------------------+----------------------+\\n\\n|   3  NVIDIA RTX A5000               On  |   00000000:61:00.0 Off |                  Off |\\n\\n| 30%   46C    P2             80W /  230W |   22939MiB /  24564MiB |     99%      Default |\\n\\n|                                         |                        |                  N/A |\\n\\n\\\\+-----------------------------------------+------------------------+----------------------+","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3vybqp","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Output while under load:&lt;/p&gt;\\n\\n&lt;p&gt;Fri Jul 18 17:09:10 2025&lt;/p&gt;\\n\\n&lt;p&gt;+-----------------------------------------------------------------------------------------+&lt;/p&gt;\\n\\n&lt;p&gt;| NVIDIA-SMI 575.57.08              Driver Version: 575.57.08      CUDA Version: 12.9     |&lt;/p&gt;\\n\\n&lt;p&gt;|-----------------------------------------+------------------------+----------------------+&lt;/p&gt;\\n\\n&lt;p&gt;| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |&lt;/p&gt;\\n\\n&lt;p&gt;| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |&lt;/p&gt;\\n\\n&lt;p&gt;|                                         |                        |               MIG M. |&lt;/p&gt;\\n\\n&lt;p&gt;|=========================================+========================+======================|&lt;/p&gt;\\n\\n&lt;p&gt;|   0  NVIDIA RTX A5000               On  |   00000000:01:00.0 Off |                  Off |&lt;/p&gt;\\n\\n&lt;p&gt;| 30%   39C    P2             85W /  230W |   22939MiB /  24564MiB |    100%      Default |&lt;/p&gt;\\n\\n&lt;p&gt;|                                         |                        |                  N/A |&lt;/p&gt;\\n\\n&lt;p&gt;+-----------------------------------------+------------------------+----------------------+&lt;/p&gt;\\n\\n&lt;p&gt;|   1  NVIDIA RTX A5000               On  |   00000000:2E:00.0 Off |                  Off |&lt;/p&gt;\\n\\n&lt;p&gt;| 30%   55C    P2             94W /  230W |   22939MiB /  24564MiB |     98%      Default |&lt;/p&gt;\\n\\n&lt;p&gt;|                                         |                        |                  N/A |&lt;/p&gt;\\n\\n&lt;p&gt;+-----------------------------------------+------------------------+----------------------+&lt;/p&gt;\\n\\n&lt;p&gt;|   2  NVIDIA RTX A5000               On  |   00000000:41:00.0 Off |                  Off |&lt;/p&gt;\\n\\n&lt;p&gt;| 30%   55C    P2             82W /  230W |   22939MiB /  24564MiB |     99%      Default |&lt;/p&gt;\\n\\n&lt;p&gt;|                                         |                        |                  N/A |&lt;/p&gt;\\n\\n&lt;p&gt;+-----------------------------------------+------------------------+----------------------+&lt;/p&gt;\\n\\n&lt;p&gt;|   3  NVIDIA RTX A5000               On  |   00000000:61:00.0 Off |                  Off |&lt;/p&gt;\\n\\n&lt;p&gt;| 30%   46C    P2             80W /  230W |   22939MiB /  24564MiB |     99%      Default |&lt;/p&gt;\\n\\n&lt;p&gt;|                                         |                        |                  N/A |&lt;/p&gt;\\n\\n&lt;p&gt;+-----------------------------------------+------------------------+----------------------+&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m3cfy9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3cfy9/looking_for_help_with_terrible_vllm_performance/n3vybqp/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752873022,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3vvuh1","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DeltaSqueezer","can_mod_post":false,"created_utc":1752872263,"send_replies":true,"parent_id":"t3_1m3cfy9","score":1,"author_fullname":"t2_8jqx3m14","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"give nvidia-smi output. otherwise we are guessing with less information than you are able to provide.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3vvuh1","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;give nvidia-smi output. otherwise we are guessing with less information than you are able to provide.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3cfy9/looking_for_help_with_terrible_vllm_performance/n3vvuh1/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752872263,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m3cfy9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3wfpn3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Ok_Needleworker_5247","can_mod_post":false,"created_utc":1752878703,"send_replies":true,"parent_id":"t3_1m3cfy9","score":1,"author_fullname":"t2_1gmprv51a1","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Check if your CPUs are bottlenecking the GPUs. Sometimes limiting factors can be CPU-bound, especially if one process is hitting 100%. You might benefit from tweaking CPU affinity settings or trying process binding to balance the load better. Also, consider swapping the CPUs if possible if there's a mismatch in processing power compared to the rented setup.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3wfpn3","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Check if your CPUs are bottlenecking the GPUs. Sometimes limiting factors can be CPU-bound, especially if one process is hitting 100%. You might benefit from tweaking CPU affinity settings or trying process binding to balance the load better. Also, consider swapping the CPUs if possible if there&amp;#39;s a mismatch in processing power compared to the rented setup.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3cfy9/looking_for_help_with_terrible_vllm_performance/n3wfpn3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752878703,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m3cfy9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3xif36","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Render_Arcana","can_mod_post":false,"created_utc":1752892855,"send_replies":true,"parent_id":"t1_n3wllxi","score":1,"author_fullname":"t2_1mzgsryavd","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I thought this was promising, but I don't see anything particularly damning there. The only nvidia related ones are related to loading, and some DRM bits.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3xif36","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I thought this was promising, but I don&amp;#39;t see anything particularly damning there. The only nvidia related ones are related to loading, and some DRM bits.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m3cfy9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3cfy9/looking_for_help_with_terrible_vllm_performance/n3xif36/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752892855,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3wllxi","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"kevin_1994","can_mod_post":false,"created_utc":1752880721,"send_replies":true,"parent_id":"t3_1m3cfy9","score":1,"author_fullname":"t2_o015g","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"This happened to me before and was caused by interference lol. Anything in dmesg? Im on a phone but try sudo dmesg | grep nvidia","edited":1752881442,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3wllxi","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;This happened to me before and was caused by interference lol. Anything in dmesg? Im on a phone but try sudo dmesg | grep nvidia&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":true,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3cfy9/looking_for_help_with_terrible_vllm_performance/n3wllxi/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752880721,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m3cfy9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3wxobz","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Conscious_Cut_6144","can_mod_post":false,"created_utc":1752884973,"send_replies":true,"parent_id":"t3_1m3cfy9","score":1,"author_fullname":"t2_9hl4ymvj","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I would try starting fresh with older drivers, an older vllm and an older model.  \\n  \\nIf you have a spare ssd swap it in and start completely fresh with a new ubuntu.    \\napt install nvidia-driver-550    \\napt install nvidia-cuda-toolkit    \\npip install vllm==0.9.0    \\nvllm serve RedHatAI/Llama-3.3-70B-Instruct-FP8-dynamic -tp4 -max model length 4000  \\n\\nIf none of that fixes it you are most likely looking at a hardware issue.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3wxobz","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I would try starting fresh with older drivers, an older vllm and an older model.  &lt;/p&gt;\\n\\n&lt;p&gt;If you have a spare ssd swap it in and start completely fresh with a new ubuntu.&lt;br/&gt;\\napt install nvidia-driver-550&lt;br/&gt;\\napt install nvidia-cuda-toolkit&lt;br/&gt;\\npip install vllm==0.9.0&lt;br/&gt;\\nvllm serve RedHatAI/Llama-3.3-70B-Instruct-FP8-dynamic -tp4 -max model length 4000  &lt;/p&gt;\\n\\n&lt;p&gt;If none of that fixes it you are most likely looking at a hardware issue.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3cfy9/looking_for_help_with_terrible_vllm_performance/n3wxobz/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752884973,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m3cfy9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n41crll","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"kmouratidis","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3vne9e","score":1,"author_fullname":"t2_k6u7rfxb","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"You *should* be able to easily offload to CPU with vLLM (I think \`--cpu-offload-gb\`, ~~but might be confusing it with sglang~~ https://docs.vllm.ai/en/latest/configuration/engine_args.html#cacheconfig), but I'm pretty sure it doesn't work like llama.cpp where it partly does CPU inferencing too. But it definitely does not offload to CPU on it's own, it will throw OOMs by default.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n41crll","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You &lt;em&gt;should&lt;/em&gt; be able to easily offload to CPU with vLLM (I think &lt;code&gt;--cpu-offload-gb&lt;/code&gt;, &lt;del&gt;but might be confusing it with sglang&lt;/del&gt; &lt;a href=\\"https://docs.vllm.ai/en/latest/configuration/engine_args.html#cacheconfig\\"&gt;https://docs.vllm.ai/en/latest/configuration/engine_args.html#cacheconfig&lt;/a&gt;), but I&amp;#39;m pretty sure it doesn&amp;#39;t work like llama.cpp where it partly does CPU inferencing too. But it definitely does not offload to CPU on it&amp;#39;s own, it will throw OOMs by default.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m3cfy9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3cfy9/looking_for_help_with_terrible_vllm_performance/n41crll/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752949971,"author_flair_text":null,"treatment_tags":[],"created_utc":1752949971,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3vne9e","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Render_Arcana","can_mod_post":false,"created_utc":1752869775,"send_replies":true,"parent_id":"t1_n3vm5cp","score":2,"author_fullname":"t2_1mzgsryavd","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I mentioned the model, but to break it out:\\n\\nI'm running the 22b param devstral at bf16, which fits pretty comfortably across the 4 cards with about 200k total tokens for cache. If vllm is for some reason deciding to offload to CPU in this situation it would be a surprise to me (and I was under the impression you had to jump through some hoops to get vllm to offload at all).\\n\\nAs far as I'm aware, it isn't offloading to disk, given relatively low disk IO, but there is the one process running high CPU, which I would believe is doing something it shouldn't. The workstation has \\\\~500gb of ram, so any offloading should stay there, although even offloading to ram I wouldn't expect it to be quite that slow. Not to mention, on almost identical hardware (sans lack of nvlink and it being a shared system, both of which should mean better performance on the workstation) I'm getting the 100+ token/s I was kind of expecting.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3vne9e","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I mentioned the model, but to break it out:&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;m running the 22b param devstral at bf16, which fits pretty comfortably across the 4 cards with about 200k total tokens for cache. If vllm is for some reason deciding to offload to CPU in this situation it would be a surprise to me (and I was under the impression you had to jump through some hoops to get vllm to offload at all).&lt;/p&gt;\\n\\n&lt;p&gt;As far as I&amp;#39;m aware, it isn&amp;#39;t offloading to disk, given relatively low disk IO, but there is the one process running high CPU, which I would believe is doing something it shouldn&amp;#39;t. The workstation has ~500gb of ram, so any offloading should stay there, although even offloading to ram I wouldn&amp;#39;t expect it to be quite that slow. Not to mention, on almost identical hardware (sans lack of nvlink and it being a shared system, both of which should mean better performance on the workstation) I&amp;#39;m getting the 100+ token/s I was kind of expecting.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m3cfy9","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3cfy9/looking_for_help_with_terrible_vllm_performance/n3vne9e/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752869775,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n3vm5cp","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"GatePorters","can_mod_post":false,"created_utc":1752869408,"send_replies":true,"parent_id":"t3_1m3cfy9","score":0,"author_fullname":"t2_vryl95sg1","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"You didn’t mention how everything is fitting. \\n\\n1.) what model are you using\\n\\n2.) how much VRAm does each card have\\n\\nNVLink allows you to use the compute from multiple cards, but you are still limited by the VRAM of one card for fastest inference without offloading. \\n\\n\\nIf CPU and Disk are high, you are offloading. The reason why it would be slow in this case would be because your system can’t feed your GPU quickly enough. \\n\\n————\\n\\nI may not be able to answer you, but you answering my questions will assist others in answering you as well.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3vm5cp","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You didn’t mention how everything is fitting. &lt;/p&gt;\\n\\n&lt;p&gt;1.) what model are you using&lt;/p&gt;\\n\\n&lt;p&gt;2.) how much VRAm does each card have&lt;/p&gt;\\n\\n&lt;p&gt;NVLink allows you to use the compute from multiple cards, but you are still limited by the VRAM of one card for fastest inference without offloading. &lt;/p&gt;\\n\\n&lt;p&gt;If CPU and Disk are high, you are offloading. The reason why it would be slow in this case would be because your system can’t feed your GPU quickly enough. &lt;/p&gt;\\n\\n&lt;p&gt;————&lt;/p&gt;\\n\\n&lt;p&gt;I may not be able to answer you, but you answering my questions will assist others in answering you as well.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3cfy9/looking_for_help_with_terrible_vllm_performance/n3vm5cp/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752869408,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m3cfy9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n40ty3n","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"jaMMint","can_mod_post":false,"created_utc":1752944162,"send_replies":true,"parent_id":"t3_1m3cfy9","score":0,"author_fullname":"t2_s4p1j","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"For what it's worth, I tried this model (mistralai/devstral-small-2507, 4Q_K_M quant @ 14,33GB size) on a RTX 6000 Pro, DDR5 6400, and Ultra 9 285K in vanilla LM Studio and got 80t/s on a simple prompt and small context.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n40ty3n","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;For what it&amp;#39;s worth, I tried this model (mistralai/devstral-small-2507, 4Q_K_M quant @ 14,33GB size) on a RTX 6000 Pro, DDR5 6400, and Ultra 9 285K in vanilla LM Studio and got 80t/s on a simple prompt and small context.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3cfy9/looking_for_help_with_terrible_vllm_performance/n40ty3n/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752944162,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m3cfy9","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}}]`),o=()=>e.jsx(t,{data:l});export{o as default};
