import{j as e}from"./index-xfnGEtuL.js";import{R as t}from"./RedditPostRenderer-DAZRIZZK.js";import"./index-BaNn5-RR.js";const l=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Just sharing my new madness, really, not much to say about it, as its very early. \\n\\nSo the idea is very simple lets have an LLM engine that can run relatively large size models on constrained hardware.\\n\\n**So what it is (or going to be if I don't disappear into the abyss):**\\n\\nStarted hacking on this today. \\n\\nA **pure C (or I try to keep it that way)** runtime for LLaMA models, built using llama.cpp and my own ideas for tiny devices, embedded systems, and portability-first scenarios.\\n\\nSo let me introduce \`llamac-lab\`, a work-in-progress open-source runtime for LLaMA-based models.\\n\\nThink llama.cpp, but:\\n\\n* Flattened into straight-up C (no C++ or STL baggage)\\n* Optimized for **minimal memory use**\\n* Dead simple to embed into **any stack** (Rust, Python, or LUA or anything else that can interface with C)\\n* Born for **edge devices**, MCUs, and other weird places LLMs don't usually go\\n\\nI'll work on some fun stuff as well, like adapting and converting large LLMs (as long as licensing allows) to this specialised runtime.\\n\\nNote: It’s super early. No model loading yet. No inference. Just early scaffolding and dreams.  \\nBut if you're into LLMs, embedded stuff, or like watching weird low-level projects grow - follow along or contribute!\\n\\nRepo: \\\\[llamac\\\\](https://github.com/llamac-lab/llamac)\\n\\nNote: about the flair, could not decide between generation or new model so I went with new model. More like new runtime.","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"New OSS project: llamac-lab or a pure C runtime for LLaMA models, made for the edge","link_flair_richtext":[{"e":"text","t":"New Model"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lx4ya7","quarantine":false,"link_flair_text_color":"light","upvote_ratio":1,"author_flair_background_color":null,"subreddit_type":"public","ups":13,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_1q37ljt3oe","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"New Model","can_mod_post":false,"score":13,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"post_hint":"self","content_categories":null,"is_self":true,"mod_note":null,"created":1752234990,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Just sharing my new madness, really, not much to say about it, as its very early. &lt;/p&gt;\\n\\n&lt;p&gt;So the idea is very simple lets have an LLM engine that can run relatively large size models on constrained hardware.&lt;/p&gt;\\n\\n&lt;p&gt;&lt;strong&gt;So what it is (or going to be if I don&amp;#39;t disappear into the abyss):&lt;/strong&gt;&lt;/p&gt;\\n\\n&lt;p&gt;Started hacking on this today. &lt;/p&gt;\\n\\n&lt;p&gt;A &lt;strong&gt;pure C (or I try to keep it that way)&lt;/strong&gt; runtime for LLaMA models, built using llama.cpp and my own ideas for tiny devices, embedded systems, and portability-first scenarios.&lt;/p&gt;\\n\\n&lt;p&gt;So let me introduce &lt;code&gt;llamac-lab&lt;/code&gt;, a work-in-progress open-source runtime for LLaMA-based models.&lt;/p&gt;\\n\\n&lt;p&gt;Think llama.cpp, but:&lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;Flattened into straight-up C (no C++ or STL baggage)&lt;/li&gt;\\n&lt;li&gt;Optimized for &lt;strong&gt;minimal memory use&lt;/strong&gt;&lt;/li&gt;\\n&lt;li&gt;Dead simple to embed into &lt;strong&gt;any stack&lt;/strong&gt; (Rust, Python, or LUA or anything else that can interface with C)&lt;/li&gt;\\n&lt;li&gt;Born for &lt;strong&gt;edge devices&lt;/strong&gt;, MCUs, and other weird places LLMs don&amp;#39;t usually go&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;p&gt;I&amp;#39;ll work on some fun stuff as well, like adapting and converting large LLMs (as long as licensing allows) to this specialised runtime.&lt;/p&gt;\\n\\n&lt;p&gt;Note: It’s super early. No model loading yet. No inference. Just early scaffolding and dreams.&lt;br/&gt;\\nBut if you&amp;#39;re into LLMs, embedded stuff, or like watching weird low-level projects grow - follow along or contribute!&lt;/p&gt;\\n\\n&lt;p&gt;Repo: [llamac](&lt;a href=\\"https://github.com/llamac-lab/llamac\\"&gt;https://github.com/llamac-lab/llamac&lt;/a&gt;)&lt;/p&gt;\\n\\n&lt;p&gt;Note: about the flair, could not decide between generation or new model so I went with new model. More like new runtime.&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"preview":{"images":[{"source":{"url":"https://external-preview.redd.it/lWjJdcJe5yyBPoySwGyf4ZMSAVXJpSrM43u2nmieDUY.png?auto=webp&amp;s=a3a05a15a5a23d6cd68aef22d033ab8f79da092f","width":1200,"height":600},"resolutions":[{"url":"https://external-preview.redd.it/lWjJdcJe5yyBPoySwGyf4ZMSAVXJpSrM43u2nmieDUY.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=843e60c1255f40c94aa5aef6086d2d53756bb490","width":108,"height":54},{"url":"https://external-preview.redd.it/lWjJdcJe5yyBPoySwGyf4ZMSAVXJpSrM43u2nmieDUY.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=c613caf7eaf7d782e6415ed96da5eabc213f1b98","width":216,"height":108},{"url":"https://external-preview.redd.it/lWjJdcJe5yyBPoySwGyf4ZMSAVXJpSrM43u2nmieDUY.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=81405381b463f0028759d01f0f39523a952042da","width":320,"height":160},{"url":"https://external-preview.redd.it/lWjJdcJe5yyBPoySwGyf4ZMSAVXJpSrM43u2nmieDUY.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=bc02ed772d7763b4c227291c7397637b3ad1206a","width":640,"height":320},{"url":"https://external-preview.redd.it/lWjJdcJe5yyBPoySwGyf4ZMSAVXJpSrM43u2nmieDUY.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=08bbc9018399efc936b038e156cf8bdfcfab3b48","width":960,"height":480},{"url":"https://external-preview.redd.it/lWjJdcJe5yyBPoySwGyf4ZMSAVXJpSrM43u2nmieDUY.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f9a769d0a1801bd9c73f0459564a58881d8821b9","width":1080,"height":540}],"variants":{},"id":"lWjJdcJe5yyBPoySwGyf4ZMSAVXJpSrM43u2nmieDUY"}],"enabled":false},"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"ced98442-f5d3-11ed-b657-66d3b15490c6","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#ffb000","id":"1lx4ya7","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"rvnllm","discussion_type":null,"num_comments":1,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lx4ya7/new_oss_project_llamaclab_or_a_pure_c_runtime_for/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lx4ya7/new_oss_project_llamaclab_or_a_pure_c_runtime_for/","subreddit_subscribers":497826,"created_utc":1752234990,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2jjzzk","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Evening_Ad6637","can_mod_post":false,"created_utc":1752239057,"send_replies":true,"parent_id":"t3_1lx4ya7","score":2,"author_fullname":"t2_p45er6oo","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Great! Thanks for sharing 👍","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2jjzzk","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Great! Thanks for sharing 👍&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lx4ya7/new_oss_project_llamaclab_or_a_pure_c_runtime_for/n2jjzzk/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752239057,"author_flair_text":"llama.cpp","treatment_tags":[],"link_id":"t3_1lx4ya7","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}}]`),o=()=>e.jsx(t,{data:l});export{o as default};
