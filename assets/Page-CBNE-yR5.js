import{j as e}from"./index-Dh2YTDbC.js";import{R as t}from"./RedditPostRenderer-BwWe7STC.js";import"./index-D7FMfiLd.js";const l=[{kind:"Listing",data:{after:null,dist:1,modhash:"",geo_filter:"",children:[{kind:"t3",data:{approved_at_utc:null,subreddit:"LocalLLaMA",selftext:`Hey folks,

I’m trying to build a local LLM that can work offline on a phone, mainly for educational purposes — like helping students with concepts, solving problems step by step, and answering basic academic questions (school or early college level).

I’m planning to fine-tune a smaller model like Phi-2, Mistral 7B, or maybe Qwen 1.5 (4B or 7B). My final goal is to run this model **completely offline** on a phone using something like llama.cpp.

So I need help with two things:

1. **Good educational datasets** – any open datasets you know of for instruction-style Q&amp;A or tutoring? Preferably stuff that’s already in a good format for fine-tuning.
2. **Model suggestions + mobile performance** – I want to use a model that won’t make my phone overheat or lag too much. I’ve heard about 4-bit quantized models (GGUF) — but which ones actually run well on phones?

Also, are there any common things to watch out for to avoid performance issues? Like:

* Which quantization type is best for smooth performance (e.g., Q4\\_K\\_M or Q6\\_K)?
* What thread settings or tweaks help reduce heat or battery drain?
* Should I go with 3B models instead of 7B for better efficiency?

Would really appreciate any tips or your own experience if you’ve tried this already. I’m still figuring it out so anything helps.

Thanks!`,user_reports:[],saved:!1,mod_reason_title:null,gilded:0,clicked:!1,title:"Need help finding educational datasets and model suggestions for offline LLM on phone",link_flair_richtext:[{e:"text",t:"Question | Help"}],subreddit_name_prefixed:"r/LocalLLaMA",hidden:!1,pwls:6,link_flair_css_class:"",downs:0,thumbnail_height:null,top_awarded_type:null,hide_score:!1,name:"t3_1lox9c4",quarantine:!1,link_flair_text_color:"dark",upvote_ratio:1,author_flair_background_color:null,subreddit_type:"public",ups:2,total_awards_received:0,media_embed:{},thumbnail_width:null,author_flair_template_id:null,is_original_content:!1,author_fullname:"t2_6sy4ogbwu",secure_media:null,is_reddit_media_domain:!1,is_meta:!1,category:null,secure_media_embed:{},link_flair_text:"Question | Help",can_mod_post:!1,score:2,approved_by:null,is_created_from_ads_ui:!1,author_premium:!1,thumbnail:"self",edited:!1,author_flair_css_class:null,author_flair_richtext:[],gildings:{},content_categories:null,is_self:!0,mod_note:null,created:1751362245,link_flair_type:"richtext",wls:6,removed_by_category:null,banned_by:null,author_flair_type:"text",domain:"self.LocalLLaMA",allow_live_comments:!1,selftext_html:`&lt;!-- SC_OFF --&gt;&lt;div class="md"&gt;&lt;p&gt;Hey folks,&lt;/p&gt;

&lt;p&gt;I’m trying to build a local LLM that can work offline on a phone, mainly for educational purposes — like helping students with concepts, solving problems step by step, and answering basic academic questions (school or early college level).&lt;/p&gt;

&lt;p&gt;I’m planning to fine-tune a smaller model like Phi-2, Mistral 7B, or maybe Qwen 1.5 (4B or 7B). My final goal is to run this model &lt;strong&gt;completely offline&lt;/strong&gt; on a phone using something like llama.cpp.&lt;/p&gt;

&lt;p&gt;So I need help with two things:&lt;/p&gt;

&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;Good educational datasets&lt;/strong&gt; – any open datasets you know of for instruction-style Q&amp;amp;A or tutoring? Preferably stuff that’s already in a good format for fine-tuning.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Model suggestions + mobile performance&lt;/strong&gt; – I want to use a model that won’t make my phone overheat or lag too much. I’ve heard about 4-bit quantized models (GGUF) — but which ones actually run well on phones?&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Also, are there any common things to watch out for to avoid performance issues? Like:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Which quantization type is best for smooth performance (e.g., Q4_K_M or Q6_K)?&lt;/li&gt;
&lt;li&gt;What thread settings or tweaks help reduce heat or battery drain?&lt;/li&gt;
&lt;li&gt;Should I go with 3B models instead of 7B for better efficiency?&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Would really appreciate any tips or your own experience if you’ve tried this already. I’m still figuring it out so anything helps.&lt;/p&gt;

&lt;p&gt;Thanks!&lt;/p&gt;
&lt;/div&gt;&lt;!-- SC_ON --&gt;`,likes:null,suggested_sort:null,banned_at_utc:null,view_count:null,archived:!1,no_follow:!1,is_crosspostable:!1,pinned:!1,over_18:!1,all_awardings:[],awarders:[],media_only:!1,link_flair_template_id:"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1",can_gild:!1,spoiler:!1,locked:!1,author_flair_text:null,treatment_tags:[],visited:!1,removed_by:null,num_reports:null,distinguished:null,subreddit_id:"t5_81eyvm",author_is_blocked:!1,mod_reason_by:null,removal_reason:null,link_flair_background_color:"#5a74cc",id:"1lox9c4",is_robot_indexable:!0,num_duplicates:0,report_reasons:null,author:"Phantomx_77",discussion_type:null,num_comments:0,send_replies:!0,media:null,contest_mode:!1,author_patreon_flair:!1,author_flair_text_color:null,permalink:"/r/LocalLLaMA/comments/1lox9c4/need_help_finding_educational_datasets_and_model/",stickied:!1,url:"https://www.reddit.com/r/LocalLLaMA/comments/1lox9c4/need_help_finding_educational_datasets_and_model/",subreddit_subscribers:493458,created_utc:1751362245,num_crossposts:0,mod_reports:[],is_video:!1}}],before:null}},{kind:"Listing",data:{after:null,dist:null,modhash:"",geo_filter:"",children:[],before:null}}],i=()=>e.jsx(t,{data:l});export{i as default};
