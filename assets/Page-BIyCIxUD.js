import{j as e}from"./index-CWmJdUH_.js";import{R as t}from"./RedditPostRenderer-D2iunoQ9.js";import"./index-BCg9RP6g.js";const l=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"A while back I posted some [Strix Halo LLM performance testing](https://www.reddit.com/r/LocalLLaMA/comments/1kmi3ra/amd_strix_halo_ryzen_ai_max_395_gpu_llm/) benchmarks. I'm back with an update that I believe is actually a fair bit more comprehensive now (although the original is still worth checking out for background).\\n\\nThe biggest difference is I wrote some automated sweeps to test different backends and flags against a full range of pp/tg on many different model architectures (including the latest MoEs) and sizes.\\n\\nThis is also using the latest drivers, ROCm (7.0 nightlies), and llama.cpp \\n\\nAll the full data and latest info is available in the Github repo: [https://github.com/lhl/strix-halo-testing/tree/main/llm-bench](https://github.com/lhl/strix-halo-testing/tree/main/llm-bench) but here are the topline stats below:\\n\\n# Strix Halo LLM Benchmark Results\\n\\nAll testing was done on pre-production [Framework Desktop](https://frame.work/desktop) systems with an AMD Ryzen Max+ 395 (Strix Halo)/128GB LPDDR5x-8000 configuration. (Thanks Nirav, Alexandru, and co!)\\n\\nExact testing/system details are in the results folders, but roughly these are running:\\n\\n* Close to production BIOS/EC\\n* Relatively up-to-date kernels: 6.15.5-arch1-1/6.15.6-arch1-1\\n* Recent TheRock/ROCm-7.0 nightly builds with Strix Halo (gfx1151) kernels\\n* Recent llama.cpp builds (eg b5863 from 2005-07-10)\\n\\nJust to get a ballpark on the hardware:\\n\\n* \\\\~215 GB/s max GPU MBW out of a 256 GB/s theoretical (256-bit 8000 MT/s)\\n* theoretical 59 FP16 TFLOPS (VPOD/WMMA) on RDNA 3.5 (gfx11); effective is *much* lower\\n\\n# Results\\n\\n# Prompt Processing (pp) Performance\\n\\nhttps://preview.redd.it/mjr2d31ujeef1.png?width=1782&amp;format=png&amp;auto=webp&amp;s=850201c7bcca2bb14085e2aa139105ffbdd5bc5f\\n\\n|Model Name|Architecture|Weights (B)|Active (B)|Backend|Flags|pp512|tg128|Memory (Max MiB)|\\n|:-|:-|:-|:-|:-|:-|:-|:-|:-|\\n|Llama 2 7B Q4\\\\_0|Llama 2|7|7|Vulkan||998.0|46.5|4237|\\n|Llama 2 7B Q4\\\\_K\\\\_M|Llama 2|7|7|HIP|hipBLASLt|906.1|40.8|4720|\\n|Shisa V2 8B i1-Q4\\\\_K\\\\_M|Llama 3|8|8|HIP|hipBLASLt|878.2|37.2|5308|\\n|Qwen 3 30B-A3B UD-Q4\\\\_K\\\\_XL|Qwen 3 MoE|30|3|Vulkan|fa=1|604.8|66.3|17527|\\n|Mistral Small 3.1 UD-Q4\\\\_K\\\\_XL|Mistral 3|24|24|HIP|hipBLASLt|316.9|13.6|14638|\\n|Hunyuan-A13B UD-Q6\\\\_K\\\\_XL|Hunyuan MoE|80|13|Vulkan|fa=1|270.5|17.1|68785|\\n|Llama 4 Scout UD-Q4\\\\_K\\\\_XL|Llama 4 MoE|109|17|HIP|hipBLASLt|264.1|17.2|59720|\\n|Shisa V2 70B i1-Q4\\\\_K\\\\_M|Llama 3|70|70|HIP rocWMMA||94.7|4.5|41522|\\n|dots1 UD-Q4\\\\_K\\\\_XL|dots1 MoE|142|14|Vulkan|fa=1 b=256|63.1|20.6|84077|\\n\\n# Text Generation (tg) Performance\\n\\nhttps://preview.redd.it/7y0pdbqujeef1.png?width=1782&amp;format=png&amp;auto=webp&amp;s=1ba61feb31fa21953a7e5df5b1072187c3c1bdd7\\n\\n|Model Name|Architecture|Weights (B)|Active (B)|Backend|Flags|pp512|tg128|Memory (Max MiB)|\\n|:-|:-|:-|:-|:-|:-|:-|:-|:-|\\n|Qwen 3 30B-A3B UD-Q4\\\\_K\\\\_XL|Qwen 3 MoE|30|3|Vulkan|b=256|591.1|72.0|17377|\\n|Llama 2 7B Q4\\\\_K\\\\_M|Llama 2|7|7|Vulkan|fa=1|620.9|47.9|4463|\\n|Llama 2 7B Q4\\\\_0|Llama 2|7|7|Vulkan|fa=1|1014.1|45.8|4219|\\n|Shisa V2 8B i1-Q4\\\\_K\\\\_M|Llama 3|8|8|Vulkan|fa=1|614.2|42.0|5333|\\n|dots1 UD-Q4\\\\_K\\\\_XL|dots1 MoE|142|14|Vulkan|fa=1 b=256|63.1|20.6|84077|\\n|Llama 4 Scout UD-Q4\\\\_K\\\\_XL|Llama 4 MoE|109|17|Vulkan|fa=1 b=256|146.1|19.3|59917|\\n|Hunyuan-A13B UD-Q6\\\\_K\\\\_XL|Hunyuan MoE|80|13|Vulkan|fa=1 b=256|223.9|17.1|68608|\\n|Mistral Small 3.1 UD-Q4\\\\_K\\\\_XL|Mistral 3|24|24|Vulkan|fa=1|119.6|14.3|14540|\\n|Shisa V2 70B i1-Q4\\\\_K\\\\_M|Llama 3|70|70|Vulkan|fa=1|26.4|5.0|41456|\\n\\n# Testing Notes\\n\\nThe best overall backend and flags were chosen for each model family tested. You can see that often times the best backend for prefill vs token generation differ. Full results for each model (including the pp/tg graphs for different context lengths for all tested backend variations) are available for review in their respective folders as which backend is the best performing will depend on your exact use-case.\\n\\nThere's a lot of performance still on the table when it comes to pp especially. Since these results should be close to optimal for when they were tested, I might add dates to the table  (adding kernel, ROCm, and llama.cpp build#'s might be a bit much).\\n\\nOne thing worth pointing out is that pp has improved significantly on some models since I last tested. For example, back in May, pp512 for Qwen3 30B-A3B was 119 t/s (Vulkan) and it's now 605 t/s. Similarly, Llama 4 Scout has a pp512 of 103 t/s, and is now 173 t/s, although the HIP backend is significantly faster at 264 t/s.\\n\\nUnlike last time, I won't be taking any model testing requests as these sweeps take quite a while to run - I feel like there are enough 395 systems out there now and the repo linked at top includes the full scripts to allow anyone to replicate (and can be easily adapted for other backends or to run with different hardware).\\n\\nFor testing, the HIP backend, I highly recommend trying \`ROCBLAS_USE_HIPBLASLT=1\` as that is almost always faster than the default rocBLAS. If you are OK with occasionally hitting the reboot switch, you might also want to test in combination with (as long as you have the gfx1100 kernels installed) \`HSA_OVERRIDE_GFX_VERSION=11.0.0\` \\\\- in prior testing I've found the gfx1100 kernels to be up 2X faster than gfx1151 kernels... ðŸ¤”","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Updated Strix Halo (Ryzen AI Max+ 395) LLM Benchmark Results","link_flair_richtext":[{"e":"text","t":"Resources"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":92,"top_awarded_type":null,"hide_score":false,"media_metadata":{"mjr2d31ujeef1":{"status":"valid","e":"Image","m":"image/png","p":[{"y":71,"x":108,"u":"https://preview.redd.it/mjr2d31ujeef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=80836e3346fcd0e6847fcb3f1d33c5f2ac3c12e3"},{"y":143,"x":216,"u":"https://preview.redd.it/mjr2d31ujeef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=5cd34de136a42ad65ac9facd37455912c1a13410"},{"y":212,"x":320,"u":"https://preview.redd.it/mjr2d31ujeef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=675c17530bfdbbd8ae2830c3b694e07b5163a1b0"},{"y":424,"x":640,"u":"https://preview.redd.it/mjr2d31ujeef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=6bdc9a2d7260d9fd3acbab23e12917b54e651493"},{"y":636,"x":960,"u":"https://preview.redd.it/mjr2d31ujeef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=7e051180cc70357e18ffe4725601fb9811ba1193"},{"y":715,"x":1080,"u":"https://preview.redd.it/mjr2d31ujeef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=75e938dac2457cc4191363a5cd8cbcaf777049ed"}],"s":{"y":1181,"x":1782,"u":"https://preview.redd.it/mjr2d31ujeef1.png?width=1782&amp;format=png&amp;auto=webp&amp;s=850201c7bcca2bb14085e2aa139105ffbdd5bc5f"},"id":"mjr2d31ujeef1"},"7y0pdbqujeef1":{"status":"valid","e":"Image","m":"image/png","p":[{"y":71,"x":108,"u":"https://preview.redd.it/7y0pdbqujeef1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=493fe8a11d9ee1d7a485e1b7233ca7e945637599"},{"y":143,"x":216,"u":"https://preview.redd.it/7y0pdbqujeef1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=c43022989f8b24ef64216d0b17642e3f80013763"},{"y":212,"x":320,"u":"https://preview.redd.it/7y0pdbqujeef1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=f7f683ec0155e0b8f196a71bccaf428b44550399"},{"y":424,"x":640,"u":"https://preview.redd.it/7y0pdbqujeef1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=bb65c24153d597ae592f5012aabff2a638b88357"},{"y":636,"x":960,"u":"https://preview.redd.it/7y0pdbqujeef1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=f57d68c01ba672fba3a5062a2ec12ac45a621fac"},{"y":715,"x":1080,"u":"https://preview.redd.it/7y0pdbqujeef1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=59b563c8992d2568941d7c73829b8f7ebc4f5585"}],"s":{"y":1181,"x":1782,"u":"https://preview.redd.it/7y0pdbqujeef1.png?width=1782&amp;format=png&amp;auto=webp&amp;s=1ba61feb31fa21953a7e5df5b1072187c3c1bdd7"},"id":"7y0pdbqujeef1"}},"name":"t3_1m6b151","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.99,"author_flair_background_color":null,"subreddit_type":"public","ups":81,"total_awards_received":0,"media_embed":{},"thumbnail_width":140,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_eztox","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Resources","can_mod_post":false,"score":81,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"https://b.thumbs.redditmedia.com/iZq9ApFg7F044Ny8obqZ27FfndXjE_7xNkH5oORO2gc.jpg","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1753182004,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;A while back I posted some &lt;a href=\\"https://www.reddit.com/r/LocalLLaMA/comments/1kmi3ra/amd_strix_halo_ryzen_ai_max_395_gpu_llm/\\"&gt;Strix Halo LLM performance testing&lt;/a&gt; benchmarks. I&amp;#39;m back with an update that I believe is actually a fair bit more comprehensive now (although the original is still worth checking out for background).&lt;/p&gt;\\n\\n&lt;p&gt;The biggest difference is I wrote some automated sweeps to test different backends and flags against a full range of pp/tg on many different model architectures (including the latest MoEs) and sizes.&lt;/p&gt;\\n\\n&lt;p&gt;This is also using the latest drivers, ROCm (7.0 nightlies), and llama.cpp &lt;/p&gt;\\n\\n&lt;p&gt;All the full data and latest info is available in the Github repo: &lt;a href=\\"https://github.com/lhl/strix-halo-testing/tree/main/llm-bench\\"&gt;https://github.com/lhl/strix-halo-testing/tree/main/llm-bench&lt;/a&gt; but here are the topline stats below:&lt;/p&gt;\\n\\n&lt;h1&gt;Strix Halo LLM Benchmark Results&lt;/h1&gt;\\n\\n&lt;p&gt;All testing was done on pre-production &lt;a href=\\"https://frame.work/desktop\\"&gt;Framework Desktop&lt;/a&gt; systems with an AMD Ryzen Max+ 395 (Strix Halo)/128GB LPDDR5x-8000 configuration. (Thanks Nirav, Alexandru, and co!)&lt;/p&gt;\\n\\n&lt;p&gt;Exact testing/system details are in the results folders, but roughly these are running:&lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;Close to production BIOS/EC&lt;/li&gt;\\n&lt;li&gt;Relatively up-to-date kernels: 6.15.5-arch1-1/6.15.6-arch1-1&lt;/li&gt;\\n&lt;li&gt;Recent TheRock/ROCm-7.0 nightly builds with Strix Halo (gfx1151) kernels&lt;/li&gt;\\n&lt;li&gt;Recent llama.cpp builds (eg b5863 from 2005-07-10)&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;p&gt;Just to get a ballpark on the hardware:&lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;~215 GB/s max GPU MBW out of a 256 GB/s theoretical (256-bit 8000 MT/s)&lt;/li&gt;\\n&lt;li&gt;theoretical 59 FP16 TFLOPS (VPOD/WMMA) on RDNA 3.5 (gfx11); effective is &lt;em&gt;much&lt;/em&gt; lower&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;h1&gt;Results&lt;/h1&gt;\\n\\n&lt;h1&gt;Prompt Processing (pp) Performance&lt;/h1&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://preview.redd.it/mjr2d31ujeef1.png?width=1782&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=850201c7bcca2bb14085e2aa139105ffbdd5bc5f\\"&gt;https://preview.redd.it/mjr2d31ujeef1.png?width=1782&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=850201c7bcca2bb14085e2aa139105ffbdd5bc5f&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;table&gt;&lt;thead&gt;\\n&lt;tr&gt;\\n&lt;th align=\\"left\\"&gt;Model Name&lt;/th&gt;\\n&lt;th align=\\"left\\"&gt;Architecture&lt;/th&gt;\\n&lt;th align=\\"left\\"&gt;Weights (B)&lt;/th&gt;\\n&lt;th align=\\"left\\"&gt;Active (B)&lt;/th&gt;\\n&lt;th align=\\"left\\"&gt;Backend&lt;/th&gt;\\n&lt;th align=\\"left\\"&gt;Flags&lt;/th&gt;\\n&lt;th align=\\"left\\"&gt;pp512&lt;/th&gt;\\n&lt;th align=\\"left\\"&gt;tg128&lt;/th&gt;\\n&lt;th align=\\"left\\"&gt;Memory (Max MiB)&lt;/th&gt;\\n&lt;/tr&gt;\\n&lt;/thead&gt;&lt;tbody&gt;\\n&lt;tr&gt;\\n&lt;td align=\\"left\\"&gt;Llama 2 7B Q4_0&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;Llama 2&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;7&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;7&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;Vulkan&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;998.0&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;46.5&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;4237&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;tr&gt;\\n&lt;td align=\\"left\\"&gt;Llama 2 7B Q4_K_M&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;Llama 2&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;7&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;7&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;HIP&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;hipBLASLt&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;906.1&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;40.8&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;4720&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;tr&gt;\\n&lt;td align=\\"left\\"&gt;Shisa V2 8B i1-Q4_K_M&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;Llama 3&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;8&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;8&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;HIP&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;hipBLASLt&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;878.2&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;37.2&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;5308&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;tr&gt;\\n&lt;td align=\\"left\\"&gt;Qwen 3 30B-A3B UD-Q4_K_XL&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;Qwen 3 MoE&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;30&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;3&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;Vulkan&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;fa=1&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;604.8&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;66.3&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;17527&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;tr&gt;\\n&lt;td align=\\"left\\"&gt;Mistral Small 3.1 UD-Q4_K_XL&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;Mistral 3&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;24&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;24&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;HIP&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;hipBLASLt&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;316.9&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;13.6&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;14638&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;tr&gt;\\n&lt;td align=\\"left\\"&gt;Hunyuan-A13B UD-Q6_K_XL&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;Hunyuan MoE&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;80&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;13&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;Vulkan&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;fa=1&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;270.5&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;17.1&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;68785&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;tr&gt;\\n&lt;td align=\\"left\\"&gt;Llama 4 Scout UD-Q4_K_XL&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;Llama 4 MoE&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;109&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;17&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;HIP&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;hipBLASLt&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;264.1&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;17.2&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;59720&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;tr&gt;\\n&lt;td align=\\"left\\"&gt;Shisa V2 70B i1-Q4_K_M&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;Llama 3&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;70&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;70&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;HIP rocWMMA&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;94.7&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;4.5&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;41522&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;tr&gt;\\n&lt;td align=\\"left\\"&gt;dots1 UD-Q4_K_XL&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;dots1 MoE&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;142&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;14&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;Vulkan&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;fa=1 b=256&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;63.1&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;20.6&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;84077&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;/tbody&gt;&lt;/table&gt;\\n\\n&lt;h1&gt;Text Generation (tg) Performance&lt;/h1&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://preview.redd.it/7y0pdbqujeef1.png?width=1782&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1ba61feb31fa21953a7e5df5b1072187c3c1bdd7\\"&gt;https://preview.redd.it/7y0pdbqujeef1.png?width=1782&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1ba61feb31fa21953a7e5df5b1072187c3c1bdd7&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;table&gt;&lt;thead&gt;\\n&lt;tr&gt;\\n&lt;th align=\\"left\\"&gt;Model Name&lt;/th&gt;\\n&lt;th align=\\"left\\"&gt;Architecture&lt;/th&gt;\\n&lt;th align=\\"left\\"&gt;Weights (B)&lt;/th&gt;\\n&lt;th align=\\"left\\"&gt;Active (B)&lt;/th&gt;\\n&lt;th align=\\"left\\"&gt;Backend&lt;/th&gt;\\n&lt;th align=\\"left\\"&gt;Flags&lt;/th&gt;\\n&lt;th align=\\"left\\"&gt;pp512&lt;/th&gt;\\n&lt;th align=\\"left\\"&gt;tg128&lt;/th&gt;\\n&lt;th align=\\"left\\"&gt;Memory (Max MiB)&lt;/th&gt;\\n&lt;/tr&gt;\\n&lt;/thead&gt;&lt;tbody&gt;\\n&lt;tr&gt;\\n&lt;td align=\\"left\\"&gt;Qwen 3 30B-A3B UD-Q4_K_XL&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;Qwen 3 MoE&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;30&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;3&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;Vulkan&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;b=256&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;591.1&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;72.0&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;17377&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;tr&gt;\\n&lt;td align=\\"left\\"&gt;Llama 2 7B Q4_K_M&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;Llama 2&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;7&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;7&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;Vulkan&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;fa=1&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;620.9&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;47.9&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;4463&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;tr&gt;\\n&lt;td align=\\"left\\"&gt;Llama 2 7B Q4_0&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;Llama 2&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;7&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;7&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;Vulkan&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;fa=1&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;1014.1&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;45.8&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;4219&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;tr&gt;\\n&lt;td align=\\"left\\"&gt;Shisa V2 8B i1-Q4_K_M&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;Llama 3&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;8&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;8&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;Vulkan&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;fa=1&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;614.2&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;42.0&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;5333&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;tr&gt;\\n&lt;td align=\\"left\\"&gt;dots1 UD-Q4_K_XL&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;dots1 MoE&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;142&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;14&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;Vulkan&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;fa=1 b=256&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;63.1&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;20.6&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;84077&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;tr&gt;\\n&lt;td align=\\"left\\"&gt;Llama 4 Scout UD-Q4_K_XL&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;Llama 4 MoE&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;109&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;17&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;Vulkan&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;fa=1 b=256&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;146.1&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;19.3&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;59917&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;tr&gt;\\n&lt;td align=\\"left\\"&gt;Hunyuan-A13B UD-Q6_K_XL&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;Hunyuan MoE&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;80&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;13&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;Vulkan&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;fa=1 b=256&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;223.9&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;17.1&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;68608&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;tr&gt;\\n&lt;td align=\\"left\\"&gt;Mistral Small 3.1 UD-Q4_K_XL&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;Mistral 3&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;24&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;24&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;Vulkan&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;fa=1&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;119.6&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;14.3&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;14540&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;tr&gt;\\n&lt;td align=\\"left\\"&gt;Shisa V2 70B i1-Q4_K_M&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;Llama 3&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;70&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;70&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;Vulkan&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;fa=1&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;26.4&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;5.0&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;41456&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;/tbody&gt;&lt;/table&gt;\\n\\n&lt;h1&gt;Testing Notes&lt;/h1&gt;\\n\\n&lt;p&gt;The best overall backend and flags were chosen for each model family tested. You can see that often times the best backend for prefill vs token generation differ. Full results for each model (including the pp/tg graphs for different context lengths for all tested backend variations) are available for review in their respective folders as which backend is the best performing will depend on your exact use-case.&lt;/p&gt;\\n\\n&lt;p&gt;There&amp;#39;s a lot of performance still on the table when it comes to pp especially. Since these results should be close to optimal for when they were tested, I might add dates to the table  (adding kernel, ROCm, and llama.cpp build#&amp;#39;s might be a bit much).&lt;/p&gt;\\n\\n&lt;p&gt;One thing worth pointing out is that pp has improved significantly on some models since I last tested. For example, back in May, pp512 for Qwen3 30B-A3B was 119 t/s (Vulkan) and it&amp;#39;s now 605 t/s. Similarly, Llama 4 Scout has a pp512 of 103 t/s, and is now 173 t/s, although the HIP backend is significantly faster at 264 t/s.&lt;/p&gt;\\n\\n&lt;p&gt;Unlike last time, I won&amp;#39;t be taking any model testing requests as these sweeps take quite a while to run - I feel like there are enough 395 systems out there now and the repo linked at top includes the full scripts to allow anyone to replicate (and can be easily adapted for other backends or to run with different hardware).&lt;/p&gt;\\n\\n&lt;p&gt;For testing, the HIP backend, I highly recommend trying &lt;code&gt;ROCBLAS_USE_HIPBLASLT=1&lt;/code&gt; as that is almost always faster than the default rocBLAS. If you are OK with occasionally hitting the reboot switch, you might also want to test in combination with (as long as you have the gfx1100 kernels installed) &lt;code&gt;HSA_OVERRIDE_GFX_VERSION=11.0.0&lt;/code&gt; - in prior testing I&amp;#39;ve found the gfx1100 kernels to be up 2X faster than gfx1151 kernels... ðŸ¤”&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#ccac2b","id":"1m6b151","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"randomfoo2","discussion_type":null,"num_comments":65,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1m6b151/updated_strix_halo_ryzen_ai_max_395_llm_benchmark/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1m6b151/updated_strix_halo_ryzen_ai_max_395_llm_benchmark/","subreddit_subscribers":503254,"created_utc":1753182004,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4ifz2c","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"AdamDhahabi","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4ib5lo","score":1,"author_fullname":"t2_x5lnbc2","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Thanks for the link, 128GB barebone is indeed quoted \\\\~2000$. In euro currency 2500â‚¬ (including cheapest NVMe) which is 2900$. I guess because of EU VAT.","edited":1753186688,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n4ifz2c","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thanks for the link, 128GB barebone is indeed quoted ~2000$. In euro currency 2500â‚¬ (including cheapest NVMe) which is 2900$. I guess because of EU VAT.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m6b151","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6b151/updated_strix_halo_ryzen_ai_max_395_llm_benchmark/n4ifz2c/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753186391,"author_flair_text":null,"treatment_tags":[],"created_utc":1753186391,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4jm9hu","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Competitive_Ideal866","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4jkm0b","score":1,"author_fullname":"t2_1d13xm6n7f","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Soz. I replied to the wrong comment.","edited":false,"author_flair_css_class":null,"name":"t1_n4jm9hu","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Soz. I replied to the wrong comment.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1m6b151","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6b151/updated_strix_halo_ryzen_ai_max_395_llm_benchmark/n4jm9hu/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753199354,"author_flair_text":null,"collapsed":false,"created_utc":1753199354,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4jkm0b","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Solaranvr","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4jblzm","score":6,"author_fullname":"t2_139o8g93yj","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"How are you getting $2900?\\n\\nThe Framework finishes at around $2100 for the 128GB config, after all the panels, cooler, ssd, and ports have been added. Storage can even be had for cheaper if you buy an M.2, as does the cooler, so you can even scrape by under $2100.\\n\\nA 128GB M4 Max Mac Studio starts at $3499 and that's with only 512gb.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4jkm0b","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;How are you getting $2900?&lt;/p&gt;\\n\\n&lt;p&gt;The Framework finishes at around $2100 for the 128GB config, after all the panels, cooler, ssd, and ports have been added. Storage can even be had for cheaper if you buy an M.2, as does the cooler, so you can even scrape by under $2100.&lt;/p&gt;\\n\\n&lt;p&gt;A 128GB M4 Max Mac Studio starts at $3499 and that&amp;#39;s with only 512gb.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m6b151","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6b151/updated_strix_halo_ryzen_ai_max_395_llm_benchmark/n4jkm0b/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753198900,"author_flair_text":null,"treatment_tags":[],"created_utc":1753198900,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4lj4a6","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Competitive_Ideal866","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4l5hqw","score":0,"author_fullname":"t2_1d13xm6n7f","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"&gt; And I believe you are talking about used/refurbished mac studio with 128GB ram?\\n\\nI got that for the M4 Max with 128GB.\\n\\n&gt; Because in apple store it's 4000 for 96GB mac studio.\\n\\nIs that the M3 Ultra?","edited":false,"author_flair_css_class":null,"name":"t1_n4lj4a6","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;And I believe you are talking about used/refurbished mac studio with 128GB ram?&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;I got that for the M4 Max with 128GB.&lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;Because in apple store it&amp;#39;s 4000 for 96GB mac studio.&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;Is that the M3 Ultra?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1m6b151","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6b151/updated_strix_halo_ryzen_ai_max_395_llm_benchmark/n4lj4a6/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753218498,"author_flair_text":null,"collapsed":false,"created_utc":1753218498,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}},"user_reports":[],"saved":false,"id":"n4l5hqw","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"uti24","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4jblzm","score":1,"author_fullname":"t2_13hbro","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"2000 is base price for AMD and 3600 is base price for mac. So for both you have to add 30% if you are not in USA. And I believe you are talking about used/refurbished mac studio with 128GB ram? Because in apple store it's 4000 for 96GB mac studio.","edited":1753216869,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4l5hqw","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;2000 is base price for AMD and 3600 is base price for mac. So for both you have to add 30% if you are not in USA. And I believe you are talking about used/refurbished mac studio with 128GB ram? Because in apple store it&amp;#39;s 4000 for 96GB mac studio.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m6b151","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6b151/updated_strix_halo_ryzen_ai_max_395_llm_benchmark/n4l5hqw/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753214714,"author_flair_text":null,"treatment_tags":[],"created_utc":1753214714,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4jblzm","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Competitive_Ideal866","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4ib5lo","score":-1,"author_fullname":"t2_1d13xm6n7f","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Wow, so $2,900 Ryzen vs $3,600 Mac Studio. 24% more money gets you 2-10x faster performance.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n4jblzm","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Wow, so $2,900 Ryzen vs $3,600 Mac Studio. 24% more money gets you 2-10x faster performance.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m6b151","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6b151/updated_strix_halo_ryzen_ai_max_395_llm_benchmark/n4jblzm/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753196406,"author_flair_text":null,"treatment_tags":[],"created_utc":1753196406,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4ib5lo","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"uti24","can_mod_post":false,"created_utc":1753184487,"send_replies":true,"parent_id":"t1_n4i8iu2","score":18,"author_fullname":"t2_13hbro","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"All Ryzen AI Max+ 395 computers has more or less same prise, because you can not change CPU or RAM\\n\\n128GB ram setup cost \\\\~2000$\\n\\n[https://frame.work/products/desktop-diy-amd-aimax300/configuration/new](https://frame.work/products/desktop-diy-amd-aimax300/configuration/new)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4ib5lo","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;All Ryzen AI Max+ 395 computers has more or less same prise, because you can not change CPU or RAM&lt;/p&gt;\\n\\n&lt;p&gt;128GB ram setup cost ~2000$&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://frame.work/products/desktop-diy-amd-aimax300/configuration/new\\"&gt;https://frame.work/products/desktop-diy-amd-aimax300/configuration/new&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m6b151","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6b151/updated_strix_halo_ryzen_ai_max_395_llm_benchmark/n4ib5lo/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753184487,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":18}}],"before":null}},"user_reports":[],"saved":false,"id":"n4i8iu2","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"AdamDhahabi","can_mod_post":false,"created_utc":1753183372,"send_replies":true,"parent_id":"t3_1m6b151","score":10,"author_fullname":"t2_x5lnbc2","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"That's quite good, how much dollars would such a setup cost?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4i8iu2","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;That&amp;#39;s quite good, how much dollars would such a setup cost?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6b151/updated_strix_halo_ryzen_ai_max_395_llm_benchmark/n4i8iu2/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753183372,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m6b151","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":10}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4jlc3z","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"randomfoo2","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4iuu6e","score":3,"author_fullname":"t2_eztox","approved_by":null,"mod_note":null,"all_awardings":[],"body":"Actually the changes have been upstreamed, you can look in ggml/src/ggml-cuda/vendors/hip.h but basically all you have to do is make sure to go to around line 140 and lower the HIP\\\\_VERSION (the ROCm 7.0 preview keeps a 6.5 version, but also, the structures were deprecated by 6.5 anyway...)","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n4jlc3z","is_submitter":true,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Actually the changes have been upstreamed, you can look in ggml/src/ggml-cuda/vendors/hip.h but basically all you have to do is make sure to go to around line 140 and lower the HIP_VERSION (the ROCm 7.0 preview keeps a 6.5 version, but also, the structures were deprecated by 6.5 anyway...)&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1m6b151","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6b151/updated_strix_halo_ryzen_ai_max_395_llm_benchmark/n4jlc3z/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753199100,"author_flair_text":null,"treatment_tags":[],"created_utc":1753199100,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n4iuu6e","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"spaceman_","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4is92u","score":1,"author_fullname":"t2_9neub","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"It works when I set the hipBLASLt env var, but not when I set the \`HSA_OVERRIDE_GFX_VERSION=11.0.0\`\\n\\nI've configured cmake with \`-DGPU_TARGETS=gfx1100,gfx1151\`\\n\\nWhat do you change to make it include the \`hip_v2_fix.h\` file?","edited":1753191968,"author_flair_css_class":null,"name":"t1_n4iuu6e","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It works when I set the hipBLASLt env var, but not when I set the &lt;code&gt;HSA_OVERRIDE_GFX_VERSION=11.0.0&lt;/code&gt;&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;ve configured cmake with &lt;code&gt;-DGPU_TARGETS=gfx1100,gfx1151&lt;/code&gt;&lt;/p&gt;\\n\\n&lt;p&gt;What do you change to make it include the &lt;code&gt;hip_v2_fix.h&lt;/code&gt; file?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1m6b151","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6b151/updated_strix_halo_ryzen_ai_max_395_llm_benchmark/n4iuu6e/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753191512,"author_flair_text":null,"collapsed":false,"created_utc":1753191512,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4is92u","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"randomfoo2","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4irubg","score":2,"author_fullname":"t2_eztox","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Runtime, but I believe ROCm 6.4 does not have gfx1151 hipBLASLt kernels... (you can grep through your ROCm folder to double check). You'll want to use the TheRock nightlies and find the gfx1151 builds.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4is92u","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Runtime, but I believe ROCm 6.4 does not have gfx1151 hipBLASLt kernels... (you can grep through your ROCm folder to double check). You&amp;#39;ll want to use the TheRock nightlies and find the gfx1151 builds.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m6b151","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6b151/updated_strix_halo_ryzen_ai_max_395_llm_benchmark/n4is92u/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753190677,"author_flair_text":null,"treatment_tags":[],"created_utc":1753190677,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n4irubg","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"spaceman_","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4ipn61","score":2,"author_fullname":"t2_9neub","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Many thanks! I totally glossed over the releases since the last release was from May, but seems like they add new artifacts to the old release occasionally. Kinda weird, but I guess it works.\\n\\nCan I set the ROCBLAS\\\\_USE\\\\_HIPBLASLT=1 env at run time or should it be set at cmake config or build time?\\n\\nI tried this with ROCm 6.4 and I keep getting crashes.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n4irubg","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Many thanks! I totally glossed over the releases since the last release was from May, but seems like they add new artifacts to the old release occasionally. Kinda weird, but I guess it works.&lt;/p&gt;\\n\\n&lt;p&gt;Can I set the ROCBLAS_USE_HIPBLASLT=1 env at run time or should it be set at cmake config or build time?&lt;/p&gt;\\n\\n&lt;p&gt;I tried this with ROCm 6.4 and I keep getting crashes.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m6b151","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6b151/updated_strix_halo_ryzen_ai_max_395_llm_benchmark/n4irubg/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753190542,"author_flair_text":null,"treatment_tags":[],"created_utc":1753190542,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n4ipn61","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"randomfoo2","can_mod_post":false,"created_utc":1753189815,"send_replies":true,"parent_id":"t1_n4ierbl","score":4,"author_fullname":"t2_eztox","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"You can just d/l any gfx1151 nightly tarball here: [https://github.com/ROCm/TheRock/releases/](https://github.com/ROCm/TheRock/releases/)\\n\\nJust untar it to /opt/rocm or any folder you like. You can use something like this to load the proper env variables: [https://github.com/lhl/strix-halo-testing/blob/main/rocm-therock-env.sh](https://github.com/lhl/strix-halo-testing/blob/main/rocm-therock-env.sh)\\n\\n    # ---- ROCm nightly from /home/lhl/therock/rocm-7.0 ----\\n    export ROCM_PATH=/home/lhl/therock/rocm-7.0\\n    export HIP_PLATFORM=amd\\n    export HIP_PATH=$ROCM_PATH\\n    export HIP_CLANG_PATH=$ROCM_PATH/llvm/bin\\n    export HIP_INCLUDE_PATH=$ROCM_PATH/include\\n    export HIP_LIB_PATH=$ROCM_PATH/lib\\n    export HIP_DEVICE_LIB_PATH=$ROCM_PATH/lib/llvm/amdgcn/bitcode\\n    \\n    # search paths -- prepend\\n    export PATH=\\"$ROCM_PATH/bin:$HIP_CLANG_PATH:$PATH\\"\\n    export LD_LIBRARY_PATH=\\"$HIP_LIB_PATH:$ROCM_PATH/lib:$ROCM_PATH/lib64:$ROCM_PATH/llvm/lib:\${LD_LIBRARY_PATH:-}\\"\\n    export LIBRARY_PATH=\\"$HIP_LIB_PATH:$ROCM_PATH/lib:$ROCM_PATH/lib64:\${LIBRARY_PATH:-}\\"\\n    export CPATH=\\"$HIP_INCLUDE_PATH:\${CPATH:-}\\"\\n    export PKG_CONFIG_PATH=\\"$ROCM_PATH/lib/pkgconfig:\${PKG_CONFIG_PATH:-}\\"","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4ipn61","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You can just d/l any gfx1151 nightly tarball here: &lt;a href=\\"https://github.com/ROCm/TheRock/releases/\\"&gt;https://github.com/ROCm/TheRock/releases/&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;Just untar it to /opt/rocm or any folder you like. You can use something like this to load the proper env variables: &lt;a href=\\"https://github.com/lhl/strix-halo-testing/blob/main/rocm-therock-env.sh\\"&gt;https://github.com/lhl/strix-halo-testing/blob/main/rocm-therock-env.sh&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;pre&gt;&lt;code&gt;# ---- ROCm nightly from /home/lhl/therock/rocm-7.0 ----\\nexport ROCM_PATH=/home/lhl/therock/rocm-7.0\\nexport HIP_PLATFORM=amd\\nexport HIP_PATH=$ROCM_PATH\\nexport HIP_CLANG_PATH=$ROCM_PATH/llvm/bin\\nexport HIP_INCLUDE_PATH=$ROCM_PATH/include\\nexport HIP_LIB_PATH=$ROCM_PATH/lib\\nexport HIP_DEVICE_LIB_PATH=$ROCM_PATH/lib/llvm/amdgcn/bitcode\\n\\n# search paths -- prepend\\nexport PATH=&amp;quot;$ROCM_PATH/bin:$HIP_CLANG_PATH:$PATH&amp;quot;\\nexport LD_LIBRARY_PATH=&amp;quot;$HIP_LIB_PATH:$ROCM_PATH/lib:$ROCM_PATH/lib64:$ROCM_PATH/llvm/lib:\${LD_LIBRARY_PATH:-}&amp;quot;\\nexport LIBRARY_PATH=&amp;quot;$HIP_LIB_PATH:$ROCM_PATH/lib:$ROCM_PATH/lib64:\${LIBRARY_PATH:-}&amp;quot;\\nexport CPATH=&amp;quot;$HIP_INCLUDE_PATH:\${CPATH:-}&amp;quot;\\nexport PKG_CONFIG_PATH=&amp;quot;$ROCM_PATH/lib/pkgconfig:\${PKG_CONFIG_PATH:-}&amp;quot;\\n&lt;/code&gt;&lt;/pre&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m6b151","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6b151/updated_strix_halo_ryzen_ai_max_395_llm_benchmark/n4ipn61/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753189815,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}}],"before":null}},"user_reports":[],"saved":false,"id":"n4ierbl","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"spaceman_","can_mod_post":false,"created_utc":1753185920,"send_replies":true,"parent_id":"t3_1m6b151","score":5,"author_fullname":"t2_9neub","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Thanks for this! I'm currently running the 395 w/64GB memory using llama.cpp and the Vulkan backend, and I'm eager to get this better performance. Are there any instructions on how to install rocm 7 nightlies anywhere I can follow?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4ierbl","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thanks for this! I&amp;#39;m currently running the 395 w/64GB memory using llama.cpp and the Vulkan backend, and I&amp;#39;m eager to get this better performance. Are there any instructions on how to install rocm 7 nightlies anywhere I can follow?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6b151/updated_strix_halo_ryzen_ai_max_395_llm_benchmark/n4ierbl/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753185920,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m6b151","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4jxf0i","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"annakhouri2150","can_mod_post":false,"created_utc":1753202484,"send_replies":true,"parent_id":"t3_1m6b151","score":4,"author_fullname":"t2_1jmvw9zdnb","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Honestly, the framework desktop, at least the 128GB version, seems custom built for this new era of ubiquitous open-source mixture of expert models, where you need a huge VRAM to fit the whole model into memory, but you don't quite need as much top tier compute because the number of active parameters is significantly smaller compared both to equivalently performing dense models and to the total number of parameters you need to load into RAM. So something like these new AMD APUs where you sacrifice cutting edge as fast as possible compute, although the compute still seems really decent in order to get that larger VRAM make perfect sense.\\n\\n\\nThe only question for me was whether the compute sacrifices would end up being large enough to negate the usefulness of larger models or not. But it seems like the performance that these APUs are able to turn out is decent enough that I'm not too worried about that, especially since we're getting pretty good numbers already and there's still a decent amount of theoretical FLOPS and memory bandwidth on the table for drivers and kernel updates to get at. It would be interesting to see calculations of what the theoretical maximum prompt and token generation speeds might be.\\n\\n\\nNow, if only they'd sell versions with 256 or even 512 gigabytes of RAM.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4jxf0i","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Honestly, the framework desktop, at least the 128GB version, seems custom built for this new era of ubiquitous open-source mixture of expert models, where you need a huge VRAM to fit the whole model into memory, but you don&amp;#39;t quite need as much top tier compute because the number of active parameters is significantly smaller compared both to equivalently performing dense models and to the total number of parameters you need to load into RAM. So something like these new AMD APUs where you sacrifice cutting edge as fast as possible compute, although the compute still seems really decent in order to get that larger VRAM make perfect sense.&lt;/p&gt;\\n\\n&lt;p&gt;The only question for me was whether the compute sacrifices would end up being large enough to negate the usefulness of larger models or not. But it seems like the performance that these APUs are able to turn out is decent enough that I&amp;#39;m not too worried about that, especially since we&amp;#39;re getting pretty good numbers already and there&amp;#39;s still a decent amount of theoretical FLOPS and memory bandwidth on the table for drivers and kernel updates to get at. It would be interesting to see calculations of what the theoretical maximum prompt and token generation speeds might be.&lt;/p&gt;\\n\\n&lt;p&gt;Now, if only they&amp;#39;d sell versions with 256 or even 512 gigabytes of RAM.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6b151/updated_strix_halo_ryzen_ai_max_395_llm_benchmark/n4jxf0i/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753202484,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m6b151","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4j3cot","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"randomfoo2","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4j0qu6","score":2,"author_fullname":"t2_eztox","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yeah, I mean, 16 Zen5 cores w/ fast memory is not too shabby!","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n4j3cot","is_submitter":true,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yeah, I mean, 16 Zen5 cores w/ fast memory is not too shabby!&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m6b151","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6b151/updated_strix_halo_ryzen_ai_max_395_llm_benchmark/n4j3cot/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753194086,"author_flair_text":null,"treatment_tags":[],"created_utc":1753194086,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n4j0qu6","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"BalorNG","can_mod_post":false,"created_utc":1753193318,"send_replies":true,"parent_id":"t1_n4i8c2h","score":1,"author_fullname":"t2_b6gw9q","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Thanks for the good work! Does not seem to be that much of a good deal w/o better drivers/software, but is small, very energy efficient and is a quite capable workstation in a pinch :)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4j0qu6","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thanks for the good work! Does not seem to be that much of a good deal w/o better drivers/software, but is small, very energy efficient and is a quite capable workstation in a pinch :)&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m6b151","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6b151/updated_strix_halo_ryzen_ai_max_395_llm_benchmark/n4j0qu6/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753193318,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4i8c2h","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"randomfoo2","can_mod_post":false,"created_utc":1753183291,"send_replies":true,"parent_id":"t3_1m6b151","score":8,"author_fullname":"t2_eztox","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"For those interested in tracking gfx1100 vs gfx1151 kernel performance regressions: [https://github.com/ROCm/ROCm/issues/4748](https://github.com/ROCm/ROCm/issues/4748)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4i8c2h","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;For those interested in tracking gfx1100 vs gfx1151 kernel performance regressions: &lt;a href=\\"https://github.com/ROCm/ROCm/issues/4748\\"&gt;https://github.com/ROCm/ROCm/issues/4748&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6b151/updated_strix_halo_ryzen_ai_max_395_llm_benchmark/n4i8c2h/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753183291,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m6b151","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":8}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4izkva","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"BalorNG","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4iqube","score":3,"author_fullname":"t2_b6gw9q","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Oh, missed your reply, indeed EPYC seems like the best bang for a buck, but not noise/power and being compact obv","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n4izkva","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Oh, missed your reply, indeed EPYC seems like the best bang for a buck, but not noise/power and being compact obv&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m6b151","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6b151/updated_strix_halo_ryzen_ai_max_395_llm_benchmark/n4izkva/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753192968,"author_flair_text":null,"treatment_tags":[],"created_utc":1753192968,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n4iqube","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"randomfoo2","can_mod_post":false,"created_utc":1753190214,"send_replies":true,"parent_id":"t1_n4ibzvg","score":5,"author_fullname":"t2_eztox","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"There is USB4, but there's also a x4 PCIe slot as well (as well as a 2nd M.2 that you'd could presumably connect to), so you have some options...\\n\\nBut IMO if you're going to go for dGPUs, take the $2K you would have spent on a HEDT/server (eg, EPYC) system w/ 300GB/s+ mbw and PCIe 5.0 and you'd be in a better spot...","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4iqube","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;There is USB4, but there&amp;#39;s also a x4 PCIe slot as well (as well as a 2nd M.2 that you&amp;#39;d could presumably connect to), so you have some options...&lt;/p&gt;\\n\\n&lt;p&gt;But IMO if you&amp;#39;re going to go for dGPUs, take the $2K you would have spent on a HEDT/server (eg, EPYC) system w/ 300GB/s+ mbw and PCIe 5.0 and you&amp;#39;d be in a better spot...&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m6b151","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6b151/updated_strix_halo_ryzen_ai_max_395_llm_benchmark/n4iqube/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753190214,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4j5jq2","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"uti24","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4iz7sn","score":4,"author_fullname":"t2_13hbro","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Getting enough 3090s is a hassle and costs more (to get same amount of VRAM), while this tiny little box â€” you just put it anywhere in your apartment and forget about it.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n4j5jq2","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Getting enough 3090s is a hassle and costs more (to get same amount of VRAM), while this tiny little box â€” you just put it anywhere in your apartment and forget about it.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m6b151","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6b151/updated_strix_halo_ryzen_ai_max_395_llm_benchmark/n4j5jq2/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753194711,"author_flair_text":null,"treatment_tags":[],"created_utc":1753194711,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4oymzy","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"simracerman","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4nsmkl","score":1,"author_fullname":"t2_vbzgnic","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"The audience of this Ryzen 385 and the Mac mini/studio are hobbyists for sure. The 395 IMO is a far better value than say M4 Max because itâ€™s Â cheaper and acts as a more versatile Windows/Linux box. Can do all current games at 1440 High settings, multimedia applications, and coding if you need it to.\\n\\nAlways read the fine print and take nothing at face value.","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n4oymzy","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The audience of this Ryzen 385 and the Mac mini/studio are hobbyists for sure. The 395 IMO is a far better value than say M4 Max because itâ€™s Â cheaper and acts as a more versatile Windows/Linux box. Can do all current games at 1440 High settings, multimedia applications, and coding if you need it to.&lt;/p&gt;\\n\\n&lt;p&gt;Always read the fine print and take nothing at face value.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1m6b151","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6b151/updated_strix_halo_ryzen_ai_max_395_llm_benchmark/n4oymzy/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753269546,"author_flair_text":null,"treatment_tags":[],"created_utc":1753269546,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":6,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4nsmkl","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"fastheadcrab","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4mg0k2","score":1,"author_fullname":"t2_c6hhv","approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yeah I saw this review, it says ~150W running LLMs which makes more sense given the TDP. Can the cooling solution handle dissipating 150W full time? It's a huge ask compared to running a few loads for just 3-4 hours a day. Having only owned big OEM mini-PCs, I might buy one of these Chinese ones and run a compute job non-stop to see when they fail lol. \\n\\nWith that said, you do make fair points. I do agree that they are very efficient compared to a bunch of GPUs, even accounting for performance/watt. You're looking at far over 1 kW probably even when undervolting a 4x 3090 setup. \\n  \\nBased on the benchmarks in the review and in the OP the speed will be passable (3-5 tok/sec with the larger models that fit). Not glacial but not fast either. For chatting it's fine but for generating a lot of code or text it might take a while. Set it up and then come back tomorrow morning for the answer lol. And the RAM size limitation will put a cap on model size which is going to limit the quality of results. \\n  \\nThis seems like a nice way to play around with some local LLMs, but I just feel people should go into buying these things with full information, especially since the consumers buying this will lean more beginner, even when it comes to computer basics. It is capable but just going to be capped in performance by iGPU capability, RAM size, and thermals. With companies slapping AI on everything consumers should be well-informed.\\n  \\nSomeone building a GPU rig will either know what they are doing or will have the commitment to figure it out. Also power bills alone will bankrupt users lmao\\n  \\nSo I basically agree with you, but just with more caveats. As always, the fast-cheap-good trade off applies here. The question is whether this is cheap enough to be \\"cheap and acceptably good.\\"","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n4nsmkl","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yeah I saw this review, it says ~150W running LLMs which makes more sense given the TDP. Can the cooling solution handle dissipating 150W full time? It&amp;#39;s a huge ask compared to running a few loads for just 3-4 hours a day. Having only owned big OEM mini-PCs, I might buy one of these Chinese ones and run a compute job non-stop to see when they fail lol. &lt;/p&gt;\\n\\n&lt;p&gt;With that said, you do make fair points. I do agree that they are very efficient compared to a bunch of GPUs, even accounting for performance/watt. You&amp;#39;re looking at far over 1 kW probably even when undervolting a 4x 3090 setup. &lt;/p&gt;\\n\\n&lt;p&gt;Based on the benchmarks in the review and in the OP the speed will be passable (3-5 tok/sec with the larger models that fit). Not glacial but not fast either. For chatting it&amp;#39;s fine but for generating a lot of code or text it might take a while. Set it up and then come back tomorrow morning for the answer lol. And the RAM size limitation will put a cap on model size which is going to limit the quality of results. &lt;/p&gt;\\n\\n&lt;p&gt;This seems like a nice way to play around with some local LLMs, but I just feel people should go into buying these things with full information, especially since the consumers buying this will lean more beginner, even when it comes to computer basics. It is capable but just going to be capped in performance by iGPU capability, RAM size, and thermals. With companies slapping AI on everything consumers should be well-informed.&lt;/p&gt;\\n\\n&lt;p&gt;Someone building a GPU rig will either know what they are doing or will have the commitment to figure it out. Also power bills alone will bankrupt users lmao&lt;/p&gt;\\n\\n&lt;p&gt;So I basically agree with you, but just with more caveats. As always, the fast-cheap-good trade off applies here. The question is whether this is cheap enough to be &amp;quot;cheap and acceptably good.&amp;quot;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1m6b151","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6b151/updated_strix_halo_ryzen_ai_max_395_llm_benchmark/n4nsmkl/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753247087,"author_flair_text":null,"treatment_tags":[],"created_utc":1753247087,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4mg0k2","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"simracerman","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4m664m","score":2,"author_fullname":"t2_vbzgnic","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Donâ€™t take my word regarding power, just read this:\\n\\nhttps://www.servethehome.com/gmktec-evo-x2-review-an-amd-ryzen-ai-max-395-powerhouse/4/\\n\\nAt idle it pulls 8-14W, with full load at 170-180W.\\n\\n\\nOn the reliability front, I have a Mini PC from Beelink running 24/7, like Never shutdown this thing since Aug 2023. Runs win 11. I game, run LLMs up to 24B in size and the thing stays cool. Pulls around 12 Watts at idle and 95 Watt full load. They really are insanely low power.\\n\\nTrue that some Mini PCs go bust in months, but we all know thatâ€™s the cheapest of the cheap. Go with a Framework, Beelink or Asus to get the best.\\n\\nIn terms of slow, yeah it is compared to a dGPU setup, but that again comes with all the headaches I listed in my last comment. OPs benchmarks donâ€™t say slow anywhere, but thatâ€™s my standard for home and tinkering use. If I serve users in Production, my calculus is quite different.","edited":false,"author_flair_css_class":null,"name":"t1_n4mg0k2","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Donâ€™t take my word regarding power, just read this:&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://www.servethehome.com/gmktec-evo-x2-review-an-amd-ryzen-ai-max-395-powerhouse/4/\\"&gt;https://www.servethehome.com/gmktec-evo-x2-review-an-amd-ryzen-ai-max-395-powerhouse/4/&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;At idle it pulls 8-14W, with full load at 170-180W.&lt;/p&gt;\\n\\n&lt;p&gt;On the reliability front, I have a Mini PC from Beelink running 24/7, like Never shutdown this thing since Aug 2023. Runs win 11. I game, run LLMs up to 24B in size and the thing stays cool. Pulls around 12 Watts at idle and 95 Watt full load. They really are insanely low power.&lt;/p&gt;\\n\\n&lt;p&gt;True that some Mini PCs go bust in months, but we all know thatâ€™s the cheapest of the cheap. Go with a Framework, Beelink or Asus to get the best.&lt;/p&gt;\\n\\n&lt;p&gt;In terms of slow, yeah it is compared to a dGPU setup, but that again comes with all the headaches I listed in my last comment. OPs benchmarks donâ€™t say slow anywhere, but thatâ€™s my standard for home and tinkering use. If I serve users in Production, my calculus is quite different.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1m6b151","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6b151/updated_strix_halo_ryzen_ai_max_395_llm_benchmark/n4mg0k2/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753228818,"author_flair_text":null,"collapsed":false,"created_utc":1753228818,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n4m664m","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"fastheadcrab","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4kq2ho","score":2,"author_fullname":"t2_c6hhv","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Much more efficient, fair, but probably a lot slower. \\n\\nNo way in hell it's pulling 10W when in use lol. And the cooling solution on these thing will likely fail pretty quickly if under constant load (&lt;1 year of 24/7). Typical mini-PCs made from these fly-by-night OEMs will not tolerate running at the thermal limits for any extended period of time, these are not server or even consumer desktop quality. Maybe the framwork will be longer lasting but even so the limited expansion options were a mystifying decision.\\n\\nBut tbf, 4x 3090s will be pulling way more than 180W lol. The idle draw alone may be that level.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4m664m","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Much more efficient, fair, but probably a lot slower. &lt;/p&gt;\\n\\n&lt;p&gt;No way in hell it&amp;#39;s pulling 10W when in use lol. And the cooling solution on these thing will likely fail pretty quickly if under constant load (&amp;lt;1 year of 24/7). Typical mini-PCs made from these fly-by-night OEMs will not tolerate running at the thermal limits for any extended period of time, these are not server or even consumer desktop quality. Maybe the framwork will be longer lasting but even so the limited expansion options were a mystifying decision.&lt;/p&gt;\\n\\n&lt;p&gt;But tbf, 4x 3090s will be pulling way more than 180W lol. The idle draw alone may be that level.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m6b151","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6b151/updated_strix_halo_ryzen_ai_max_395_llm_benchmark/n4m664m/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753225639,"author_flair_text":null,"treatment_tags":[],"created_utc":1753225639,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n4kq2ho","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"simracerman","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4iz7sn","score":4,"author_fullname":"t2_vbzgnic","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I did a breakdown of 4x 3090s rig in terms of power consumption and heat vs the 395 in a different post couple weeks ago. The result is:\\n\\n- expect idle + inference power bill difference of anything from $30-$50 monthly.\\n\\n- Heat and noise. This box is cool as ice pulling 10W from the wall. 4x 3090 pulls around 140-180W (total system includes everything).\\n\\nCost is something else. 4x 3090 and the tower to go with cost around $3500-$4000 if you carefully pickup the parts. Otherwise, itâ€™s more.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n4kq2ho","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I did a breakdown of 4x 3090s rig in terms of power consumption and heat vs the 395 in a different post couple weeks ago. The result is:&lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;&lt;p&gt;expect idle + inference power bill difference of anything from $30-$50 monthly.&lt;/p&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;p&gt;Heat and noise. This box is cool as ice pulling 10W from the wall. 4x 3090 pulls around 140-180W (total system includes everything).&lt;/p&gt;&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;p&gt;Cost is something else. 4x 3090 and the tower to go with cost around $3500-$4000 if you carefully pickup the parts. Otherwise, itâ€™s more.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m6b151","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6b151/updated_strix_halo_ryzen_ai_max_395_llm_benchmark/n4kq2ho/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753210322,"author_flair_text":null,"treatment_tags":[],"created_utc":1753210322,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}}],"before":null}},"user_reports":[],"saved":false,"id":"n4iz7sn","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"BalorNG","can_mod_post":false,"created_utc":1753192858,"send_replies":true,"parent_id":"t1_n4ibzvg","score":2,"author_fullname":"t2_b6gw9q","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Multiple 3090 will be faster tho.\\nA used EPYC rig will be faster and more expandable at a fairly similar price point I think, but much less energy... And space efficient :)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4iz7sn","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Multiple 3090 will be faster tho.\\nA used EPYC rig will be faster and more expandable at a fairly similar price point I think, but much less energy... And space efficient :)&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m6b151","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6b151/updated_strix_halo_ryzen_ai_max_395_llm_benchmark/n4iz7sn/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753192858,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4jh96g","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Rich_Repeat_22","can_mod_post":false,"created_utc":1753197978,"send_replies":true,"parent_id":"t1_n4ibzvg","score":1,"author_fullname":"t2_viufiki6","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"There are miniPCs with Oculink or you can use M2 to Oculink adapter. \\n\\nFYI there is a barebones board from China with 3 M2.s so you can connect 2 M.2s to oculink and have 1 M.2 for a drive","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4jh96g","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;There are miniPCs with Oculink or you can use M2 to Oculink adapter. &lt;/p&gt;\\n\\n&lt;p&gt;FYI there is a barebones board from China with 3 M2.s so you can connect 2 M.2s to oculink and have 1 M.2 for a drive&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m6b151","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6b151/updated_strix_halo_ryzen_ai_max_395_llm_benchmark/n4jh96g/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753197978,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4ibzvg","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"uti24","can_mod_post":false,"created_utc":1753184826,"send_replies":true,"parent_id":"t3_1m6b151","score":5,"author_fullname":"t2_13hbro","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Thank you for the detailed benchmarks.  \\nIt actually looks pretty reasonable. So, for a budget build, you either tinker with multiple used 3090s or just take this.  \\nBy the way, can this system support something like OcuLink or USB4 for an external GPU? People say you can improve MOE speed like 2 times with just a single GPU.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4ibzvg","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thank you for the detailed benchmarks.&lt;br/&gt;\\nIt actually looks pretty reasonable. So, for a budget build, you either tinker with multiple used 3090s or just take this.&lt;br/&gt;\\nBy the way, can this system support something like OcuLink or USB4 for an external GPU? People say you can improve MOE speed like 2 times with just a single GPU.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6b151/updated_strix_halo_ryzen_ai_max_395_llm_benchmark/n4ibzvg/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753184826,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m6b151","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4iilxt","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"grigio","can_mod_post":false,"created_utc":1753187375,"send_replies":true,"parent_id":"t3_1m6b151","score":4,"author_fullname":"t2_37dhn","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Thanks for testing q4","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4iilxt","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thanks for testing q4&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6b151/updated_strix_halo_ryzen_ai_max_395_llm_benchmark/n4iilxt/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753187375,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m6b151","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4jos14","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"sleepy_roger","can_mod_post":false,"created_utc":1753200060,"send_replies":true,"parent_id":"t3_1m6b151","score":4,"author_fullname":"t2_usojvms","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"&gt; There's a lot of performance still on the table when it comes to pp especially. \\n\\nI've been telling my wife this for years.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4jos14","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;There&amp;#39;s a lot of performance still on the table when it comes to pp especially. &lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;I&amp;#39;ve been telling my wife this for years.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6b151/updated_strix_halo_ryzen_ai_max_395_llm_benchmark/n4jos14/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753200060,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m6b151","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4j80fc","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Murhie","can_mod_post":false,"created_utc":1753195409,"send_replies":true,"parent_id":"t3_1m6b151","score":3,"author_fullname":"t2_fezak","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Thanks for the detailed benchmarking! Im expecting to get one of these systems delivered this quarter. After seeing some benchmarks in the gmktec system I was worried but im not disappointed with what im seeing in this post.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4j80fc","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thanks for the detailed benchmarking! Im expecting to get one of these systems delivered this quarter. After seeing some benchmarks in the gmktec system I was worried but im not disappointed with what im seeing in this post.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6b151/updated_strix_halo_ryzen_ai_max_395_llm_benchmark/n4j80fc/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753195409,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m6b151","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4nh60j","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"randomfoo2","can_mod_post":false,"created_utc":1753242089,"send_replies":true,"parent_id":"t1_n4llskk","score":2,"author_fullname":"t2_eztox","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"There's only one HIP\\\\_VERSION change you need to make to get it to compile: [https://www.reddit.com/r/LocalLLaMA/comments/1m6b151/comment/n4jlc3z](https://www.reddit.com/r/LocalLLaMA/comments/1m6b151/comment/n4jlc3z)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4nh60j","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;There&amp;#39;s only one HIP_VERSION change you need to make to get it to compile: &lt;a href=\\"https://www.reddit.com/r/LocalLLaMA/comments/1m6b151/comment/n4jlc3z\\"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1m6b151/comment/n4jlc3z&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m6b151","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6b151/updated_strix_halo_ryzen_ai_max_395_llm_benchmark/n4nh60j/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753242089,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n4llskk","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"fizban007","can_mod_post":false,"created_utc":1753219277,"send_replies":true,"parent_id":"t3_1m6b151","score":3,"author_fullname":"t2_fwmct","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"How do you get llama.cpp to compile with the new ROCm 7.0 nightlies? Is there any PR that specifically addresses this?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4llskk","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;How do you get llama.cpp to compile with the new ROCm 7.0 nightlies? Is there any PR that specifically addresses this?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6b151/updated_strix_halo_ryzen_ai_max_395_llm_benchmark/n4llskk/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753219277,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m6b151","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4l05fo","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"jfowers_amd","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4kzp4f","score":4,"author_fullname":"t2_1m2ckixcqh","approved_by":null,"mod_note":null,"all_awardings":[],"body":"The most relevant project we're working on right now is to bring fresh ROCm from TheRock into Lemonade. Whether that fresh ROCm will help MOE models any time soon is not in my scope, but if ROCm provides Lemonade will serve it.","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n4l05fo","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The most relevant project we&amp;#39;re working on right now is to bring fresh ROCm from TheRock into Lemonade. Whether that fresh ROCm will help MOE models any time soon is not in my scope, but if ROCm provides Lemonade will serve it.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1m6b151","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6b151/updated_strix_halo_ryzen_ai_max_395_llm_benchmark/n4l05fo/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753213222,"author_flair_text":null,"treatment_tags":[],"created_utc":1753213222,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}}],"before":null}},"user_reports":[],"saved":false,"id":"n4kzp4f","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Zyguard7777777","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4j60gf","score":3,"author_fullname":"t2_zo1h5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I look forward to all and any new features. I don't suppose you could give a hint if any of these new features would improve the performance of these MOE models?","edited":false,"author_flair_css_class":null,"name":"t1_n4kzp4f","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I look forward to all and any new features. I don&amp;#39;t suppose you could give a hint if any of these new features would improve the performance of these MOE models?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1m6b151","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6b151/updated_strix_halo_ryzen_ai_max_395_llm_benchmark/n4kzp4f/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753213091,"author_flair_text":null,"collapsed":false,"created_utc":1753213091,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n4j60gf","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"jfowers_amd","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4iscby","score":7,"author_fullname":"t2_1m2ckixcqh","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"We're currently working on some new GPU-only features specifically for STX Halo in Lemonade Server, stay tuned!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4j60gf","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;We&amp;#39;re currently working on some new GPU-only features specifically for STX Halo in Lemonade Server, stay tuned!&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m6b151","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6b151/updated_strix_halo_ryzen_ai_max_395_llm_benchmark/n4j60gf/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753194844,"author_flair_text":null,"treatment_tags":[],"created_utc":1753194844,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":7}}],"before":null}},"user_reports":[],"saved":false,"id":"n4iscby","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Zyguard7777777","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4ipwel","score":1,"author_fullname":"t2_zo1h5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Oh, that's a little sad :,(  \\nDefo too expensive for me to justify at the moment then, will wait for the next generation, hopefully that will have a higher memory bandwidth as well","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n4iscby","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Oh, that&amp;#39;s a little sad :,(&lt;br/&gt;\\nDefo too expensive for me to justify at the moment then, will wait for the next generation, hopefully that will have a higher memory bandwidth as well&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m6b151","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6b151/updated_strix_halo_ryzen_ai_max_395_llm_benchmark/n4iscby/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753190706,"author_flair_text":null,"treatment_tags":[],"created_utc":1753190706,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4kzer1","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Zyguard7777777","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4j5vmn","score":2,"author_fullname":"t2_zo1h5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"That's what I was hoping for tbh","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4kzer1","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;That&amp;#39;s what I was hoping for tbh&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m6b151","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6b151/updated_strix_halo_ryzen_ai_max_395_llm_benchmark/n4kzer1/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753213008,"author_flair_text":null,"treatment_tags":[],"created_utc":1753213008,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n4j5vmn","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Awwtifishal","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4ipwel","score":1,"author_fullname":"t2_1d96a8k10t","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I wonder if that only accounts for using the NPU \\\\*instead\\\\* of the GPU and if there would be any benefit in using both at the same time, by e.g. splitting some tensors and sharing the load.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n4j5vmn","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I wonder if that only accounts for using the NPU *instead* of the GPU and if there would be any benefit in using both at the same time, by e.g. splitting some tensors and sharing the load.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m6b151","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6b151/updated_strix_halo_ryzen_ai_max_395_llm_benchmark/n4j5vmn/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753194806,"author_flair_text":null,"treatment_tags":[],"created_utc":1753194806,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4ipwel","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"randomfoo2","can_mod_post":false,"created_utc":1753189899,"send_replies":true,"parent_id":"t1_n4iecuh","score":6,"author_fullname":"t2_eztox","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"This is unlikely. From an AMD Lemonade dev: [https://github.com/lemonade-sdk/lemonade/issues/5#issuecomment-3096694964](https://github.com/lemonade-sdk/lemonade/issues/5#issuecomment-3096694964)\\n\\n&gt;just to set expectations, on Strix Halo I would not expect a performance benefit from NPU vs. GPU. On that platform I would suggest using the NPU for LLMs when the GPU is already busy with something else, for example the NPU runs an AI gaming assistant while the GPU runs the game.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4ipwel","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;This is unlikely. From an AMD Lemonade dev: &lt;a href=\\"https://github.com/lemonade-sdk/lemonade/issues/5#issuecomment-3096694964\\"&gt;https://github.com/lemonade-sdk/lemonade/issues/5#issuecomment-3096694964&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;just to set expectations, on Strix Halo I would not expect a performance benefit from NPU vs. GPU. On that platform I would suggest using the NPU for LLMs when the GPU is already busy with something else, for example the NPU runs an AI gaming assistant while the GPU runs the game.&lt;/p&gt;\\n&lt;/blockquote&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m6b151","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6b151/updated_strix_halo_ryzen_ai_max_395_llm_benchmark/n4ipwel/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753189899,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}}],"before":null}},"user_reports":[],"saved":false,"id":"n4iecuh","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Zyguard7777777","can_mod_post":false,"created_utc":1753185760,"send_replies":true,"parent_id":"t3_1m6b151","score":2,"author_fullname":"t2_zo1h5","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I look forward to the hybrid pp using both igpu and npu, should increase pp significantly","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4iecuh","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I look forward to the hybrid pp using both igpu and npu, should increase pp significantly&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6b151/updated_strix_halo_ryzen_ai_max_395_llm_benchmark/n4iecuh/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753185760,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m6b151","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4o24z9","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Kamal965","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4j5xce","score":1,"author_fullname":"t2_gp8z8","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Hey, no worries! Iâ€™ve been following Lemonade Serverâ€™s development pretty closely out of interest (even though I donâ€™t have one of the new Ryzen AI NPUs lol). Quick question if you donâ€™t mind: Iâ€™ve gotten fairly deep into ROCm recently, as I've pulled and patched the 6.3/6.4 source to get it running on my RX 590, and, as a test, managed to train a small physics-informed neural net on it using the PyTorch 2.5 ROCm fork.\\n\\nThatâ€™s gotten me curious about the NPU/software side like the ONNX Runtime, Vitis, etc but Iâ€™m starting from scratch there. Any recommendations for beginner-friendly guides or docs to get up to speed with NPU development? Also curious: how do you see the new Strix Halo GPU features intersecting with NPU workflows going forward?","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n4o24z9","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Hey, no worries! Iâ€™ve been following Lemonade Serverâ€™s development pretty closely out of interest (even though I donâ€™t have one of the new Ryzen AI NPUs lol). Quick question if you donâ€™t mind: Iâ€™ve gotten fairly deep into ROCm recently, as I&amp;#39;ve pulled and patched the 6.3/6.4 source to get it running on my RX 590, and, as a test, managed to train a small physics-informed neural net on it using the PyTorch 2.5 ROCm fork.&lt;/p&gt;\\n\\n&lt;p&gt;Thatâ€™s gotten me curious about the NPU/software side like the ONNX Runtime, Vitis, etc but Iâ€™m starting from scratch there. Any recommendations for beginner-friendly guides or docs to get up to speed with NPU development? Also curious: how do you see the new Strix Halo GPU features intersecting with NPU workflows going forward?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m6b151","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6b151/updated_strix_halo_ryzen_ai_max_395_llm_benchmark/n4o24z9/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753251927,"author_flair_text":null,"treatment_tags":[],"created_utc":1753251927,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4j5xce","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"jfowers_amd","can_mod_post":false,"created_utc":1753194820,"send_replies":true,"parent_id":"t1_n4iqfbu","score":4,"author_fullname":"t2_1m2ckixcqh","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Thanks for the shoutout! We're currently working on some new GPU-only features specifically for STX Halo in Lemonade Server, stay tuned.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4j5xce","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thanks for the shoutout! We&amp;#39;re currently working on some new GPU-only features specifically for STX Halo in Lemonade Server, stay tuned.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m6b151","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6b151/updated_strix_halo_ryzen_ai_max_395_llm_benchmark/n4j5xce/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753194820,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4mdqnt","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"cafedude","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4ishzb","score":1,"author_fullname":"t2_23o6","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":":-(\\n\\nAny idea if there are plans to support Linux?","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n4mdqnt","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;:-(&lt;/p&gt;\\n\\n&lt;p&gt;Any idea if there are plans to support Linux?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m6b151","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6b151/updated_strix_halo_ryzen_ai_max_395_llm_benchmark/n4mdqnt/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753228070,"author_flair_text":null,"treatment_tags":[],"created_utc":1753228070,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4ishzb","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"randomfoo2","can_mod_post":false,"created_utc":1753190757,"send_replies":true,"parent_id":"t1_n4iqfbu","score":2,"author_fullname":"t2_eztox","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"The Lemonade NPU support is currently Windows only.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4ishzb","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The Lemonade NPU support is currently Windows only.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m6b151","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6b151/updated_strix_halo_ryzen_ai_max_395_llm_benchmark/n4ishzb/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753190757,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n4iqfbu","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Kamal965","can_mod_post":false,"created_utc":1753190074,"send_replies":true,"parent_id":"t3_1m6b151","score":2,"author_fullname":"t2_gp8z8","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Sweet, thanks for sharing the results! Have you considered trying AMD's new Lemonade Server inference? It actually integrates NPU support due to having the ONNX Runtime, so you can finally run NPU + GPU inference through that, but I don't know what the performance looks like there.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4iqfbu","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Sweet, thanks for sharing the results! Have you considered trying AMD&amp;#39;s new Lemonade Server inference? It actually integrates NPU support due to having the ONNX Runtime, so you can finally run NPU + GPU inference through that, but I don&amp;#39;t know what the performance looks like there.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6b151/updated_strix_halo_ryzen_ai_max_395_llm_benchmark/n4iqfbu/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753190074,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m6b151","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4j42ky","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"randomfoo2","can_mod_post":false,"created_utc":1753194292,"send_replies":true,"parent_id":"t1_n4iyi2u","score":3,"author_fullname":"t2_eztox","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Looks like I forgot to include a Qwen3 32B Q8 I had run: [https://github.com/lhl/strix-halo-testing/tree/main/llm-bench/Qwen3-32B-Q8\\\\_0](https://github.com/lhl/strix-halo-testing/tree/main/llm-bench/Qwen3-32B-Q8_0)\\n\\n235B requires RPC/multiple machines unless you are running and a ridiculously bad quant.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4j42ky","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Looks like I forgot to include a Qwen3 32B Q8 I had run: &lt;a href=\\"https://github.com/lhl/strix-halo-testing/tree/main/llm-bench/Qwen3-32B-Q8_0\\"&gt;https://github.com/lhl/strix-halo-testing/tree/main/llm-bench/Qwen3-32B-Q8_0&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;235B requires RPC/multiple machines unless you are running and a ridiculously bad quant.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m6b151","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6b151/updated_strix_halo_ryzen_ai_max_395_llm_benchmark/n4j42ky/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753194292,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n4iyi2u","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Secure_Reflection409","can_mod_post":false,"created_utc":1753192644,"send_replies":true,"parent_id":"t3_1m6b151","score":2,"author_fullname":"t2_by77ogdhr","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I would love to see Qwen3 32b and 235b results, if possible.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4iyi2u","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I would love to see Qwen3 32b and 235b results, if possible.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6b151/updated_strix_halo_ryzen_ai_max_395_llm_benchmark/n4iyi2u/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753192644,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m6b151","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4iyqfh","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"jfowers_amd","can_mod_post":false,"created_utc":1753192712,"send_replies":true,"parent_id":"t3_1m6b151","score":2,"author_fullname":"t2_1m2ckixcqh","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Love to see this, thanks for sharing!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4iyqfh","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Love to see this, thanks for sharing!&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6b151/updated_strix_halo_ryzen_ai_max_395_llm_benchmark/n4iyqfh/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753192712,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m6b151","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4jchpj","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"randomfoo2","can_mod_post":false,"created_utc":1753196651,"send_replies":true,"parent_id":"t1_n4j6fyh","score":3,"author_fullname":"t2_eztox","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"TBT, personally I'd recommend a rolling distro (Arch, Fedora Rawhide, etc):\\n\\n* You **100%** should be using a recent kernel. 6.15.x at least, but tbt, on one of my systems I'm running the latest 6.16 rcs\\n* The latest linux-firmware is also recommended, the latest (by latest I mean like this past week or so) has a fix for some intermittent lockups\\n* AFAIK there is no up-to-date Docker for gfx1151. You should use one of the TheRock gfx1151 nightly-tarballs for your ROCm: [https://github.com/ROCm/TheRock/releases/](https://github.com/ROCm/TheRock/releases/) (you can use a 6.4 nightly if you want better compatibility but still want gfx1151 kernels) - you can look at my repo for what env variables I load up.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4jchpj","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;TBT, personally I&amp;#39;d recommend a rolling distro (Arch, Fedora Rawhide, etc):&lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;You &lt;strong&gt;100%&lt;/strong&gt; should be using a recent kernel. 6.15.x at least, but tbt, on one of my systems I&amp;#39;m running the latest 6.16 rcs&lt;/li&gt;\\n&lt;li&gt;The latest linux-firmware is also recommended, the latest (by latest I mean like this past week or so) has a fix for some intermittent lockups&lt;/li&gt;\\n&lt;li&gt;AFAIK there is no up-to-date Docker for gfx1151. You should use one of the TheRock gfx1151 nightly-tarballs for your ROCm: &lt;a href=\\"https://github.com/ROCm/TheRock/releases/\\"&gt;https://github.com/ROCm/TheRock/releases/&lt;/a&gt; (you can use a 6.4 nightly if you want better compatibility but still want gfx1151 kernels) - you can look at my repo for what env variables I load up.&lt;/li&gt;\\n&lt;/ul&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m6b151","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6b151/updated_strix_halo_ryzen_ai_max_395_llm_benchmark/n4jchpj/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753196651,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n4j6fyh","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"cowmix","can_mod_post":false,"created_utc":1753194967,"send_replies":true,"parent_id":"t3_1m6b151","score":2,"author_fullname":"t2_5dbvs","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I've been following your progress pretty closely -- and I'm super jazzed to see this summary status.\\n\\nI have the 128GB EVO-X2 sitting in a box (since mid-May) -- I was waiting for some of the issues you found to be ironed out. It looks like things are in much better shape so the time has come to finally unbox the thing.\\n\\nThis weekend I'm making it my goal to your test suite on it.  \\n\\nI'm planning to bootstrap the rig with Ubuntu 25.04 and run everything in Docker.  Is that a good way to go?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4j6fyh","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;ve been following your progress pretty closely -- and I&amp;#39;m super jazzed to see this summary status.&lt;/p&gt;\\n\\n&lt;p&gt;I have the 128GB EVO-X2 sitting in a box (since mid-May) -- I was waiting for some of the issues you found to be ironed out. It looks like things are in much better shape so the time has come to finally unbox the thing.&lt;/p&gt;\\n\\n&lt;p&gt;This weekend I&amp;#39;m making it my goal to your test suite on it.  &lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;m planning to bootstrap the rig with Ubuntu 25.04 and run everything in Docker.  Is that a good way to go?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6b151/updated_strix_halo_ryzen_ai_max_395_llm_benchmark/n4j6fyh/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753194967,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m6b151","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4jqu2s","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"randomfoo2","can_mod_post":false,"created_utc":1753200646,"send_replies":true,"parent_id":"t1_n4jnocm","score":3,"author_fullname":"t2_eztox","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"tbt, I'm not sure I'd call pp512/tg128 100t/s/5t/s a decent speed. If your main target is a 70B dense model I think 2 x 3090 will run you \\\\~$1500 and run a 70B Q4 much faster (\\\\~20 tok/s). That being said, there's a fair argument to be made for sticking this thing in a corner somewhere for a bunch of these new MoEs.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4jqu2s","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;tbt, I&amp;#39;m not sure I&amp;#39;d call pp512/tg128 100t/s/5t/s a decent speed. If your main target is a 70B dense model I think 2 x 3090 will run you ~$1500 and run a 70B Q4 much faster (~20 tok/s). That being said, there&amp;#39;s a fair argument to be made for sticking this thing in a corner somewhere for a bunch of these new MoEs.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m6b151","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6b151/updated_strix_halo_ryzen_ai_max_395_llm_benchmark/n4jqu2s/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753200646,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n4jnocm","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"ttkciar","can_mod_post":false,"created_utc":1753199748,"send_replies":true,"parent_id":"t3_1m6b151","score":2,"author_fullname":"t2_cpegz","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Thank you! Saving this :-)\\n\\nOne of the motivations for buying this, for me, would be running Tulu3-70B at a decent speed with llama.cpp.  It, too, is based on Llama 3, so the Shisa benchmark should be nicely representative.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4jnocm","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thank you! Saving this :-)&lt;/p&gt;\\n\\n&lt;p&gt;One of the motivations for buying this, for me, would be running Tulu3-70B at a decent speed with llama.cpp.  It, too, is based on Llama 3, so the Shisa benchmark should be nicely representative.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6b151/updated_strix_halo_ryzen_ai_max_395_llm_benchmark/n4jnocm/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753199748,"author_flair_text":"llama.cpp","treatment_tags":[],"link_id":"t3_1m6b151","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4i6rta","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"randomfoo2","can_mod_post":false,"created_utc":1753182597,"send_replies":true,"parent_id":"t1_n4i6b3m","score":5,"author_fullname":"t2_eztox","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"There are no issues w/ different sized quants, but Q3/Q4 XLs are just IMO the sweet spot for perf (accuracy/speed). As you can see, your tg is closely tied to your weight size, so you can just divide by 2 or 4 if you want an idea of how fast a Q8 or FP16 will inference.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4i6rta","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;There are no issues w/ different sized quants, but Q3/Q4 XLs are just IMO the sweet spot for perf (accuracy/speed). As you can see, your tg is closely tied to your weight size, so you can just divide by 2 or 4 if you want an idea of how fast a Q8 or FP16 will inference.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m6b151","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6b151/updated_strix_halo_ryzen_ai_max_395_llm_benchmark/n4i6rta/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753182597,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}}],"before":null}},"user_reports":[],"saved":false,"id":"n4i6b3m","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Jotschi","can_mod_post":false,"created_utc":1753182386,"send_replies":true,"parent_id":"t3_1m6b151","score":2,"author_fullname":"t2_xsvaa","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Thanks for listing exact version info you used. Side question: is there are reason why so many q4 were used? Does q8 or fp16 cause issues?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4i6b3m","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thanks for listing exact version info you used. Side question: is there are reason why so many q4 were used? Does q8 or fp16 cause issues?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6b151/updated_strix_halo_ryzen_ai_max_395_llm_benchmark/n4i6b3m/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753182386,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m6b151","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4iv5l2","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"No-Assist-4041","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4irw8u","score":2,"author_fullname":"t2_ly870h93f","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"\\\\&gt; The sad this with RDNA is the potential is there\\n\\nHaha agreed, the problem I see with ROCm is that they're locked into the Tensile backend that's used by all their BLAS libraries - which provides some inflexibility.\\n\\nThat link is a bit misleading as the benchmark that the guy ran was just a throughput benchmark for the instructions (which seem to have now been removed), but yea, even in my own tests I can see that rocBLAS falls behind. Heck, I was able to write my own FP32/FP16 GEMMs for my 7900 GRE that in most cases beat rocBLAS (I didn't really focus on smaller matrix sies)\\n\\n[adelj88/rocm\\\\_wmma\\\\_gemm: WMMA GEMM in ROCm for RDNA GPUs](https://github.com/adelj88/rocm_wmma_gemm)\\n\\n[adelj88/rocm\\\\_sgemm: Single-precision GEMM in ROCm](https://github.com/adelj88/rocm_sgemm/tree/master)\\n\\nThese two are already primed to be tuned for either RDNA3.5 or RDNA4. While I think the RDNA4 would be a lot more fun to tinker with, I just wonder if I'll be missing out on running larger LLM models if I'm just limited to 32GB VRAM.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n4iv5l2","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;&amp;gt; The sad this with RDNA is the potential is there&lt;/p&gt;\\n\\n&lt;p&gt;Haha agreed, the problem I see with ROCm is that they&amp;#39;re locked into the Tensile backend that&amp;#39;s used by all their BLAS libraries - which provides some inflexibility.&lt;/p&gt;\\n\\n&lt;p&gt;That link is a bit misleading as the benchmark that the guy ran was just a throughput benchmark for the instructions (which seem to have now been removed), but yea, even in my own tests I can see that rocBLAS falls behind. Heck, I was able to write my own FP32/FP16 GEMMs for my 7900 GRE that in most cases beat rocBLAS (I didn&amp;#39;t really focus on smaller matrix sies)&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://github.com/adelj88/rocm_wmma_gemm\\"&gt;adelj88/rocm_wmma_gemm: WMMA GEMM in ROCm for RDNA GPUs&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://github.com/adelj88/rocm_sgemm/tree/master\\"&gt;adelj88/rocm_sgemm: Single-precision GEMM in ROCm&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;These two are already primed to be tuned for either RDNA3.5 or RDNA4. While I think the RDNA4 would be a lot more fun to tinker with, I just wonder if I&amp;#39;ll be missing out on running larger LLM models if I&amp;#39;m just limited to 32GB VRAM.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m6b151","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6b151/updated_strix_halo_ryzen_ai_max_395_llm_benchmark/n4iv5l2/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753191613,"author_flair_text":null,"treatment_tags":[],"created_utc":1753191613,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n4irw8u","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"randomfoo2","can_mod_post":false,"created_utc":1753190561,"send_replies":true,"parent_id":"t1_n4iozvq","score":1,"author_fullname":"t2_eztox","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"tbt, if your goal is to tinker, I think RDNA4 would be a lot more fun: [https://gpuopen.com/learn/using\\\\_matrix\\\\_core\\\\_amd\\\\_rdna4/](https://gpuopen.com/learn/using_matrix_core_amd_rdna4/)\\n\\nThe sad this with RDNA is the potential is there, someone even managed to hit theoretical TFLOPS out of a 7900 XTX a few years back: [https://cprimozic.net/notes/posts/machine-learning-benchmarks-on-the-7900-xtx/#tinygrad-rdna3-matrix-multiplication-benchmark](https://cprimozic.net/notes/posts/machine-learning-benchmarks-on-the-7900-xtx/#tinygrad-rdna3-matrix-multiplication-benchmark) \\\\- but nothing close in efficiency has ever into ROCm...","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4irw8u","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;tbt, if your goal is to tinker, I think RDNA4 would be a lot more fun: &lt;a href=\\"https://gpuopen.com/learn/using_matrix_core_amd_rdna4/\\"&gt;https://gpuopen.com/learn/using_matrix_core_amd_rdna4/&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;The sad this with RDNA is the potential is there, someone even managed to hit theoretical TFLOPS out of a 7900 XTX a few years back: &lt;a href=\\"https://cprimozic.net/notes/posts/machine-learning-benchmarks-on-the-7900-xtx/#tinygrad-rdna3-matrix-multiplication-benchmark\\"&gt;https://cprimozic.net/notes/posts/machine-learning-benchmarks-on-the-7900-xtx/#tinygrad-rdna3-matrix-multiplication-benchmark&lt;/a&gt; - but nothing close in efficiency has ever into ROCm...&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m6b151","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6b151/updated_strix_halo_ryzen_ai_max_395_llm_benchmark/n4irw8u/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753190561,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4iozvq","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"No-Assist-4041","can_mod_post":false,"created_utc":1753189595,"send_replies":true,"parent_id":"t3_1m6b151","score":1,"author_fullname":"t2_ly870h93f","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Nice, I'm currently considering between this or the R9700 as I'm planning to just tinker around and optimize more HIP kernels (no plan to upstream, just as practice). I'm curious, what are the main bottlenecks that you see right now on the ROCm side vs the Vulkan side?\\n\\nI'm glad that my repository helped you file a report concerning the rocBLAS performance though.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4iozvq","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Nice, I&amp;#39;m currently considering between this or the R9700 as I&amp;#39;m planning to just tinker around and optimize more HIP kernels (no plan to upstream, just as practice). I&amp;#39;m curious, what are the main bottlenecks that you see right now on the ROCm side vs the Vulkan side?&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;m glad that my repository helped you file a report concerning the rocBLAS performance though.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6b151/updated_strix_halo_ryzen_ai_max_395_llm_benchmark/n4iozvq/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753189595,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m6b151","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4nhdmt","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"randomfoo2","can_mod_post":false,"created_utc":1753242176,"send_replies":true,"parent_id":"t1_n4jpfg9","score":2,"author_fullname":"t2_eztox","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"HIP **is** the ROCm backend for llama.cpp. Review the repo results to see the head to head for each model tested.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4nhdmt","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;HIP &lt;strong&gt;is&lt;/strong&gt; the ROCm backend for llama.cpp. Review the repo results to see the head to head for each model tested.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m6b151","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6b151/updated_strix_halo_ryzen_ai_max_395_llm_benchmark/n4nhdmt/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753242176,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n4jpfg9","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"No_Influence175","can_mod_post":false,"created_utc":1753200246,"send_replies":true,"parent_id":"t3_1m6b151","score":1,"author_fullname":"t2_6lc8qdqr","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Great jobs! Github has just update the ROCm which says AI Max is supported, could u help to use ROCm and make a compare with Vulkan? Thanks.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4jpfg9","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Great jobs! Github has just update the ROCm which says AI Max is supported, could u help to use ROCm and make a compare with Vulkan? Thanks.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6b151/updated_strix_halo_ryzen_ai_max_395_llm_benchmark/n4jpfg9/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753200246,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m6b151","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4ke5aa","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Snoo-83094","can_mod_post":false,"created_utc":1753206995,"send_replies":true,"parent_id":"t3_1m6b151","score":1,"author_fullname":"t2_752tmplg","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"im waiting for cluster benchmarks with these","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4ke5aa","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;im waiting for cluster benchmarks with these&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6b151/updated_strix_halo_ryzen_ai_max_395_llm_benchmark/n4ke5aa/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753206995,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m6b151","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4kmj7e","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"paul_tu","can_mod_post":false,"created_utc":1753209326,"send_replies":true,"parent_id":"t3_1m6b151","score":1,"author_fullname":"t2_whtsd","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Could you please share setup guide for this please?\\n\\nAs a GMKTEC Evo x-2 owner I'd be very interested\\n\\nWindows still missing all the necessary backends","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4kmj7e","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Could you please share setup guide for this please?&lt;/p&gt;\\n\\n&lt;p&gt;As a GMKTEC Evo x-2 owner I&amp;#39;d be very interested&lt;/p&gt;\\n\\n&lt;p&gt;Windows still missing all the necessary backends&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6b151/updated_strix_halo_ryzen_ai_max_395_llm_benchmark/n4kmj7e/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753209326,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m6b151","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4kqzpg","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"simracerman","can_mod_post":false,"created_utc":1753210580,"send_replies":true,"parent_id":"t3_1m6b151","score":1,"author_fullname":"t2_vbzgnic","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Whatâ€™s the state of ROCm in Windows for the 395? AMD said they will accelerate their development but not sure if that meant Windows or Linux.\\n\\nI want to get a similar box, but now Iâ€™m torn because I really donâ€™t want to migrate my main PC to Linux.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4kqzpg","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Whatâ€™s the state of ROCm in Windows for the 395? AMD said they will accelerate their development but not sure if that meant Windows or Linux.&lt;/p&gt;\\n\\n&lt;p&gt;I want to get a similar box, but now Iâ€™m torn because I really donâ€™t want to migrate my main PC to Linux.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6b151/updated_strix_halo_ryzen_ai_max_395_llm_benchmark/n4kqzpg/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753210580,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m6b151","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),o=()=>e.jsx(t,{data:l});export{o as default};
