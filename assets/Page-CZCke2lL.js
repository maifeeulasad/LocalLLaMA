import{j as e}from"./index-DLSqWzaI.js";import{R as l}from"./RedditPostRenderer-CysRo2D_.js";import"./index-COXiL3Lo.js";const t=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Love LocalLLM and have been hosting smaller models on my 4090 for a long time.  Local LLM seems to be viable now so I got 2x 5090s.  I'm trying to run Devstral small 8Q.  It uses about 85-90% of the dual 5090 memory with full context.    \\n  \\nThe issue I'm having is they don't hit 100% utilization.   Both GPUs sit at about 40-50% utilization.\\n\\nThreadripper 7960x  \\n256gb ddr5 6000mt/s\\n\\nTYIA","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"GPUs low utilization?","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":79,"top_awarded_type":null,"hide_score":false,"name":"t3_1m16h0b","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.65,"author_flair_background_color":null,"ups":7,"total_awards_received":0,"media_embed":{},"thumbnail_width":140,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_53nkx","secure_media":null,"is_reddit_media_domain":true,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":7,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"https://b.thumbs.redditmedia.com/dW52RcEDaQiJjX4tw9tmdCuS78YJESEaHqh0m4LDmfc.jpg","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"post_hint":"image","content_categories":null,"is_self":false,"subreddit_type":"public","created":1752651627,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"i.redd.it","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Love LocalLLM and have been hosting smaller models on my 4090 for a long time.  Local LLM seems to be viable now so I got 2x 5090s.  I&amp;#39;m trying to run Devstral small 8Q.  It uses about 85-90% of the dual 5090 memory with full context.    &lt;/p&gt;\\n\\n&lt;p&gt;The issue I&amp;#39;m having is they don&amp;#39;t hit 100% utilization.   Both GPUs sit at about 40-50% utilization.&lt;/p&gt;\\n\\n&lt;p&gt;Threadripper 7960x&lt;br/&gt;\\n256gb ddr5 6000mt/s&lt;/p&gt;\\n\\n&lt;p&gt;TYIA&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"url_overridden_by_dest":"https://i.redd.it/kdwq8atcw6df1.png","view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"preview":{"images":[{"source":{"url":"https://preview.redd.it/kdwq8atcw6df1.png?auto=webp&amp;s=cdf9107daf62d89291b5ae0aa6cc67d4a46c4c6b","width":353,"height":201},"resolutions":[{"url":"https://preview.redd.it/kdwq8atcw6df1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=2a600e6922e30f4a88be9074b33181ff02b90084","width":108,"height":61},{"url":"https://preview.redd.it/kdwq8atcw6df1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=62af8e712a102326b711b75c6eabe23286c2a8ad","width":216,"height":122},{"url":"https://preview.redd.it/kdwq8atcw6df1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=17eaed0b5acc766f0c26616616de7556cfa64e4b","width":320,"height":182}],"variants":{},"id":"ms7iiYVGCNzRK0IIW2Y-Wq-je6rK1N0HVMw9Y8hAeWg"}],"enabled":true},"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"mod_note":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"num_reports":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1m16h0b","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"rymn","discussion_type":null,"num_comments":12,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1m16h0b/gpus_low_utilization/","stickied":false,"url":"https://i.redd.it/kdwq8atcw6df1.png","subreddit_subscribers":499773,"created_utc":1752651627,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3er4j7","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"rymn","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3equ13","score":1,"author_fullname":"t2_53nkx","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Thanks. Also, they're in stock at MSRP.  I got mine open box from newegg for less than MSRP","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3er4j7","is_submitter":true,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thanks. Also, they&amp;#39;re in stock at MSRP.  I got mine open box from newegg for less than MSRP&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m16h0b","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m16h0b/gpus_low_utilization/n3er4j7/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752652292,"author_flair_text":null,"treatment_tags":[],"created_utc":1752652292,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3erd90","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"rymn","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3equ13","score":1,"author_fullname":"t2_53nkx","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Do you mean I should increase or decrease batch size?","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3erd90","is_submitter":true,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Do you mean I should increase or decrease batch size?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m16h0b","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m16h0b/gpus_low_utilization/n3erd90/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752652431,"author_flair_text":null,"treatment_tags":[],"created_utc":1752652431,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3equ13","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"michaelsoft__binbows","can_mod_post":false,"created_utc":1752652127,"send_replies":true,"parent_id":"t1_n3eqaq6","score":6,"author_fullname":"t2_iifi6ul2l","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"i can go from 160tok/s running one inference to nearly 700tok/s total throughput on a 3090 with sglang (qwen3-a3b)\\n\\nI believe a 5090 has even more arithmetic intensity than a 3090 so it may benefit from even more batching. \\n\\nAlso save some of these chips for people like me. Its been so hard to get a 5090 I don't even want one anymore.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3equ13","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;i can go from 160tok/s running one inference to nearly 700tok/s total throughput on a 3090 with sglang (qwen3-a3b)&lt;/p&gt;\\n\\n&lt;p&gt;I believe a 5090 has even more arithmetic intensity than a 3090 so it may benefit from even more batching. &lt;/p&gt;\\n\\n&lt;p&gt;Also save some of these chips for people like me. Its been so hard to get a 5090 I don&amp;#39;t even want one anymore.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m16h0b","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m16h0b/gpus_low_utilization/n3equ13/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752652127,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}}],"before":null}},"user_reports":[],"saved":false,"id":"n3eqaq6","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"LoSboccacc","can_mod_post":false,"created_utc":1752651824,"send_replies":true,"parent_id":"t3_1m16h0b","score":15,"author_fullname":"t2_dievh","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Yeah bottleneck is memory bandwidth. ","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3eqaq6","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yeah bottleneck is memory bandwidth. &lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m16h0b/gpus_low_utilization/n3eqaq6/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752651824,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m16h0b","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":15}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3eurnb","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"MaxKruse96","can_mod_post":false,"created_utc":1752654422,"send_replies":true,"parent_id":"t3_1m16h0b","score":7,"author_fullname":"t2_pfi81","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"use the cuda12 runtime, also windows task manager doesnt show you cuda usage... use hwinfo or smth similar","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3eurnb","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;use the cuda12 runtime, also windows task manager doesnt show you cuda usage... use hwinfo or smth similar&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m16h0b/gpus_low_utilization/n3eurnb/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752654422,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m16h0b","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":7}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"50c36eba-fdca-11ee-9735-92a88d7e3b87","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3exjw7","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"maifee","can_mod_post":false,"created_utc":1752656058,"send_replies":true,"parent_id":"t3_1m16h0b","score":5,"author_fullname":"t2_1fuhylzi","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Use \`nvidia-smi\` to view actual usage.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3exjw7","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"Ollama"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Use &lt;code&gt;nvidia-smi&lt;/code&gt; to view actual usage.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m16h0b/gpus_low_utilization/n3exjw7/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752656058,"author_flair_text":"Ollama","treatment_tags":[],"link_id":"t3_1m16h0b","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3er5cg","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"rymn","can_mod_post":false,"created_utc":1752652305,"send_replies":true,"parent_id":"t1_n3eqehu","score":1,"author_fullname":"t2_53nkx","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"lm studio","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3er5cg","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;lm studio&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m16h0b","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m16h0b/gpus_low_utilization/n3er5cg/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752652305,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3eqehu","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"fizzy1242","can_mod_post":false,"created_utc":1752651883,"send_replies":true,"parent_id":"t3_1m16h0b","score":1,"author_fullname":"t2_16zcsx","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"which backend are you using for the llms? llama.cpp isn't the best option for pure gpu inference.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3eqehu","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;which backend are you using for the llms? llama.cpp isn&amp;#39;t the best option for pure gpu inference.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m16h0b/gpus_low_utilization/n3eqehu/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752651883,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m16h0b","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3eu6l6","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"catgirl_liker","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3erzpu","score":3,"author_fullname":"t2_w7qdzn48","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"That's a different batch size","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3eu6l6","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;That&amp;#39;s a different batch size&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m16h0b","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m16h0b/gpus_low_utilization/n3eu6l6/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752654073,"author_flair_text":null,"treatment_tags":[],"created_utc":1752654073,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3et2mj","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"triynizzles1","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3erzpu","score":2,"author_fullname":"t2_zr0g49ixt","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Changing the batch size will only make a difference if you’re sending multiple requests at once. If you are the only person using the system the AI model only has your prompt to process.\\n\\nBasically there are so many cores and compute available the vram cant keep the cores fed with data to process. This is where batching comes in. If you send one prompt, the model will start to be read from memory and computed a few megabytes at a time. The cores will finish computing before memory can provide new data to process. If you have five prompts sent to the model at once, it will use the idle time to compute the other request. This shifts the bottleneck off of memory bandwidth and onto raw compute.\\n\\nFor your use case 50% utilization per GPU is the best you will get. If this was a server processing a bunch of requests at once then you would be able to take advantage of batching and would see higher GPU utilization.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3et2mj","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Changing the batch size will only make a difference if you’re sending multiple requests at once. If you are the only person using the system the AI model only has your prompt to process.&lt;/p&gt;\\n\\n&lt;p&gt;Basically there are so many cores and compute available the vram cant keep the cores fed with data to process. This is where batching comes in. If you send one prompt, the model will start to be read from memory and computed a few megabytes at a time. The cores will finish computing before memory can provide new data to process. If you have five prompts sent to the model at once, it will use the idle time to compute the other request. This shifts the bottleneck off of memory bandwidth and onto raw compute.&lt;/p&gt;\\n\\n&lt;p&gt;For your use case 50% utilization per GPU is the best you will get. If this was a server processing a bunch of requests at once then you would be able to take advantage of batching and would see higher GPU utilization.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m16h0b","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m16h0b/gpus_low_utilization/n3et2mj/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752653419,"author_flair_text":null,"treatment_tags":[],"created_utc":1752653419,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n3erzpu","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"rymn","can_mod_post":false,"created_utc":1752652792,"send_replies":true,"parent_id":"t1_n3er1si","score":1,"author_fullname":"t2_53nkx","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Default batch is lm studio is 512.  I changed it to 1024 and didn't notice any changes.  I'll change it to 1 and try again.  I'm noticing that eventhough there is multiple gb of vram available, \\"Shared GPU memory\\" for both gpus is 2/128GB","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3erzpu","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Default batch is lm studio is 512.  I changed it to 1024 and didn&amp;#39;t notice any changes.  I&amp;#39;ll change it to 1 and try again.  I&amp;#39;m noticing that eventhough there is multiple gb of vram available, &amp;quot;Shared GPU memory&amp;quot; for both gpus is 2/128GB&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m16h0b","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m16h0b/gpus_low_utilization/n3erzpu/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752652792,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3er1si","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"triynizzles1","can_mod_post":false,"created_utc":1752652248,"send_replies":true,"parent_id":"t3_1m16h0b","score":1,"author_fullname":"t2_zr0g49ixt","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"If you are only processing one prompt at a time e.g. Batch size of one. The first 30 GB of the model are processed on GPU 0 then the next 30 gb of layers are processed on GPU 1 while GPU 0 idles. If he had a batch size of five for example and sent five requests at once, you would see better gpu utilization and likely not have a drop in token output per prompt. Even though the gpus are outputting five times the tokens","edited":1752652631,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3er1si","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;If you are only processing one prompt at a time e.g. Batch size of one. The first 30 GB of the model are processed on GPU 0 then the next 30 gb of layers are processed on GPU 1 while GPU 0 idles. If he had a batch size of five for example and sent five requests at once, you would see better gpu utilization and likely not have a drop in token output per prompt. Even though the gpus are outputting five times the tokens&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m16h0b/gpus_low_utilization/n3er1si/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752652248,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m16h0b","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),n=()=>e.jsx(l,{data:t});export{n as default};
