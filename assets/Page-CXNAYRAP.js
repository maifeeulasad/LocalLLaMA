import{j as e}from"./index-BOnf-UhU.js";import{R as l}from"./RedditPostRenderer-Ce39qICS.js";import"./index-CDase6VD.js";const t=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Hi everyone ðŸ‘‹ \\nI am trying to build an offline coding assistant. For that I have to do POC. Anyone having any idea about this? To implement this in limited environment?","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Offline Coding Assistant","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1m5ew98","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.5,"author_flair_background_color":null,"subreddit_type":"public","ups":0,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_bb6pp919","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":0,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1753092188,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Hi everyone ðŸ‘‹ \\nI am trying to build an offline coding assistant. For that I have to do POC. Anyone having any idea about this? To implement this in limited environment?&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":true,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1m5ew98","is_robot_indexable":true,"num_duplicates":1,"report_reasons":null,"author":"eternalHarsh","discussion_type":null,"num_comments":11,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1m5ew98/offline_coding_assistant/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1m5ew98/offline_coding_assistant/","subreddit_subscribers":502721,"created_utc":1753092188,"num_crossposts":2,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4dxcqd","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"GPTrack_ai","can_mod_post":false,"created_utc":1753122697,"send_replies":true,"parent_id":"t1_n4bedg8","score":2,"author_fullname":"t2_1tpuoj72sa","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Only people who do not know what the Apple logo really means buy Apple. Real men prefer the one and only Nvidia. IMHO GH200 624GB is a good start. If you can afford it DGX Station and HGX B200 are beasts.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4dxcqd","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Only people who do not know what the Apple logo really means buy Apple. Real men prefer the one and only Nvidia. IMHO GH200 624GB is a good start. If you can afford it DGX Station and HGX B200 are beasts.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m5ew98","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5ew98/offline_coding_assistant/n4dxcqd/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753122697,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4hw7nk","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Rich_Repeat_22","can_mod_post":false,"created_utc":1753177204,"send_replies":true,"parent_id":"t1_n4bedg8","score":2,"author_fullname":"t2_viufiki6","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"What I don't get is why people still push for Mac Studio when for half the money someone can build an Xeon 4 server 2x8480 QS + MS73HB1 mobo + 512GB (16x32) RAM + RTX5090 utilizing Intel AMX and ktransformers ðŸ¤”","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4hw7nk","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;What I don&amp;#39;t get is why people still push for Mac Studio when for half the money someone can build an Xeon 4 server 2x8480 QS + MS73HB1 mobo + 512GB (16x32) RAM + RTX5090 utilizing Intel AMX and ktransformers ðŸ¤”&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m5ew98","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5ew98/offline_coding_assistant/n4hw7nk/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753177204,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n4bedg8","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DepthHour1669","can_mod_post":false,"created_utc":1753093158,"send_replies":true,"parent_id":"t3_1m5ew98","score":2,"author_fullname":"t2_t6glzswk","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Depends on your budget. \\n\\nEasiest highest performance answer is buy a Mac Studio 512GB for $10k and run Deepseek R1 with llama.cpp on it.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4bedg8","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Depends on your budget. &lt;/p&gt;\\n\\n&lt;p&gt;Easiest highest performance answer is buy a Mac Studio 512GB for $10k and run Deepseek R1 with llama.cpp on it.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5ew98/offline_coding_assistant/n4bedg8/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753093158,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m5ew98","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4bd4dj","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AppearanceHeavy6724","can_mod_post":false,"created_utc":1753092489,"send_replies":true,"parent_id":"t3_1m5ew98","score":3,"author_fullname":"t2_uz37qfx5","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Start with any of Qwen2.5 coder or Qwen 3.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4bd4dj","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Start with any of Qwen2.5 coder or Qwen 3.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5ew98/offline_coding_assistant/n4bd4dj/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753092489,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m5ew98","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4cbnsb","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"anarchos","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4c0fkm","score":1,"author_fullname":"t2_44chy","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"on macOS anyways, applications are just zip files, so you can just open it up, find the js bundle and look in it.  The code is minified but the prompts are there in plain text.  Not sure about other platforms.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n4cbnsb","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;on macOS anyways, applications are just zip files, so you can just open it up, find the js bundle and look in it.  The code is minified but the prompts are there in plain text.  Not sure about other platforms.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m5ew98","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5ew98/offline_coding_assistant/n4cbnsb/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753106282,"author_flair_text":null,"treatment_tags":[],"created_utc":1753106282,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4c0fkm","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"eternalHarsh","can_mod_post":false,"created_utc":1753102557,"send_replies":true,"parent_id":"t1_n4bj1t0","score":1,"author_fullname":"t2_bb6pp919","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Where can I get claude binaries?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4c0fkm","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Where can I get claude binaries?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m5ew98","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5ew98/offline_coding_assistant/n4c0fkm/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753102557,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4bj1t0","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"anarchos","can_mod_post":false,"created_utc":1753095487,"send_replies":true,"parent_id":"t3_1m5ew98","score":3,"author_fullname":"t2_44chy","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"1. Open up the Claude Code binary  \\n2. Steal the prompt  \\n3. Look at the tools it has and steal their prompts  \\n4. Use something like the OpenAI Agent SDK (for python or typescript), for TS you can use the Vercel AI SDK to make the OAI-Agent-SDK work with pretty much any model, including local models (ollama plugin)  \\n5. Reimplement all the tools including their input types  \\n6. Make a basic CLI app.  You might consider using Ink, which is a react renderer for the command line, it's what Claude Code/Gemini/etc use\\n\\nYou can more or less re-implement a very basic version Claude Code in \\\\~300 LOC (not including prompts/instructions).  This will be a super basic version, YOLO'ing it all without permissions or anything fancy like plan mode, but running it through Claude models outputs more or less the same code quality as Claude Code itself.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4bj1t0","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;ol&gt;\\n&lt;li&gt;Open up the Claude Code binary&lt;br/&gt;&lt;/li&gt;\\n&lt;li&gt;Steal the prompt&lt;br/&gt;&lt;/li&gt;\\n&lt;li&gt;Look at the tools it has and steal their prompts&lt;br/&gt;&lt;/li&gt;\\n&lt;li&gt;Use something like the OpenAI Agent SDK (for python or typescript), for TS you can use the Vercel AI SDK to make the OAI-Agent-SDK work with pretty much any model, including local models (ollama plugin)&lt;br/&gt;&lt;/li&gt;\\n&lt;li&gt;Reimplement all the tools including their input types&lt;br/&gt;&lt;/li&gt;\\n&lt;li&gt;Make a basic CLI app.  You might consider using Ink, which is a react renderer for the command line, it&amp;#39;s what Claude Code/Gemini/etc use&lt;/li&gt;\\n&lt;/ol&gt;\\n\\n&lt;p&gt;You can more or less re-implement a very basic version Claude Code in ~300 LOC (not including prompts/instructions).  This will be a super basic version, YOLO&amp;#39;ing it all without permissions or anything fancy like plan mode, but running it through Claude models outputs more or less the same code quality as Claude Code itself.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5ew98/offline_coding_assistant/n4bj1t0/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753095487,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m5ew98","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4bnt94","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"eternalHarsh","can_mod_post":false,"created_utc":1753097670,"send_replies":true,"parent_id":"t1_n4bddsf","score":0,"author_fullname":"t2_bb6pp919","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I plan on customizing the vscode plugin to incorporate local llm for chat like features but the local llm which is good for coding is hard for me to find out. Also do I need to customise it for a use case or can I use it raw?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4bnt94","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I plan on customizing the vscode plugin to incorporate local llm for chat like features but the local llm which is good for coding is hard for me to find out. Also do I need to customise it for a use case or can I use it raw?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m5ew98","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5ew98/offline_coding_assistant/n4bnt94/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753097670,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}},"user_reports":[],"saved":false,"id":"n4bddsf","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Round_Mixture_7541","can_mod_post":false,"created_utc":1753092633,"send_replies":true,"parent_id":"t3_1m5ew98","score":2,"author_fullname":"t2_114cnblv7x","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Clone a random chatbot repo that is OpenAI API compatible. Point your locally running LLM instance in your newly created chatbot. Want more? Throw in another model trained on FIM tokens to handle autocompletion (you can even use the same model for both tasks, look Qwen 2.5 Coder or Codestral). \\n\\nDone.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4bddsf","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Clone a random chatbot repo that is OpenAI API compatible. Point your locally running LLM instance in your newly created chatbot. Want more? Throw in another model trained on FIM tokens to handle autocompletion (you can even use the same model for both tasks, look Qwen 2.5 Coder or Codestral). &lt;/p&gt;\\n\\n&lt;p&gt;Done.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5ew98/offline_coding_assistant/n4bddsf/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753092633,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m5ew98","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4cmd8u","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"dolomitt","can_mod_post":false,"created_utc":1753109551,"send_replies":true,"parent_id":"t3_1m5ew98","score":3,"author_fullname":"t2_15nfya8","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Vscode with continue pointing to local ollama","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4cmd8u","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Vscode with continue pointing to local ollama&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5ew98/offline_coding_assistant/n4cmd8u/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753109551,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m5ew98","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4hvv1l","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Rich_Repeat_22","can_mod_post":false,"created_utc":1753177005,"send_replies":true,"parent_id":"t3_1m5ew98","score":2,"author_fullname":"t2_viufiki6","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Depends your budget. For normal home system GLM4-32B is AMAZING. So what you need is something like a AMD 395 APU based system with minimum 64GB preferably 128GB RAM. That's imho the CHEAPEST option to run something like that with big context window since we are in $1600-$1900 range for a full miniPC which can be used for gaming also (around 6700XT desktop perf) and as workstation (effectively has 9950X in it). \\n\\nAfter that the whole point is how much money you want to spend to load big models. \\n\\nA single GPU (like R9700/RTX5090) + dual 8480 QS + MS73HB1 mobo + 512GB RAM (16x32 RDIMM DDR5 modules) will set you back â‚¬4000-â‚¬5000 (depending the GPU) and you can use Intel AMX and ktransformers to run full size Deepseek R1 at respectable speeds.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4hvv1l","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Depends your budget. For normal home system GLM4-32B is AMAZING. So what you need is something like a AMD 395 APU based system with minimum 64GB preferably 128GB RAM. That&amp;#39;s imho the CHEAPEST option to run something like that with big context window since we are in $1600-$1900 range for a full miniPC which can be used for gaming also (around 6700XT desktop perf) and as workstation (effectively has 9950X in it). &lt;/p&gt;\\n\\n&lt;p&gt;After that the whole point is how much money you want to spend to load big models. &lt;/p&gt;\\n\\n&lt;p&gt;A single GPU (like R9700/RTX5090) + dual 8480 QS + MS73HB1 mobo + 512GB RAM (16x32 RDIMM DDR5 modules) will set you back â‚¬4000-â‚¬5000 (depending the GPU) and you can use Intel AMX and ktransformers to run full size Deepseek R1 at respectable speeds.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5ew98/offline_coding_assistant/n4hvv1l/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753177005,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m5ew98","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}}]`),r=()=>e.jsx(l,{data:t});export{r as default};
