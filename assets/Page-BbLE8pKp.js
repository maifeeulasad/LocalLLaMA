import{j as e}from"./index-BlGsFJYy.js";import{R as l}from"./RedditPostRenderer-B6uvq_Zl.js";import"./index-DDvVtNwD.js";const a=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Hey folks!\\n\\nWeâ€™ve been building a browser-native, on-device AI assistant that runs entirely locally â€” no servers, no telemetry, no API calls. All inference happens through Ollama (for now), though we also support running lightweight models via WebLLM for quick testing directly in the browser.\\n\\nThe motivation: while local model support is getting better, the UX side is still pretty raw. We wanted to explore what it would take to make LLMs feel like a real part of your workflow â€” embedded in the browser environment itself, not just in a terminal or Electron wrapper.\\n\\nA few things weâ€™re trying to solve:\\n\\n* persistent context across tabs\\n* writing tools with local context awareness\\n* in-page bilingual translation\\n* quick actions (right-click to trigger prompt templates)\\n* local-first web search (open and process real pages in browser tabs)\\n* file-based conversations (PDFs, image files), user-controlled memory (Both coming very soon)\\n\\nWeâ€™re using Ollama as the local model backend, but what weâ€™ve really focused on is designing and building the GUI and interaction layer itâ€™s missing â€” something more usable for non-technical users, and more native to the browser environment.\\n\\nCodeâ€™s open here if you want to poke around: [https://github.com/NativeMindBrowser/NativeMindExtension](https://github.com/NativeMindBrowser/NativeMindExtension)\\n\\nWould love to hear thoughts from others working on local tooling.","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Anyone else interested in a 100% on-device browser AI assistant?","link_flair_richtext":[{"e":"text","t":"Discussion"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lzimcq","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.47,"author_flair_background_color":null,"subreddit_type":"public","ups":0,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_1nibexa2mk","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Discussion","can_mod_post":false,"score":0,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":1752549636,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"post_hint":"self","content_categories":null,"is_self":true,"mod_note":null,"created":1752488062,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Hey folks!&lt;/p&gt;\\n\\n&lt;p&gt;Weâ€™ve been building a browser-native, on-device AI assistant that runs entirely locally â€” no servers, no telemetry, no API calls. All inference happens through Ollama (for now), though we also support running lightweight models via WebLLM for quick testing directly in the browser.&lt;/p&gt;\\n\\n&lt;p&gt;The motivation: while local model support is getting better, the UX side is still pretty raw. We wanted to explore what it would take to make LLMs feel like a real part of your workflow â€” embedded in the browser environment itself, not just in a terminal or Electron wrapper.&lt;/p&gt;\\n\\n&lt;p&gt;A few things weâ€™re trying to solve:&lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;persistent context across tabs&lt;/li&gt;\\n&lt;li&gt;writing tools with local context awareness&lt;/li&gt;\\n&lt;li&gt;in-page bilingual translation&lt;/li&gt;\\n&lt;li&gt;quick actions (right-click to trigger prompt templates)&lt;/li&gt;\\n&lt;li&gt;local-first web search (open and process real pages in browser tabs)&lt;/li&gt;\\n&lt;li&gt;file-based conversations (PDFs, image files), user-controlled memory (Both coming very soon)&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;p&gt;Weâ€™re using Ollama as the local model backend, but what weâ€™ve really focused on is designing and building the GUI and interaction layer itâ€™s missing â€” something more usable for non-technical users, and more native to the browser environment.&lt;/p&gt;\\n\\n&lt;p&gt;Codeâ€™s open here if you want to poke around: &lt;a href=\\"https://github.com/NativeMindBrowser/NativeMindExtension\\"&gt;https://github.com/NativeMindBrowser/NativeMindExtension&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;Would love to hear thoughts from others working on local tooling.&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":true,"is_crosspostable":false,"pinned":false,"over_18":false,"preview":{"images":[{"source":{"url":"https://external-preview.redd.it/SCRdaLyzDoTV55-cdYQI2oN4jk8P9OR4Ai2vp6y5bdg.png?auto=webp&amp;s=8042ecc768e0ad4bb27467181ccc60fb64733c12","width":1200,"height":600},"resolutions":[{"url":"https://external-preview.redd.it/SCRdaLyzDoTV55-cdYQI2oN4jk8P9OR4Ai2vp6y5bdg.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=307824a95b90814f4b420f060a268603bfc26a80","width":108,"height":54},{"url":"https://external-preview.redd.it/SCRdaLyzDoTV55-cdYQI2oN4jk8P9OR4Ai2vp6y5bdg.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=fb1b315a93a4b4ce546d598ae89831e0041a7c25","width":216,"height":108},{"url":"https://external-preview.redd.it/SCRdaLyzDoTV55-cdYQI2oN4jk8P9OR4Ai2vp6y5bdg.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=b8f78b55a419b2be3251f47373f246de4cb2e2a1","width":320,"height":160},{"url":"https://external-preview.redd.it/SCRdaLyzDoTV55-cdYQI2oN4jk8P9OR4Ai2vp6y5bdg.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=073ef6e17c32cb48adaf9e9ccc4b2d4de33710b2","width":640,"height":320},{"url":"https://external-preview.redd.it/SCRdaLyzDoTV55-cdYQI2oN4jk8P9OR4Ai2vp6y5bdg.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=1c7e1613aa961ad578e6da9fda71e63ffd3cb433","width":960,"height":480},{"url":"https://external-preview.redd.it/SCRdaLyzDoTV55-cdYQI2oN4jk8P9OR4Ai2vp6y5bdg.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=98e89b83c879133a94111d825fb8cb2d951e541e","width":1080,"height":540}],"variants":{},"id":"SCRdaLyzDoTV55-cdYQI2oN4jk8P9OR4Ai2vp6y5bdg"}],"enabled":false},"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"5f921ea4-c7bc-11ed-9c23-3a00622979b4","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#646d73","id":"1lzimcq","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"InfiniteJX","discussion_type":null,"num_comments":33,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lzimcq/anyone_else_interested_in_a_100_ondevice_browser/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lzimcq/anyone_else_interested_in_a_100_ondevice_browser/","subreddit_subscribers":499296,"created_utc":1752488062,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n32iyt4","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Rakhmat3","can_mod_post":false,"created_utc":1752498469,"send_replies":true,"parent_id":"t1_n31zxyl","score":1,"author_fullname":"t2_m2ujq54i","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"It works for website calls, too.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n32iyt4","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It works for website calls, too.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lzimcq","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzimcq/anyone_else_interested_in_a_100_ondevice_browser/n32iyt4/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752498469,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n31zxyl","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"rditorx","can_mod_post":false,"created_utc":1752490837,"send_replies":true,"parent_id":"t3_1lzimcq","score":4,"author_fullname":"t2_l5da600d","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Chrome is doing it for extensions\\n\\nhttps://developer.chrome.com/docs/ai/prompt-api","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n31zxyl","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Chrome is doing it for extensions&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://developer.chrome.com/docs/ai/prompt-api\\"&gt;https://developer.chrome.com/docs/ai/prompt-api&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzimcq/anyone_else_interested_in_a_100_ondevice_browser/n31zxyl/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752490837,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lzimcq","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n32i7uz","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"henfiber","can_mod_post":false,"created_utc":1752498210,"send_replies":true,"parent_id":"t1_n31zzh3","score":1,"author_fullname":"t2_lw9me25","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"And they can interface with both local and remote  models (e.g., openrouter) this way. \\n\\nJust fyi, in case you really want to use some SW only compatible with ollama, you can use llama-swappo: https://github.com/kooshi/llama-swappo\\n\\nwhich is a patched version of llama-swap exposing ollama compatible endpoints and forwards the requests to your downstream OpenAI-API-compatible engine (llama.cpp server, vllm, sglang etc.)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n32i7uz","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;And they can interface with both local and remote  models (e.g., openrouter) this way. &lt;/p&gt;\\n\\n&lt;p&gt;Just fyi, in case you really want to use some SW only compatible with ollama, you can use llama-swappo: &lt;a href=\\"https://github.com/kooshi/llama-swappo\\"&gt;https://github.com/kooshi/llama-swappo&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;which is a patched version of llama-swap exposing ollama compatible endpoints and forwards the requests to your downstream OpenAI-API-compatible engine (llama.cpp server, vllm, sglang etc.)&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lzimcq","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzimcq/anyone_else_interested_in_a_100_ondevice_browser/n32i7uz/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752498210,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n31zzh3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"plankalkul-z1","can_mod_post":false,"created_utc":1752490858,"send_replies":true,"parent_id":"t3_1lzimcq","score":10,"author_fullname":"t2_w73n3yrsx","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"&gt; It runs local models via Ollama\\n\\n\\nYou would be much better off if you interlaced with local models via OpenAI-compatible API, and not via Ollama's. It's not more difficult, AND Ollama *will keep working*, since it supports it.\\n\\n\\n&gt; Anyone else interested in a 100% on-device browser AI assistant?\\n\\n\\nOh yeah, I am. But in its current form, your solution is completely useless to me: I *have* to run most models using vLLM or SGLang, for performance reasons. So I won't even try it.\\n\\n\\nNow, I'm not one of Ollama haters, I use it as well (via OpenAI's API!), and I admire the simplicity they managed to achieve in several key areas (such as memory management).\\n\\n\\nBut my goodness, every time I see another Ollama-only solution, I'm completely baffled... Why do you limit your audience like that for no apparent benefit? It's completely beyond me...","edited":1752492055,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n31zzh3","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;It runs local models via Ollama&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;You would be much better off if you interlaced with local models via OpenAI-compatible API, and not via Ollama&amp;#39;s. It&amp;#39;s not more difficult, AND Ollama &lt;em&gt;will keep working&lt;/em&gt;, since it supports it.&lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;Anyone else interested in a 100% on-device browser AI assistant?&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;Oh yeah, I am. But in its current form, your solution is completely useless to me: I &lt;em&gt;have&lt;/em&gt; to run most models using vLLM or SGLang, for performance reasons. So I won&amp;#39;t even try it.&lt;/p&gt;\\n\\n&lt;p&gt;Now, I&amp;#39;m not one of Ollama haters, I use it as well (via OpenAI&amp;#39;s API!), and I admire the simplicity they managed to achieve in several key areas (such as memory management).&lt;/p&gt;\\n\\n&lt;p&gt;But my goodness, every time I see another Ollama-only solution, I&amp;#39;m completely baffled... Why do you limit your audience like that for no apparent benefit? It&amp;#39;s completely beyond me...&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzimcq/anyone_else_interested_in_a_100_ondevice_browser/n31zzh3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752490858,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lzimcq","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":10}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3209jj","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"StoneCypher","can_mod_post":false,"created_utc":1752490989,"send_replies":true,"parent_id":"t3_1lzimcq","score":10,"author_fullname":"t2_6nze","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"we already have that in every browser and operating systemÂ \\n\\nyouâ€™ve never heard of it because nobody wants itÂ ","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3209jj","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;we already have that in every browser and operating systemÂ &lt;/p&gt;\\n\\n&lt;p&gt;youâ€™ve never heard of it because nobody wants itÂ &lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzimcq/anyone_else_interested_in_a_100_ondevice_browser/n3209jj/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752490989,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lzimcq","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":10}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n32bq2m","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Novel-Mechanic3448","can_mod_post":false,"created_utc":1752495849,"send_replies":true,"parent_id":"t3_1lzimcq","score":3,"author_fullname":"t2_1nxd0u4yoq","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"There dozens of these already. Get a clue buddy","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n32bq2m","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;There dozens of these already. Get a clue buddy&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzimcq/anyone_else_interested_in_a_100_ondevice_browser/n32bq2m/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752495849,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lzimcq","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n37x66e","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"omansharora","can_mod_post":false,"send_replies":true,"parent_id":"t1_n372v1d","score":1,"author_fullname":"t2_7qqmqfin","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"will you help me know how did you ? i wanna learn honestly","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n37x66e","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;will you help me know how did you ? i wanna learn honestly&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lzimcq","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzimcq/anyone_else_interested_in_a_100_ondevice_browser/n37x66e/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752562560,"author_flair_text":null,"treatment_tags":[],"created_utc":1752562560,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n372v1d","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"InfiniteJX","can_mod_post":false,"created_utc":1752548416,"send_replies":true,"parent_id":"t1_n32pc5r","score":1,"author_fullname":"t2_1nibexa2mk","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Hey! Thanks for your interest â€” sounds like youâ€™re working on a really cool idea.\\n\\nWeâ€™ve already implemented AI-powered automatic browsing and searching in our extension. As for AI-based form filling with custom prompts, that part is still under development â€” but itâ€™s definitely on our roadmap.\\n\\nIf youâ€™re open to trying our plugin and sharing feedback, weâ€™d love to hear how it works for your use case! ðŸ™Œ","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n372v1d","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Hey! Thanks for your interest â€” sounds like youâ€™re working on a really cool idea.&lt;/p&gt;\\n\\n&lt;p&gt;Weâ€™ve already implemented AI-powered automatic browsing and searching in our extension. As for AI-based form filling with custom prompts, that part is still under development â€” but itâ€™s definitely on our roadmap.&lt;/p&gt;\\n\\n&lt;p&gt;If youâ€™re open to trying our plugin and sharing feedback, weâ€™d love to hear how it works for your use case! ðŸ™Œ&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lzimcq","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzimcq/anyone_else_interested_in_a_100_ondevice_browser/n372v1d/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752548416,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n32pc5r","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"omansharora","can_mod_post":false,"created_utc":1752500602,"send_replies":true,"parent_id":"t3_1lzimcq","score":2,"author_fullname":"t2_7qqmqfin","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"i m interested, but i have a question. i have been trying to make the ai automatic broswer seacher and form filler as i want custom prompt for filling i m trying to build it up but still getting so many errors like sometimes it work sometyms i doesnt it would be really kind if someone cn help me thanks","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n32pc5r","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;i m interested, but i have a question. i have been trying to make the ai automatic broswer seacher and form filler as i want custom prompt for filling i m trying to build it up but still getting so many errors like sometimes it work sometyms i doesnt it would be really kind if someone cn help me thanks&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzimcq/anyone_else_interested_in_a_100_ondevice_browser/n32pc5r/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752500602,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lzimcq","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n36xkde","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Fussy-Fur3608","can_mod_post":false,"created_utc":1752546477,"send_replies":true,"parent_id":"t3_1lzimcq","score":2,"author_fullname":"t2_8383arktn","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"i've had a play with [https://huggingface.co/docs/transformers.js/en/index](https://huggingface.co/docs/transformers.js/en/index) and can recommend it for small models. but the small model trade-off might make it too \\"dumb\\" to achieve anything useful.  \\nOnce the model is cached, it's quick to load.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n36xkde","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;i&amp;#39;ve had a play with &lt;a href=\\"https://huggingface.co/docs/transformers.js/en/index\\"&gt;https://huggingface.co/docs/transformers.js/en/index&lt;/a&gt; and can recommend it for small models. but the small model trade-off might make it too &amp;quot;dumb&amp;quot; to achieve anything useful.&lt;br/&gt;\\nOnce the model is cached, it&amp;#39;s quick to load.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzimcq/anyone_else_interested_in_a_100_ondevice_browser/n36xkde/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752546477,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lzimcq","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n31x8vd","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"FullstackSensei","can_mod_post":false,"created_utc":1752489525,"send_replies":true,"parent_id":"t1_n31ussf","score":5,"author_fullname":"t2_17n3nqtj56","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"This. The moment I read ollama, I lost all interest.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n31x8vd","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;This. The moment I read ollama, I lost all interest.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lzimcq","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzimcq/anyone_else_interested_in_a_100_ondevice_browser/n31x8vd/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752489525,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":1,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3237p6","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Anduin1357","can_mod_post":false,"send_replies":true,"parent_id":"t1_n31xnwg","score":3,"author_fullname":"t2_lpy03","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"LiteLLM literally supports Ollama, and practically everything else that you might want to use, both local models and API models.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3237p6","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;LiteLLM literally supports Ollama, and practically everything else that you might want to use, both local models and API models.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lzimcq","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzimcq/anyone_else_interested_in_a_100_ondevice_browser/n3237p6/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752492343,"author_flair_text":null,"treatment_tags":[],"created_utc":1752492343,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n31xnwg","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"jferments","can_mod_post":false,"created_utc":1752489739,"send_replies":true,"parent_id":"t1_n31ussf","score":3,"author_fullname":"t2_xs66a5h67","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Probably when other solutions become as easy to use as Ollama.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n31xnwg","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Probably when other solutions become as easy to use as Ollama.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lzimcq","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzimcq/anyone_else_interested_in_a_100_ondevice_browser/n31xnwg/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752489739,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":1,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n31yzc4","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"muxxington","can_mod_post":false,"created_utc":1752490383,"send_replies":true,"parent_id":"t1_n31ussf","score":1,"author_fullname":"t2_1ktdmsvo","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"This.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n31yzc4","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;This.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lzimcq","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzimcq/anyone_else_interested_in_a_100_ondevice_browser/n31yzc4/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752490383,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":1,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n33ilkl","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Anduin1357","can_mod_post":false,"send_replies":true,"parent_id":"t1_n32i6nq","score":1,"author_fullname":"t2_lpy03","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Oh no, it's worse than that. Your GGUF files got duplicated as imports into a new format, and you had to configure settings files for each model, or you get stuck with a really low amount of context, silently.\\n\\nThe concept of the update is a badge on a little icon in the system tray.\\n\\nAnd yes, it already has a system tray icon. Why doesn't it have a GUI?\\n\\nPassing Ollama to normal users feels like giving them a PUP.","edited":false,"author_flair_css_class":null,"name":"t1_n33ilkl","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Oh no, it&amp;#39;s worse than that. Your GGUF files got duplicated as imports into a new format, and you had to configure settings files for each model, or you get stuck with a really low amount of context, silently.&lt;/p&gt;\\n\\n&lt;p&gt;The concept of the update is a badge on a little icon in the system tray.&lt;/p&gt;\\n\\n&lt;p&gt;And yes, it already has a system tray icon. Why doesn&amp;#39;t it have a GUI?&lt;/p&gt;\\n\\n&lt;p&gt;Passing Ollama to normal users feels like giving them a PUP.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lzimcq","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzimcq/anyone_else_interested_in_a_100_ondevice_browser/n33ilkl/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752509180,"author_flair_text":null,"collapsed":false,"created_utc":1752509180,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n32m1lu","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Illustrious-Dot-6888","can_mod_post":false,"send_replies":true,"parent_id":"t1_n32i6nq","score":0,"author_fullname":"t2_gelzgtkby","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Nothing wrong with Ollama, your memory on the other hand...","edited":false,"author_flair_css_class":null,"name":"t1_n32m1lu","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Nothing wrong with Ollama, your memory on the other hand...&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lzimcq","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzimcq/anyone_else_interested_in_a_100_ondevice_browser/n32m1lu/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752499511,"author_flair_text":null,"collapsed":false,"created_utc":1752499511,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}},"user_reports":[],"saved":false,"id":"n32i6nq","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Electronic-Metal2391","can_mod_post":false,"send_replies":true,"parent_id":"t1_n32bwig","score":1,"author_fullname":"t2_58y73n8zk","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"the terminal. No GUI. I'm not a Linux guru and can't remember long syntaxes.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n32i6nq","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;the terminal. No GUI. I&amp;#39;m not a Linux guru and can&amp;#39;t remember long syntaxes.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lzimcq","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzimcq/anyone_else_interested_in_a_100_ondevice_browser/n32i6nq/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752498198,"author_flair_text":null,"treatment_tags":[],"created_utc":1752498198,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n32bwig","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MrMrsPotts","can_mod_post":false,"send_replies":true,"parent_id":"t1_n32724q","score":2,"author_fullname":"t2_sm168dt0h","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"What's wrong with it?","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n32bwig","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;What&amp;#39;s wrong with it?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lzimcq","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzimcq/anyone_else_interested_in_a_100_ondevice_browser/n32bwig/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752495918,"author_flair_text":null,"treatment_tags":[],"created_utc":1752495918,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n374c5o","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"InfiniteJX","can_mod_post":false,"send_replies":true,"parent_id":"t1_n32724q","score":1,"author_fullname":"t2_1nibexa2mk","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yeah, we ran into the same pain â€” no GUI, hard to recommend to non-tech friends.\\n\\nThatâ€™s actually why we started working on our thing. Just wanted to make it a bit more usable without needing the terminal.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n374c5o","is_submitter":true,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yeah, we ran into the same pain â€” no GUI, hard to recommend to non-tech friends.&lt;/p&gt;\\n\\n&lt;p&gt;Thatâ€™s actually why we started working on our thing. Just wanted to make it a bit more usable without needing the terminal.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lzimcq","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzimcq/anyone_else_interested_in_a_100_ondevice_browser/n374c5o/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752548987,"author_flair_text":null,"treatment_tags":[],"created_utc":1752548987,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n32724q","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Electronic-Metal2391","can_mod_post":false,"created_utc":1752493998,"send_replies":true,"parent_id":"t1_n31ussf","score":0,"author_fullname":"t2_58y73n8zk","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I hate Ollama, used it once for 30 minutes few months back and swore never to touch it again. I only use Koboldcpp.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n32724q","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I hate Ollama, used it once for 30 minutes few months back and swore never to touch it again. I only use Koboldcpp.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lzimcq","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzimcq/anyone_else_interested_in_a_100_ondevice_browser/n32724q/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752493998,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}},"user_reports":[],"saved":false,"id":"n31ussf","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Anduin1357","can_mod_post":false,"created_utc":1752488234,"send_replies":true,"parent_id":"t3_1lzimcq","score":6,"author_fullname":"t2_lpy03","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Man. When will Ollama stop being such a go-to solution? Go use LiteLLM or something.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n31ussf","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Man. When will Ollama stop being such a go-to solution? Go use LiteLLM or something.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzimcq/anyone_else_interested_in_a_100_ondevice_browser/n31ussf/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752488234,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lzimcq","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":1,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3282uy","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"leonbollerup","can_mod_post":false,"created_utc":1752494412,"send_replies":true,"parent_id":"t3_1lzimcq","score":1,"author_fullname":"t2_1gky5x","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Ya, on a Mac mini .. there is a work done for using the on device model using OpenAI API but in macOS Beta3 itâ€™s constantly hitting guard rails","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3282uy","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Ya, on a Mac mini .. there is a work done for using the on device model using OpenAI API but in macOS Beta3 itâ€™s constantly hitting guard rails&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzimcq/anyone_else_interested_in_a_100_ondevice_browser/n3282uy/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752494412,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lzimcq","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n32aroq","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"-lq_pl-","can_mod_post":false,"created_utc":1752495484,"send_replies":true,"parent_id":"t3_1lzimcq","score":1,"author_fullname":"t2_16rvbe","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Aren't there a million projects which do exactly that? E.g. [https://webllm.mlc.ai/](https://webllm.mlc.ai/)\\n\\nIn contrast to your vision, that runs also the LLM inside the browser thanks to webassembly and webgpu. Edit: I see you are also planning to support webllm as a backend, good.","edited":1752495778,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n32aroq","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Aren&amp;#39;t there a million projects which do exactly that? E.g. &lt;a href=\\"https://webllm.mlc.ai/\\"&gt;https://webllm.mlc.ai/&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;In contrast to your vision, that runs also the LLM inside the browser thanks to webassembly and webgpu. Edit: I see you are also planning to support webllm as a backend, good.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzimcq/anyone_else_interested_in_a_100_ondevice_browser/n32aroq/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752495484,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lzimcq","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n32iwy8","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Rakhmat3","can_mod_post":false,"created_utc":1752498450,"send_replies":true,"parent_id":"t3_1lzimcq","score":1,"author_fullname":"t2_m2ujq54i","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Yes, Chrome already deploys a local Gemini. It works from the website side.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n32iwy8","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yes, Chrome already deploys a local Gemini. It works from the website side.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzimcq/anyone_else_interested_in_a_100_ondevice_browser/n32iwy8/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752498450,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lzimcq","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n32qe98","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"dbuildofficial","can_mod_post":false,"created_utc":1752500944,"send_replies":true,"parent_id":"t3_1lzimcq","score":1,"author_fullname":"t2_5k8ox0g9","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"did that with [litechat.dev](http://litechat.dev) :) you have to self-host on your machine to be able to use http endpoint, but yup, works well. A friend of mine runs it with lmstudio like a charm !","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n32qe98","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;did that with &lt;a href=\\"http://litechat.dev\\"&gt;litechat.dev&lt;/a&gt; :) you have to self-host on your machine to be able to use http endpoint, but yup, works well. A friend of mine runs it with lmstudio like a charm !&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzimcq/anyone_else_interested_in_a_100_ondevice_browser/n32qe98/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752500944,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lzimcq","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n372ycf","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"InfiniteJX","can_mod_post":false,"created_utc":1752548450,"send_replies":true,"parent_id":"t1_n322ckk","score":1,"author_fullname":"t2_1nibexa2mk","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Thanks so much! Really glad you like it ðŸ˜Š Let us know if you have any suggestions or ideas too!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n372ycf","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thanks so much! Really glad you like it ðŸ˜Š Let us know if you have any suggestions or ideas too!&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lzimcq","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzimcq/anyone_else_interested_in_a_100_ondevice_browser/n372ycf/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752548450,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n322ckk","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Famku","can_mod_post":false,"created_utc":1752491957,"send_replies":true,"parent_id":"t3_1lzimcq","score":0,"author_fullname":"t2_13esso","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I think itâ€˜s a nice app","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n322ckk","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I think itâ€˜s a nice app&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzimcq/anyone_else_interested_in_a_100_ondevice_browser/n322ckk/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752491957,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lzimcq","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n32qtco","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"dbuildofficial","can_mod_post":false,"created_utc":1752501076,"send_replies":true,"parent_id":"t1_n31ypva","score":1,"author_fullname":"t2_5k8ox0g9","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Sure. it's only a wrapper after all.\\n\\nXD","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n32qtco","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Sure. it&amp;#39;s only a wrapper after all.&lt;/p&gt;\\n\\n&lt;p&gt;XD&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lzimcq","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzimcq/anyone_else_interested_in_a_100_ondevice_browser/n32qtco/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752501076,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n380hs8","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Tiny_Judge_2119","can_mod_post":false,"send_replies":true,"parent_id":"t1_n373txv","score":1,"author_fullname":"t2_aqcxxu50","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"ollama already looks bad enough by a wrapper of llama.cpp and if you are building a layer of wrapper of it, whether you like it or not, you are wasting your effort.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n380hs8","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;ollama already looks bad enough by a wrapper of llama.cpp and if you are building a layer of wrapper of it, whether you like it or not, you are wasting your effort.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lzimcq","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzimcq/anyone_else_interested_in_a_100_ondevice_browser/n380hs8/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752564419,"author_flair_text":null,"treatment_tags":[],"created_utc":1752564419,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n373txv","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"InfiniteJX","can_mod_post":false,"created_utc":1752548790,"send_replies":true,"parent_id":"t1_n31ypva","score":1,"author_fullname":"t2_1nibexa2mk","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Hey, fair point â€” Ollama definitely simplifies local model integration. But what weâ€™re building goes further than just connecting a model to a UI.\\n\\nOur goal is to create a true on-device assistant that understands context, remembers interactions, supports multi-tab workflows, and can execute actions â€” all within the browser, no cloud involved.\\n\\nWeâ€™re also rethinking interaction design: adding in-page bilingual translation, flexible quick actions, and UI elements that adapt to what youâ€™re doing. \\n\\nSure, a lot of AI apps today *are* essentially wrappers â€” but that doesnâ€™t stop them from helping people get things done better and faster :)\\n\\nAppreciate the pushback â€” itâ€™s always good to hear different perspectives.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n373txv","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Hey, fair point â€” Ollama definitely simplifies local model integration. But what weâ€™re building goes further than just connecting a model to a UI.&lt;/p&gt;\\n\\n&lt;p&gt;Our goal is to create a true on-device assistant that understands context, remembers interactions, supports multi-tab workflows, and can execute actions â€” all within the browser, no cloud involved.&lt;/p&gt;\\n\\n&lt;p&gt;Weâ€™re also rethinking interaction design: adding in-page bilingual translation, flexible quick actions, and UI elements that adapt to what youâ€™re doing. &lt;/p&gt;\\n\\n&lt;p&gt;Sure, a lot of AI apps today &lt;em&gt;are&lt;/em&gt; essentially wrappers â€” but that doesnâ€™t stop them from helping people get things done better and faster :)&lt;/p&gt;\\n\\n&lt;p&gt;Appreciate the pushback â€” itâ€™s always good to hear different perspectives.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lzimcq","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzimcq/anyone_else_interested_in_a_100_ondevice_browser/n373txv/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752548790,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n31ypva","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Tiny_Judge_2119","can_mod_post":false,"created_utc":1752490257,"send_replies":true,"parent_id":"t3_1lzimcq","score":-3,"author_fullname":"t2_aqcxxu50","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"If things are using ollama, you only build ui? Which would be done by claude code in 5 mins?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n31ypva","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;If things are using ollama, you only build ui? Which would be done by claude code in 5 mins?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzimcq/anyone_else_interested_in_a_100_ondevice_browser/n31ypva/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752490257,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lzimcq","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":1,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3248cz","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ROOFisonFIRE_usa","can_mod_post":false,"created_utc":1752492794,"send_replies":true,"parent_id":"t3_1lzimcq","score":0,"author_fullname":"t2_1jwmlwo64i","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"More interested in a assistant on my phone for my apps. We already have browser assistants, but not really app assistants.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3248cz","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;More interested in a assistant on my phone for my apps. We already have browser assistants, but not really app assistants.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzimcq/anyone_else_interested_in_a_100_ondevice_browser/n3248cz/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752492794,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lzimcq","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}}]`),o=()=>e.jsx(l,{data:a});export{o as default};
