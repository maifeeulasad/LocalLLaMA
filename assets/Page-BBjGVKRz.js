import{j as e}from"./index-F0NXdzZX.js";import{R as l}from"./RedditPostRenderer-CoEZB1dt.js";import"./index-DrtravXm.js";const t=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"I am currently using Gemini 2.5 pro, and I seem to be using about $100 per month. I plan to increase the usage by 10 fold, so then I thought of using my 4090+3090 on open source models as a possibility cheaper alternative (and protect my assets). I'm currently testing Deep seek r1 70b and 8b. 70b takes a while, 8b seems much faster, but I continued using Gemini because of the context window.\\n\\nNow I'm just wondering if deepseek r1 is my best bet for programming locally or Kimi 2 is worth more, even if the inference it's much slower? Or something else? \\n\\nAnd perhaps I should be using some better flavor than pure Deep seek r1?","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"What best model(s) to use for inference using a 4090+3090 for Aider?","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lzh0cf","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.67,"author_flair_background_color":null,"subreddit_type":"public","ups":1,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_cv5ft","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":1,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1752481760,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;I am currently using Gemini 2.5 pro, and I seem to be using about $100 per month. I plan to increase the usage by 10 fold, so then I thought of using my 4090+3090 on open source models as a possibility cheaper alternative (and protect my assets). I&amp;#39;m currently testing Deep seek r1 70b and 8b. 70b takes a while, 8b seems much faster, but I continued using Gemini because of the context window.&lt;/p&gt;\\n\\n&lt;p&gt;Now I&amp;#39;m just wondering if deepseek r1 is my best bet for programming locally or Kimi 2 is worth more, even if the inference it&amp;#39;s much slower? Or something else? &lt;/p&gt;\\n\\n&lt;p&gt;And perhaps I should be using some better flavor than pure Deep seek r1?&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":true,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1lzh0cf","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"cGalaxy","discussion_type":null,"num_comments":11,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lzh0cf/what_best_models_to_use_for_inference_using_a/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lzh0cf/what_best_models_to_use_for_inference_using_a/","subreddit_subscribers":499296,"created_utc":1752481760,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n31o9za","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"cGalaxy","can_mod_post":false,"created_utc":1752484527,"send_replies":true,"parent_id":"t1_n31lh85","score":2,"author_fullname":"t2_cv5ft","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Thank you for the recommendation, will check both of them out. Thanks for going a step further and suggesting coding &amp; planning model\\n\\nAs to why I chose Deep Seek R1: lack of knowledge in the space, thus my post for help","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n31o9za","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thank you for the recommendation, will check both of them out. Thanks for going a step further and suggesting coding &amp;amp; planning model&lt;/p&gt;\\n\\n&lt;p&gt;As to why I chose Deep Seek R1: lack of knowledge in the space, thus my post for help&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lzh0cf","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzh0cf/what_best_models_to_use_for_inference_using_a/n31o9za/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752484527,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n31nre9","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MaxKruse96","can_mod_post":false,"send_replies":true,"parent_id":"t1_n31nbyz","score":3,"author_fullname":"t2_pfi81","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"qwen3 as a whole isnt a good coder (maybe that changes with finetunes...). devstral is a good instruction following model with agentic back-and-forth trained in, as well as coding finetune specifically. its just superior, esp with context size in mind.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n31nre9","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;qwen3 as a whole isnt a good coder (maybe that changes with finetunes...). devstral is a good instruction following model with agentic back-and-forth trained in, as well as coding finetune specifically. its just superior, esp with context size in mind.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lzh0cf","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzh0cf/what_best_models_to_use_for_inference_using_a/n31nre9/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752484219,"author_flair_text":null,"treatment_tags":[],"created_utc":1752484219,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n330jk1","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"tomz17","can_mod_post":false,"send_replies":true,"parent_id":"t1_n31nbyz","score":2,"author_fullname":"t2_1mhx5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Op was asking for tool-use (e.g. Aider).   Devstral will be better than any other model currently out there AFAIK, but still likely to fall FAR short of the closed models at this point in history.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n330jk1","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Op was asking for tool-use (e.g. Aider).   Devstral will be better than any other model currently out there AFAIK, but still likely to fall FAR short of the closed models at this point in history.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lzh0cf","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzh0cf/what_best_models_to_use_for_inference_using_a/n330jk1/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752504010,"author_flair_text":null,"treatment_tags":[],"created_utc":1752504010,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n35gmot","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"randomqhacker","can_mod_post":false,"send_replies":true,"parent_id":"t1_n31nbyz","score":1,"author_fullname":"t2_4nw3v","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"235B A22B is 4x faster than a 24B?!  What quants are you using?","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n35gmot","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;235B A22B is 4x faster than a 24B?!  What quants are you using?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lzh0cf","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzh0cf/what_best_models_to_use_for_inference_using_a/n35gmot/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752528882,"author_flair_text":null,"treatment_tags":[],"created_utc":1752528882,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n31nbyz","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"grasza","can_mod_post":false,"created_utc":1752483966,"send_replies":true,"parent_id":"t1_n31lh85","score":1,"author_fullname":"t2_1w1tr9d5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Why wouldn't you recommend Qwen3 235b for coding? I tried to use Devstral, but on my (AMD 395) setup it's too slow, quantized Qwen3 235b being 4x faster.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n31nbyz","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Why wouldn&amp;#39;t you recommend Qwen3 235b for coding? I tried to use Devstral, but on my (AMD 395) setup it&amp;#39;s too slow, quantized Qwen3 235b being 4x faster.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lzh0cf","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzh0cf/what_best_models_to_use_for_inference_using_a/n31nbyz/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752483966,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n31lh85","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"MaxKruse96","can_mod_post":false,"created_utc":1752482879,"send_replies":true,"parent_id":"t3_1lzh0cf","score":7,"author_fullname":"t2_pfi81","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"neither of the local models you mentioned are good for coding. Not sure why u tried to use them.\\n\\nIf you want the best tradeoff of quality and speed, i have 2 suggestions:\\n\\n1. Devstral (whichever version of the 2 works better for you, 2505 or 2507)  \\n2. Planning with Qwen3 235b (e.g. structure and gotchas), then devstral for implementing\\n\\nIf you want to have a 1tb memory machine that then gets 1t/s for kimi 2, by all means go ahead but i hope u dont pay for electricity at that point","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n31lh85","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;neither of the local models you mentioned are good for coding. Not sure why u tried to use them.&lt;/p&gt;\\n\\n&lt;p&gt;If you want the best tradeoff of quality and speed, i have 2 suggestions:&lt;/p&gt;\\n\\n&lt;ol&gt;\\n&lt;li&gt;Devstral (whichever version of the 2 works better for you, 2505 or 2507)&lt;br/&gt;&lt;/li&gt;\\n&lt;li&gt;Planning with Qwen3 235b (e.g. structure and gotchas), then devstral for implementing&lt;/li&gt;\\n&lt;/ol&gt;\\n\\n&lt;p&gt;If you want to have a 1tb memory machine that then gets 1t/s for kimi 2, by all means go ahead but i hope u dont pay for electricity at that point&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzh0cf/what_best_models_to_use_for_inference_using_a/n31lh85/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752482879,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lzh0cf","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":7}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n31o2hg","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"KernQ","can_mod_post":false,"created_utc":1752484403,"send_replies":true,"parent_id":"t3_1lzh0cf","score":3,"author_fullname":"t2_1pozn81kn1","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Devstral is worth exploring for local.\\n\\nDeepseek R1 0528 (architect) and R3 (editor) is my daily driver for API use with Aider. Token cost is much lower, even with R1 being chatty.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n31o2hg","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Devstral is worth exploring for local.&lt;/p&gt;\\n\\n&lt;p&gt;Deepseek R1 0528 (architect) and R3 (editor) is my daily driver for API use with Aider. Token cost is much lower, even with R1 being chatty.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzh0cf/what_best_models_to_use_for_inference_using_a/n31o2hg/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752484403,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lzh0cf","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n33438b","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"VegaKH","can_mod_post":false,"created_utc":1752505029,"send_replies":true,"parent_id":"t1_n31yroo","score":3,"author_fullname":"t2_11wjla","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I also suggest [Devstral Small 1.1](https://huggingface.co/mistralai/Devstral-Small-2507) (aka 25.07). It's a finetune of Mistral Small 3.1 focused on agentic coding. Per their HF repo:\\n\\n&gt;Devstral excels at using tools to explore codebases, editing multiple files and power software engineering agents.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n33438b","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I also suggest &lt;a href=\\"https://huggingface.co/mistralai/Devstral-Small-2507\\"&gt;Devstral Small 1.1&lt;/a&gt; (aka 25.07). It&amp;#39;s a finetune of Mistral Small 3.1 focused on agentic coding. Per their HF repo:&lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;Devstral excels at using tools to explore codebases, editing multiple files and power software engineering agents.&lt;/p&gt;\\n&lt;/blockquote&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lzh0cf","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzh0cf/what_best_models_to_use_for_inference_using_a/n33438b/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752505029,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n31yroo","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"tempetemplar","can_mod_post":false,"created_utc":1752490281,"send_replies":true,"parent_id":"t3_1lzh0cf","score":4,"author_fullname":"t2_atvw2aj","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Devstral 25.07 or Kimi (api)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n31yroo","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Devstral 25.07 or Kimi (api)&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzh0cf/what_best_models_to_use_for_inference_using_a/n31yroo/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752490281,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lzh0cf","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n37zzyd","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"cGalaxy","can_mod_post":false,"created_utc":1752564139,"send_replies":true,"parent_id":"t1_n35k9k3","score":1,"author_fullname":"t2_cv5ft","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Do you mean to use Qwen for both planning and implementing?\\n\\nIf yes, would you suggest planning and implementing both to be Qwen3 32B Q8 or one with lower parameters?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n37zzyd","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Do you mean to use Qwen for both planning and implementing?&lt;/p&gt;\\n\\n&lt;p&gt;If yes, would you suggest planning and implementing both to be Qwen3 32B Q8 or one with lower parameters?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lzh0cf","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzh0cf/what_best_models_to_use_for_inference_using_a/n37zzyd/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752564139,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n35k9k3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"wwabbbitt","can_mod_post":false,"created_utc":1752529959,"send_replies":true,"parent_id":"t3_1lzh0cf","score":2,"author_fullname":"t2_51z2oqmy","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Be aware that Aider benchmark performance does not always agree with other SWE benchmarks. Devstral is one of a few that do not fare well in Aider despite what other benchmarks suggest.\\n\\nCheck out the Aider community Discord where other users report Aider benchmarks they have performed for various models.\\n\\nI recommend the Qwen3 32B Q8 as tested by neolithic\\n\\nhttps://discord.com/channels/1131200896827654144/1393170679863447553","edited":1752530241,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n35k9k3","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Be aware that Aider benchmark performance does not always agree with other SWE benchmarks. Devstral is one of a few that do not fare well in Aider despite what other benchmarks suggest.&lt;/p&gt;\\n\\n&lt;p&gt;Check out the Aider community Discord where other users report Aider benchmarks they have performed for various models.&lt;/p&gt;\\n\\n&lt;p&gt;I recommend the Qwen3 32B Q8 as tested by neolithic&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://discord.com/channels/1131200896827654144/1393170679863447553\\"&gt;https://discord.com/channels/1131200896827654144/1393170679863447553&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzh0cf/what_best_models_to_use_for_inference_using_a/n35k9k3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752529959,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lzh0cf","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}}]`),n=()=>e.jsx(l,{data:t});export{n as default};
