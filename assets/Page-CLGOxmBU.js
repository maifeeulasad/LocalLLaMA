import{j as e}from"./index-xfnGEtuL.js";import{R as t}from"./RedditPostRenderer-DAZRIZZK.js";import"./index-BaNn5-RR.js";const l=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Hi all,\\n\\nI'm working on an internal tool where we are provided with **only a URL** — no description, metadata, or prior context — and our goal is to automatically classify the website into one of two categories. The categories could be something like:\\n\\n* **Category A**: Websites that promote or belong to academic institutions\\n* **Category B**: Websites that do not relate to academics at all\\n\\n**The Goal:**\\n\\nGiven a URL like [\`example.com\`](http://example.com), we want to classify it as either Category A or Category B with decent accuracy. There is no prior knowledge or labeled data about the site — we need to infer the classification based on the actual content.\\n\\n**What I’ve Tried:**\\n\\n\\\\- I’ve tried Gemini API (2.5 Flash) with Grounded Google Search and also with URL Context tool — both didn’t provide satisfactory results.\\n\\n**The Challenge with using google searchs:**\\n\\n\\\\-  Some sites don’t show up at all in google search.\\n\\n\\\\- Others return results, but snippets don’t belong to the actual domain but to similar domains.\\n\\n**Considered Scraping:**\\n\\n\\\\- One possible route is to scrape the target websites and analyze the content directly.\\n\\n\\\\- However, this comes with a **context window limitation** — scraping just the homepage or a single page might not give the full picture, especially if relevant content is nested deeper in About, Services, or FAQ pages.\\n\\n\\\\- To address this, we may need to **crawl and scrape all primary pages** of the website (e.g., top-level links and their children), but that quickly escalates both cost and processing time, and still doesn't solve the context summarization issue unless chunked well.\\n\\n\\\\- Using LLMs on long content is tricky — even with chunking and summarization maintaining context fidelity and avoiding hallucinations remains a challenge.\\n\\n# My Question:\\n\\nHow would you approach this classification problem? I would appreciate any help with this. I am a novice in this field.\\n\\nThanks in advance","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Struggling with NLP classification pipeline for web content – seeking advice","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1m7aefj","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":1,"author_flair_background_color":null,"subreddit_type":"public","ups":2,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_tv03725","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":2,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":1753285588,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1753280082,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Hi all,&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;m working on an internal tool where we are provided with &lt;strong&gt;only a URL&lt;/strong&gt; — no description, metadata, or prior context — and our goal is to automatically classify the website into one of two categories. The categories could be something like:&lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;&lt;strong&gt;Category A&lt;/strong&gt;: Websites that promote or belong to academic institutions&lt;/li&gt;\\n&lt;li&gt;&lt;strong&gt;Category B&lt;/strong&gt;: Websites that do not relate to academics at all&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;p&gt;&lt;strong&gt;The Goal:&lt;/strong&gt;&lt;/p&gt;\\n\\n&lt;p&gt;Given a URL like &lt;a href=\\"http://example.com\\"&gt;&lt;code&gt;example.com&lt;/code&gt;&lt;/a&gt;, we want to classify it as either Category A or Category B with decent accuracy. There is no prior knowledge or labeled data about the site — we need to infer the classification based on the actual content.&lt;/p&gt;\\n\\n&lt;p&gt;&lt;strong&gt;What I’ve Tried:&lt;/strong&gt;&lt;/p&gt;\\n\\n&lt;p&gt;- I’ve tried Gemini API (2.5 Flash) with Grounded Google Search and also with URL Context tool — both didn’t provide satisfactory results.&lt;/p&gt;\\n\\n&lt;p&gt;&lt;strong&gt;The Challenge with using google searchs:&lt;/strong&gt;&lt;/p&gt;\\n\\n&lt;p&gt;-  Some sites don’t show up at all in google search.&lt;/p&gt;\\n\\n&lt;p&gt;- Others return results, but snippets don’t belong to the actual domain but to similar domains.&lt;/p&gt;\\n\\n&lt;p&gt;&lt;strong&gt;Considered Scraping:&lt;/strong&gt;&lt;/p&gt;\\n\\n&lt;p&gt;- One possible route is to scrape the target websites and analyze the content directly.&lt;/p&gt;\\n\\n&lt;p&gt;- However, this comes with a &lt;strong&gt;context window limitation&lt;/strong&gt; — scraping just the homepage or a single page might not give the full picture, especially if relevant content is nested deeper in About, Services, or FAQ pages.&lt;/p&gt;\\n\\n&lt;p&gt;- To address this, we may need to &lt;strong&gt;crawl and scrape all primary pages&lt;/strong&gt; of the website (e.g., top-level links and their children), but that quickly escalates both cost and processing time, and still doesn&amp;#39;t solve the context summarization issue unless chunked well.&lt;/p&gt;\\n\\n&lt;p&gt;- Using LLMs on long content is tricky — even with chunking and summarization maintaining context fidelity and avoiding hallucinations remains a challenge.&lt;/p&gt;\\n\\n&lt;h1&gt;My Question:&lt;/h1&gt;\\n\\n&lt;p&gt;How would you approach this classification problem? I would appreciate any help with this. I am a novice in this field.&lt;/p&gt;\\n\\n&lt;p&gt;Thanks in advance&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":true,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1m7aefj","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"amir_shehzad","discussion_type":null,"num_comments":5,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1m7aefj/struggling_with_nlp_classification_pipeline_for/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1m7aefj/struggling_with_nlp_classification_pipeline_for/","subreddit_subscribers":503757,"created_utc":1753280082,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4qckcq","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"amir_shehzad","can_mod_post":false,"created_utc":1753285542,"send_replies":true,"parent_id":"t1_n4q41av","score":1,"author_fullname":"t2_tv03725","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Hi, Thanks for replying. Can you please see the start of the post again? I have updated it","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4qckcq","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Hi, Thanks for replying. Can you please see the start of the post again? I have updated it&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m7aefj","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7aefj/struggling_with_nlp_classification_pipeline_for/n4qckcq/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753285542,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4q41av","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"No_Efficiency_1144","can_mod_post":false,"created_utc":1753283196,"send_replies":true,"parent_id":"t3_1m7aefj","score":2,"author_fullname":"t2_1nkj9l14b0","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Could you explain what you are actually trying to do?\\n\\n\\nWhat are your class labels like?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4q41av","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Could you explain what you are actually trying to do?&lt;/p&gt;\\n\\n&lt;p&gt;What are your class labels like?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7aefj/struggling_with_nlp_classification_pipeline_for/n4q41av/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753283196,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m7aefj","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4qckx5","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"amir_shehzad","can_mod_post":false,"created_utc":1753285546,"send_replies":true,"parent_id":"t1_n4q89ra","score":1,"author_fullname":"t2_tv03725","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Hi, Thanks for replying. Can you please see the start of the post again? I have updated it","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4qckx5","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Hi, Thanks for replying. Can you please see the start of the post again? I have updated it&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m7aefj","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7aefj/struggling_with_nlp_classification_pipeline_for/n4qckx5/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753285546,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4q89ra","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"IllllIIlIllIllllIIIl","can_mod_post":false,"created_utc":1753284363,"send_replies":true,"parent_id":"t3_1m7aefj","score":1,"author_fullname":"t2_q34vj4lcw","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":" It sounds like you're just tossing web pages into the LLM and asking \\"Which category does this content belong to?\\", is that correct? You might consider calculating embeddings and then running a classification or clustering algorithm on those instead. But it's hard to know without a better picture of the problem and goal.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4q89ra","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It sounds like you&amp;#39;re just tossing web pages into the LLM and asking &amp;quot;Which category does this content belong to?&amp;quot;, is that correct? You might consider calculating embeddings and then running a classification or clustering algorithm on those instead. But it&amp;#39;s hard to know without a better picture of the problem and goal.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7aefj/struggling_with_nlp_classification_pipeline_for/n4q89ra/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753284363,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m7aefj","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4r20d9","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"kevin_1994","can_mod_post":false,"created_utc":1753292472,"send_replies":true,"parent_id":"t3_1m7aefj","score":1,"author_fullname":"t2_o015g","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Have you thought about using a simpler ML model such as FFNN classifier? Or maybe even something way quicker and \\"stupider\\" like a naive bayes classifier?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4r20d9","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Have you thought about using a simpler ML model such as FFNN classifier? Or maybe even something way quicker and &amp;quot;stupider&amp;quot; like a naive bayes classifier?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":true,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7aefj/struggling_with_nlp_classification_pipeline_for/n4r20d9/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753292472,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m7aefj","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),i=()=>e.jsx(t,{data:l});export{i as default};
