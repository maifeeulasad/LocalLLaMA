import{j as e}from"./index-BgwOAK4-.js";import{R as l}from"./RedditPostRenderer-BOBjDTFu.js";import"./index-BL22wVg5.js";const t=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"I'm looking to run the 70B sized models but with large context sizes. Like 10k or more. I'd like to avoid offloading to the cpu. What would you recommend hardware set up to be on a budget?\\n\\n2 x 3090 still best value?\\nSwitch to Radeon like the 2x  mi50 32gb?\\n\\nIt would be just for inference and as long as its faster than cpu only. Currently with Qwen2.5 72b q3km is 119 t/s pp and 1.03 t/s tg with a 8k context window as cpu only on ddr5 ram. Goes up to 162 t/s pp and 1.5 t/s tg with partial offload to one 3090","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Running the 70B sized models on a budget","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1m49p7w","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.6,"author_flair_background_color":null,"subreddit_type":"public","ups":1,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_40xsg56g","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":1,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1752966325,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;m looking to run the 70B sized models but with large context sizes. Like 10k or more. I&amp;#39;d like to avoid offloading to the cpu. What would you recommend hardware set up to be on a budget?&lt;/p&gt;\\n\\n&lt;p&gt;2 x 3090 still best value?\\nSwitch to Radeon like the 2x  mi50 32gb?&lt;/p&gt;\\n\\n&lt;p&gt;It would be just for inference and as long as its faster than cpu only. Currently with Qwen2.5 72b q3km is 119 t/s pp and 1.03 t/s tg with a 8k context window as cpu only on ddr5 ram. Goes up to 162 t/s pp and 1.5 t/s tg with partial offload to one 3090&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":true,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1m49p7w","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"fgoricha","discussion_type":null,"num_comments":16,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1m49p7w/running_the_70b_sized_models_on_a_budget/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1m49p7w/running_the_70b_sized_models_on_a_budget/","subreddit_subscribers":502030,"created_utc":1752966325,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n45rrtc","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"fgoricha","can_mod_post":false,"send_replies":true,"parent_id":"t1_n45b83d","score":3,"author_fullname":"t2_40xsg56g","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I was disappointed that Qwen did not release a new 70B tier model with the recent Qwen3 release. But from my testing, if found that I liked Qwen 2.5 72B the best out of the new Qwen lineup that runs on my current hardware. I do not deviate much from Qwen since it can become overwhelming to try them all without automating the evaluation process","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n45rrtc","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I was disappointed that Qwen did not release a new 70B tier model with the recent Qwen3 release. But from my testing, if found that I liked Qwen 2.5 72B the best out of the new Qwen lineup that runs on my current hardware. I do not deviate much from Qwen since it can become overwhelming to try them all without automating the evaluation process&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m49p7w","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m49p7w/running_the_70b_sized_models_on_a_budget/n45rrtc/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753016787,"author_flair_text":null,"treatment_tags":[],"created_utc":1753016787,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n45b83d","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Admirable-Star7088","can_mod_post":false,"send_replies":true,"parent_id":"t1_n42za1y","score":3,"author_fullname":"t2_qhlcbiy3k","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Qwen2.5 72b is a good model, but it's kind of old and outdated now. More recent, smaller models such as Mistral Small 3.2 are actually smarter, in my experience. The main advantage older 70b models still have are their greater scope of knowledge.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n45b83d","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Qwen2.5 72b is a good model, but it&amp;#39;s kind of old and outdated now. More recent, smaller models such as Mistral Small 3.2 are actually smarter, in my experience. The main advantage older 70b models still have are their greater scope of knowledge.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m49p7w","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m49p7w/running_the_70b_sized_models_on_a_budget/n45b83d/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753009491,"author_flair_text":null,"treatment_tags":[],"created_utc":1753009491,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n462o0k","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"techmago","can_mod_post":false,"send_replies":true,"parent_id":"t1_n42za1y","score":0,"author_fullname":"t2_azy5rpp","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"nope. I used to hate Qwen before. I only started using it in qwen3","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n462o0k","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;nope. I used to hate Qwen before. I only started using it in qwen3&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m49p7w","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m49p7w/running_the_70b_sized_models_on_a_budget/n462o0k/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753020680,"author_flair_text":null,"treatment_tags":[],"created_utc":1753020680,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}},"user_reports":[],"saved":false,"id":"n42za1y","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"fgoricha","can_mod_post":false,"created_utc":1752969568,"send_replies":true,"parent_id":"t1_n42umww","score":2,"author_fullname":"t2_40xsg56g","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Have you tried qwen2.5 72b?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n42za1y","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Have you tried qwen2.5 72b?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m49p7w","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m49p7w/running_the_70b_sized_models_on_a_budget/n42za1y/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752969568,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n42umww","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"techmago","can_mod_post":false,"created_utc":1752967905,"send_replies":true,"parent_id":"t3_1m49p7w","score":3,"author_fullname":"t2_azy5rpp","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"i run 70B models(q4) with about \\\\~12k  at 9 token/s\\n\\nI find out that   \\nquewn3-q8  \\nand mistral-q8 look better than lamma3.3  \\nand they run at 13 token/s and 23 token/s\\n\\nand i can handle larger contexts... even til 64K without cpu offloading.  \\nAll this with 2x3090.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n42umww","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;i run 70B models(q4) with about ~12k  at 9 token/s&lt;/p&gt;\\n\\n&lt;p&gt;I find out that&lt;br/&gt;\\nquewn3-q8&lt;br/&gt;\\nand mistral-q8 look better than lamma3.3&lt;br/&gt;\\nand they run at 13 token/s and 23 token/s&lt;/p&gt;\\n\\n&lt;p&gt;and i can handle larger contexts... even til 64K without cpu offloading.&lt;br/&gt;\\nAll this with 2x3090.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m49p7w/running_the_70b_sized_models_on_a_budget/n42umww/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752967905,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m49p7w","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n46l4pt","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"fgoricha","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4699y0","score":1,"author_fullname":"t2_40xsg56g","approved_by":null,"mod_note":null,"all_awardings":[],"body":"Thanks for the input! That would be valuable information when considering alternative hardware","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n46l4pt","is_submitter":true,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thanks for the input! That would be valuable information when considering alternative hardware&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1m49p7w","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m49p7w/running_the_70b_sized_models_on_a_budget/n46l4pt/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753026521,"author_flair_text":null,"treatment_tags":[],"created_utc":1753026521,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4699y0","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MLDataScientist","can_mod_post":false,"send_replies":true,"parent_id":"t1_n45m5ki","score":1,"author_fullname":"t2_3zy7pnf1","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"At 8k context, I was getting ~10t/s for qwen2.5 72b. But haven't checked the PP. I should probably do more tests at larger contexts.","edited":false,"author_flair_css_class":null,"name":"t1_n4699y0","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;At 8k context, I was getting ~10t/s for qwen2.5 72b. But haven&amp;#39;t checked the PP. I should probably do more tests at larger contexts.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1m49p7w","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m49p7w/running_the_70b_sized_models_on_a_budget/n4699y0/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753022848,"author_flair_text":null,"collapsed":false,"created_utc":1753022848,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n45m5ki","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"fgoricha","can_mod_post":false,"send_replies":true,"parent_id":"t1_n448y3z","score":1,"author_fullname":"t2_40xsg56g","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Cool! Thank you! How is pp and tg speed impacted as the context window increases?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n45m5ki","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Cool! Thank you! How is pp and tg speed impacted as the context window increases?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m49p7w","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m49p7w/running_the_70b_sized_models_on_a_budget/n45m5ki/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753014584,"author_flair_text":null,"treatment_tags":[],"created_utc":1753014584,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n448y3z","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MLDataScientist","can_mod_post":false,"send_replies":true,"parent_id":"t1_n43b6hr","score":2,"author_fullname":"t2_3zy7pnf1","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"For MI50 32gb, check here: https://www.reddit.com/r/LocalLLaMA/comments/1lspzn3/128gb_vram_for_600_qwen3_moe_235ba22b_reaching_20/ .\\n\\n\\nTldr: I have 8x MI50 32GB and I tested various models.\\nTo your question, Qwen2.5 72B gptq 4bit runs at around 20 t/s with 2xMI50 (two cards with tensor parallelism) in vLLM. Prompt processing speed is around 150 t/s for that model.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n448y3z","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;For MI50 32gb, check here: &lt;a href=\\"https://www.reddit.com/r/LocalLLaMA/comments/1lspzn3/128gb_vram_for_600_qwen3_moe_235ba22b_reaching_20/\\"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1lspzn3/128gb_vram_for_600_qwen3_moe_235ba22b_reaching_20/&lt;/a&gt; .&lt;/p&gt;\\n\\n&lt;p&gt;Tldr: I have 8x MI50 32GB and I tested various models.\\nTo your question, Qwen2.5 72B gptq 4bit runs at around 20 t/s with 2xMI50 (two cards with tensor parallelism) in vLLM. Prompt processing speed is around 150 t/s for that model.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m49p7w","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m49p7w/running_the_70b_sized_models_on_a_budget/n448y3z/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752988162,"author_flair_text":null,"treatment_tags":[],"created_utc":1752988162,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n43b6hr","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"fgoricha","can_mod_post":false,"created_utc":1752974050,"send_replies":true,"parent_id":"t1_n42vlzd","score":2,"author_fullname":"t2_40xsg56g","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"What kind of results do you get when running those models? I am torn if the small speed increases running on old hardware is worth the upgrade","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n43b6hr","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;What kind of results do you get when running those models? I am torn if the small speed increases running on old hardware is worth the upgrade&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m49p7w","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m49p7w/running_the_70b_sized_models_on_a_budget/n43b6hr/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752974050,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n42vlzd","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ttkciar","can_mod_post":false,"created_utc":1752968251,"send_replies":true,"parent_id":"t3_1m49p7w","score":3,"author_fullname":"t2_cpegz","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I've seen posts recently about people having trouble with MI50, like only being able to use 16GB of their 32GB.\\n\\nMy MI60 has been pretty pain-free, though, and last I checked it was only $450 on eBay.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n42vlzd","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;ve seen posts recently about people having trouble with MI50, like only being able to use 16GB of their 32GB.&lt;/p&gt;\\n\\n&lt;p&gt;My MI60 has been pretty pain-free, though, and last I checked it was only $450 on eBay.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m49p7w/running_the_70b_sized_models_on_a_budget/n42vlzd/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752968251,"author_flair_text":"llama.cpp","treatment_tags":[],"link_id":"t3_1m49p7w","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n45m895","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"fgoricha","can_mod_post":false,"created_utc":1753014614,"send_replies":true,"parent_id":"t1_n44945w","score":1,"author_fullname":"t2_40xsg56g","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I have been thinking about this route as well. Is your set up in a open air rig?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n45m895","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I have been thinking about this route as well. Is your set up in a open air rig?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m49p7w","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m49p7w/running_the_70b_sized_models_on_a_budget/n45m895/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753014614,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n44945w","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"jacek2023","can_mod_post":false,"created_utc":1752988245,"send_replies":true,"parent_id":"t3_1m49p7w","score":1,"author_fullname":"t2_vqgbql9w","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"70B models in Q4 are not difficult, two 3090s are enough, I use 3","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n44945w","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;70B models in Q4 are not difficult, two 3090s are enough, I use 3&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m49p7w/running_the_70b_sized_models_on_a_budget/n44945w/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752988245,"author_flair_text":"llama.cpp","treatment_tags":[],"link_id":"t3_1m49p7w","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n47s9y5","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"GPTshop_ai","can_mod_post":false,"send_replies":true,"parent_id":"t1_n45rf6i","score":1,"author_fullname":"t2_rkmud0isr","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Anyone can afford 7k, if he really needs to. When I was young, I slept for 6 months at work on the floor, to be able to afford a decent PC. Better spend 7k on someting decent, than 4k on something terrible.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n47s9y5","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Anyone can afford 7k, if he really needs to. When I was young, I slept for 6 months at work on the floor, to be able to afford a decent PC. Better spend 7k on someting decent, than 4k on something terrible.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m49p7w","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m49p7w/running_the_70b_sized_models_on_a_budget/n47s9y5/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753039531,"author_flair_text":null,"treatment_tags":[],"created_utc":1753039531,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n45rf6i","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"fgoricha","can_mod_post":false,"created_utc":1753016655,"send_replies":true,"parent_id":"t1_n45fb85","score":1,"author_fullname":"t2_40xsg56g","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I would love to get a single gpu like the pro 6000 but that is out of my budget","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n45rf6i","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I would love to get a single gpu like the pro 6000 but that is out of my budget&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m49p7w","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m49p7w/running_the_70b_sized_models_on_a_budget/n45rf6i/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753016655,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n45fb85","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"GPTshop_ai","can_mod_post":false,"created_utc":1753011554,"send_replies":true,"parent_id":"t3_1m49p7w","score":1,"author_fullname":"t2_rkmud0isr","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"One single GPU is to be very much preferred over multiple, because the slow PCIe connection will impact performance...","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n45fb85","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;One single GPU is to be very much preferred over multiple, because the slow PCIe connection will impact performance...&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m49p7w/running_the_70b_sized_models_on_a_budget/n45fb85/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753011554,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m49p7w","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),o=()=>e.jsx(l,{data:t});export{o as default};
