import{j as e}from"./index-CmSyeZDT.js";import{R as t}from"./RedditPostRenderer-C2Zg39IK.js";import"./index-CiTZuv6Z.js";const l=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Hey there, I'm just starting here, I will work into a company that has privacy concerns with using external AI agents so I'm willing to build a local server to use at home.\\n\\nIt seems that the ideal to code inference is to use a 70b model, so I'm willing to make a setup with 4  rtx 3090 with 24g vram each (I think I need a bit less than 96 vram but I want to have some extra resources to play around and test stuff)\\n\\nAfter researching the last 2 days, I found some items that it seems I need to consider outside vram.\\n\\n1 - heat - it seems that using a eth miner structure as case works well right? With risers to connect the GPU to the mother board. Do you think it does make sense to have water-cooler?\\n\\n2 - motherboard - it seems that if I get a Mobo with multiple tracks on each pcie I get speed improvements to train stuff (which is not my main goal, but I would like to see the pricing difference to choose)\\n\\n3 - no clue about how much cpu and ram.\\n\\n4 - energy - I do have a decent infrastructure for energy, I do have some solar panels that are giving me extra 100kw/month and 220v with support for 32A, so my concern is just which how many Watts should my power supply part does need to support.\\n\\n\\nCould you give me some help to figure out a good set of Mobo, Processor and amount of Ram that I  could buy for inference only, and for inference and training?\\n\\nI live in Brazil so importing has 100% taxes on top of the price, so I'm trying to find stuff that is already here.","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Help with defining hardware multi GPU setup","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lqwylx","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.5,"author_flair_background_color":null,"subreddit_type":"public","ups":0,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_7j4as586","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":0,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":1751567542,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1751567126,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Hey there, I&amp;#39;m just starting here, I will work into a company that has privacy concerns with using external AI agents so I&amp;#39;m willing to build a local server to use at home.&lt;/p&gt;\\n\\n&lt;p&gt;It seems that the ideal to code inference is to use a 70b model, so I&amp;#39;m willing to make a setup with 4  rtx 3090 with 24g vram each (I think I need a bit less than 96 vram but I want to have some extra resources to play around and test stuff)&lt;/p&gt;\\n\\n&lt;p&gt;After researching the last 2 days, I found some items that it seems I need to consider outside vram.&lt;/p&gt;\\n\\n&lt;p&gt;1 - heat - it seems that using a eth miner structure as case works well right? With risers to connect the GPU to the mother board. Do you think it does make sense to have water-cooler?&lt;/p&gt;\\n\\n&lt;p&gt;2 - motherboard - it seems that if I get a Mobo with multiple tracks on each pcie I get speed improvements to train stuff (which is not my main goal, but I would like to see the pricing difference to choose)&lt;/p&gt;\\n\\n&lt;p&gt;3 - no clue about how much cpu and ram.&lt;/p&gt;\\n\\n&lt;p&gt;4 - energy - I do have a decent infrastructure for energy, I do have some solar panels that are giving me extra 100kw/month and 220v with support for 32A, so my concern is just which how many Watts should my power supply part does need to support.&lt;/p&gt;\\n\\n&lt;p&gt;Could you give me some help to figure out a good set of Mobo, Processor and amount of Ram that I  could buy for inference only, and for inference and training?&lt;/p&gt;\\n\\n&lt;p&gt;I live in Brazil so importing has 100% taxes on top of the price, so I&amp;#39;m trying to find stuff that is already here.&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":true,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1lqwylx","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"haruanmj","discussion_type":null,"num_comments":6,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lqwylx/help_with_defining_hardware_multi_gpu_setup/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lqwylx/help_with_defining_hardware_multi_gpu_setup/","subreddit_subscribers":494198,"created_utc":1751567126,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n16em9d","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"haruanmj","can_mod_post":false,"created_utc":1751569546,"send_replies":true,"parent_id":"t1_n16b7xs","score":1,"author_fullname":"t2_7j4as586","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Thanks a lot, that makes me comfortable with my direction","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n16em9d","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thanks a lot, that makes me comfortable with my direction&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lqwylx","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqwylx/help_with_defining_hardware_multi_gpu_setup/n16em9d/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751569546,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n17v8gf","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Unlikely_Track_5154","can_mod_post":false,"created_utc":1751586091,"send_replies":true,"parent_id":"t1_n16b7xs","score":1,"author_fullname":"t2_r783n0ey","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I have a similar setup.\\n\\nGigabyte m32 rev3 ( i think it is rev 3, whichever one can do 7003 ) w/ 64 core 7002 epyc.\\n\\nPretty good value in these systems if you take your time and do your research.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n17v8gf","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I have a similar setup.&lt;/p&gt;\\n\\n&lt;p&gt;Gigabyte m32 rev3 ( i think it is rev 3, whichever one can do 7003 ) w/ 64 core 7002 epyc.&lt;/p&gt;\\n\\n&lt;p&gt;Pretty good value in these systems if you take your time and do your research.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lqwylx","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqwylx/help_with_defining_hardware_multi_gpu_setup/n17v8gf/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751586091,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n16b7xs","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"bick_nyers","can_mod_post":false,"created_utc":1751568558,"send_replies":true,"parent_id":"t3_1lqwylx","score":2,"author_fullname":"t2_6nwld4d3","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"1: Miner style case seems fine, for air cooling you do want them spaced a bit apart. 4 non-blower GPU this can be hard to do in a rackmount chassis. I personally don't think water-cooling is necessary but if you did want to reduce noise then water-cooling is one way to squeeze those cards into a chassis. Keep in mind that the risers used for mining are super slow and will not be a good fit for inference/training.\\n\\n\\n2: Check out used EPYC Motherboard combos on eBay to get cheap PCIE 4.0 lanes. For training tasks you want full PCIE 4.0 x16 for each individual card. I use the Zen 2 EPYC, I think it's called Rome.\\n\\n\\n3: Ideally RAM should be slightly more than 2x your VRAM. Since you're aiming for 96GB of VRAM, 256GB is a good number. You could get away with 128GB especially if you mostly do inference.\\n\\n\\n4: I personally aim for 50-75% utilization on a PSU. Each 3090 is about 300w, if we quote the rest of the system at 300w, then we have a 1500w power draw. Aim for 2000w-3000w power supply.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n16b7xs","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;1: Miner style case seems fine, for air cooling you do want them spaced a bit apart. 4 non-blower GPU this can be hard to do in a rackmount chassis. I personally don&amp;#39;t think water-cooling is necessary but if you did want to reduce noise then water-cooling is one way to squeeze those cards into a chassis. Keep in mind that the risers used for mining are super slow and will not be a good fit for inference/training.&lt;/p&gt;\\n\\n&lt;p&gt;2: Check out used EPYC Motherboard combos on eBay to get cheap PCIE 4.0 lanes. For training tasks you want full PCIE 4.0 x16 for each individual card. I use the Zen 2 EPYC, I think it&amp;#39;s called Rome.&lt;/p&gt;\\n\\n&lt;p&gt;3: Ideally RAM should be slightly more than 2x your VRAM. Since you&amp;#39;re aiming for 96GB of VRAM, 256GB is a good number. You could get away with 128GB especially if you mostly do inference.&lt;/p&gt;\\n\\n&lt;p&gt;4: I personally aim for 50-75% utilization on a PSU. Each 3090 is about 300w, if we quote the rest of the system at 300w, then we have a 1500w power draw. Aim for 2000w-3000w power supply.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqwylx/help_with_defining_hardware_multi_gpu_setup/n16b7xs/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751568558,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lqwylx","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n17cpjm","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"eloquentemu","can_mod_post":false,"send_replies":true,"parent_id":"t1_n16xx3e","score":2,"author_fullname":"t2_lpdsy","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yeah, that's rough...  Well 3090s are probably your best bet then.  Just keep in mind that 4 will need 12 slots of space if you keep the normal air coolers.  (Also worth mentioning that you can run a 70B model with 2x 3090 at q4)","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n17cpjm","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yeah, that&amp;#39;s rough...  Well 3090s are probably your best bet then.  Just keep in mind that 4 will need 12 slots of space if you keep the normal air coolers.  (Also worth mentioning that you can run a 70B model with 2x 3090 at q4)&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lqwylx","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqwylx/help_with_defining_hardware_multi_gpu_setup/n17cpjm/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751579836,"author_flair_text":null,"treatment_tags":[],"created_utc":1751579836,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n16xx3e","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"haruanmj","can_mod_post":false,"created_utc":1751575269,"send_replies":true,"parent_id":"t1_n16r7t1","score":1,"author_fullname":"t2_7j4as586","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"So, this setup is just for myself, to replace my Github copilot.\\nI'm searching for used 3090s, which will cost me around 4k USD total.\\n4 new 5090 which is what they are selling now cost the same as a Blackwell, 12k USD.\\nI'm just getting sad about how crazy expensive this stuff can be here 😔.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n16xx3e","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;So, this setup is just for myself, to replace my Github copilot.\\nI&amp;#39;m searching for used 3090s, which will cost me around 4k USD total.\\n4 new 5090 which is what they are selling now cost the same as a Blackwell, 12k USD.\\nI&amp;#39;m just getting sad about how crazy expensive this stuff can be here 😔.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lqwylx","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqwylx/help_with_defining_hardware_multi_gpu_setup/n16xx3e/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751575269,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n16r7t1","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"eloquentemu","can_mod_post":false,"created_utc":1751573292,"send_replies":true,"parent_id":"t3_1lqwylx","score":1,"author_fullname":"t2_lpdsy","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"&gt; I live in Brazil so importing has 100% taxes on top of the price\\n\\nWhat's the local 3090 market look like versus getting a 6000 Blackwell?  While the 3090s are fine for a home setup it's definitely a little sketchy for a business and will be hard to grow.  Fitting 4 stock (triple slot) 3090s in a rack mount case is going to be tricky, especially when most GPU server setups will expect dual slot cards.  I'd also say to not discount the 3090 power consumption... Mine would idle at 30-35W.\\n\\nOf course, I could see the 6000 being vastly more expensive that then 6000 and you might be able to get better performance out of the 4x 3090 setup.  It's just the the 6000 is basically made for businesses wanting to self-host 70B models so I think it's worthy of double checking that it's outside your price range.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n16r7t1","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;I live in Brazil so importing has 100% taxes on top of the price&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;What&amp;#39;s the local 3090 market look like versus getting a 6000 Blackwell?  While the 3090s are fine for a home setup it&amp;#39;s definitely a little sketchy for a business and will be hard to grow.  Fitting 4 stock (triple slot) 3090s in a rack mount case is going to be tricky, especially when most GPU server setups will expect dual slot cards.  I&amp;#39;d also say to not discount the 3090 power consumption... Mine would idle at 30-35W.&lt;/p&gt;\\n\\n&lt;p&gt;Of course, I could see the 6000 being vastly more expensive that then 6000 and you might be able to get better performance out of the 4x 3090 setup.  It&amp;#39;s just the the 6000 is basically made for businesses wanting to self-host 70B models so I think it&amp;#39;s worthy of double checking that it&amp;#39;s outside your price range.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqwylx/help_with_defining_hardware_multi_gpu_setup/n16r7t1/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751573292,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lqwylx","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),s=()=>e.jsx(t,{data:l});export{s as default};
