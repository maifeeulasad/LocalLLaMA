import{j as e}from"./index-CmSyeZDT.js";import{R as t}from"./RedditPostRenderer-C2Zg39IK.js";import"./index-CiTZuv6Z.js";const l=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"GLM-4.1V-Thinking","link_flair_richtext":[{"e":"text","t":"New Model"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":75,"top_awarded_type":null,"hide_score":false,"name":"t3_1lpl656","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.98,"author_flair_background_color":"#bbbdbf","ups":155,"total_awards_received":0,"media_embed":{},"thumbnail_width":140,"author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","is_original_content":false,"author_fullname":"t2_4gc7hf3m","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"New Model","can_mod_post":false,"score":155,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"https://external-preview.redd.it/bgfOhKzxcgalBLIa5eyKdcFfts71dHE0qj65OmHVMu0.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=d059c66b44f14873c0e09506c4c5798e53dba956","edited":false,"author_flair_css_class":null,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"gildings":{},"post_hint":"link","content_categories":null,"is_self":false,"subreddit_type":"public","created":1751425417,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"richtext","domain":"huggingface.co","allow_live_comments":false,"selftext_html":null,"likes":null,"suggested_sort":null,"banned_at_utc":null,"url_overridden_by_dest":"https://huggingface.co/collections/THUDM/glm-41v-thinking-6862bbfc44593a8601c2578d","view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"preview":{"images":[{"source":{"url":"https://external-preview.redd.it/bgfOhKzxcgalBLIa5eyKdcFfts71dHE0qj65OmHVMu0.png?auto=webp&amp;s=838e2433753fcd0f4293f62c52233c048beee8d6","width":1200,"height":648},"resolutions":[{"url":"https://external-preview.redd.it/bgfOhKzxcgalBLIa5eyKdcFfts71dHE0qj65OmHVMu0.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=df5b1dd936b0b8b133d5cb8154c5965ed1f7cf6d","width":108,"height":58},{"url":"https://external-preview.redd.it/bgfOhKzxcgalBLIa5eyKdcFfts71dHE0qj65OmHVMu0.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=b3d0c7a91a7de8c1033542bfbb90cb23715d9c54","width":216,"height":116},{"url":"https://external-preview.redd.it/bgfOhKzxcgalBLIa5eyKdcFfts71dHE0qj65OmHVMu0.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=7ae83d707a4008f00072b51019ea2ae1d235b787","width":320,"height":172},{"url":"https://external-preview.redd.it/bgfOhKzxcgalBLIa5eyKdcFfts71dHE0qj65OmHVMu0.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=d1f66974e5478d143d6f55b57fcf633e79edaf66","width":640,"height":345},{"url":"https://external-preview.redd.it/bgfOhKzxcgalBLIa5eyKdcFfts71dHE0qj65OmHVMu0.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=0e5e87c4269cb2905b9afb9f33e9a45c185acb3e","width":960,"height":518},{"url":"https://external-preview.redd.it/bgfOhKzxcgalBLIa5eyKdcFfts71dHE0qj65OmHVMu0.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=11f7d439086090b970ea5d7cd38cee3f8fdce322","width":1080,"height":583}],"variants":{},"id":"bgfOhKzxcgalBLIa5eyKdcFfts71dHE0qj65OmHVMu0"}],"enabled":false},"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"ced98442-f5d3-11ed-b657-66d3b15490c6","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":"llama.cpp","treatment_tags":[],"visited":false,"removed_by":null,"mod_note":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"num_reports":null,"removal_reason":null,"link_flair_background_color":"#ffb000","id":"1lpl656","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"AaronFeng47","discussion_type":null,"num_comments":47,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":"light","permalink":"/r/LocalLLaMA/comments/1lpl656/glm41vthinking/","stickied":false,"url":"https://huggingface.co/collections/THUDM/glm-41v-thinking-6862bbfc44593a8601c2578d","subreddit_subscribers":494001,"created_utc":1751425417,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0wgrt5","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"ShengrenR","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0vt5ru","score":13,"author_fullname":"t2_ji4n4","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Magistral speaks a bunch of languages as well, no?","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0wgrt5","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Magistral speaks a bunch of languages as well, no?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lpl656","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpl656/glm41vthinking/n0wgrt5/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751439327,"author_flair_text":null,"treatment_tags":[],"created_utc":1751439327,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":13}}],"before":null}},"user_reports":[],"saved":false,"id":"n0vt5ru","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Emport1","can_mod_post":false,"created_utc":1751427791,"send_replies":true,"parent_id":"t1_n0vnreu","score":22,"author_fullname":"t2_ubae0chn0","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"You're probably talking about smaller models but doesn't deepseek also do that?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0vt5ru","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You&amp;#39;re probably talking about smaller models but doesn&amp;#39;t deepseek also do that?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lpl656","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpl656/glm41vthinking/n0vt5ru/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751427791,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":22}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"2b12e2b8-fdc0-11ee-9a03-6e2f48afd456","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"2b12e2b8-fdc0-11ee-9a03-6e2f48afd456","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n10kjoz","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Former-Ad-5757","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0zvlyj","score":2,"author_fullname":"t2_ihsdiwk6k","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"So basically you want to give user some eye candy and you don’t care about the real thinking, just split your workflow up into multiple questions, one just asking for 10 items of eye candy in language x which you can roll and show in your app and second the real question for the answer. Because of kv cache it costs almost nothing more than just one question.\\nThe current state of thinking isn’t chain of thought alone any more, and certainly not chain of thought in a specific language.\\n\\nJust look at a qwq model, it produced for its time good answers, but it’s thinking was plainly a lot of garbage and beyond chain of thought, you really want to show that.\\nOr look at o3 pro, there is a tweet out there which showed 14 minutes thinking and a huge amount of tokens used on just responding to hello.\\n\\nWhat is called thinking is not what we humans consider thinking, it is just a way of expanding the context and cot is just a small part of that. If you want eye candy cot then you have to create it yourself or not use a good current model, because what you want is not the current state.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n10kjoz","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"Llama 3"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;So basically you want to give user some eye candy and you don’t care about the real thinking, just split your workflow up into multiple questions, one just asking for 10 items of eye candy in language x which you can roll and show in your app and second the real question for the answer. Because of kv cache it costs almost nothing more than just one question.\\nThe current state of thinking isn’t chain of thought alone any more, and certainly not chain of thought in a specific language.&lt;/p&gt;\\n\\n&lt;p&gt;Just look at a qwq model, it produced for its time good answers, but it’s thinking was plainly a lot of garbage and beyond chain of thought, you really want to show that.\\nOr look at o3 pro, there is a tweet out there which showed 14 minutes thinking and a huge amount of tokens used on just responding to hello.&lt;/p&gt;\\n\\n&lt;p&gt;What is called thinking is not what we humans consider thinking, it is just a way of expanding the context and cot is just a small part of that. If you want eye candy cot then you have to create it yourself or not use a good current model, because what you want is not the current state.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lpl656","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpl656/glm41vthinking/n10kjoz/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751491563,"author_flair_text":"Llama 3","treatment_tags":[],"created_utc":1751491563,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":"#c7b594","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n0zvlyj","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"celsowm","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0zu0ee","score":1,"author_fullname":"t2_dyvrh","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"My app could be able to mimick chatgpt reasoning accordion, and the user could be able to see the chain of thoughts in our own language","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0zvlyj","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;My app could be able to mimick chatgpt reasoning accordion, and the user could be able to see the chain of thoughts in our own language&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lpl656","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpl656/glm41vthinking/n0zvlyj/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751484223,"author_flair_text":null,"treatment_tags":[],"created_utc":1751484223,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n12dhn0","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"PlasticKey6704","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0zu0ee","score":1,"author_fullname":"t2_sqg6bx99s","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I often get inspired by thinking tokens, readable thinking helps a lot to many.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n12dhn0","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I often get inspired by thinking tokens, readable thinking helps a lot to many.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lpl656","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpl656/glm41vthinking/n12dhn0/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751513369,"author_flair_text":null,"treatment_tags":[],"created_utc":1751513369,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0zu0ee","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Former-Ad-5757","can_mod_post":false,"created_utc":1751483738,"send_replies":true,"parent_id":"t1_n0vnreu","score":3,"author_fullname":"t2_ihsdiwk6k","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"What is the added value of that? It is not real thinking, it is just a way to inject more context into the prompt. In theory you should basically get the same response in qwen 3 nothinking if you just add the thinking part to your prompt. It is a tool to enhance the user prompt and you are only limiting it if you limit it to not the largest language in its training data.\\n\\nWhy do you think most closed models are not showing it complete anymore, a part of it is anticompetitive of course, but I also believe a part is just introducing the concept of hidden tokens which are for humans complete nonsense while they help the model.\\n\\nOne of the biggest problems with llm’s is that people use extremely bad prompts which can easily be enhanced with a relative small cost of tokens (cq thinking), but in the current costing structure you can’t eat the costs and just higher your general price, and if you give the user the choice they will go for the cheapest option (because everybody knows best) and complain your model is not good enough. The only real workable solution is introduce hidden tokens which are paid for but basically never shown as otherwise people will try to cheat it for getting lower costs.\\n\\nAnd you are happy that it is thinking in other than the best language, I seriously ask… Why???","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0zu0ee","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"Llama 3"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;What is the added value of that? It is not real thinking, it is just a way to inject more context into the prompt. In theory you should basically get the same response in qwen 3 nothinking if you just add the thinking part to your prompt. It is a tool to enhance the user prompt and you are only limiting it if you limit it to not the largest language in its training data.&lt;/p&gt;\\n\\n&lt;p&gt;Why do you think most closed models are not showing it complete anymore, a part of it is anticompetitive of course, but I also believe a part is just introducing the concept of hidden tokens which are for humans complete nonsense while they help the model.&lt;/p&gt;\\n\\n&lt;p&gt;One of the biggest problems with llm’s is that people use extremely bad prompts which can easily be enhanced with a relative small cost of tokens (cq thinking), but in the current costing structure you can’t eat the costs and just higher your general price, and if you give the user the choice they will go for the cheapest option (because everybody knows best) and complain your model is not good enough. The only real workable solution is introduce hidden tokens which are paid for but basically never shown as otherwise people will try to cheat it for getting lower costs.&lt;/p&gt;\\n\\n&lt;p&gt;And you are happy that it is thinking in other than the best language, I seriously ask… Why???&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lpl656","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpl656/glm41vthinking/n0zu0ee/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751483738,"author_flair_text":"Llama 3","treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":"#c7b594","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n12iyxv","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Neither-Phone-7264","can_mod_post":false,"send_replies":true,"parent_id":"t1_n12dkgi","score":1,"author_fullname":"t2_e7yz4055","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"well, yeah, but if you just say hi, it'll start thinking in mandarin","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n12iyxv","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;well, yeah, but if you just say hi, it&amp;#39;ll start thinking in mandarin&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lpl656","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpl656/glm41vthinking/n12iyxv/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751515639,"author_flair_text":null,"treatment_tags":[],"created_utc":1751515639,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n12dkgi","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"PlasticKey6704","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0xxujc","score":2,"author_fullname":"t2_sqg6bx99s","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"depends on your prompt.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n12dkgi","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;depends on your prompt.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lpl656","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpl656/glm41vthinking/n12dkgi/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751513401,"author_flair_text":null,"treatment_tags":[],"created_utc":1751513401,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n0xxujc","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Neither-Phone-7264","can_mod_post":false,"created_utc":1751464143,"send_replies":true,"parent_id":"t1_n0vnreu","score":1,"author_fullname":"t2_e7yz4055","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"deepseek and qwen are chinese by default, no?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0xxujc","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;deepseek and qwen are chinese by default, no?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lpl656","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpl656/glm41vthinking/n0xxujc/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751464143,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0wyd7m","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"d3lay","can_mod_post":false,"created_utc":1751449648,"send_replies":true,"parent_id":"t1_n0vnreu","score":1,"author_fullname":"t2_3sqkr","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"It's a useful feature, but Deepseek developed it first, and that was quite a long time ago...","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0wyd7m","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It&amp;#39;s a useful feature, but Deepseek developed it first, and that was quite a long time ago...&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lpl656","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpl656/glm41vthinking/n0wyd7m/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751449648,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0vnreu","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"celsowm","can_mod_post":false,"created_utc":1751425641,"send_replies":true,"parent_id":"t3_1lpl656","score":25,"author_fullname":"t2_dyvrh","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"https://preview.redd.it/8j97cdmkndaf1.png?width=1031&amp;format=png&amp;auto=webp&amp;s=09eda73e39c216ada7a269993689c60c06118ce0\\n\\nfinally a non-only-english thinking open LLM !","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0vnreu","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;&lt;a href=\\"https://preview.redd.it/8j97cdmkndaf1.png?width=1031&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=09eda73e39c216ada7a269993689c60c06118ce0\\"&gt;https://preview.redd.it/8j97cdmkndaf1.png?width=1031&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=09eda73e39c216ada7a269993689c60c06118ce0&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;finally a non-only-english thinking open LLM !&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpl656/glm41vthinking/n0vnreu/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751425641,"media_metadata":{"8j97cdmkndaf1":{"status":"valid","e":"Image","m":"image/png","p":[{"y":47,"x":108,"u":"https://preview.redd.it/8j97cdmkndaf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=9ff62c3d79d7f10195b0fd939362e5d8ced4bb93"},{"y":95,"x":216,"u":"https://preview.redd.it/8j97cdmkndaf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=55cca16e81b208ba2d71cece6c758957fbf5417b"},{"y":141,"x":320,"u":"https://preview.redd.it/8j97cdmkndaf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=dab7ca0f9082e2f296ebc98b1f4aaded45b6db85"},{"y":283,"x":640,"u":"https://preview.redd.it/8j97cdmkndaf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=56fbc060a0f73eb14bbe3cc7240b74a7c2ac6fdc"},{"y":425,"x":960,"u":"https://preview.redd.it/8j97cdmkndaf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=1f39bf39895f8d6fd49d5a4b7b0daf91fe811ce1"}],"s":{"y":457,"x":1031,"u":"https://preview.redd.it/8j97cdmkndaf1.png?width=1031&amp;format=png&amp;auto=webp&amp;s=09eda73e39c216ada7a269993689c60c06118ce0"},"id":"8j97cdmkndaf1"}},"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lpl656","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":25}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0x3uz3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"RMCPhoto","can_mod_post":false,"created_utc":1751452576,"send_replies":true,"parent_id":"t3_1lpl656","score":4,"author_fullname":"t2_ehhvb","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"These benchmark results are absolutely wild...  Looking forward to seeing how this compares in the real world.  It's hard to believe that a 9b model could outclass a relatively recent 72b across generalized Vision/Language domains.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0x3uz3","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;These benchmark results are absolutely wild...  Looking forward to seeing how this compares in the real world.  It&amp;#39;s hard to believe that a 9b model could outclass a relatively recent 72b across generalized Vision/Language domains.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpl656/glm41vthinking/n0x3uz3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751452576,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lpl656","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"7dba5c08-72f1-11ee-9b6f-ca195bc297d4","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0xue2s","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Freonr2","can_mod_post":false,"created_utc":1751463026,"send_replies":true,"parent_id":"t1_n0wgpwq","score":8,"author_fullname":"t2_8xi6x","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"There are very few vision enabled models with thinking, so that's probably the most interesting part.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0xue2s","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;There are very few vision enabled models with thinking, so that&amp;#39;s probably the most interesting part.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lpl656","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpl656/glm41vthinking/n0xue2s/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751463026,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":8}}],"before":null}},"user_reports":[],"saved":false,"id":"n0wgpwq","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"PraxisOG","can_mod_post":false,"created_utc":1751439297,"send_replies":true,"parent_id":"t3_1lpl656","score":4,"author_fullname":"t2_3f9vjjno","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Unfortunately it only comes in a 9b flavor. Cool to see other thinking models though","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0wgpwq","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"Llama 70B"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Unfortunately it only comes in a 9b flavor. Cool to see other thinking models though&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpl656/glm41vthinking/n0wgpwq/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751439297,"author_flair_text":"Llama 70B","treatment_tags":[],"link_id":"t3_1lpl656","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0xttkv","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Freonr2","can_mod_post":false,"created_utc":1751462840,"send_replies":true,"parent_id":"t3_1lpl656","score":2,"author_fullname":"t2_8xi6x","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"There are not many thinking VLMs.  Kimi was recently one of the first (?) VLM models with thinking but I'm not sure it is well supported by common inference packages/apps. \\n\\nWaiting for llamacpp/vllm/lmstudio/ollama support.  \\n\\nAlso wish they used Gemma 3 27B in the comparisons, even if it is quite a bit larger, that's been my general gold standard for VLMs lately.  9B with thinking might end up being similar total latency as 27B non-thinking depending on how wordy it is, and 27B is still reasonable for local use at ~19.5GB in Q4.\\n\\nAnd at least THUDM actually integrated the GLM4 model code (Glm4vForConditionalGeneration) into the transformers package.  Some of THUDM's previous models, like CogVLM (which was amazing at the time and still very solid today), broke because they just shoved modeling.py in with the weights and not the actual transformers package and it broke within a few weeks of package updates.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0xttkv","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;There are not many thinking VLMs.  Kimi was recently one of the first (?) VLM models with thinking but I&amp;#39;m not sure it is well supported by common inference packages/apps. &lt;/p&gt;\\n\\n&lt;p&gt;Waiting for llamacpp/vllm/lmstudio/ollama support.  &lt;/p&gt;\\n\\n&lt;p&gt;Also wish they used Gemma 3 27B in the comparisons, even if it is quite a bit larger, that&amp;#39;s been my general gold standard for VLMs lately.  9B with thinking might end up being similar total latency as 27B non-thinking depending on how wordy it is, and 27B is still reasonable for local use at ~19.5GB in Q4.&lt;/p&gt;\\n\\n&lt;p&gt;And at least THUDM actually integrated the GLM4 model code (Glm4vForConditionalGeneration) into the transformers package.  Some of THUDM&amp;#39;s previous models, like CogVLM (which was amazing at the time and still very solid today), broke because they just shoved modeling.py in with the weights and not the actual transformers package and it broke within a few weeks of package updates.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpl656/glm41vthinking/n0xttkv/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751462840,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lpl656","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n13rdmb","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"nullmove","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0zuf7x","score":1,"author_fullname":"t2_aq4j0","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"&gt; Arcee-AI claim to have fixed it in base model, can't wait for fixed GLM-4 isntruct\\n\\nSadly I doubt they are gonna do that. They basically used that as test bed to validate technique for their own model:\\n\\nhttps://www.arcee.ai/blog/extending-afm-4-5b-to-64k-context-length\\n\\nHappy to be wrong but I doubt they are motivated to do more.","edited":false,"author_flair_css_class":null,"name":"t1_n13rdmb","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;Arcee-AI claim to have fixed it in base model, can&amp;#39;t wait for fixed GLM-4 isntruct&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;Sadly I doubt they are gonna do that. They basically used that as test bed to validate technique for their own model:&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://www.arcee.ai/blog/extending-afm-4-5b-to-64k-context-length\\"&gt;https://www.arcee.ai/blog/extending-afm-4-5b-to-64k-context-length&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;Happy to be wrong but I doubt they are motivated to do more.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lpl656","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpl656/glm41vthinking/n13rdmb/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751539370,"author_flair_text":null,"collapsed":false,"created_utc":1751539370,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0zuf7x","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AppearanceHeavy6724","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0zov87","score":0,"author_fullname":"t2_uz37qfx5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Yep, exactly, right now I am using it to edit a short story.  \\n  \\nGLM4-32b is an interesting model. Lack of proper context handling (falling apart after around 8k, although Arcee-AI claim to have fixed it in base model, can't wait for fixed GLM-4 isntruct) certainly hurts and default heavy sloppy style is not for everyone either, but it is smart and generally follow instructions well. Overall I'd put in the same bin as Mistral Nemo, Gamma 3 and perhaps Mistral Small 3.2 as one of not many models useable for fiction.\\n\\nOne technical oddity about GLM4-32b is that it has only 2 KV heads vs usual 8. How it manages to work at all I am puzzled.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0zuf7x","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yep, exactly, right now I am using it to edit a short story.  &lt;/p&gt;\\n\\n&lt;p&gt;GLM4-32b is an interesting model. Lack of proper context handling (falling apart after around 8k, although Arcee-AI claim to have fixed it in base model, can&amp;#39;t wait for fixed GLM-4 isntruct) certainly hurts and default heavy sloppy style is not for everyone either, but it is smart and generally follow instructions well. Overall I&amp;#39;d put in the same bin as Mistral Nemo, Gamma 3 and perhaps Mistral Small 3.2 as one of not many models useable for fiction.&lt;/p&gt;\\n\\n&lt;p&gt;One technical oddity about GLM4-32b is that it has only 2 KV heads vs usual 8. How it manages to work at all I am puzzled.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lpl656","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpl656/glm41vthinking/n0zuf7x/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751483863,"author_flair_text":null,"treatment_tags":[],"created_utc":1751483863,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}},"user_reports":[],"saved":false,"id":"n0zov87","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"LicensedTerrapin","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0w56mk","score":3,"author_fullname":"t2_97zi8wea","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Offtopic but I love GLM4 32b as an editor. Much better than Gemma 27b. Gemma wants to change too much of my writing and style while GLM4 is like eh, you do you buddy.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0zov87","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Offtopic but I love GLM4 32b as an editor. Much better than Gemma 27b. Gemma wants to change too much of my writing and style while GLM4 is like eh, you do you buddy.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lpl656","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpl656/glm41vthinking/n0zov87/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751482200,"author_flair_text":null,"treatment_tags":[],"created_utc":1751482200,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0zzin5","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AppearanceHeavy6724","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0zyepb","score":0,"author_fullname":"t2_uz37qfx5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Gemma 3 is also a vision model FYI.","edited":false,"author_flair_css_class":null,"name":"t1_n0zzin5","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Gemma 3 is also a vision model FYI.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lpl656","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpl656/glm41vthinking/n0zzin5/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751485415,"author_flair_text":null,"collapsed":false,"created_utc":1751485415,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}},"user_reports":[],"saved":false,"id":"n0zyepb","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Cool-Chemical-5629","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0wjany","score":5,"author_fullname":"t2_qz1qjc86","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Umm, but this is a vision model. Imho they aren't the best for fiction in general.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0zyepb","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Umm, but this is a vision model. Imho they aren&amp;#39;t the best for fiction in general.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lpl656","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpl656/glm41vthinking/n0zyepb/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751485077,"author_flair_text":null,"treatment_tags":[],"created_utc":1751485077,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}}],"before":null}},"user_reports":[],"saved":false,"id":"n0wjany","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"IrisColt","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0w56mk","score":2,"author_fullname":"t2_c2f558x","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I can confirm this.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0wjany","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I can confirm this.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lpl656","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpl656/glm41vthinking/n0wjany/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751440750,"author_flair_text":null,"treatment_tags":[],"created_utc":1751440750,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n0w56mk","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"AppearanceHeavy6724","can_mod_post":false,"created_utc":1751433199,"send_replies":true,"parent_id":"t1_n0vzt0i","score":21,"author_fullname":"t2_uz37qfx5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"just checked. for fiction it is awful.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0w56mk","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;just checked. for fiction it is awful.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lpl656","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpl656/glm41vthinking/n0w56mk/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751433199,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":21}}],"before":null}},"user_reports":[],"saved":false,"id":"n0vzt0i","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"BreakfastFriendly728","can_mod_post":false,"created_utc":1751430645,"send_replies":true,"parent_id":"t3_1lpl656","score":2,"author_fullname":"t2_xdw24u3am","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"how's that compared to gemma3-12b-it?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0vzt0i","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;how&amp;#39;s that compared to gemma3-12b-it?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpl656/glm41vthinking/n0vzt0i/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751430645,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lpl656","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0xh820","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Coconut_Reddit","can_mod_post":false,"created_utc":1751458387,"send_replies":true,"parent_id":"t3_1lpl656","score":1,"author_fullname":"t2_9f9nnmcr","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"How much performance is different from qwen30b ?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0xh820","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;How much performance is different from qwen30b ?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpl656/glm41vthinking/n0xh820/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751458387,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lpl656","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n100f8u","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AppearanceHeavy6724","can_mod_post":false,"created_utc":1751485689,"send_replies":true,"parent_id":"t3_1lpl656","score":0,"author_fullname":"t2_uz37qfx5","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I asked to generate a simple elmentary code, even Llama 3.2 1b does right. This one flopped.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n100f8u","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I asked to generate a simple elmentary code, even Llama 3.2 1b does right. This one flopped.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpl656/glm41vthinking/n100f8u/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751485689,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lpl656","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"total_awards_received":0,"approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"ups":0,"removal_reason":null,"link_id":"t3_1lpl656","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n11nrta","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DataLearnerAI","can_mod_post":false,"send_replies":true,"parent_id":"t1_n10flqm","score":0,"author_fullname":"t2_ilxa9crpd","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I am not, just use AI to rewrite my text, haha","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n11nrta","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I am not, just use AI to rewrite my text, haha&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lpl656","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpl656/glm41vthinking/n11nrta/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751504236,"author_flair_text":null,"treatment_tags":[],"created_utc":1751504236,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}},"user_reports":[],"saved":false,"id":"n10flqm","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":"DELETED","no_follow":true,"author":"[deleted]","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0wmucp","score":0,"approved_by":null,"report_reasons":null,"all_awardings":[],"subreddit_id":"t5_81eyvm","body":"[deleted]","edited":false,"author_flair_css_class":null,"collapsed":true,"downs":0,"is_submitter":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;[deleted]&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"associated_award":null,"stickied":false,"subreddit_type":"public","can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":"dark","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpl656/glm41vthinking/n10flqm/","num_reports":null,"locked":false,"name":"t1_n10flqm","created":1751490092,"subreddit":"LocalLLaMA","author_flair_text":null,"treatment_tags":[],"created_utc":1751490092,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":"","collapsed_because_crowd_control":null,"mod_reports":[],"mod_note":null,"distinguished":null}}],"before":null}},"user_reports":[],"saved":false,"id":"n0wmucp","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":"LOW_SCORE","no_follow":true,"author":"DataLearnerAI","can_mod_post":false,"created_utc":1751442849,"send_replies":true,"parent_id":"t3_1lpl656","score":-6,"author_fullname":"t2_ilxa9crpd","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":true,"body":"This model demonstrates remarkable competitiveness across a diverse range of benchmark tasks, including STEM reasoning, visual question answering, OCR processing, long-document understanding, and agent-based scenarios. The benchmark results reveal performance on par with the 72B-parameter counterpart (Qwen2.5-72B-VL), with notable superiority over GPT-4o in specific tasks. Particularly impressive is its 9B-parameter architecture under the MIT license, showcasing exceptional capability from a Chinese startup. This achievement highlights the growing innovation power of domestic AI research, offering a compelling open-source alternative with strong practical value.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0wmucp","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;This model demonstrates remarkable competitiveness across a diverse range of benchmark tasks, including STEM reasoning, visual question answering, OCR processing, long-document understanding, and agent-based scenarios. The benchmark results reveal performance on par with the 72B-parameter counterpart (Qwen2.5-72B-VL), with notable superiority over GPT-4o in specific tasks. Particularly impressive is its 9B-parameter architecture under the MIT license, showcasing exceptional capability from a Chinese startup. This achievement highlights the growing innovation power of domestic AI research, offering a compelling open-source alternative with strong practical value.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":"comment score below threshold","distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpl656/glm41vthinking/n0wmucp/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751442849,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lpl656","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-6}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0y01u3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"CheatCodesOfLife","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0xt0gp","score":2,"author_fullname":"t2_32el727b","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Ah, when I said \\"Bagel can do it\\", I meant the [ByteDance-Seed/BAGEL](https://huggingface.co/spaces/ByteDance-Seed/BAGEL) model.\\n\\nIt can do count out of distribution / weird things easily. Eg. this 5-legged Zebra's legs:\\n\\nhttps://files.catbox.moe/6s3780.png","edited":false,"author_flair_css_class":null,"name":"t1_n0y01u3","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Ah, when I said &amp;quot;Bagel can do it&amp;quot;, I meant the &lt;a href=\\"https://huggingface.co/spaces/ByteDance-Seed/BAGEL\\"&gt;ByteDance-Seed/BAGEL&lt;/a&gt; model.&lt;/p&gt;\\n\\n&lt;p&gt;It can do count out of distribution / weird things easily. Eg. this 5-legged Zebra&amp;#39;s legs:&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://files.catbox.moe/6s3780.png\\"&gt;https://files.catbox.moe/6s3780.png&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lpl656","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpl656/glm41vthinking/n0y01u3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751464825,"author_flair_text":null,"collapsed":false,"created_utc":1751464825,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n0xt0gp","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"thirteen-bit","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0xg987","score":1,"author_fullname":"t2_9l12dgc5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Interesting!\\n\\nWhat was your prompt? It shows 24 pcs that is total.\\n\\nWhen I've tried this image and prompt \\"how many strawberries are in the letter \\"R\\"\\" with GLM-4.1V-Thinking HF space at all default settings it correctly recognized that I'm asking only the center \\"R\\" letter strawberries and tried to count them but errored, got 9 instead of 10.\\n\\nMaybe some parameter tweaking will improve the results or maybe image tokens are encoded in too low resolution to count this image.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0xt0gp","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Interesting!&lt;/p&gt;\\n\\n&lt;p&gt;What was your prompt? It shows 24 pcs that is total.&lt;/p&gt;\\n\\n&lt;p&gt;When I&amp;#39;ve tried this image and prompt &amp;quot;how many strawberries are in the letter &amp;quot;R&amp;quot;&amp;quot; with GLM-4.1V-Thinking HF space at all default settings it correctly recognized that I&amp;#39;m asking only the center &amp;quot;R&amp;quot; letter strawberries and tried to count them but errored, got 9 instead of 10.&lt;/p&gt;\\n\\n&lt;p&gt;Maybe some parameter tweaking will improve the results or maybe image tokens are encoded in too low resolution to count this image.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lpl656","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpl656/glm41vthinking/n0xt0gp/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751462574,"author_flair_text":null,"treatment_tags":[],"created_utc":1751462574,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0y0ors","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"thirteen-bit","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0xzc80","score":1,"author_fullname":"t2_9l12dgc5","approved_by":null,"mod_note":null,"all_awardings":[],"body":"Impressive result!","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n0y0ors","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Impressive result!&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lpl656","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpl656/glm41vthinking/n0y0ors/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751465019,"author_flair_text":null,"treatment_tags":[],"created_utc":1751465019,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0xzc80","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"CheatCodesOfLife","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0xttj1","score":2,"author_fullname":"t2_32el727b","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Heh, I failed the Turing test myself. I thought we wanted to count the total number of strawberries lol\\n\\nNew prompt:\\n\\n&gt; How many strawberries in the letter \\"R\\" ?\\n\\nResponse:\\n\\n&gt; &lt;think&gt;&lt;point&gt; [0.409, 0.546] &lt;/point&gt;&lt;point&gt; [0.417, 0.652] &lt;/point&gt;&lt;point&gt; [0.420, 0.440] &lt;/point&gt;&lt;point&gt; [0.427, 0.340] &lt;/point&gt;&lt;point&gt; [0.487, 0.507] &lt;/point&gt;&lt;point&gt; [0.492, 0.321] &lt;/point&gt;&lt;point&gt; [0.507, 0.588] &lt;/point&gt;&lt;point&gt; [0.537, 0.458] &lt;/point&gt;&lt;point&gt; [0.542, 0.662] &lt;/point&gt;&lt;point&gt; [0.547, 0.372] &lt;/point&gt; &lt;/think&gt;There are 10 strawberries in the letter \\"R\\" in the picture","edited":false,"author_flair_css_class":null,"name":"t1_n0xzc80","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Heh, I failed the Turing test myself. I thought we wanted to count the total number of strawberries lol&lt;/p&gt;\\n\\n&lt;p&gt;New prompt:&lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;How many strawberries in the letter &amp;quot;R&amp;quot; ?&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;Response:&lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;&amp;lt;think&amp;gt;&amp;lt;point&amp;gt; [0.409, 0.546] &amp;lt;/point&amp;gt;&amp;lt;point&amp;gt; [0.417, 0.652] &amp;lt;/point&amp;gt;&amp;lt;point&amp;gt; [0.420, 0.440] &amp;lt;/point&amp;gt;&amp;lt;point&amp;gt; [0.427, 0.340] &amp;lt;/point&amp;gt;&amp;lt;point&amp;gt; [0.487, 0.507] &amp;lt;/point&amp;gt;&amp;lt;point&amp;gt; [0.492, 0.321] &amp;lt;/point&amp;gt;&amp;lt;point&amp;gt; [0.507, 0.588] &amp;lt;/point&amp;gt;&amp;lt;point&amp;gt; [0.537, 0.458] &amp;lt;/point&amp;gt;&amp;lt;point&amp;gt; [0.542, 0.662] &amp;lt;/point&amp;gt;&amp;lt;point&amp;gt; [0.547, 0.372] &amp;lt;/point&amp;gt; &amp;lt;/think&amp;gt;There are 10 strawberries in the letter &amp;quot;R&amp;quot; in the picture&lt;/p&gt;\\n&lt;/blockquote&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lpl656","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpl656/glm41vthinking/n0xzc80/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751464607,"author_flair_text":null,"collapsed":false,"created_utc":1751464607,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0xu78j","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"thirteen-bit","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0xttj1","score":1,"author_fullname":"t2_9l12dgc5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Mistral 3.2 gives the same answer but elaborates:\\n\\nhttps://preview.redd.it/lmvbgq2lqgaf1.png?width=903&amp;format=png&amp;auto=webp&amp;s=a6a857858c07e46517147e7d614cb1c09184ba63","edited":false,"author_flair_css_class":null,"name":"t1_n0xu78j","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Mistral 3.2 gives the same answer but elaborates:&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://preview.redd.it/lmvbgq2lqgaf1.png?width=903&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a6a857858c07e46517147e7d614cb1c09184ba63\\"&gt;https://preview.redd.it/lmvbgq2lqgaf1.png?width=903&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a6a857858c07e46517147e7d614cb1c09184ba63&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lpl656","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpl656/glm41vthinking/n0xu78j/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751462964,"media_metadata":{"lmvbgq2lqgaf1":{"status":"valid","e":"Image","m":"image/png","p":[{"y":91,"x":108,"u":"https://preview.redd.it/lmvbgq2lqgaf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=044a111e3cfe0f31cd33e92d11b0176cab685142"},{"y":182,"x":216,"u":"https://preview.redd.it/lmvbgq2lqgaf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=9a20dfeebfb1391318a0e729db90a87173e1feb8"},{"y":269,"x":320,"u":"https://preview.redd.it/lmvbgq2lqgaf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=a421d9442509b67b5b70e439d3d1bfa0a7c08380"},{"y":539,"x":640,"u":"https://preview.redd.it/lmvbgq2lqgaf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=154564051559b23e668002ed59719271610294c8"}],"s":{"y":761,"x":903,"u":"https://preview.redd.it/lmvbgq2lqgaf1.png?width=903&amp;format=png&amp;auto=webp&amp;s=a6a857858c07e46517147e7d614cb1c09184ba63"},"id":"lmvbgq2lqgaf1"}},"author_flair_text":null,"collapsed":false,"created_utc":1751462964,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0xufbw","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"thirteen-bit","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0xttj1","score":1,"author_fullname":"t2_9l12dgc5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Joycaption is almost correct:\\n\\nhttps://preview.redd.it/mk8zqgqtqgaf1.png?width=856&amp;format=png&amp;auto=webp&amp;s=ecda6003356d4f1dc77308ec53bf7c826503ca04","edited":false,"author_flair_css_class":null,"name":"t1_n0xufbw","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Joycaption is almost correct:&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://preview.redd.it/mk8zqgqtqgaf1.png?width=856&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ecda6003356d4f1dc77308ec53bf7c826503ca04\\"&gt;https://preview.redd.it/mk8zqgqtqgaf1.png?width=856&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=ecda6003356d4f1dc77308ec53bf7c826503ca04&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lpl656","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpl656/glm41vthinking/n0xufbw/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751463038,"media_metadata":{"mk8zqgqtqgaf1":{"status":"valid","e":"Image","m":"image/png","p":[{"y":58,"x":108,"u":"https://preview.redd.it/mk8zqgqtqgaf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=45d6f072ddbc1db99467b1ac6ade0df26a1ca109"},{"y":117,"x":216,"u":"https://preview.redd.it/mk8zqgqtqgaf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=450f8323d2abca05df138376b88dd32aed288da9"},{"y":173,"x":320,"u":"https://preview.redd.it/mk8zqgqtqgaf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=025457ed0c5e6c8c88ddc6afa3d98ccb8c37c1e6"},{"y":346,"x":640,"u":"https://preview.redd.it/mk8zqgqtqgaf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=c35aba0f564d9b5d55f2339a1c6402458bff999b"}],"s":{"y":464,"x":856,"u":"https://preview.redd.it/mk8zqgqtqgaf1.png?width=856&amp;format=png&amp;auto=webp&amp;s=ecda6003356d4f1dc77308ec53bf7c826503ca04"},"id":"mk8zqgqtqgaf1"}},"author_flair_text":null,"collapsed":false,"created_utc":1751463038,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0xusd1","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"thirteen-bit","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0xttj1","score":1,"author_fullname":"t2_9l12dgc5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"And granite vision 3.2 2B Q8 just said:\\n\\n&gt; answering does not require reading text in the image","edited":false,"author_flair_css_class":null,"name":"t1_n0xusd1","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;And granite vision 3.2 2B Q8 just said:&lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;answering does not require reading text in the image&lt;/p&gt;\\n&lt;/blockquote&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lpl656","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpl656/glm41vthinking/n0xusd1/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751463159,"author_flair_text":null,"collapsed":false,"created_utc":1751463159,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0xttj1","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"thirteen-bit","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0xg987","score":1,"author_fullname":"t2_9l12dgc5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Gemma3 27B Q4 confidently incorrect:\\n\\nhttps://preview.redd.it/ghhh5oe8qgaf1.png?width=1002&amp;format=png&amp;auto=webp&amp;s=3c2023a05bf071319c63295d22ff9c7ff512d721","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0xttj1","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Gemma3 27B Q4 confidently incorrect:&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://preview.redd.it/ghhh5oe8qgaf1.png?width=1002&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3c2023a05bf071319c63295d22ff9c7ff512d721\\"&gt;https://preview.redd.it/ghhh5oe8qgaf1.png?width=1002&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=3c2023a05bf071319c63295d22ff9c7ff512d721&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lpl656","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpl656/glm41vthinking/n0xttj1/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751462839,"media_metadata":{"ghhh5oe8qgaf1":{"status":"valid","e":"Image","m":"image/png","p":[{"y":50,"x":108,"u":"https://preview.redd.it/ghhh5oe8qgaf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=a0b1c825fc9edb298d3e0ca3262721628fbb9ed6"},{"y":100,"x":216,"u":"https://preview.redd.it/ghhh5oe8qgaf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=82192e762813063925e72980c959df9f3db32628"},{"y":148,"x":320,"u":"https://preview.redd.it/ghhh5oe8qgaf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=6def504218bf762fdc2c83aa239c9cff9b90b69c"},{"y":297,"x":640,"u":"https://preview.redd.it/ghhh5oe8qgaf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=719fa12d2b581bbd6a6a9cc6884b7b2ef0ed5e17"},{"y":445,"x":960,"u":"https://preview.redd.it/ghhh5oe8qgaf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=3db16e774767715e63b0dbb7c1a9f44a7ca22fd3"}],"s":{"y":465,"x":1002,"u":"https://preview.redd.it/ghhh5oe8qgaf1.png?width=1002&amp;format=png&amp;auto=webp&amp;s=3c2023a05bf071319c63295d22ff9c7ff512d721"},"id":"ghhh5oe8qgaf1"}},"author_flair_text":null,"treatment_tags":[],"created_utc":1751462839,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0xg987","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"CheatCodesOfLife","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0wcx0h","score":2,"author_fullname":"t2_32el727b","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"\`\`\`\\n&lt;think&gt;&lt;point&gt; [0.146, 0.664] &lt;/point&gt;&lt;point&gt; [0.160, 0.280] &lt;/point&gt;&lt;point&gt; [0.166, 0.471] &lt;/point&gt;&lt;point&gt; [0.170, 0.374] &lt;/point&gt;&lt;point&gt; [0.180, 0.566] &lt;/point&gt;&lt;point&gt; [0.214, 0.652] &lt;/point&gt;&lt;point&gt; [0.286, 0.652] &lt;/point&gt;&lt;point&gt; [0.410, 0.546] &lt;/point&gt;&lt;point&gt; [0.414, 0.652] &lt;/point&gt;&lt;point&gt; [0.420, 0.440] &lt;/point&gt;&lt;point&gt; [0.426, 0.340] &lt;/point&gt;&lt;point&gt; [0.484, 0.506] &lt;/point&gt;&lt;point&gt; [0.494, 0.324] &lt;/point&gt;&lt;point&gt; [0.506, 0.586] &lt;/point&gt;&lt;point&gt; [0.536, 0.456] &lt;/point&gt;&lt;point&gt; [0.540, 0.664] &lt;/point&gt;&lt;point&gt; [0.546, 0.374] &lt;/point&gt;&lt;point&gt; [0.674, 0.664] &lt;/point&gt;&lt;point&gt; [0.686, 0.586] &lt;/point&gt;&lt;point&gt; [0.690, 0.384] &lt;/point&gt;&lt;point&gt; [0.694, 0.294] &lt;/point&gt;&lt;point&gt; [0.694, 0.494] &lt;/point&gt;&lt;point&gt; [0.750, 0.652] &lt;/point&gt;&lt;point&gt; [0.814, 0.652] &lt;/point&gt; &lt;/think&gt;There are 24 strawberries in the picture\\n\`\`\`\\n\\nBagel can do it.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0xg987","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;&lt;code&gt;\\n&amp;lt;think&amp;gt;&amp;lt;point&amp;gt; [0.146, 0.664] &amp;lt;/point&amp;gt;&amp;lt;point&amp;gt; [0.160, 0.280] &amp;lt;/point&amp;gt;&amp;lt;point&amp;gt; [0.166, 0.471] &amp;lt;/point&amp;gt;&amp;lt;point&amp;gt; [0.170, 0.374] &amp;lt;/point&amp;gt;&amp;lt;point&amp;gt; [0.180, 0.566] &amp;lt;/point&amp;gt;&amp;lt;point&amp;gt; [0.214, 0.652] &amp;lt;/point&amp;gt;&amp;lt;point&amp;gt; [0.286, 0.652] &amp;lt;/point&amp;gt;&amp;lt;point&amp;gt; [0.410, 0.546] &amp;lt;/point&amp;gt;&amp;lt;point&amp;gt; [0.414, 0.652] &amp;lt;/point&amp;gt;&amp;lt;point&amp;gt; [0.420, 0.440] &amp;lt;/point&amp;gt;&amp;lt;point&amp;gt; [0.426, 0.340] &amp;lt;/point&amp;gt;&amp;lt;point&amp;gt; [0.484, 0.506] &amp;lt;/point&amp;gt;&amp;lt;point&amp;gt; [0.494, 0.324] &amp;lt;/point&amp;gt;&amp;lt;point&amp;gt; [0.506, 0.586] &amp;lt;/point&amp;gt;&amp;lt;point&amp;gt; [0.536, 0.456] &amp;lt;/point&amp;gt;&amp;lt;point&amp;gt; [0.540, 0.664] &amp;lt;/point&amp;gt;&amp;lt;point&amp;gt; [0.546, 0.374] &amp;lt;/point&amp;gt;&amp;lt;point&amp;gt; [0.674, 0.664] &amp;lt;/point&amp;gt;&amp;lt;point&amp;gt; [0.686, 0.586] &amp;lt;/point&amp;gt;&amp;lt;point&amp;gt; [0.690, 0.384] &amp;lt;/point&amp;gt;&amp;lt;point&amp;gt; [0.694, 0.294] &amp;lt;/point&amp;gt;&amp;lt;point&amp;gt; [0.694, 0.494] &amp;lt;/point&amp;gt;&amp;lt;point&amp;gt; [0.750, 0.652] &amp;lt;/point&amp;gt;&amp;lt;point&amp;gt; [0.814, 0.652] &amp;lt;/point&amp;gt; &amp;lt;/think&amp;gt;There are 24 strawberries in the picture\\n&lt;/code&gt;&lt;/p&gt;\\n\\n&lt;p&gt;Bagel can do it.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lpl656","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpl656/glm41vthinking/n0xg987/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751458011,"author_flair_text":null,"treatment_tags":[],"created_utc":1751458011,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0wdx24","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Lazy-Pattern-5171","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0wcx0h","score":1,"author_fullname":"t2_1lyjk8is25","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Sucks. All these strawberries and no R’s.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0wdx24","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Sucks. All these strawberries and no R’s.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lpl656","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpl656/glm41vthinking/n0wdx24/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751437747,"author_flair_text":null,"treatment_tags":[],"created_utc":1751437747,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0wcx0h","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"thirteen-bit","can_mod_post":false,"created_utc":1751437201,"send_replies":true,"parent_id":"t1_n0vypgv","score":8,"author_fullname":"t2_9l12dgc5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Well, as it's a multimodal model you'll have to ask how many strawberries are in the letter \\"R\\":\\n\\nhttps://preview.redd.it/ozyyh7a0meaf1.jpeg?width=1152&amp;format=pjpg&amp;auto=webp&amp;s=07fe13a466b7b090b82f70efcebf4b16743c25df","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0wcx0h","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Well, as it&amp;#39;s a multimodal model you&amp;#39;ll have to ask how many strawberries are in the letter &amp;quot;R&amp;quot;:&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://preview.redd.it/ozyyh7a0meaf1.jpeg?width=1152&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=07fe13a466b7b090b82f70efcebf4b16743c25df\\"&gt;https://preview.redd.it/ozyyh7a0meaf1.jpeg?width=1152&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=07fe13a466b7b090b82f70efcebf4b16743c25df&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lpl656","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpl656/glm41vthinking/n0wcx0h/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751437201,"media_metadata":{"ozyyh7a0meaf1":{"status":"valid","e":"Image","m":"image/jpeg","p":[{"y":84,"x":108,"u":"https://preview.redd.it/ozyyh7a0meaf1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=1fe9cf951c8c795131f13e8d0f0fed6a0634426b"},{"y":168,"x":216,"u":"https://preview.redd.it/ozyyh7a0meaf1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=6f27d52f95a08a3954968b479682a75bd721ec3a"},{"y":248,"x":320,"u":"https://preview.redd.it/ozyyh7a0meaf1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=569d7809176e77d6b1a15547a89959dbe8268f8e"},{"y":497,"x":640,"u":"https://preview.redd.it/ozyyh7a0meaf1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=ca8b6740e5dc12205e37c84e0047e5e9e89e4ec5"},{"y":746,"x":960,"u":"https://preview.redd.it/ozyyh7a0meaf1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=f9c635db88c85f446e65a1a47b4715188cfd2f26"},{"y":840,"x":1080,"u":"https://preview.redd.it/ozyyh7a0meaf1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=7f386e63795d1ae210043fedf0577ceca1e16518"}],"s":{"y":896,"x":1152,"u":"https://preview.redd.it/ozyyh7a0meaf1.jpeg?width=1152&amp;format=pjpg&amp;auto=webp&amp;s=07fe13a466b7b090b82f70efcebf4b16743c25df"},"id":"ozyyh7a0meaf1"}},"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":8}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n11gue1","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"RMCPhoto","can_mod_post":false,"created_utc":1751501969,"send_replies":true,"parent_id":"t1_n107ed6","score":1,"author_fullname":"t2_ehhvb","approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yeah, it does seem strange doesn't it...  Some of this abstraction related confusion would be resolved by moving towards character level tokens, but this would reduce the throughput and require significantly more predictions.\\n\\nThe tokens have also been adjusted over time to improve comprehension of specific content.  Like tabbed codeblocks.  I believe various tab/space combinations were explicitly added to improve code comprehension, as it was previously a bit unpredictable and would vary depending on the first characters in the code blocks.   \\n\\nThe error rate of early llama models would also vary WILDLY with very small changes to tokens.  Something as simple as starting the user query with a space would swing error 40%. \\n\\nThis is still a major issue all over the place.  Small changes to text can have unpredictable impacts on the resulting prediction even though to a person it would mean the same thing.","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n11gue1","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yeah, it does seem strange doesn&amp;#39;t it...  Some of this abstraction related confusion would be resolved by moving towards character level tokens, but this would reduce the throughput and require significantly more predictions.&lt;/p&gt;\\n\\n&lt;p&gt;The tokens have also been adjusted over time to improve comprehension of specific content.  Like tabbed codeblocks.  I believe various tab/space combinations were explicitly added to improve code comprehension, as it was previously a bit unpredictable and would vary depending on the first characters in the code blocks.   &lt;/p&gt;\\n\\n&lt;p&gt;The error rate of early llama models would also vary WILDLY with very small changes to tokens.  Something as simple as starting the user query with a space would swing error 40%. &lt;/p&gt;\\n\\n&lt;p&gt;This is still a major issue all over the place.  Small changes to text can have unpredictable impacts on the resulting prediction even though to a person it would mean the same thing.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lpl656","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpl656/glm41vthinking/n11gue1/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751501969,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":7,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n107ed6","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Lazy-Pattern-5171","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0zqndg","score":1,"author_fullname":"t2_1lyjk8is25","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"That does feel like we haven’t really unlocked the key to having brain like systems yet. We just have a way now of generating infinite coherent looking even conscious like text but the system that generates this coherent looking text does not itself have an understanding of it. \\n\\nThat’s interesting to me because Multi Head attention is exactly designed to do that. It’s designed for one token to be aware of its semantic meaning in relation to all the other tokens (hence the N^2 complexity of Transformers). So you would think that A 1 B 2 C 3 etc appearing in input text would give each of those a mathematical semantic meaning however it doesn’t seem like math is an emergent property of such a function of convergence. Even when it’s generalized over the entire fineweb corpus.","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n107ed6","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;That does feel like we haven’t really unlocked the key to having brain like systems yet. We just have a way now of generating infinite coherent looking even conscious like text but the system that generates this coherent looking text does not itself have an understanding of it. &lt;/p&gt;\\n\\n&lt;p&gt;That’s interesting to me because Multi Head attention is exactly designed to do that. It’s designed for one token to be aware of its semantic meaning in relation to all the other tokens (hence the N&lt;sup&gt;2&lt;/sup&gt; complexity of Transformers). So you would think that A 1 B 2 C 3 etc appearing in input text would give each of those a mathematical semantic meaning however it doesn’t seem like math is an emergent property of such a function of convergence. Even when it’s generalized over the entire fineweb corpus.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lpl656","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpl656/glm41vthinking/n107ed6/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751487745,"author_flair_text":null,"treatment_tags":[],"created_utc":1751487745,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":6,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0zqndg","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"RMCPhoto","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0y9173","score":1,"author_fullname":"t2_ehhvb","approved_by":null,"mod_note":null,"all_awardings":[],"body":"If it's in the dataset and is important enough to be known verbatim, then yes, it would work.  \\n\\nThink of it this way, LLMs are also not good at counting the words in a paragraph, the number of periods in \\"..........\\" Or other similar methods of evaluating the numerical or structural or character level nature of the prompt via prediction.    It can get close because of its exposure in training data to labeled paragraphs of certain word counts, or similar to make a rough inference, but there is no efficient reasoning / reinforcement learning method that can be used to do this accurately.     I'm sure you could find a step by step decomposition process that might work, but it's silly to teach a model this. \\n\\nIn essence, the language model is not self aware and does not know that the prompt / context is tokens instead of text...I think they should instead ensure that RL/fine tuning instills knowledge of it's own limitations rather than wasting parameter configurations on fruitlessly 🍓 trying to solve this low value issue.  \\n\\nIn fact, even the dumbest language models can easily solve all of the problems above...very easily... I'm sure even a 3b model could.  \\n\\nThe solution is to ask it to write a python script to provide the answer.   \\n\\nMost models / agents will hopefully have this capability.  (Python in sandbox).  And this is the right approach.   \\n\\n1. Use a llm for what it is good for.\\n2. Identify it's blind spots, and understand why those blind spots exist.\\n3.  Teach the model about those blindspots in fine tuning and provide the correct tool to answer those problems.","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n0zqndg","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;If it&amp;#39;s in the dataset and is important enough to be known verbatim, then yes, it would work.  &lt;/p&gt;\\n\\n&lt;p&gt;Think of it this way, LLMs are also not good at counting the words in a paragraph, the number of periods in &amp;quot;..........&amp;quot; Or other similar methods of evaluating the numerical or structural or character level nature of the prompt via prediction.    It can get close because of its exposure in training data to labeled paragraphs of certain word counts, or similar to make a rough inference, but there is no efficient reasoning / reinforcement learning method that can be used to do this accurately.     I&amp;#39;m sure you could find a step by step decomposition process that might work, but it&amp;#39;s silly to teach a model this. &lt;/p&gt;\\n\\n&lt;p&gt;In essence, the language model is not self aware and does not know that the prompt / context is tokens instead of text...I think they should instead ensure that RL/fine tuning instills knowledge of it&amp;#39;s own limitations rather than wasting parameter configurations on fruitlessly 🍓 trying to solve this low value issue.  &lt;/p&gt;\\n\\n&lt;p&gt;In fact, even the dumbest language models can easily solve all of the problems above...very easily... I&amp;#39;m sure even a 3b model could.  &lt;/p&gt;\\n\\n&lt;p&gt;The solution is to ask it to write a python script to provide the answer.   &lt;/p&gt;\\n\\n&lt;p&gt;Most models / agents will hopefully have this capability.  (Python in sandbox).  And this is the right approach.   &lt;/p&gt;\\n\\n&lt;ol&gt;\\n&lt;li&gt;Use a llm for what it is good for.&lt;/li&gt;\\n&lt;li&gt;Identify it&amp;#39;s blind spots, and understand why those blind spots exist.&lt;/li&gt;\\n&lt;li&gt; Teach the model about those blindspots in fine tuning and provide the correct tool to answer those problems.&lt;/li&gt;\\n&lt;/ol&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lpl656","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpl656/glm41vthinking/n0zqndg/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751482724,"author_flair_text":null,"treatment_tags":[],"created_utc":1751482724,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0y9173","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Lazy-Pattern-5171","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0y2eb5","score":1,"author_fullname":"t2_1lyjk8is25","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"No I understand how tokenizers work they’re the most commonly occurring byte pair sequences in a given corpus where we pick a fixed amount of vocabulary. However, it seems to be tokenizing it and “recognizing” A B C etc. it doesn’t converge to counting correctly and overthinks, this seems to be an issue with the RL no? Given that I’m asking something that at this point should also be in the dataset.","edited":false,"author_flair_css_class":null,"name":"t1_n0y9173","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;No I understand how tokenizers work they’re the most commonly occurring byte pair sequences in a given corpus where we pick a fixed amount of vocabulary. However, it seems to be tokenizing it and “recognizing” A B C etc. it doesn’t converge to counting correctly and overthinks, this seems to be an issue with the RL no? Given that I’m asking something that at this point should also be in the dataset.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lpl656","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpl656/glm41vthinking/n0y9173/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751467464,"author_flair_text":null,"collapsed":false,"created_utc":1751467464,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0y2eb5","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"RMCPhoto","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0xnjcv","score":1,"author_fullname":"t2_ehhvb","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"No, not necessarily.  And those will vary based on what comes before or after.  IE a space before 'A', or your period after 'B'.  Etc etc.  You can try the openai tokenizer yourself with various combinations and see how an AI model sees it.   [https://platform.openai.com/tokenizer](https://platform.openai.com/tokenizer)\\n\\nThe tokens are not necessarily \\"logical\\" to you.  They are not fixed either.  They are derrived statistically based on massive amounts of training data.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0y2eb5","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;No, not necessarily.  And those will vary based on what comes before or after.  IE a space before &amp;#39;A&amp;#39;, or your period after &amp;#39;B&amp;#39;.  Etc etc.  You can try the openai tokenizer yourself with various combinations and see how an AI model sees it.   &lt;a href=\\"https://platform.openai.com/tokenizer\\"&gt;https://platform.openai.com/tokenizer&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;The tokens are not necessarily &amp;quot;logical&amp;quot; to you.  They are not fixed either.  They are derrived statistically based on massive amounts of training data.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lpl656","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpl656/glm41vthinking/n0y2eb5/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751465530,"author_flair_text":null,"treatment_tags":[],"created_utc":1751465530,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0xnjcv","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Lazy-Pattern-5171","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0x0o2m","score":1,"author_fullname":"t2_1lyjk8is25","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Isn’t ‘A’’B’. ‘C’ etc a token also?","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0xnjcv","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Isn’t ‘A’’B’. ‘C’ etc a token also?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lpl656","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpl656/glm41vthinking/n0xnjcv/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751460715,"author_flair_text":null,"treatment_tags":[],"created_utc":1751460715,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0x0o2m","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"RMCPhoto","can_mod_post":false,"created_utc":1751450908,"send_replies":true,"parent_id":"t1_n0vypgv","score":1,"author_fullname":"t2_ehhvb","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"No, look into how tokenizers / llms function.  Even a 400b parameter model would not be \\"expected\\" to count characters correctly.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0x0o2m","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;No, look into how tokenizers / llms function.  Even a 400b parameter model would not be &amp;quot;expected&amp;quot; to count characters correctly.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lpl656","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpl656/glm41vthinking/n0x0o2m/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751450908,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0vypgv","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":"LOW_SCORE","no_follow":true,"author":"Lazy-Pattern-5171","can_mod_post":false,"created_utc":1751430154,"send_replies":true,"parent_id":"t3_1lpl656","score":-9,"author_fullname":"t2_1lyjk8is25","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":true,"body":"Doesn’t count R’s in strawberry correctly. I’m guessing 9Bs should be able to do that no?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0vypgv","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Doesn’t count R’s in strawberry correctly. I’m guessing 9Bs should be able to do that no?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":"comment score below threshold","distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpl656/glm41vthinking/n0vypgv/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751430154,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lpl656","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-9}}],"before":null}}]`),r=()=>e.jsx(t,{data:l});export{r as default};
