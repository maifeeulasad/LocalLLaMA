import{j as e}from"./index-DQXiEb7D.js";import{R as t}from"./RedditPostRenderer-BjndLgq8.js";import"./index-B-ILyjT1.js";const a=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"I'm releasing a v1.0 of my [Orpheus TTS FastAPI Server](https://github.com/prakharsr/Orpheus-TTS-FastAPI). Its a high-performance FastAPI-based server that provides OpenAI-compatible Text-to-Speech (TTS) endpoints using the [Orpheus TTS](https://github.com/canopyai/Orpheus-TTS) model. The server supports async parallel chunk processing for significantly faster audio generation. This project improves the original implementation in the \`orpheus-speech\` python package.\\n\\nThe project solves existing issues in audio generation when using Orpheus (repeated lines in audio/ extended audio with no spoken text but weird noises/ audio hallucinations/ infinite audio looping/ some other issues) by:\\n\\n1. Using higher precision formats requiring more VRAM but eliminating audio quality issues and artifacts commonly found in quantized models or alternative inference engines.\\n2. **Intelligent Retry Logic:** Automatic retry on audio decoding errors for improved reliability. The original implementation in \`orpheus-speech\` skipped tokens leading to incomplete words, this is now fixed by retrying automatically on detection of such errors.\\n3. **Token Repetition Detection**: Prevents infinite audio loops with adaptive pattern detection and automatic retry with adjusted parameters. The original implementation in \`orpheus-speech\` sometimes generated infinite audio loops, this is now fixed by automatic detection of such repetitions and retrying with higher repetition penalty.\\n4. **Async Parallel Processing**: Processes multiple text chunks simultaneously for faster generation. The original implementation in \`orpheus-speech\` was synchronous, this is now fixed by adding support for concurrent async calls.\\n5. **Text Chunking**: Automatic intelligent text splitting for long content.\\n\\nLink to the repo: [https://github.com/prakharsr/Orpheus-TTS-FastAPI](https://github.com/prakharsr/Orpheus-TTS-FastAPI)\\n\\nLet me know how it works and also checkout my [Audiobook Creator Project here](https://github.com/prakharsr/audiobook-creator) which supports Kokoro and Orpheus.","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Orpheus TTS FastAPI Server Release v1.0 (Async and Audio Issues Fixes)","link_flair_richtext":[{"e":"text","t":"Resources"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lyvsqv","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.94,"author_flair_background_color":null,"subreddit_type":"public","ups":44,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_hi3epx7","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Resources","can_mod_post":false,"score":44,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"post_hint":"self","content_categories":null,"is_self":true,"mod_note":null,"created":1752421053,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;m releasing a v1.0 of my &lt;a href=\\"https://github.com/prakharsr/Orpheus-TTS-FastAPI\\"&gt;Orpheus TTS FastAPI Server&lt;/a&gt;. Its a high-performance FastAPI-based server that provides OpenAI-compatible Text-to-Speech (TTS) endpoints using the &lt;a href=\\"https://github.com/canopyai/Orpheus-TTS\\"&gt;Orpheus TTS&lt;/a&gt; model. The server supports async parallel chunk processing for significantly faster audio generation. This project improves the original implementation in the &lt;code&gt;orpheus-speech&lt;/code&gt; python package.&lt;/p&gt;\\n\\n&lt;p&gt;The project solves existing issues in audio generation when using Orpheus (repeated lines in audio/ extended audio with no spoken text but weird noises/ audio hallucinations/ infinite audio looping/ some other issues) by:&lt;/p&gt;\\n\\n&lt;ol&gt;\\n&lt;li&gt;Using higher precision formats requiring more VRAM but eliminating audio quality issues and artifacts commonly found in quantized models or alternative inference engines.&lt;/li&gt;\\n&lt;li&gt;&lt;strong&gt;Intelligent Retry Logic:&lt;/strong&gt; Automatic retry on audio decoding errors for improved reliability. The original implementation in &lt;code&gt;orpheus-speech&lt;/code&gt; skipped tokens leading to incomplete words, this is now fixed by retrying automatically on detection of such errors.&lt;/li&gt;\\n&lt;li&gt;&lt;strong&gt;Token Repetition Detection&lt;/strong&gt;: Prevents infinite audio loops with adaptive pattern detection and automatic retry with adjusted parameters. The original implementation in &lt;code&gt;orpheus-speech&lt;/code&gt; sometimes generated infinite audio loops, this is now fixed by automatic detection of such repetitions and retrying with higher repetition penalty.&lt;/li&gt;\\n&lt;li&gt;&lt;strong&gt;Async Parallel Processing&lt;/strong&gt;: Processes multiple text chunks simultaneously for faster generation. The original implementation in &lt;code&gt;orpheus-speech&lt;/code&gt; was synchronous, this is now fixed by adding support for concurrent async calls.&lt;/li&gt;\\n&lt;li&gt;&lt;strong&gt;Text Chunking&lt;/strong&gt;: Automatic intelligent text splitting for long content.&lt;/li&gt;\\n&lt;/ol&gt;\\n\\n&lt;p&gt;Link to the repo: &lt;a href=\\"https://github.com/prakharsr/Orpheus-TTS-FastAPI\\"&gt;https://github.com/prakharsr/Orpheus-TTS-FastAPI&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;Let me know how it works and also checkout my &lt;a href=\\"https://github.com/prakharsr/audiobook-creator\\"&gt;Audiobook Creator Project here&lt;/a&gt; which supports Kokoro and Orpheus.&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"preview":{"images":[{"source":{"url":"https://external-preview.redd.it/IESpl2ifzDFrsnd4p7lWGb7dUcwjRO9ltHokb4hE5Co.png?auto=webp&amp;s=0cc860c55c5d39a8725b904b4561a9f80e0f99d0","width":1200,"height":600},"resolutions":[{"url":"https://external-preview.redd.it/IESpl2ifzDFrsnd4p7lWGb7dUcwjRO9ltHokb4hE5Co.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=45d52cf1200f1189b715c76836164ff9cecf79b9","width":108,"height":54},{"url":"https://external-preview.redd.it/IESpl2ifzDFrsnd4p7lWGb7dUcwjRO9ltHokb4hE5Co.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=b408822642b2470faba4b74a50ce8516a253a9e3","width":216,"height":108},{"url":"https://external-preview.redd.it/IESpl2ifzDFrsnd4p7lWGb7dUcwjRO9ltHokb4hE5Co.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=948de85aa715b4f5e80a8759acad2fb62df83251","width":320,"height":160},{"url":"https://external-preview.redd.it/IESpl2ifzDFrsnd4p7lWGb7dUcwjRO9ltHokb4hE5Co.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=b0c92ff268493c06d6bca23d519a6e658e83f3c0","width":640,"height":320},{"url":"https://external-preview.redd.it/IESpl2ifzDFrsnd4p7lWGb7dUcwjRO9ltHokb4hE5Co.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=4c66abb6a8bf083ae94087d05f70bde8121c8436","width":960,"height":480},{"url":"https://external-preview.redd.it/IESpl2ifzDFrsnd4p7lWGb7dUcwjRO9ltHokb4hE5Co.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=ec93d51f819b3e6c11763f4435e0e77ffbdb9b84","width":1080,"height":540}],"variants":{},"id":"IESpl2ifzDFrsnd4p7lWGb7dUcwjRO9ltHokb4hE5Co"}],"enabled":false},"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#ccac2b","id":"1lyvsqv","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"prakharsr","discussion_type":null,"num_comments":20,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lyvsqv/orpheus_tts_fastapi_server_release_v10_async_and/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lyvsqv/orpheus_tts_fastapi_server_release_v10_async_and/","subreddit_subscribers":499297,"created_utc":1752421053,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2xyacg","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"prakharsr","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2xtxk7","score":2,"author_fullname":"t2_hi3epx7","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I tested with fp16 and q8 gguf on my mac actually. Will test bf16 and fp32 on a nvidia runpod and see if the issues lie with gguf implementations.\\n\\nYeah, I had been using linux only for vllm testing but it makes sense that having a stable llama.cpp support would be better for memory footprint and inference speed also.\\n\\nAh, got it. I mostly did the batching thing so that the audio wouldnt extend to the max model length param leading to audio clipping. Yeaah, expanding the dataset for keywords to split on makes sense. Thanks for the tip !","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2xyacg","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I tested with fp16 and q8 gguf on my mac actually. Will test bf16 and fp32 on a nvidia runpod and see if the issues lie with gguf implementations.&lt;/p&gt;\\n\\n&lt;p&gt;Yeah, I had been using linux only for vllm testing but it makes sense that having a stable llama.cpp support would be better for memory footprint and inference speed also.&lt;/p&gt;\\n\\n&lt;p&gt;Ah, got it. I mostly did the batching thing so that the audio wouldnt extend to the max model length param leading to audio clipping. Yeaah, expanding the dataset for keywords to split on makes sense. Thanks for the tip !&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lyvsqv","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyvsqv/orpheus_tts_fastapi_server_release_v10_async_and/n2xyacg/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752432323,"author_flair_text":null,"treatment_tags":[],"created_utc":1752432323,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n2xtxk7","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Chromix_","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2xml07","score":3,"author_fullname":"t2_k7w2h","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Did you also test for issues with BF16 GGUFs, to see if there's maybe an implementation issue?\\n\\n[vLLM on Windows](https://github.com/SystemPanic/vllm-windows) is a relatively new thing, which is why supporting llama.cpp as an alternative can be nicer for those not running Linux.\\n\\n&gt;I haven't benchmarked if any text gets lost\\n\\nOh, I didn't mean that text would get lost, but that the code contains a manual check for words such as \\"said\\" and \\"exclaimed\\". Maybe more can be found in an automated way by using a small LLM on a bulk of stories.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2xtxk7","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Did you also test for issues with BF16 GGUFs, to see if there&amp;#39;s maybe an implementation issue?&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://github.com/SystemPanic/vllm-windows\\"&gt;vLLM on Windows&lt;/a&gt; is a relatively new thing, which is why supporting llama.cpp as an alternative can be nicer for those not running Linux.&lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;I haven&amp;#39;t benchmarked if any text gets lost&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;Oh, I didn&amp;#39;t mean that text would get lost, but that the code contains a manual check for words such as &amp;quot;said&amp;quot; and &amp;quot;exclaimed&amp;quot;. Maybe more can be found in an automated way by using a small LLM on a bulk of stories.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lyvsqv","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyvsqv/orpheus_tts_fastapi_server_release_v10_async_and/n2xtxk7/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752431021,"author_flair_text":null,"treatment_tags":[],"created_utc":1752431021,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n2xml07","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"prakharsr","can_mod_post":false,"created_utc":1752428855,"send_replies":true,"parent_id":"t1_n2xcmwm","score":4,"author_fullname":"t2_hi3epx7","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I had been testing orpheus with ggufs and llama.cpp only earlier. I found that the audio related issues were very prevalent so I decided to try the higher precision vLLM implementation. There I noticed that the issues became fewer as compared to quantized versions but some issues would still popup randomly. So I decided to investigate what was happening and came up with the token decoding fixes and audio looping related fixes. In theory, the same fixes should work with llama.cpp ggufs also. So, I'll surely try and test the ggufs and release support for them if all goes well.\\n\\nRegarding the text splitting, I haven't benchmarked if any text gets lost but I can verify that and probably make it better.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2xml07","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I had been testing orpheus with ggufs and llama.cpp only earlier. I found that the audio related issues were very prevalent so I decided to try the higher precision vLLM implementation. There I noticed that the issues became fewer as compared to quantized versions but some issues would still popup randomly. So I decided to investigate what was happening and came up with the token decoding fixes and audio looping related fixes. In theory, the same fixes should work with llama.cpp ggufs also. So, I&amp;#39;ll surely try and test the ggufs and release support for them if all goes well.&lt;/p&gt;\\n\\n&lt;p&gt;Regarding the text splitting, I haven&amp;#39;t benchmarked if any text gets lost but I can verify that and probably make it better.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lyvsqv","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyvsqv/orpheus_tts_fastapi_server_release_v10_async_and/n2xml07/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752428855,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}}],"before":null}},"user_reports":[],"saved":false,"id":"n2xcmwm","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Chromix_","can_mod_post":false,"created_utc":1752425988,"send_replies":true,"parent_id":"t3_1lyvsqv","score":9,"author_fullname":"t2_k7w2h","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"The project currently requires vLLM. Maybe support for a REST call to an endpoint like provided by the llama.cpp server can be added as an alternative? Some sort of Orpheus support was added a while ago. That would also allow to use quantized GGUF version to reduce the VRAM usage, if the current state works correctly.\\n\\nThere's a bunch of [hardcoded logic](https://github.com/prakharsr/Orpheus-TTS-FastAPI/blob/e1d64bdf4c7af018cce6caeac734518791016e01/text_processor.py#L53) for splitting dialogue text to maintain a consistent voice. It could be interesting to batch-process a bunch of text with it and let a small LLM check what wasn't picked up, to see if there's anything missing that should be added to the splitting logic.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2xcmwm","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The project currently requires vLLM. Maybe support for a REST call to an endpoint like provided by the llama.cpp server can be added as an alternative? Some sort of Orpheus support was added a while ago. That would also allow to use quantized GGUF version to reduce the VRAM usage, if the current state works correctly.&lt;/p&gt;\\n\\n&lt;p&gt;There&amp;#39;s a bunch of &lt;a href=\\"https://github.com/prakharsr/Orpheus-TTS-FastAPI/blob/e1d64bdf4c7af018cce6caeac734518791016e01/text_processor.py#L53\\"&gt;hardcoded logic&lt;/a&gt; for splitting dialogue text to maintain a consistent voice. It could be interesting to batch-process a bunch of text with it and let a small LLM check what wasn&amp;#39;t picked up, to see if there&amp;#39;s anything missing that should be added to the splitting logic.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyvsqv/orpheus_tts_fastapi_server_release_v10_async_and/n2xcmwm/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752425988,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lyvsqv","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":9}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2ypy7s","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"simracerman","can_mod_post":false,"created_utc":1752440561,"send_replies":true,"parent_id":"t1_n2x1lp8","score":2,"author_fullname":"t2_vbzgnic","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"This is not OP's repo, but I just found this, and it worked great!\\n\\n[https://github.com/Lex-au/Orpheus-FastAPI](https://github.com/Lex-au/Orpheus-FastAPI)\\n\\n  \\nToo bad I don't have a compatible GPU to be passed to Docker, so it defaulted to CPU. The sound quality is insane! However, it takes a really long time to generate.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2ypy7s","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;This is not OP&amp;#39;s repo, but I just found this, and it worked great!&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://github.com/Lex-au/Orpheus-FastAPI\\"&gt;https://github.com/Lex-au/Orpheus-FastAPI&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;Too bad I don&amp;#39;t have a compatible GPU to be passed to Docker, so it defaulted to CPU. The sound quality is insane! However, it takes a really long time to generate.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lyvsqv","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyvsqv/orpheus_tts_fastapi_server_release_v10_async_and/n2ypy7s/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752440561,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2x6egq","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"prakharsr","can_mod_post":false,"created_utc":1752424109,"send_replies":true,"parent_id":"t1_n2x1lp8","score":1,"author_fullname":"t2_hi3epx7","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I thought of creating a docker image but I currently dont have a compatible GPU to run and test it right now. I had been working on this project using a runpod and there I couldn't figure out how to get docker running to be able to create an image. So, currently I can't create and test one but would accept PRs if you or anybody else is interested in it.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2x6egq","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I thought of creating a docker image but I currently dont have a compatible GPU to run and test it right now. I had been working on this project using a runpod and there I couldn&amp;#39;t figure out how to get docker running to be able to create an image. So, currently I can&amp;#39;t create and test one but would accept PRs if you or anybody else is interested in it.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lyvsqv","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyvsqv/orpheus_tts_fastapi_server_release_v10_async_and/n2x6egq/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752424109,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2x1lp8","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Flashy_Management962","can_mod_post":false,"created_utc":1752422653,"send_replies":true,"parent_id":"t3_1lyvsqv","score":3,"author_fullname":"t2_n9dnke1h","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Do you have any interest in releasing a docker/podman image for that?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2x1lp8","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Do you have any interest in releasing a docker/podman image for that?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyvsqv/orpheus_tts_fastapi_server_release_v10_async_and/n2x1lp8/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752422653,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lyvsqv","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2xzqka","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"prakharsr","can_mod_post":false,"created_utc":1752432757,"send_replies":true,"parent_id":"t1_n2xr6sn","score":2,"author_fullname":"t2_hi3epx7","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"This doesnt support streaming yet but I can look into it coming up.\\n\\nIt used to give me audio issues when i first tested it without my existing fixes. After some testing and fixes, I was able to make it usable for zero shot (my fastapi server implementation retries if its detects any audio issues and fixes the audio). I tried creating short audiobooks out of it and till now it looks usable to me.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2xzqka","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;This doesnt support streaming yet but I can look into it coming up.&lt;/p&gt;\\n\\n&lt;p&gt;It used to give me audio issues when i first tested it without my existing fixes. After some testing and fixes, I was able to make it usable for zero shot (my fastapi server implementation retries if its detects any audio issues and fixes the audio). I tried creating short audiobooks out of it and till now it looks usable to me.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lyvsqv","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyvsqv/orpheus_tts_fastapi_server_release_v10_async_and/n2xzqka/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752432757,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2xx5ot","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"RobotDoorBuilder","can_mod_post":false,"created_utc":1752431987,"send_replies":true,"parent_id":"t1_n2xr6sn","score":1,"author_fullname":"t2_pz6zika0","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"need finetune to be reliable.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2xx5ot","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;need finetune to be reliable.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lyvsqv","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyvsqv/orpheus_tts_fastapi_server_release_v10_async_and/n2xx5ot/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752431987,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n30pv3o","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Unfair-Enthusiasm-30","can_mod_post":false,"created_utc":1752466067,"send_replies":true,"parent_id":"t1_n2xr6sn","score":1,"author_fullname":"t2_vgd5f4x3","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Baseten has streaming support for \\"production grade\\" where they have done benchmark on H100 and can support 16-24 concurrent sessions. (I am not with Baseten at all. I just happened to experimented with Orpheus, fine-tuned it and tested it)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n30pv3o","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Baseten has streaming support for &amp;quot;production grade&amp;quot; where they have done benchmark on H100 and can support 16-24 concurrent sessions. (I am not with Baseten at all. I just happened to experimented with Orpheus, fine-tuned it and tested it)&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lyvsqv","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyvsqv/orpheus_tts_fastapi_server_release_v10_async_and/n30pv3o/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752466067,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2xr6sn","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"UAAgency","can_mod_post":false,"created_utc":1752430203,"send_replies":true,"parent_id":"t3_1lyvsqv","score":4,"author_fullname":"t2_1jx1fa8","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"What about streaming? This is just for generation?\\n\\nBtw how reliable is Orpheus, could it work well as a real time tts zero shot every time?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2xr6sn","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;What about streaming? This is just for generation?&lt;/p&gt;\\n\\n&lt;p&gt;Btw how reliable is Orpheus, could it work well as a real time tts zero shot every time?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyvsqv/orpheus_tts_fastapi_server_release_v10_async_and/n2xr6sn/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752430203,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lyvsqv","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2y3rrq","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"prakharsr","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2y34wr","score":1,"author_fullname":"t2_hi3epx7","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Cool, I'll check it out. Though I haven't implemented the vLLM integration, I forked this off the existing implementation that orpheus guys had, so I haven't dived much deep into the inference part.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2y3rrq","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Cool, I&amp;#39;ll check it out. Though I haven&amp;#39;t implemented the vLLM integration, I forked this off the existing implementation that orpheus guys had, so I haven&amp;#39;t dived much deep into the inference part.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lyvsqv","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyvsqv/orpheus_tts_fastapi_server_release_v10_async_and/n2y3rrq/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752433963,"author_flair_text":null,"treatment_tags":[],"created_utc":1752433963,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2y34wr","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Traditional_Tap1708","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2y2kha","score":2,"author_fullname":"t2_aejvth7b","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Great. I would suggest you to try tensorrt-llm its faster than vllm for orpheus llama backbone. I was getting ~160ms ttfb with it on a 4090 (with some decoding optimisations)","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2y34wr","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Great. I would suggest you to try tensorrt-llm its faster than vllm for orpheus llama backbone. I was getting ~160ms ttfb with it on a 4090 (with some decoding optimisations)&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lyvsqv","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyvsqv/orpheus_tts_fastapi_server_release_v10_async_and/n2y34wr/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752433771,"author_flair_text":null,"treatment_tags":[],"created_utc":1752433771,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n2y2kha","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"prakharsr","can_mod_post":false,"created_utc":1752433605,"send_replies":true,"parent_id":"t1_n2y0vw6","score":1,"author_fullname":"t2_hi3epx7","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I havent benchmarked it yet but when i ran it on a rtx 3090, I was getting 1-2 seconds per line of text while making 16 parallel calls to the fastapi server.\\n\\nyeah, i used to have audio related issues (repeated lines in audio/ extended audio with no spoken text but weird noises/ audio hallucinations/ infinite audio looping/ some other issues) but I was able to fix most of them by using bf16/ fp32 precision and some tweaks and retry mechanisms to handle errors.\\n\\nI havent fine tuned the model, I just did some fixes to the audio generation pipeline that I mentioned above. Regarding the default env variables to use while using the app, you can use the same config as in .env.sample, I have tested that config well.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2y2kha","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I havent benchmarked it yet but when i ran it on a rtx 3090, I was getting 1-2 seconds per line of text while making 16 parallel calls to the fastapi server.&lt;/p&gt;\\n\\n&lt;p&gt;yeah, i used to have audio related issues (repeated lines in audio/ extended audio with no spoken text but weird noises/ audio hallucinations/ infinite audio looping/ some other issues) but I was able to fix most of them by using bf16/ fp32 precision and some tweaks and retry mechanisms to handle errors.&lt;/p&gt;\\n\\n&lt;p&gt;I havent fine tuned the model, I just did some fixes to the audio generation pipeline that I mentioned above. Regarding the default env variables to use while using the app, you can use the same config as in .env.sample, I have tested that config well.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lyvsqv","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyvsqv/orpheus_tts_fastapi_server_release_v10_async_and/n2y2kha/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752433605,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2y0vw6","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Traditional_Tap1708","can_mod_post":false,"created_utc":1752433100,"send_replies":true,"parent_id":"t3_1lyvsqv","score":2,"author_fullname":"t2_aejvth7b","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Whats the latency in generating the first audio byte? I have noticed orpheus skips short phrases sometimes? Have you observed this issue before? Any tips on fine-tuning you wanna share?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2y0vw6","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Whats the latency in generating the first audio byte? I have noticed orpheus skips short phrases sometimes? Have you observed this issue before? Any tips on fine-tuning you wanna share?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyvsqv/orpheus_tts_fastapi_server_release_v10_async_and/n2y0vw6/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752433100,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lyvsqv","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n31670p","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"prakharsr","can_mod_post":false,"created_utc":1752474093,"send_replies":true,"parent_id":"t1_n2z95rx","score":1,"author_fullname":"t2_hi3epx7","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"not right now but i'm looking into if using llama cpp ggufs is viable and stable. If it works, then will have mac support too.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n31670p","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;not right now but i&amp;#39;m looking into if using llama cpp ggufs is viable and stable. If it works, then will have mac support too.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lyvsqv","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyvsqv/orpheus_tts_fastapi_server_release_v10_async_and/n31670p/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752474093,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2z95rx","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"rm-rf-rm","can_mod_post":false,"created_utc":1752446732,"send_replies":true,"parent_id":"t3_1lyvsqv","score":2,"author_fullname":"t2_xucqa0ilr","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"is it mac compatible?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2z95rx","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;is it mac compatible?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyvsqv/orpheus_tts_fastapi_server_release_v10_async_and/n2z95rx/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752446732,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lyvsqv","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n319px3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"dahara111","can_mod_post":false,"created_utc":1752476040,"send_replies":true,"parent_id":"t3_1lyvsqv","score":2,"author_fullname":"t2_6sew99etq","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Nice!\\n\\nI'm currently developing the Japanese version of Orpheus and building a demo site with TensorRT-LLM. I agree that quantization has a big impact. I haven't done enough tuning yet, but I feel that maybe FP8 is the limit of what's acceptable.\\n\\nI'll check your implementation once I've finished my work.\\n\\nThank you.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n319px3","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Nice!&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;m currently developing the Japanese version of Orpheus and building a demo site with TensorRT-LLM. I agree that quantization has a big impact. I haven&amp;#39;t done enough tuning yet, but I feel that maybe FP8 is the limit of what&amp;#39;s acceptable.&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;ll check your implementation once I&amp;#39;ve finished my work.&lt;/p&gt;\\n\\n&lt;p&gt;Thank you.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyvsqv/orpheus_tts_fastapi_server_release_v10_async_and/n319px3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752476040,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lyvsqv","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n316hta","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"prakharsr","can_mod_post":false,"created_utc":1752474259,"send_replies":true,"parent_id":"t1_n2z8iyc","score":1,"author_fullname":"t2_hi3epx7","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I didnt know much about it actually but I'll look into it and add it","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n316hta","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I didnt know much about it actually but I&amp;#39;ll look into it and add it&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lyvsqv","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyvsqv/orpheus_tts_fastapi_server_release_v10_async_and/n316hta/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752474259,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2z8iyc","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"rm-rf-rm","can_mod_post":false,"created_utc":1752446519,"send_replies":true,"parent_id":"t3_1lyvsqv","score":0,"author_fullname":"t2_xucqa0ilr","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"any reason you arent using a pyproject.toml? Then its a simple \`uv sync\`","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2z8iyc","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;any reason you arent using a pyproject.toml? Then its a simple &lt;code&gt;uv sync&lt;/code&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyvsqv/orpheus_tts_fastapi_server_release_v10_async_and/n2z8iyc/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752446519,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lyvsqv","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}}]`),n=()=>e.jsx(t,{data:a});export{n as default};
