import{j as e}from"./index-Bu7qcPAU.js";import{R as l}from"./RedditPostRenderer-CbHA7O5q.js";import"./index-BKgbfxhf.js";const a=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Hi,\\n\\nI have evaluated mistral small 3.2 for OCR tasks using ollama. The accuracy has been very satisfying while some bugs cause it to run on CPU solely with a rtx 4090 (about 5t/s). \\n\\nSo I switched to llama.cpp and obtain between 20-40t/s using the model + mmproj from unsloth. Both models are Q4\\\\_K\\\\_M. The accuracy is way worse than what I get when using ollama. How can that be? \\n\\nIs it using another vision projector, or am I doing sth wrong? I use 32k context, temp=0, all other settings are defaults. I do not explicitely use quantized kvcache or flash attention.\\n\\nAny idea how to get on par with ollamas excellent OCR accuracy?\\n\\nthanks &amp; greets","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"mistral-small-3.2 OCR accuracy way too bad with llama.cpp compared to ollama?","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1m5mms1","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.5,"author_flair_background_color":null,"subreddit_type":"public","ups":0,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_2b6dk0nt","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":0,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1753113210,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\\n\\n&lt;p&gt;I have evaluated mistral small 3.2 for OCR tasks using ollama. The accuracy has been very satisfying while some bugs cause it to run on CPU solely with a rtx 4090 (about 5t/s). &lt;/p&gt;\\n\\n&lt;p&gt;So I switched to llama.cpp and obtain between 20-40t/s using the model + mmproj from unsloth. Both models are Q4_K_M. The accuracy is way worse than what I get when using ollama. How can that be? &lt;/p&gt;\\n\\n&lt;p&gt;Is it using another vision projector, or am I doing sth wrong? I use 32k context, temp=0, all other settings are defaults. I do not explicitely use quantized kvcache or flash attention.&lt;/p&gt;\\n\\n&lt;p&gt;Any idea how to get on par with ollamas excellent OCR accuracy?&lt;/p&gt;\\n\\n&lt;p&gt;thanks &amp;amp; greets&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":true,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1m5mms1","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"caetydid","discussion_type":null,"num_comments":21,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1m5mms1/mistralsmall32_ocr_accuracy_way_too_bad_with/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1m5mms1/mistralsmall32_ocr_accuracy_way_too_bad_with/","subreddit_subscribers":502515,"created_utc":1753113210,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4dpcpn","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"caetydid","can_mod_post":false,"created_utc":1753120467,"send_replies":true,"parent_id":"t1_n4d8dz7","score":1,"author_fullname":"t2_2b6dk0nt","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"maybe the vision projector implementation in llama.cpp is just inferior? I do not know how the vision projector in ollama works, and I\\"d prefer to use it but I cannot live with 5t/s\\n\\nbtw i evaluated qwen2.5-vl and although it is supposed to be better, it is not. I am mostly processing European names/addresses, and it messes them up quite often. I guess it is heavily biased towards Chinese and English.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4dpcpn","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;maybe the vision projector implementation in llama.cpp is just inferior? I do not know how the vision projector in ollama works, and I&amp;quot;d prefer to use it but I cannot live with 5t/s&lt;/p&gt;\\n\\n&lt;p&gt;btw i evaluated qwen2.5-vl and although it is supposed to be better, it is not. I am mostly processing European names/addresses, and it messes them up quite often. I guess it is heavily biased towards Chinese and English.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m5mms1","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5mms1/mistralsmall32_ocr_accuracy_way_too_bad_with/n4dpcpn/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753120467,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4etjw6","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Gregory-Wolf","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4eggc4","score":1,"author_fullname":"t2_gethr3mh","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"My bad, I posted in the wrong place. The comment was to the OP actually :-P\\n\\nAnyways, thanks for the answer. I see that ollama moved from llama.cpp directly to ggml lib. I guess it's still gguf though, but since they don't rely on mmproj anymore, perhaps that could affect the results.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4etjw6","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;My bad, I posted in the wrong place. The comment was to the OP actually :-P&lt;/p&gt;\\n\\n&lt;p&gt;Anyways, thanks for the answer. I see that ollama moved from llama.cpp directly to ggml lib. I guess it&amp;#39;s still gguf though, but since they don&amp;#39;t rely on mmproj anymore, perhaps that could affect the results.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m5mms1","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5mms1/mistralsmall32_ocr_accuracy_way_too_bad_with/n4etjw6/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753131871,"author_flair_text":null,"treatment_tags":[],"created_utc":1753131871,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4eggc4","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"HumanAppointment5","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4dwk3b","score":1,"author_fullname":"t2_1tt3gtupry","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"To clarify: I tried the GGUF versions created by bartowski and others. Used these with llama.cpp and LM Studio. When you look at the list of files on Hugging Face, they all have a separate .mmproj file used for the vision functionality. Ollama on the other side embed this with their version of the model.\\n\\nOllama gives some more details at https://ollama.com/blog/multimodal-models\\n\\nThey say, \\"llama.cpp offers first-class support for text-only models. For multimodal systems, however, the text decoder and vision encoder are split into separate models and executed independently. Passing image embeddings from the vision model into the text model therefore demands model-specific logic in the orchestration layer that can break specific model implementations.Within Ollama, each model is fully self-contained and can expose its own projection layer, aligned with how that model was trained.\\"","edited":1753129606,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n4eggc4","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;To clarify: I tried the GGUF versions created by bartowski and others. Used these with llama.cpp and LM Studio. When you look at the list of files on Hugging Face, they all have a separate .mmproj file used for the vision functionality. Ollama on the other side embed this with their version of the model.&lt;/p&gt;\\n\\n&lt;p&gt;Ollama gives some more details at &lt;a href=\\"https://ollama.com/blog/multimodal-models\\"&gt;https://ollama.com/blog/multimodal-models&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;They say, &amp;quot;llama.cpp offers first-class support for text-only models. For multimodal systems, however, the text decoder and vision encoder are split into separate models and executed independently. Passing image embeddings from the vision model into the text model therefore demands model-specific logic in the orchestration layer that can break specific model implementations.Within Ollama, each model is fully self-contained and can expose its own projection layer, aligned with how that model was trained.&amp;quot;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m5mms1","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5mms1/mistralsmall32_ocr_accuracy_way_too_bad_with/n4eggc4/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753128130,"author_flair_text":null,"treatment_tags":[],"created_utc":1753128130,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4dwk3b","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Gregory-Wolf","can_mod_post":false,"created_utc":1753122474,"send_replies":true,"parent_id":"t1_n4d8dz7","score":1,"author_fullname":"t2_gethr3mh","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I kind of didn't understand half of what you are saying :)\\n\\n\\"Tried the GGUF versions and they were all a lot worse\\" - so you are saying \\"ollama has different versions from GGUF\\"? no, ollama uses GGUFs.  \\n\\"Ollama vision model\\" - ollama is an inference software, what vision model does it have?  \\n\\"Ollama has the vision projector embedded into the model, where the GGUF files use a separate mmproj file.\\" - ollama incapsulates llama.cpp as an inference engine (at least it did, not sure if it changed, I never used ollama because why)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4dwk3b","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I kind of didn&amp;#39;t understand half of what you are saying :)&lt;/p&gt;\\n\\n&lt;p&gt;&amp;quot;Tried the GGUF versions and they were all a lot worse&amp;quot; - so you are saying &amp;quot;ollama has different versions from GGUF&amp;quot;? no, ollama uses GGUFs.&lt;br/&gt;\\n&amp;quot;Ollama vision model&amp;quot; - ollama is an inference software, what vision model does it have?&lt;br/&gt;\\n&amp;quot;Ollama has the vision projector embedded into the model, where the GGUF files use a separate mmproj file.&amp;quot; - ollama incapsulates llama.cpp as an inference engine (at least it did, not sure if it changed, I never used ollama because why)&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m5mms1","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5mms1/mistralsmall32_ocr_accuracy_way_too_bad_with/n4dwk3b/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753122474,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4d8dz7","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"HumanAppointment5","can_mod_post":false,"created_utc":1753115834,"send_replies":true,"parent_id":"t3_1m5mms1","score":2,"author_fullname":"t2_1tt3gtupry","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I found the best OCR accuracy to date with the Ollama version of Qwen-2.5-VL. Tried the GGUF versions with llama.cpp and they were all a lot worse. \\nI've also been baffled why the Ollama [version of the] vision model is so much better.\\nOllama has the vision projector embedded into the model, where the GGUF files use a separate mmproj file.\\n\\nI especially use it for handwritten notes, and there you quickly see the accuracy differences.\\n\\nHave you tried Qwen2.5-VL?","edited":1753128868,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4d8dz7","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I found the best OCR accuracy to date with the Ollama version of Qwen-2.5-VL. Tried the GGUF versions with llama.cpp and they were all a lot worse. \\nI&amp;#39;ve also been baffled why the Ollama [version of the] vision model is so much better.\\nOllama has the vision projector embedded into the model, where the GGUF files use a separate mmproj file.&lt;/p&gt;\\n\\n&lt;p&gt;I especially use it for handwritten notes, and there you quickly see the accuracy differences.&lt;/p&gt;\\n\\n&lt;p&gt;Have you tried Qwen2.5-VL?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5mms1/mistralsmall32_ocr_accuracy_way_too_bad_with/n4d8dz7/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753115834,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m5mms1","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4dpiv6","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"caetydid","can_mod_post":false,"created_utc":1753120513,"send_replies":true,"parent_id":"t1_n4diam5","score":2,"author_fullname":"t2_2b6dk0nt","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"when i compared numbers it was always performing worse than qwen2.5-vl and mistral. If the setup is easy I might give it a try.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4dpiv6","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;when i compared numbers it was always performing worse than qwen2.5-vl and mistral. If the setup is easy I might give it a try.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m5mms1","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5mms1/mistralsmall32_ocr_accuracy_way_too_bad_with/n4dpiv6/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753120513,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n4diam5","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Cergorach","can_mod_post":false,"created_utc":1753118547,"send_replies":true,"parent_id":"t3_1m5mms1","score":2,"author_fullname":"t2_cs4w88d2l","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Try looking at olmocr or rolmocr.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4diam5","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Try looking at olmocr or rolmocr.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5mms1/mistralsmall32_ocr_accuracy_way_too_bad_with/n4diam5/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753118547,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m5mms1","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4dvj76","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Gregory-Wolf","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4doyp7","score":1,"author_fullname":"t2_gethr3mh","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Our case is kind of non-standard - the image data are of very variable quality. But let's say, it gives 50% accuracy, which is kind of OK because it reduces 50% of manual labour.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4dvj76","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Our case is kind of non-standard - the image data are of very variable quality. But let&amp;#39;s say, it gives 50% accuracy, which is kind of OK because it reduces 50% of manual labour.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m5mms1","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5mms1/mistralsmall32_ocr_accuracy_way_too_bad_with/n4dvj76/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753122185,"author_flair_text":null,"treatment_tags":[],"created_utc":1753122185,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4doyp7","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"caetydid","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4dfmk9","score":1,"author_fullname":"t2_2b6dk0nt","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"and are you satisfied with accuracy?","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n4doyp7","is_submitter":true,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;and are you satisfied with accuracy?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m5mms1","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5mms1/mistralsmall32_ocr_accuracy_way_too_bad_with/n4doyp7/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753120360,"author_flair_text":null,"treatment_tags":[],"created_utc":1753120360,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4dfmk9","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Gregory-Wolf","can_mod_post":false,"created_utc":1753117819,"send_replies":true,"parent_id":"t1_n4d7vtx","score":2,"author_fullname":"t2_gethr3mh","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"same. running in production q4\\\\_km with mmproj.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4dfmk9","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;same. running in production q4_km with mmproj.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m5mms1","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5mms1/mistralsmall32_ocr_accuracy_way_too_bad_with/n4dfmk9/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753117819,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4doh7r","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"caetydid","can_mod_post":false,"created_utc":1753120229,"send_replies":true,"parent_id":"t1_n4d7vtx","score":1,"author_fullname":"t2_2b6dk0nt","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"nah...temp=0 yielded the least inaccuracies in ollama so I applied it here as well. it does not seem to make a big difference until up to 0.2 which is the recommended default for the model.\\n\\ni dont have examples but i am processing scans of medical forms and it is just basically messing up person names, dates, addresses occasionally. but when i have  a look at the source image it is not like the text is illegible.\\n\\nmistral small with ollama never did this kind of mistakes","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4doh7r","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;nah...temp=0 yielded the least inaccuracies in ollama so I applied it here as well. it does not seem to make a big difference until up to 0.2 which is the recommended default for the model.&lt;/p&gt;\\n\\n&lt;p&gt;i dont have examples but i am processing scans of medical forms and it is just basically messing up person names, dates, addresses occasionally. but when i have  a look at the source image it is not like the text is illegible.&lt;/p&gt;\\n\\n&lt;p&gt;mistral small with ollama never did this kind of mistakes&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m5mms1","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5mms1/mistralsmall32_ocr_accuracy_way_too_bad_with/n4doh7r/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753120229,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4d7vtx","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"fp4guru","can_mod_post":false,"created_utc":1753115693,"send_replies":true,"parent_id":"t3_1m5mms1","score":1,"author_fullname":"t2_1tp8zldw5g","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Any examples? I happen to have mistral small q4 running with mmproj. Is it related to temp= 0?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4d7vtx","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Any examples? I happen to have mistral small q4 running with mmproj. Is it related to temp= 0?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5mms1/mistralsmall32_ocr_accuracy_way_too_bad_with/n4d7vtx/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753115693,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m5mms1","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4e7xv9","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"pseudonerv","can_mod_post":false,"created_utc":1753125692,"send_replies":true,"parent_id":"t3_1m5mms1","score":1,"author_fullname":"t2_eerln","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Yeah. Something is definitely off with mistral small vision adapter in llama.cpp. But I’m not a good programmer to figure out what that is","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4e7xv9","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yeah. Something is definitely off with mistral small vision adapter in llama.cpp. But I’m not a good programmer to figure out what that is&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5mms1/mistralsmall32_ocr_accuracy_way_too_bad_with/n4e7xv9/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753125692,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m5mms1","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4ee01a","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"HumanAppointment5","can_mod_post":false,"created_utc":1753127430,"send_replies":true,"parent_id":"t3_1m5mms1","score":1,"author_fullname":"t2_1tt3gtupry","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"For OCR and HTR the temperature, the Top P and Min P should all be 0. Top K must be 1. Basically it means no \\"creativity\\", ensure there is only 1 option to choose from in the list (K).","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4ee01a","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;For OCR and HTR the temperature, the Top P and Min P should all be 0. Top K must be 1. Basically it means no &amp;quot;creativity&amp;quot;, ensure there is only 1 option to choose from in the list (K).&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5mms1/mistralsmall32_ocr_accuracy_way_too_bad_with/n4ee01a/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753127430,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m5mms1","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4eff9v","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"HumanAppointment5","can_mod_post":false,"created_utc":1753127834,"send_replies":true,"parent_id":"t3_1m5mms1","score":1,"author_fullname":"t2_1tt3gtupry","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I read somewhere that you can copy (or link) the Ollama files and then use them with llama.cpp or via LM Studio. You can then test to see if you can get the same Ollama accuracy with the better 20-40 t/s.\\n\\nIf the accuracy is not the same then we know the magic is in the new Ollama engine.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4eff9v","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I read somewhere that you can copy (or link) the Ollama files and then use them with llama.cpp or via LM Studio. You can then test to see if you can get the same Ollama accuracy with the better 20-40 t/s.&lt;/p&gt;\\n\\n&lt;p&gt;If the accuracy is not the same then we know the magic is in the new Ollama engine.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5mms1/mistralsmall32_ocr_accuracy_way_too_bad_with/n4eff9v/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753127834,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m5mms1","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4eq9c8","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"HumanAppointment5","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4enrbz","score":1,"author_fullname":"t2_1tt3gtupry","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Thank you. I will have to try that. I've been using images of 1024 pixels with Qwen. If Ollama's Qwen does not have the limit of 560 pixels, and the bartowski (and others) do it could explain my results.\\nI'll crop to 560 pixels and test again (tomorrow).\\n\\nI'm also downloading the Ollama Mistral version to test it for my use case of handwritten notes.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4eq9c8","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thank you. I will have to try that. I&amp;#39;ve been using images of 1024 pixels with Qwen. If Ollama&amp;#39;s Qwen does not have the limit of 560 pixels, and the bartowski (and others) do it could explain my results.\\nI&amp;#39;ll crop to 560 pixels and test again (tomorrow).&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;m also downloading the Ollama Mistral version to test it for my use case of handwritten notes.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m5mms1","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5mms1/mistralsmall32_ocr_accuracy_way_too_bad_with/n4eq9c8/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753130927,"author_flair_text":null,"treatment_tags":[],"created_utc":1753130927,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4f8qn9","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"HumanAppointment5","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4enrbz","score":1,"author_fullname":"t2_1tt3gtupry","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"OK, I tested Mistral 3.2 bf16: Ollama vs. Bartowski vs MLX. All with the same handwritten note at 1024 pixels. The winner is very clearly the Ollama version with better accuracy.\\n\\nAlso tested them with the note resized to 560 pixels. Again the Ollama version has much better accuracy.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4f8qn9","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;OK, I tested Mistral 3.2 bf16: Ollama vs. Bartowski vs MLX. All with the same handwritten note at 1024 pixels. The winner is very clearly the Ollama version with better accuracy.&lt;/p&gt;\\n\\n&lt;p&gt;Also tested them with the note resized to 560 pixels. Again the Ollama version has much better accuracy.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m5mms1","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5mms1/mistralsmall32_ocr_accuracy_way_too_bad_with/n4f8qn9/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753136592,"author_flair_text":null,"treatment_tags":[],"created_utc":1753136592,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4enrbz","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Awwtifishal","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4elc31","score":1,"author_fullname":"t2_1d96a8k10t","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Ollama embedded mmproj weights are FP16 for the most part.\\n\\nCould you share an example that is different between the two? Or if not, can you try rescaling the picture to a maximum of 1540 pixels for mistral or 560 for qwen? maybe each application is doing something different when the picture is too big.\\n\\nI just realized ollama's qwen doesn't have the limit of 560 in the gguf. Unless it's hardcoded somewhere else, that may be an important difference. But if that's the case I can't explain the difference in the case of mistral (the limit is in both).","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n4enrbz","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Ollama embedded mmproj weights are FP16 for the most part.&lt;/p&gt;\\n\\n&lt;p&gt;Could you share an example that is different between the two? Or if not, can you try rescaling the picture to a maximum of 1540 pixels for mistral or 560 for qwen? maybe each application is doing something different when the picture is too big.&lt;/p&gt;\\n\\n&lt;p&gt;I just realized ollama&amp;#39;s qwen doesn&amp;#39;t have the limit of 560 in the gguf. Unless it&amp;#39;s hardcoded somewhere else, that may be an important difference. But if that&amp;#39;s the case I can&amp;#39;t explain the difference in the case of mistral (the limit is in both).&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m5mms1","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5mms1/mistralsmall32_ocr_accuracy_way_too_bad_with/n4enrbz/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753130217,"author_flair_text":null,"treatment_tags":[],"created_utc":1753130217,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4elc31","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"HumanAppointment5","can_mod_post":false,"created_utc":1753129527,"send_replies":true,"parent_id":"t1_n4ejdx8","score":1,"author_fullname":"t2_1tt3gtupry","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"As an example, have a look at https://huggingface.co/bartowski/Qwen_Qwen2.5-VL-72B-Instruct-GGUF/tree/main\\n\\nRegardless of the quant you select, it always uses the 16-bit quant of the mmproj. It still does not make the OCR result as accurate as that of the Ollama one.\\n\\nOllama however embeds the mmproj file into their quant.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4elc31","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;As an example, have a look at &lt;a href=\\"https://huggingface.co/bartowski/Qwen_Qwen2.5-VL-72B-Instruct-GGUF/tree/main\\"&gt;https://huggingface.co/bartowski/Qwen_Qwen2.5-VL-72B-Instruct-GGUF/tree/main&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;Regardless of the quant you select, it always uses the 16-bit quant of the mmproj. It still does not make the OCR result as accurate as that of the Ollama one.&lt;/p&gt;\\n\\n&lt;p&gt;Ollama however embeds the mmproj file into their quant.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m5mms1","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5mms1/mistralsmall32_ocr_accuracy_way_too_bad_with/n4elc31/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753129527,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4elgvz","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"caetydid","can_mod_post":false,"created_utc":1753129565,"send_replies":true,"parent_id":"t1_n4ejdx8","score":1,"author_fullname":"t2_2b6dk0nt","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"F16","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4elgvz","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;F16&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m5mms1","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5mms1/mistralsmall32_ocr_accuracy_way_too_bad_with/n4elgvz/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753129565,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4ejdx8","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Awwtifishal","can_mod_post":false,"created_utc":1753128958,"send_replies":true,"parent_id":"t3_1m5mms1","score":1,"author_fullname":"t2_1d96a8k10t","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Which mmproj quant are you using with llama.cpp?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4ejdx8","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Which mmproj quant are you using with llama.cpp?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5mms1/mistralsmall32_ocr_accuracy_way_too_bad_with/n4ejdx8/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753128958,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m5mms1","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),s=()=>e.jsx(l,{data:a});export{s as default};
