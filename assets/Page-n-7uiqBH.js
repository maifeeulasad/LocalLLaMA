import{j as e}from"./index-BOnf-UhU.js";import{R as l}from"./RedditPostRenderer-Ce39qICS.js";import"./index-CDase6VD.js";const a=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"as the title says, i have 250,000 6000 word files and i want to be able to query them. they are legal documents, what model would run flawlessly on my mac air m2. thanks","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"best local llm for 250,000 json with 6000 words each","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lqeogc","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.4,"author_flair_background_color":null,"subreddit_type":"public","ups":0,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_1nb285wcw0","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":0,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1751510910,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;as the title says, i have 250,000 6000 word files and i want to be able to query them. they are legal documents, what model would run flawlessly on my mac air m2. thanks&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":true,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1lqeogc","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"Substantial-Gear1150","discussion_type":null,"num_comments":36,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lqeogc/best_local_llm_for_250000_json_with_6000_words/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lqeogc/best_local_llm_for_250000_json_with_6000_words/","subreddit_subscribers":494198,"created_utc":1751510910,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n12b1do","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"dqUu3QlS","can_mod_post":false,"created_utc":1751512392,"send_replies":true,"parent_id":"t1_n1285wx","score":1,"author_fullname":"t2_35zxc5xb","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"For querying, would you need RAG, or just R?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n12b1do","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;For querying, would you need RAG, or just R?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lqeogc","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqeogc/best_local_llm_for_250000_json_with_6000_words/n12b1do/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751512392,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n12le6x","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ArsNeph","can_mod_post":false,"send_replies":true,"parent_id":"t1_n12k33n","score":1,"author_fullname":"t2_vt0xkv60d","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Probably Qwen embedding 0.6B and Qwen 3 14B at Q4KM and as much context as you can fit.","edited":false,"author_flair_css_class":null,"name":"t1_n12le6x","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Probably Qwen embedding 0.6B and Qwen 3 14B at Q4KM and as much context as you can fit.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lqeogc","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqeogc/best_local_llm_for_250000_json_with_6000_words/n12le6x/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751516700,"author_flair_text":null,"collapsed":false,"created_utc":1751516700,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n12k33n","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Substantial-Gear1150","can_mod_post":false,"send_replies":true,"parent_id":"t1_n12iger","score":1,"author_fullname":"t2_1nb285wcw0","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Any recommendations for embedding and LLMs for Canadian legal spanning 250,000 files with 6000 words each and running on Mac air 16gb ram and m2 chip","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n12k33n","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Any recommendations for embedding and LLMs for Canadian legal spanning 250,000 files with 6000 words each and running on Mac air 16gb ram and m2 chip&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lqeogc","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqeogc/best_local_llm_for_250000_json_with_6000_words/n12k33n/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751516125,"author_flair_text":null,"treatment_tags":[],"created_utc":1751516125,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n12iger","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ArsNeph","can_mod_post":false,"send_replies":true,"parent_id":"t1_n128kqr","score":2,"author_fullname":"t2_vt0xkv60d","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"For questions that involve the whole data set, you'd probably want something like GraphRAG, but also a more intelligent model overall. ChatGPT's accuracy is a mix of the open AI embedding model, and the intelligence of GPT 4o. In terms of embedding models, the Qwen embedding series should be equivalent or better. As for the intelligence of the model itself, smaller models are not going to compare, but you'd want the largest model you can run, with as high of a context length as you can get.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n12iger","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;For questions that involve the whole data set, you&amp;#39;d probably want something like GraphRAG, but also a more intelligent model overall. ChatGPT&amp;#39;s accuracy is a mix of the open AI embedding model, and the intelligence of GPT 4o. In terms of embedding models, the Qwen embedding series should be equivalent or better. As for the intelligence of the model itself, smaller models are not going to compare, but you&amp;#39;d want the largest model you can run, with as high of a context length as you can get.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lqeogc","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqeogc/best_local_llm_for_250000_json_with_6000_words/n12iger/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751515418,"author_flair_text":null,"treatment_tags":[],"created_utc":1751515418,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n134iv9","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Substantial-Gear1150","can_mod_post":false,"created_utc":1751526366,"send_replies":true,"parent_id":"t1_n12x187","score":1,"author_fullname":"t2_1nb285wcw0","approved_by":null,"mod_note":null,"all_awardings":[],"body":"Using a **flat FAISS index** instead of HNSW. the HNSW did not work at all","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n134iv9","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Using a &lt;strong&gt;flat FAISS index&lt;/strong&gt; instead of HNSW. the HNSW did not work at all&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lqeogc","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqeogc/best_local_llm_for_250000_json_with_6000_words/n134iv9/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751526366,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":7,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n12x187","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"lxgrf","can_mod_post":false,"send_replies":true,"parent_id":"t1_n12anuf","score":2,"author_fullname":"t2_55cvbroq","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Vector storage is very lightweight, I run it without any issue at all on my M2 mac. What did you try, specifically?","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n12x187","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Vector storage is very lightweight, I run it without any issue at all on my M2 mac. What did you try, specifically?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lqeogc","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqeogc/best_local_llm_for_250000_json_with_6000_words/n12x187/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751522300,"author_flair_text":null,"treatment_tags":[],"created_utc":1751522300,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":6,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n12anuf","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Substantial-Gear1150","can_mod_post":false,"send_replies":true,"parent_id":"t1_n12aiqi","score":-2,"author_fullname":"t2_1nb285wcw0","approved_by":null,"mod_note":null,"all_awardings":[],"body":"Vector storage is too much on a Mac , I tried . I am using just linear storage now","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n12anuf","is_submitter":true,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Vector storage is too much on a Mac , I tried . I am using just linear storage now&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lqeogc","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqeogc/best_local_llm_for_250000_json_with_6000_words/n12anuf/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751512243,"author_flair_text":null,"treatment_tags":[],"created_utc":1751512243,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-2}}],"before":null}},"user_reports":[],"saved":false,"id":"n12aiqi","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"fizzy1242","can_mod_post":false,"send_replies":true,"parent_id":"t1_n129xvu","score":1,"author_fullname":"t2_16zcsx","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"you can still use it. from my understanding, rag uses vector storage and retrieves the info in chunks. \\n\\nIf possible, it would be good to preprocess the data and if there's alot of text, split it into paragraphs","edited":false,"author_flair_css_class":null,"name":"t1_n12aiqi","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;you can still use it. from my understanding, rag uses vector storage and retrieves the info in chunks. &lt;/p&gt;\\n\\n&lt;p&gt;If possible, it would be good to preprocess the data and if there&amp;#39;s alot of text, split it into paragraphs&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lqeogc","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqeogc/best_local_llm_for_250000_json_with_6000_words/n12aiqi/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751512186,"author_flair_text":null,"collapsed":false,"created_utc":1751512186,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n12pyuv","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Conscious_Cut_6144","can_mod_post":false,"send_replies":true,"parent_id":"t1_n129xvu","score":1,"author_fullname":"t2_9hl4ymvj","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"When setting up RAG you define how many segments, and how large each segment is.  \\nThen those get returned to the LLM.\\n\\nSo if your data could come from 100 different places in your files, you tell your Rag to return the top 100 segments from your files.","edited":false,"author_flair_css_class":null,"name":"t1_n12pyuv","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;When setting up RAG you define how many segments, and how large each segment is.&lt;br/&gt;\\nThen those get returned to the LLM.&lt;/p&gt;\\n\\n&lt;p&gt;So if your data could come from 100 different places in your files, you tell your Rag to return the top 100 segments from your files.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lqeogc","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqeogc/best_local_llm_for_250000_json_with_6000_words/n12pyuv/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751518789,"author_flair_text":null,"collapsed":false,"created_utc":1751518789,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n129xvu","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Substantial-Gear1150","can_mod_post":false,"send_replies":true,"parent_id":"t1_n12902f","score":1,"author_fullname":"t2_1nb285wcw0","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"What if some fields I want from data are unstructured and could be anywhere in a file ?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n129xvu","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;What if some fields I want from data are unstructured and could be anywhere in a file ?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lqeogc","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqeogc/best_local_llm_for_250000_json_with_6000_words/n129xvu/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751511956,"author_flair_text":null,"treatment_tags":[],"created_utc":1751511956,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n12902f","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"fizzy1242","can_mod_post":false,"send_replies":true,"parent_id":"t1_n128kqr","score":2,"author_fullname":"t2_16zcsx","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"rag is what you want for this sort of thing, so you'll get responses based on the factual data. Finetuning only nudges model weights but it doesn't actually add that data.\\n\\nthe model you can run depends on the amount of memory your mac has","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n12902f","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;rag is what you want for this sort of thing, so you&amp;#39;ll get responses based on the factual data. Finetuning only nudges model weights but it doesn&amp;#39;t actually add that data.&lt;/p&gt;\\n\\n&lt;p&gt;the model you can run depends on the amount of memory your mac has&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lqeogc","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqeogc/best_local_llm_for_250000_json_with_6000_words/n12902f/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751511583,"author_flair_text":null,"treatment_tags":[],"created_utc":1751511583,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n12anbc","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"TSG-AYAN","can_mod_post":false,"send_replies":true,"parent_id":"t1_n128kqr","score":1,"author_fullname":"t2_5ml0rsi7","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I am not really sure, you need someone with a lot more knowledge about this stuff","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n12anbc","is_submitter":false,"collapsed":false,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I am not really sure, you need someone with a lot more knowledge about this stuff&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lqeogc","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqeogc/best_local_llm_for_250000_json_with_6000_words/n12anbc/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751512237,"author_flair_text":"llama.cpp","treatment_tags":[],"created_utc":1751512237,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n12put8","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Substantial-Gear1150","can_mod_post":false,"send_replies":true,"parent_id":"t1_n12pozl","score":1,"author_fullname":"t2_1nb285wcw0","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Yea what rag strategies do you recommend","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n12put8","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yea what rag strategies do you recommend&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lqeogc","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqeogc/best_local_llm_for_250000_json_with_6000_words/n12put8/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751518736,"author_flair_text":null,"treatment_tags":[],"created_utc":1751518736,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n12pozl","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Conscious_Cut_6144","can_mod_post":false,"send_replies":true,"parent_id":"t1_n128kqr","score":0,"author_fullname":"t2_9hl4ymvj","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"RAG isn't optional, no LLM is going to hold 1.5 Billion tokens, not even close.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n12pozl","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;RAG isn&amp;#39;t optional, no LLM is going to hold 1.5 Billion tokens, not even close.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lqeogc","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqeogc/best_local_llm_for_250000_json_with_6000_words/n12pozl/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751518660,"author_flair_text":null,"treatment_tags":[],"created_utc":1751518660,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}},"user_reports":[],"saved":false,"id":"n128kqr","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Substantial-Gear1150","can_mod_post":false,"created_utc":1751511415,"send_replies":true,"parent_id":"t1_n1285wx","score":1,"author_fullname":"t2_1nb285wcw0","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"its not that accurate with a RAG if i am asking quetsions that are more boroad and onvolve the whoel dataset. if i am asking specific lookup style quetsions then ya RAG is descent. like chatgpt is so accuarte and perfect. how do i get that for this dataset","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n128kqr","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;its not that accurate with a RAG if i am asking quetsions that are more boroad and onvolve the whoel dataset. if i am asking specific lookup style quetsions then ya RAG is descent. like chatgpt is so accuarte and perfect. how do i get that for this dataset&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lqeogc","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqeogc/best_local_llm_for_250000_json_with_6000_words/n128kqr/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751511415,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1285wx","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"TSG-AYAN","can_mod_post":false,"created_utc":1751511257,"send_replies":true,"parent_id":"t3_1lqeogc","score":16,"author_fullname":"t2_5ml0rsi7","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"That's a task for RAG, not a model by itself.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1285wx","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;That&amp;#39;s a task for RAG, not a model by itself.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqeogc/best_local_llm_for_250000_json_with_6000_words/n1285wx/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751511257,"author_flair_text":"llama.cpp","treatment_tags":[],"link_id":"t3_1lqeogc","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":16}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n12epaq","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Substantial-Gear1150","can_mod_post":false,"created_utc":1751513865,"send_replies":true,"parent_id":"t1_n12cjir","score":1,"author_fullname":"t2_1nb285wcw0","approved_by":null,"mod_note":null,"all_awardings":[],"body":"16 gb ram and mistral 7b qb , mistral 7b q4 and now qwen 7b q4","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n12epaq","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;16 gb ram and mistral 7b qb , mistral 7b q4 and now qwen 7b q4&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lqeogc","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqeogc/best_local_llm_for_250000_json_with_6000_words/n12epaq/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751513865,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":7,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n12cjir","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Spiritual-Ruin8007","can_mod_post":false,"send_replies":true,"parent_id":"t1_n12c7v6","score":1,"author_fullname":"t2_4jpvzaui","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"What LLM and how much RAM do you have?","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n12cjir","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;What LLM and how much RAM do you have?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lqeogc","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqeogc/best_local_llm_for_250000_json_with_6000_words/n12cjir/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751512989,"author_flair_text":null,"treatment_tags":[],"created_utc":1751512989,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":6,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n12c7v6","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Substantial-Gear1150","can_mod_post":false,"send_replies":true,"parent_id":"t1_n12bzec","score":1,"author_fullname":"t2_1nb285wcw0","approved_by":null,"mod_note":null,"all_awardings":[],"body":"Each question I ask the llm takes like 30 min for a reply and it’s only doing 200 files rn. I have played with context size and different models and chunk sizes . Some are faster but none are like 10 sec . And the faster ones are wildly inaccurate with bad overall context","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n12c7v6","is_submitter":true,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Each question I ask the llm takes like 30 min for a reply and it’s only doing 200 files rn. I have played with context size and different models and chunk sizes . Some are faster but none are like 10 sec . And the faster ones are wildly inaccurate with bad overall context&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lqeogc","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqeogc/best_local_llm_for_250000_json_with_6000_words/n12c7v6/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751512858,"author_flair_text":null,"treatment_tags":[],"created_utc":1751512858,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n12bzec","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Spiritual-Ruin8007","can_mod_post":false,"send_replies":true,"parent_id":"t1_n12b0nq","score":1,"author_fullname":"t2_4jpvzaui","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Sounds like a pattern matching problem. You can ask the LLM a question have it parse your question's topics/keywords out then do regex across your entire corpus of files and it will be able to tell you how many times it appears. This is assuming that you set up an LLM agent with function calling.","edited":false,"author_flair_css_class":null,"name":"t1_n12bzec","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Sounds like a pattern matching problem. You can ask the LLM a question have it parse your question&amp;#39;s topics/keywords out then do regex across your entire corpus of files and it will be able to tell you how many times it appears. This is assuming that you set up an LLM agent with function calling.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lqeogc","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqeogc/best_local_llm_for_250000_json_with_6000_words/n12bzec/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751512764,"author_flair_text":null,"collapsed":false,"created_utc":1751512764,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n12b0nq","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Substantial-Gear1150","can_mod_post":false,"send_replies":true,"parent_id":"t1_n12apxk","score":2,"author_fullname":"t2_1nb285wcw0","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Yea I have already included that . However if I ask a question like how many times does this appear , and it’s from even 500 files let alone 250,000, it can’t do it and only gives me a very minimal number","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n12b0nq","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yea I have already included that . However if I ask a question like how many times does this appear , and it’s from even 500 files let alone 250,000, it can’t do it and only gives me a very minimal number&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lqeogc","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqeogc/best_local_llm_for_250000_json_with_6000_words/n12b0nq/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751512384,"author_flair_text":null,"treatment_tags":[],"created_utc":1751512384,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n12apxk","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Spiritual-Ruin8007","can_mod_post":false,"send_replies":true,"parent_id":"t1_n12abta","score":1,"author_fullname":"t2_4jpvzaui","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Like the name or title of the file, author, date, number of tokens of each 6000 word document. All these are examples of metadata.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n12apxk","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Like the name or title of the file, author, date, number of tokens of each 6000 word document. All these are examples of metadata.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lqeogc","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqeogc/best_local_llm_for_250000_json_with_6000_words/n12apxk/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751512267,"author_flair_text":null,"treatment_tags":[],"created_utc":1751512267,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n12abta","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Substantial-Gear1150","can_mod_post":false,"created_utc":1751512109,"send_replies":true,"parent_id":"t1_n12a3x2","score":1,"author_fullname":"t2_1nb285wcw0","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"What do you mean add metadata to the chunks ? I thought the chunks would be all the data","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n12abta","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;What do you mean add metadata to the chunks ? I thought the chunks would be all the data&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lqeogc","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqeogc/best_local_llm_for_250000_json_with_6000_words/n12abta/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751512109,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n12a3x2","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Spiritual-Ruin8007","can_mod_post":false,"created_utc":1751512022,"send_replies":true,"parent_id":"t3_1lqeogc","score":3,"author_fullname":"t2_4jpvzaui","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"You didn't specify how much RAM your mac air m2 has so I am gonna make the assumption you have anywhere between 16gb and 32gb. I'd recommend:\\n\\n\\n\\nApriel-Nemotron-15b-Thinker\\n\\nMistral-Small-3.2-24B-Instruct-2506\\n\\nQwen3-30B-A3B\\n\\ngemma-3-12b-it\\n\\n  \\nRAG is simple. You want your documents to be split on sentence boundaries and to have chunk sizes between 512-1024 tokens generally for a blend of chunk precision and context. You should add metadata to the chunks so that the LLM can retrieve the whole document (a 6000 word document can fit into LLM context easily) as needed for more context to answer your questions.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n12a3x2","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You didn&amp;#39;t specify how much RAM your mac air m2 has so I am gonna make the assumption you have anywhere between 16gb and 32gb. I&amp;#39;d recommend:&lt;/p&gt;\\n\\n&lt;p&gt;Apriel-Nemotron-15b-Thinker&lt;/p&gt;\\n\\n&lt;p&gt;Mistral-Small-3.2-24B-Instruct-2506&lt;/p&gt;\\n\\n&lt;p&gt;Qwen3-30B-A3B&lt;/p&gt;\\n\\n&lt;p&gt;gemma-3-12b-it&lt;/p&gt;\\n\\n&lt;p&gt;RAG is simple. You want your documents to be split on sentence boundaries and to have chunk sizes between 512-1024 tokens generally for a blend of chunk precision and context. You should add metadata to the chunks so that the LLM can retrieve the whole document (a 6000 word document can fit into LLM context easily) as needed for more context to answer your questions.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqeogc/best_local_llm_for_250000_json_with_6000_words/n12a3x2/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751512022,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lqeogc","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n13d598","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"arqn22","can_mod_post":false,"created_utc":1751531370,"send_replies":true,"parent_id":"t1_n12qkwv","score":2,"author_fullname":"t2_9qymwgnh","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Also, most RAG setups have a parameter for the max results that a query can return.  It might be as simple as increasing the value of this param since it could be the true 'bottleneck' you are experiencing","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n13d598","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Also, most RAG setups have a parameter for the max results that a query can return.  It might be as simple as increasing the value of this param since it could be the true &amp;#39;bottleneck&amp;#39; you are experiencing&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lqeogc","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqeogc/best_local_llm_for_250000_json_with_6000_words/n13d598/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751531370,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n12qkwv","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"arqn22","can_mod_post":false,"created_utc":1751519083,"send_replies":true,"parent_id":"t3_1lqeogc","score":3,"author_fullname":"t2_9qymwgnh","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Sorry this isn't better organized, but here's a brain dump of ideas / questions to consider:\\n\\nYou should get much more specific about your current rag tech stack because it sounds like you might have inefficiencies there.  \\nI see that you also mention you are using linear storage, not vector.  Can you describe your setup if that's still the case?  Are you just using some sort of in memory data structure like a list/map/dictionary that contains the content of each of your json files as an in memory object?\\n\\nIf you were using RAG, What RAG solution were you using that didn't work?  \\n\\nFor instance, If your RAG is currently stored in a flat file instead of a database, that could also be part of your problem.\\n\\nYou might need more hardware (buy a new pc, or use a cloud hosting provider).\\n\\nThere are a number of vector dbs you can run/store in the cloud (pinecone, weaviate, cromadb, etc../even postgres has a vector option via some sort of extension / option at this point).  Maybe getting your data into a cloud hosted vector db would allow your local LLM to call it for what it needs?\\n\\n\\n\\nAlso, are you searching for keywords or concepts?  Because if you just need to find specific keywords, regex or standard search tools will be much better suited to your use case.  An LLM can write some python to just traverse your folders/file structure or your database or 'linear storage' to find and count the instances.\\n\\nThe benefit of RAG / Vector DBs is sementic search based on concepts when a keyword or list of keywords isn't going to find every instance.  If you can narrow it down to a list of jlkeywords for each search, then non-llm based regex / search will be much more reliable, straightforward, and manageable on your hardware.\\n\\nIt's possible that really just don't have a powerful enough machine for your use case though.  It could be that the model plus the large context window you're generating + other programs in your OS are chewing up all the available RAM well before finding all the answers you need.\\n\\nIn this case you want the smallest chunk sizes you can reasonably set (to limit the info you add to the context window for each match), and to make sure that you get back citations with each chunk telling you which doc and line number each chunk is coming from so you can manually open the docs and review each of them in context.  You'd probably want a low overlap value for your chunks as well to reduce returning the same content in multiple chunks.\\n\\nIf you really need RAG and more horsepower, I think the big players (openAI at least) have solutions to upload your files to a RAG they host, and use their models to interact with it all on their hardware.  Obviously the privacy concerns with this are considerable, so it depends on your content.  But if this is  generic non-propriatery case law info without PII in it, it might be a great solution.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n12qkwv","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Sorry this isn&amp;#39;t better organized, but here&amp;#39;s a brain dump of ideas / questions to consider:&lt;/p&gt;\\n\\n&lt;p&gt;You should get much more specific about your current rag tech stack because it sounds like you might have inefficiencies there.&lt;br/&gt;\\nI see that you also mention you are using linear storage, not vector.  Can you describe your setup if that&amp;#39;s still the case?  Are you just using some sort of in memory data structure like a list/map/dictionary that contains the content of each of your json files as an in memory object?&lt;/p&gt;\\n\\n&lt;p&gt;If you were using RAG, What RAG solution were you using that didn&amp;#39;t work?  &lt;/p&gt;\\n\\n&lt;p&gt;For instance, If your RAG is currently stored in a flat file instead of a database, that could also be part of your problem.&lt;/p&gt;\\n\\n&lt;p&gt;You might need more hardware (buy a new pc, or use a cloud hosting provider).&lt;/p&gt;\\n\\n&lt;p&gt;There are a number of vector dbs you can run/store in the cloud (pinecone, weaviate, cromadb, etc../even postgres has a vector option via some sort of extension / option at this point).  Maybe getting your data into a cloud hosted vector db would allow your local LLM to call it for what it needs?&lt;/p&gt;\\n\\n&lt;p&gt;Also, are you searching for keywords or concepts?  Because if you just need to find specific keywords, regex or standard search tools will be much better suited to your use case.  An LLM can write some python to just traverse your folders/file structure or your database or &amp;#39;linear storage&amp;#39; to find and count the instances.&lt;/p&gt;\\n\\n&lt;p&gt;The benefit of RAG / Vector DBs is sementic search based on concepts when a keyword or list of keywords isn&amp;#39;t going to find every instance.  If you can narrow it down to a list of jlkeywords for each search, then non-llm based regex / search will be much more reliable, straightforward, and manageable on your hardware.&lt;/p&gt;\\n\\n&lt;p&gt;It&amp;#39;s possible that really just don&amp;#39;t have a powerful enough machine for your use case though.  It could be that the model plus the large context window you&amp;#39;re generating + other programs in your OS are chewing up all the available RAM well before finding all the answers you need.&lt;/p&gt;\\n\\n&lt;p&gt;In this case you want the smallest chunk sizes you can reasonably set (to limit the info you add to the context window for each match), and to make sure that you get back citations with each chunk telling you which doc and line number each chunk is coming from so you can manually open the docs and review each of them in context.  You&amp;#39;d probably want a low overlap value for your chunks as well to reduce returning the same content in multiple chunks.&lt;/p&gt;\\n\\n&lt;p&gt;If you really need RAG and more horsepower, I think the big players (openAI at least) have solutions to upload your files to a RAG they host, and use their models to interact with it all on their hardware.  Obviously the privacy concerns with this are considerable, so it depends on your content.  But if this is  generic non-propriatery case law info without PII in it, it might be a great solution.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqeogc/best_local_llm_for_250000_json_with_6000_words/n12qkwv/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751519083,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lqeogc","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n14d0xt","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"false79","can_mod_post":false,"send_replies":true,"parent_id":"t1_n12jt3q","score":1,"author_fullname":"t2_wn888","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Structured or not, it's not a requirement to use regEx","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n14d0xt","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Structured or not, it&amp;#39;s not a requirement to use regEx&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lqeogc","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqeogc/best_local_llm_for_250000_json_with_6000_words/n14d0xt/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751548137,"author_flair_text":null,"treatment_tags":[],"created_utc":1751548137,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n12jt3q","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Substantial-Gear1150","can_mod_post":false,"created_utc":1751516003,"send_replies":true,"parent_id":"t1_n12gsor","score":1,"author_fullname":"t2_1nb285wcw0","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"It’s not structured","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n12jt3q","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It’s not structured&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lqeogc","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqeogc/best_local_llm_for_250000_json_with_6000_words/n12jt3q/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751516003,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n12gsor","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"false79","can_mod_post":false,"created_utc":1751514718,"send_replies":true,"parent_id":"t3_1lqeogc","score":2,"author_fullname":"t2_wn888","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"You would be better off with a batch script that finds what you want using regEx. Parallelize it to maximize all cores.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n12gsor","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You would be better off with a batch script that finds what you want using regEx. Parallelize it to maximize all cores.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqeogc/best_local_llm_for_250000_json_with_6000_words/n12gsor/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751514718,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lqeogc","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n15br3d","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Substantial-Gear1150","can_mod_post":false,"created_utc":1751558477,"send_replies":true,"parent_id":"t1_n13d10z","score":1,"author_fullname":"t2_1nb285wcw0","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"no joke , i assure u, catch me at the top","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n15br3d","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;no joke , i assure u, catch me at the top&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lqeogc","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqeogc/best_local_llm_for_250000_json_with_6000_words/n15br3d/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751558477,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n13d10z","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"One-Employment3759","can_mod_post":false,"created_utc":1751531301,"send_replies":true,"parent_id":"t3_1lqeogc","score":2,"author_fullname":"t2_1f6wnmakwr","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Haha, good joke","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n13d10z","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Haha, good joke&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqeogc/best_local_llm_for_250000_json_with_6000_words/n13d10z/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751531301,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lqeogc","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n15ft8t","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Substantial-Gear1150","can_mod_post":false,"created_utc":1751559640,"send_replies":true,"parent_id":"t3_1lqeogc","score":1,"author_fullname":"t2_1nb285wcw0","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"i built an inverted index for questions related to how many time doe ssomethign appear and its awesome and quick but it goes to the llm, its so slow. whats the context you think i should set? i set 4000 and its fast and inaccurate but 50,000 is slow af","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n15ft8t","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;i built an inverted index for questions related to how many time doe ssomethign appear and its awesome and quick but it goes to the llm, its so slow. whats the context you think i should set? i set 4000 and its fast and inaccurate but 50,000 is slow af&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqeogc/best_local_llm_for_250000_json_with_6000_words/n15ft8t/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751559640,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lqeogc","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n17nbn5","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"TerminatedProccess","can_mod_post":false,"created_utc":1751583388,"send_replies":true,"parent_id":"t3_1lqeogc","score":1,"author_fullname":"t2_45fzg6q7","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I would start with github. There's a number of AI RAG projects there you could research their popularity and comments, issues, so on.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n17nbn5","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I would start with github. There&amp;#39;s a number of AI RAG projects there you could research their popularity and comments, issues, so on.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqeogc/best_local_llm_for_250000_json_with_6000_words/n17nbn5/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751583388,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lqeogc","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n12dbua","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Substantial-Gear1150","can_mod_post":false,"created_utc":1751513303,"send_replies":true,"parent_id":"t3_1lqeogc","score":0,"author_fullname":"t2_1nb285wcw0","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"16 gb ram and mistral 7b qb , mistral 7b q4 and now qwen 7b q4","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n12dbua","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;16 gb ram and mistral 7b qb , mistral 7b q4 and now qwen 7b q4&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqeogc/best_local_llm_for_250000_json_with_6000_words/n12dbua/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751513303,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lqeogc","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}}]`),n=()=>e.jsx(l,{data:a});export{n as default};
