import{j as t}from"./index-DOAmItP2.js";import{R as e}from"./RedditPostRenderer-KKgzpPpv.js";import"./index-YSfz60vQ.js";const n=JSON.parse('[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Yesterday, we discussed *what* [positional embeddings ](https://www.ideaweaver.ai/blog/day6.html)are and *why* they’re essential in Transformer models. Today, let’s jump into the code and see exactly how they\'re implemented.\\n\\nThe reference implementation comes from an open-source GPT-style model I’ve been experimenting with  [Tiny Children Stories 30M](https://github.com/ideaweaver-ai/Tiny-Children-Stories-30M-model). It\'s designed to generate short children\'s stories and offers a clean, minimal setup perfect for understanding the internals.\\n\\n# Quick Recap: Why Transformers Need Positional Embeddings\\n\\nTransformer models process all tokens in parallel (unlike RNNs), so they don’t naturally understand word order. For example:\\n\\n    \\"The cat sat on the mat\\"\\n    \\"The mat sat on the cat\\"\\n    \\n\\nTo a transformer without positional embeddings, those look identical, same tokens, shuffled order, same representation. That’s a problem.\\n\\n# What Are Positional Embeddings?\\n\\nThey’re additional vectors that encode the *position* of each token in the sequence. These are added to token embeddings so that the model knows what the token is and where it is located.\\n\\n# [Step-by-Step Code Walkthrough](https://github.com/ideaweaver-ai/Tiny-Children-Stories-30M-model/blob/main/src/model/gpt.py) \\n\\n# 1. Model Config\\n\\n    u/dataclass\\n    class GPTConfig:\\n        vocab_size: int = 50257\\n        block_size: int = 1024\\n        n_layer: int = 6\\n        n_head: int = 8\\n        n_embd: int = 512\\n        dropout: float = 0.1\\n        bias: bool = True\\n    \\n\\n`block_size` defines the maximum sequence length and thus the number of positional embeddings needed.\\n\\n# 2. Defining the Embedding Layers\\n\\n    self.transformer = nn.ModuleDict(dict(\\n        wte=nn.Embedding(config.vocab_size, config.n_embd),  # token embeddings\\n        wpe=nn.Embedding(config.block_size, config.n_embd),  # positional embeddings\\n        ...\\n    ))\\n    \\n\\nBoth embeddings are of shape `(sequence_length, embedding_dim)`, so they can be added together.\\n\\n# 3. Forward Pass\\n\\n    pos = torch.arange(0, t, dtype=torch.long, device=device)\\n    tok_emb = self.transformer.wte(idx)\\n    pos_emb = self.transformer.wpe(pos)\\n    x = self.transformer.drop(tok_emb + pos_emb)\\n    \\n\\nThis does:\\n\\n* Generate position indices `[0, 1, 2, ..., t-1]`\\n* Look up token and position embeddings\\n* Add them\\n* Apply dropout\\n\\n# Example\\n\\nInput: `\\"The cat sat\\"`  \\nToken IDs: `[464, 2368, 3290]`\\n\\n|Token|Token Embedding|Positional Embedding|Combined Embedding|\\n|:-|:-|:-|:-|\\n|The|`[0.1, -0.3, …]`|`[0.0, 0.1, …]`|`[0.1, -0.2, …]`|\\n|cat|`[0.5, 0.2, …]`|`[0.1, 0.0, …]`|`[0.6, 0.2, …]`|\\n|sat|`[-0.2, 0.8, …]`|`[0.2, -0.1, …]`|`[0.0, 0.7, …]`|\\n\\nNow the model knows both the identity and the order of the tokens.\\n\\n# Now the question is why This Matters\\n\\nBy adding token + position, the model learns:\\n\\n* **Semantics** (what the word is)\\n* **Context** (where the word is)\\n\\nThis is crucial in generation tasks like storytelling, where position changes meaning.\\n\\n# Limitations\\n\\n* **Fixed length**: Can’t handle sequences longer than `block_size`.\\n* **No relative awareness**: Doesn\'t know how far two tokens are apart.\\n* **Sparse training**: If you never train on long sequences, performance drops.\\n\\n# Alternatives\\n\\n# Sinusoidal Positional Embeddings\\n\\n    def get_sinusoidal_embeddings(seq_len, embed_dim):\\n        pos = torch.arange(seq_len).unsqueeze(1)\\n        div_term = torch.exp(torch.arange(0, embed_dim, 2) * -(math.log(10000.0) / embed_dim))\\n        pe = torch.zeros(seq_len, embed_dim)\\n        pe[:, 0::2] = torch.sin(pos * div_term)\\n        pe[:, 1::2] = torch.cos(pos * div_term)\\n        return pe\\n    \\n\\n* Infinite length\\n* No learned parameters\\n\\n# Relative Positional Embeddings\\n\\nRather than saying \\"this is position 5\\", you tell the model \\"this token is 3 positions to the left of that one.\\"\\n\\nGreat for:\\n\\n* Reasoning\\n* Long document understanding\\n* Question answering\\n\\n#  Tips\\n\\n* Don’t overextend `block_size,` it increases memory consumption fast.\\n* Ensure your training data has diverse sequence lengths.\\n* For long inputs, check out RoPE or relative embeddings.\\n\\n# Final Thoughts\\n\\nPositional embeddings are the quiet workhorses of transformer models. Just by combining two vectors (token + position), we enable the model to process ordered text meaningfully.\\n\\nWithout this, a model wouldn\'t know if “The End” belongs at the start or the finish of your story.\\n\\n**Coming Up Next:**  \\nTomorrow we’ll dive into Rotary Positional Embeddings (RoPE), a more scalable and elegant solution to position encoding.\\n\\nIf you\'re following this series, feel free to share or [connect](https://www.linkedin.com/in/prashant-lakhera-696119b/).","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Day 7/50: Building a Small Language Model from Scratch – Coding Positional Embeddings","link_flair_richtext":[{"e":"text","t":"Discussion"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lp5pt0","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.92,"author_flair_background_color":null,"subreddit_type":"public","ups":28,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_8ht7a116","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Discussion","can_mod_post":false,"score":28,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1751386194,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Yesterday, we discussed &lt;em&gt;what&lt;/em&gt; &lt;a href=\\"https://www.ideaweaver.ai/blog/day6.html\\"&gt;positional embeddings &lt;/a&gt;are and &lt;em&gt;why&lt;/em&gt; they’re essential in Transformer models. Today, let’s jump into the code and see exactly how they&amp;#39;re implemented.&lt;/p&gt;\\n\\n&lt;p&gt;The reference implementation comes from an open-source GPT-style model I’ve been experimenting with  &lt;a href=\\"https://github.com/ideaweaver-ai/Tiny-Children-Stories-30M-model\\"&gt;Tiny Children Stories 30M&lt;/a&gt;. It&amp;#39;s designed to generate short children&amp;#39;s stories and offers a clean, minimal setup perfect for understanding the internals.&lt;/p&gt;\\n\\n&lt;h1&gt;Quick Recap: Why Transformers Need Positional Embeddings&lt;/h1&gt;\\n\\n&lt;p&gt;Transformer models process all tokens in parallel (unlike RNNs), so they don’t naturally understand word order. For example:&lt;/p&gt;\\n\\n&lt;pre&gt;&lt;code&gt;&amp;quot;The cat sat on the mat&amp;quot;\\n&amp;quot;The mat sat on the cat&amp;quot;\\n&lt;/code&gt;&lt;/pre&gt;\\n\\n&lt;p&gt;To a transformer without positional embeddings, those look identical, same tokens, shuffled order, same representation. That’s a problem.&lt;/p&gt;\\n\\n&lt;h1&gt;What Are Positional Embeddings?&lt;/h1&gt;\\n\\n&lt;p&gt;They’re additional vectors that encode the &lt;em&gt;position&lt;/em&gt; of each token in the sequence. These are added to token embeddings so that the model knows what the token is and where it is located.&lt;/p&gt;\\n\\n&lt;h1&gt;&lt;a href=\\"https://github.com/ideaweaver-ai/Tiny-Children-Stories-30M-model/blob/main/src/model/gpt.py\\"&gt;Step-by-Step Code Walkthrough&lt;/a&gt;&lt;/h1&gt;\\n\\n&lt;h1&gt;1. Model Config&lt;/h1&gt;\\n\\n&lt;pre&gt;&lt;code&gt;u/dataclass\\nclass GPTConfig:\\n    vocab_size: int = 50257\\n    block_size: int = 1024\\n    n_layer: int = 6\\n    n_head: int = 8\\n    n_embd: int = 512\\n    dropout: float = 0.1\\n    bias: bool = True\\n&lt;/code&gt;&lt;/pre&gt;\\n\\n&lt;p&gt;&lt;code&gt;block_size&lt;/code&gt; defines the maximum sequence length and thus the number of positional embeddings needed.&lt;/p&gt;\\n\\n&lt;h1&gt;2. Defining the Embedding Layers&lt;/h1&gt;\\n\\n&lt;pre&gt;&lt;code&gt;self.transformer = nn.ModuleDict(dict(\\n    wte=nn.Embedding(config.vocab_size, config.n_embd),  # token embeddings\\n    wpe=nn.Embedding(config.block_size, config.n_embd),  # positional embeddings\\n    ...\\n))\\n&lt;/code&gt;&lt;/pre&gt;\\n\\n&lt;p&gt;Both embeddings are of shape &lt;code&gt;(sequence_length, embedding_dim)&lt;/code&gt;, so they can be added together.&lt;/p&gt;\\n\\n&lt;h1&gt;3. Forward Pass&lt;/h1&gt;\\n\\n&lt;pre&gt;&lt;code&gt;pos = torch.arange(0, t, dtype=torch.long, device=device)\\ntok_emb = self.transformer.wte(idx)\\npos_emb = self.transformer.wpe(pos)\\nx = self.transformer.drop(tok_emb + pos_emb)\\n&lt;/code&gt;&lt;/pre&gt;\\n\\n&lt;p&gt;This does:&lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;Generate position indices &lt;code&gt;[0, 1, 2, ..., t-1]&lt;/code&gt;&lt;/li&gt;\\n&lt;li&gt;Look up token and position embeddings&lt;/li&gt;\\n&lt;li&gt;Add them&lt;/li&gt;\\n&lt;li&gt;Apply dropout&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;h1&gt;Example&lt;/h1&gt;\\n\\n&lt;p&gt;Input: &lt;code&gt;&amp;quot;The cat sat&amp;quot;&lt;/code&gt;&lt;br/&gt;\\nToken IDs: &lt;code&gt;[464, 2368, 3290]&lt;/code&gt;&lt;/p&gt;\\n\\n&lt;table&gt;&lt;thead&gt;\\n&lt;tr&gt;\\n&lt;th align=\\"left\\"&gt;Token&lt;/th&gt;\\n&lt;th align=\\"left\\"&gt;Token Embedding&lt;/th&gt;\\n&lt;th align=\\"left\\"&gt;Positional Embedding&lt;/th&gt;\\n&lt;th align=\\"left\\"&gt;Combined Embedding&lt;/th&gt;\\n&lt;/tr&gt;\\n&lt;/thead&gt;&lt;tbody&gt;\\n&lt;tr&gt;\\n&lt;td align=\\"left\\"&gt;The&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;&lt;code&gt;[0.1, -0.3, …]&lt;/code&gt;&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;&lt;code&gt;[0.0, 0.1, …]&lt;/code&gt;&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;&lt;code&gt;[0.1, -0.2, …]&lt;/code&gt;&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;tr&gt;\\n&lt;td align=\\"left\\"&gt;cat&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;&lt;code&gt;[0.5, 0.2, …]&lt;/code&gt;&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;&lt;code&gt;[0.1, 0.0, …]&lt;/code&gt;&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;&lt;code&gt;[0.6, 0.2, …]&lt;/code&gt;&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;tr&gt;\\n&lt;td align=\\"left\\"&gt;sat&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;&lt;code&gt;[-0.2, 0.8, …]&lt;/code&gt;&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;&lt;code&gt;[0.2, -0.1, …]&lt;/code&gt;&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;&lt;code&gt;[0.0, 0.7, …]&lt;/code&gt;&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;/tbody&gt;&lt;/table&gt;\\n\\n&lt;p&gt;Now the model knows both the identity and the order of the tokens.&lt;/p&gt;\\n\\n&lt;h1&gt;Now the question is why This Matters&lt;/h1&gt;\\n\\n&lt;p&gt;By adding token + position, the model learns:&lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;&lt;strong&gt;Semantics&lt;/strong&gt; (what the word is)&lt;/li&gt;\\n&lt;li&gt;&lt;strong&gt;Context&lt;/strong&gt; (where the word is)&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;p&gt;This is crucial in generation tasks like storytelling, where position changes meaning.&lt;/p&gt;\\n\\n&lt;h1&gt;Limitations&lt;/h1&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;&lt;strong&gt;Fixed length&lt;/strong&gt;: Can’t handle sequences longer than &lt;code&gt;block_size&lt;/code&gt;.&lt;/li&gt;\\n&lt;li&gt;&lt;strong&gt;No relative awareness&lt;/strong&gt;: Doesn&amp;#39;t know how far two tokens are apart.&lt;/li&gt;\\n&lt;li&gt;&lt;strong&gt;Sparse training&lt;/strong&gt;: If you never train on long sequences, performance drops.&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;h1&gt;Alternatives&lt;/h1&gt;\\n\\n&lt;h1&gt;Sinusoidal Positional Embeddings&lt;/h1&gt;\\n\\n&lt;pre&gt;&lt;code&gt;def get_sinusoidal_embeddings(seq_len, embed_dim):\\n    pos = torch.arange(seq_len).unsqueeze(1)\\n    div_term = torch.exp(torch.arange(0, embed_dim, 2) * -(math.log(10000.0) / embed_dim))\\n    pe = torch.zeros(seq_len, embed_dim)\\n    pe[:, 0::2] = torch.sin(pos * div_term)\\n    pe[:, 1::2] = torch.cos(pos * div_term)\\n    return pe\\n&lt;/code&gt;&lt;/pre&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;Infinite length&lt;/li&gt;\\n&lt;li&gt;No learned parameters&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;h1&gt;Relative Positional Embeddings&lt;/h1&gt;\\n\\n&lt;p&gt;Rather than saying &amp;quot;this is position 5&amp;quot;, you tell the model &amp;quot;this token is 3 positions to the left of that one.&amp;quot;&lt;/p&gt;\\n\\n&lt;p&gt;Great for:&lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;Reasoning&lt;/li&gt;\\n&lt;li&gt;Long document understanding&lt;/li&gt;\\n&lt;li&gt;Question answering&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;h1&gt;Tips&lt;/h1&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;Don’t overextend &lt;code&gt;block_size,&lt;/code&gt; it increases memory consumption fast.&lt;/li&gt;\\n&lt;li&gt;Ensure your training data has diverse sequence lengths.&lt;/li&gt;\\n&lt;li&gt;For long inputs, check out RoPE or relative embeddings.&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;h1&gt;Final Thoughts&lt;/h1&gt;\\n\\n&lt;p&gt;Positional embeddings are the quiet workhorses of transformer models. Just by combining two vectors (token + position), we enable the model to process ordered text meaningfully.&lt;/p&gt;\\n\\n&lt;p&gt;Without this, a model wouldn&amp;#39;t know if “The End” belongs at the start or the finish of your story.&lt;/p&gt;\\n\\n&lt;p&gt;&lt;strong&gt;Coming Up Next:&lt;/strong&gt;&lt;br/&gt;\\nTomorrow we’ll dive into Rotary Positional Embeddings (RoPE), a more scalable and elegant solution to position encoding.&lt;/p&gt;\\n\\n&lt;p&gt;If you&amp;#39;re following this series, feel free to share or &lt;a href=\\"https://www.linkedin.com/in/prashant-lakhera-696119b/\\"&gt;connect&lt;/a&gt;.&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"5f921ea4-c7bc-11ed-9c23-3a00622979b4","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#646d73","id":"1lp5pt0","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"Prashant-Lakhera","discussion_type":null,"num_comments":1,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lp5pt0/day_750_building_a_small_language_model_from/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lp5pt0/day_750_building_a_small_language_model_from/","subreddit_subscribers":493457,"created_utc":1751386194,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0ufufz","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"No_Afternoon_4260","can_mod_post":false,"created_utc":1751410276,"send_replies":true,"parent_id":"t3_1lp5pt0","score":1,"author_fullname":"t2_cj9kap4bx","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Keep up the good work that\'s amazing!!!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0ufufz","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Keep up the good work that&amp;#39;s amazing!!!&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp5pt0/day_750_building_a_small_language_model_from/n0ufufz/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751410276,"author_flair_text":"llama.cpp","treatment_tags":[],"link_id":"t3_1lp5pt0","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]'),i=()=>t.jsx(e,{data:n});export{i as default};
