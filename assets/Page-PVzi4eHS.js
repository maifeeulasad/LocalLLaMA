import{j as e}from"./index-BOnf-UhU.js";import{R as l}from"./RedditPostRenderer-Ce39qICS.js";import"./index-CDase6VD.js";const t=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"i was wondering what the lowest cost hardware and model i need in order to run a language model locally for my office of 11 people. i was looking at llama70B, Jamba large, and Mistral (if you have any better ones would love to hear). For the Gpu i was looking at 2 xtx7900 24GB Amd gpus just because they are much cheaper than nvidias. also would i be able to have everyone in my office using the inference setup concurrently? ","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"office AI","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lrwjnx","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.54,"author_flair_background_color":null,"subreddit_type":"public","ups":1,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_a0q7cu31","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":1,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1751672395,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;i was wondering what the lowest cost hardware and model i need in order to run a language model locally for my office of 11 people. i was looking at llama70B, Jamba large, and Mistral (if you have any better ones would love to hear). For the Gpu i was looking at 2 xtx7900 24GB Amd gpus just because they are much cheaper than nvidias. also would i be able to have everyone in my office using the inference setup concurrently? &lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":true,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1lrwjnx","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"Odd_Translator_3026","discussion_type":null,"num_comments":8,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lrwjnx/office_ai/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lrwjnx/office_ai/","subreddit_subscribers":494987,"created_utc":1751672395,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1eb5ym","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"No_Professional_582","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1ea9j7","score":1,"author_fullname":"t2_wrb36r6lc","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"If you're running two concurrent models, one on each GPU, that would allow you to have two concurrent streams of data being processed based on two different inputs simultaneously. This does reduce the overall size of the model that you can use, such as going down to a 14B model vs 32B. Can't remember right now what graphics cards you were looking at, but basically if you're going from two down to one you are cutting the size of model in half. That's not to say that you're going to get a whole lot of difference as some of these smaller models are pretty well refined. \\n\\nFrom what I've been reading, until you get above 70B, not a lot of difference in quality of outputs. But perhaps somebody else can provide more insight into that, as I am playing with 7B/8B models.","edited":false,"author_flair_css_class":null,"name":"t1_n1eb5ym","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;If you&amp;#39;re running two concurrent models, one on each GPU, that would allow you to have two concurrent streams of data being processed based on two different inputs simultaneously. This does reduce the overall size of the model that you can use, such as going down to a 14B model vs 32B. Can&amp;#39;t remember right now what graphics cards you were looking at, but basically if you&amp;#39;re going from two down to one you are cutting the size of model in half. That&amp;#39;s not to say that you&amp;#39;re going to get a whole lot of difference as some of these smaller models are pretty well refined. &lt;/p&gt;\\n\\n&lt;p&gt;From what I&amp;#39;ve been reading, until you get above 70B, not a lot of difference in quality of outputs. But perhaps somebody else can provide more insight into that, as I am playing with 7B/8B models.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lrwjnx","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrwjnx/office_ai/n1eb5ym/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751675925,"author_flair_text":null,"collapsed":false,"created_utc":1751675925,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1ea9j7","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Odd_Translator_3026","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1e7t40","score":1,"author_fullname":"t2_a0q7cu31","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"if i load up one model per gpu wouldn’t that severely limit the size of the model and speed of its just one person using it?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1ea9j7","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;if i load up one model per gpu wouldn’t that severely limit the size of the model and speed of its just one person using it?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lrwjnx","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrwjnx/office_ai/n1ea9j7/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751675548,"author_flair_text":null,"treatment_tags":[],"created_utc":1751675548,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1e7t40","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"No_Professional_582","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1e788q","score":3,"author_fullname":"t2_wrb36r6lc","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"If requests are time sensitive and you have multiple users submitting simultaneously, then creating two instances of your LLM software/processor, each with its own GPU, makes more sense to me. You would have to use smaller models that would fit onto a single GPU VRAM, but would be able to run two inferences simultaneously. \\n\\nThe other option is to have both GPUs run in parallel allowing you to use a much larger model, however this would limit you to processing a single request and multiple users would have to be queued in order to complete one inference before moving on to the next.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n1e7t40","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;If requests are time sensitive and you have multiple users submitting simultaneously, then creating two instances of your LLM software/processor, each with its own GPU, makes more sense to me. You would have to use smaller models that would fit onto a single GPU VRAM, but would be able to run two inferences simultaneously. &lt;/p&gt;\\n\\n&lt;p&gt;The other option is to have both GPUs run in parallel allowing you to use a much larger model, however this would limit you to processing a single request and multiple users would have to be queued in order to complete one inference before moving on to the next.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lrwjnx","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrwjnx/office_ai/n1e7t40/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751674531,"author_flair_text":null,"treatment_tags":[],"created_utc":1751674531,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n1e788q","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Odd_Translator_3026","can_mod_post":false,"created_utc":1751674297,"send_replies":true,"parent_id":"t1_n1e65jv","score":3,"author_fullname":"t2_a0q7cu31","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"More so the first one. i’m assuming that sending stuff at different times wouldn’t affect it much (assuming you mean like after one persons inference completes). \\n\\nI’m honestly a bit confused by this. so your saying if i have like multiple users to instead of in a way combining the gpus into 1 gpu in parallel to instead allow each gpu to handle each users process? i am very new to the hardware aspect of this so if what im saying sounds dumb i apologize","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1e788q","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;More so the first one. i’m assuming that sending stuff at different times wouldn’t affect it much (assuming you mean like after one persons inference completes). &lt;/p&gt;\\n\\n&lt;p&gt;I’m honestly a bit confused by this. so your saying if i have like multiple users to instead of in a way combining the gpus into 1 gpu in parallel to instead allow each gpu to handle each users process? i am very new to the hardware aspect of this so if what im saying sounds dumb i apologize&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lrwjnx","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrwjnx/office_ai/n1e788q/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751674297,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n1e65jv","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"No_Professional_582","can_mod_post":false,"created_utc":1751673861,"send_replies":true,"parent_id":"t3_1lrwjnx","score":3,"author_fullname":"t2_wrb36r6lc","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"By concurrent use, are you talking about multiple people submitting large prompts/data to the API at once? Or just multiple people sending stuff to the API at different times? \\n\\nIf you are maxing out the larger context models, then processing is going to take a bit (and increase errors/hallucinations), in which case if you have multiple users needing simultaneous access, it may be better to set up two instances of your LLM (such as ollama) where each instance is tied to 1x GPU. Otherwise, you can run a setup that allows your ollama to spread the workload across both GPUs to be able to max out the size of the model.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1e65jv","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;By concurrent use, are you talking about multiple people submitting large prompts/data to the API at once? Or just multiple people sending stuff to the API at different times? &lt;/p&gt;\\n\\n&lt;p&gt;If you are maxing out the larger context models, then processing is going to take a bit (and increase errors/hallucinations), in which case if you have multiple users needing simultaneous access, it may be better to set up two instances of your LLM (such as ollama) where each instance is tied to 1x GPU. Otherwise, you can run a setup that allows your ollama to spread the workload across both GPUs to be able to max out the size of the model.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrwjnx/office_ai/n1e65jv/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751673861,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lrwjnx","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1ewhwq","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ArsNeph","can_mod_post":false,"created_utc":1751685244,"send_replies":true,"parent_id":"t3_1lrwjnx","score":1,"author_fullname":"t2_vt0xkv60d","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"In order to serve all of the members of your office concurrently, you would be best off running an inference engine that supports batch inference, specifically VLLM. You would be best using it with tensor parallelism.\\n\\nI highly recommend against buying AMD gpus for this use case, as they have very messy support from most  inference software. They're also a nightmare to get running with different types of AI models, such as diffusion models. I would go with used RTX 3090s, as they can be found for about $600-700 on Facebook Marketplace.\\n\\nCurrently, the larger open source model space is kind of dead in terms of size to performance but I would recommend Llama 3.3 70B at 4 bit, or Hunyuan 80B MoE when it is better supported.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1ewhwq","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;In order to serve all of the members of your office concurrently, you would be best off running an inference engine that supports batch inference, specifically VLLM. You would be best using it with tensor parallelism.&lt;/p&gt;\\n\\n&lt;p&gt;I highly recommend against buying AMD gpus for this use case, as they have very messy support from most  inference software. They&amp;#39;re also a nightmare to get running with different types of AI models, such as diffusion models. I would go with used RTX 3090s, as they can be found for about $600-700 on Facebook Marketplace.&lt;/p&gt;\\n\\n&lt;p&gt;Currently, the larger open source model space is kind of dead in terms of size to performance but I would recommend Llama 3.3 70B at 4 bit, or Hunyuan 80B MoE when it is better supported.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrwjnx/office_ai/n1ewhwq/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751685244,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lrwjnx","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1iz2fq","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"NoVibeCoding","can_mod_post":false,"created_utc":1751747347,"send_replies":true,"parent_id":"t3_1lrwjnx","score":1,"author_fullname":"t2_1neapdttam","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"The RTX PRO 6000 (96GB VRAM) would be the most versatile. It is fast and can run many models. The workstation will cost you like $12K, though. Any multi-GPU setups with the same amount of VRAM will cost about the same, but the performance will drop dramatically due to the need to move data between GPUs.\\n\\nAt small scale pay-per-token LLM providers or GPU rental will almost certainly be cheaper and much more flexible as you'll be able to try any big or small model. You need to load the GPU for 90% for 2 years to justify the investment and it is typically quite hard to achieve. Of course, if you're in regulated industry, you have to go on-prem.\\n\\nYou can try renting RTX PRO 6000 from us. However, these are getting rented out almost immediately as we  bring them to the console.\\n\\n[https://www.cloudrift.ai](https://www.cloudrift.ai)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1iz2fq","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The RTX PRO 6000 (96GB VRAM) would be the most versatile. It is fast and can run many models. The workstation will cost you like $12K, though. Any multi-GPU setups with the same amount of VRAM will cost about the same, but the performance will drop dramatically due to the need to move data between GPUs.&lt;/p&gt;\\n\\n&lt;p&gt;At small scale pay-per-token LLM providers or GPU rental will almost certainly be cheaper and much more flexible as you&amp;#39;ll be able to try any big or small model. You need to load the GPU for 90% for 2 years to justify the investment and it is typically quite hard to achieve. Of course, if you&amp;#39;re in regulated industry, you have to go on-prem.&lt;/p&gt;\\n\\n&lt;p&gt;You can try renting RTX PRO 6000 from us. However, these are getting rented out almost immediately as we  bring them to the console.&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://www.cloudrift.ai\\"&gt;https://www.cloudrift.ai&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":true,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrwjnx/office_ai/n1iz2fq/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751747347,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lrwjnx","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1eg9tu","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"complead","can_mod_post":false,"created_utc":1751678103,"send_replies":true,"parent_id":"t3_1lrwjnx","score":-1,"author_fullname":"t2_uzn88fhh","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"For an office setup, efficiency is key. You might want to look into models like LLaMA-2 or smaller versions optimized for inference. They balance performance and resource use well. Also, if utilizing AMD GPUs, check compatibility with the libraries you plan to use, as some might be optimized for NVIDIA. Experimenting with model quantization can help reduce resource demands while maintaining decent output quality. This makes concurrent inferences smoother for multiple users.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1eg9tu","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;For an office setup, efficiency is key. You might want to look into models like LLaMA-2 or smaller versions optimized for inference. They balance performance and resource use well. Also, if utilizing AMD GPUs, check compatibility with the libraries you plan to use, as some might be optimized for NVIDIA. Experimenting with model quantization can help reduce resource demands while maintaining decent output quality. This makes concurrent inferences smoother for multiple users.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrwjnx/office_ai/n1eg9tu/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751678103,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lrwjnx","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-1}}],"before":null}}]`),n=()=>e.jsx(l,{data:t});export{n as default};
