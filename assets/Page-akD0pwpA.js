import{j as e}from"./index-BOnf-UhU.js";import{R as l}from"./RedditPostRenderer-Ce39qICS.js";import"./index-CDase6VD.js";const t=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"hey guys!\\n\\nbecause of privacy conerns and censorship i;ve decided to give local LLM a try.\\n\\ndownloaded studio LM and installed mistarl 7B and so far things are  fine. might give ollama a chance as well in the future. \\n\\ncouple of questions:\\n\\ncan the model collect data? I asked it and he said he does communicate with the internet to get some more accurate information. isn't it fully oflline? \\n\\ndo you have any other models that you recommended?\\n\\nis there a way to \\"stream\\" the model to my network so I will be able to acsses and ask things from othe computers? \\n\\nis there something else i need to know about local LLMs?\\n\\n  \\nThank you!","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"running local LLM for the first time","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lwafqm","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.83,"author_flair_background_color":null,"subreddit_type":"public","ups":12,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_e6v0plyv","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":12,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1752147328,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;hey guys!&lt;/p&gt;\\n\\n&lt;p&gt;because of privacy conerns and censorship i;ve decided to give local LLM a try.&lt;/p&gt;\\n\\n&lt;p&gt;downloaded studio LM and installed mistarl 7B and so far things are  fine. might give ollama a chance as well in the future. &lt;/p&gt;\\n\\n&lt;p&gt;couple of questions:&lt;/p&gt;\\n\\n&lt;p&gt;can the model collect data? I asked it and he said he does communicate with the internet to get some more accurate information. isn&amp;#39;t it fully oflline? &lt;/p&gt;\\n\\n&lt;p&gt;do you have any other models that you recommended?&lt;/p&gt;\\n\\n&lt;p&gt;is there a way to &amp;quot;stream&amp;quot; the model to my network so I will be able to acsses and ask things from othe computers? &lt;/p&gt;\\n\\n&lt;p&gt;is there something else i need to know about local LLMs?&lt;/p&gt;\\n\\n&lt;p&gt;Thank you!&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1lwafqm","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"Routine_Author961","discussion_type":null,"num_comments":15,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lwafqm/running_local_llm_for_the_first_time/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lwafqm/running_local_llm_for_the_first_time/","subreddit_subscribers":497354,"created_utc":1752147328,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2clik2","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"MaxKruse96","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2ck1bc","score":5,"author_fullname":"t2_pfi81","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Mistral Models are generally the least restricting. If you want actual unhinged content, check \\"JOSIEFIED\\" or \\"Abliterated\\" models, although these can then suffer in general use, because \\\\*something\\\\* had to be removed to make them behave more unhinged, so they are a little less stable","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2clik2","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Mistral Models are generally the least restricting. If you want actual unhinged content, check &amp;quot;JOSIEFIED&amp;quot; or &amp;quot;Abliterated&amp;quot; models, although these can then suffer in general use, because *something* had to be removed to make them behave more unhinged, so they are a little less stable&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lwafqm","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwafqm/running_local_llm_for_the_first_time/n2clik2/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752149264,"author_flair_text":null,"treatment_tags":[],"created_utc":1752149264,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2dm29q","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"i-eat-kittens","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2ck1bc","score":3,"author_fullname":"t2_z290n","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Try tiger gemma: https://old.reddit.com/r/LocalLLaMA/comments/1lvnkuk/drummers_big_tiger_gemma_27b_v3_and_tiger_gemma/\\n or other models from [TheDrummer](https://huggingface.co/TheDrummer/models). They're generally less stiff and censored than official releases.","edited":1752161923,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2dm29q","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Try tiger gemma: &lt;a href=\\"https://old.reddit.com/r/LocalLLaMA/comments/1lvnkuk/drummers_big_tiger_gemma_27b_v3_and_tiger_gemma/\\"&gt;https://old.reddit.com/r/LocalLLaMA/comments/1lvnkuk/drummers_big_tiger_gemma_27b_v3_and_tiger_gemma/&lt;/a&gt;\\n or other models from &lt;a href=\\"https://huggingface.co/TheDrummer/models\\"&gt;TheDrummer&lt;/a&gt;. They&amp;#39;re generally less stiff and censored than official releases.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lwafqm","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwafqm/running_local_llm_for_the_first_time/n2dm29q/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752160745,"author_flair_text":null,"treatment_tags":[],"created_utc":1752160745,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n2ck1bc","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Routine_Author961","can_mod_post":false,"created_utc":1752148681,"send_replies":true,"parent_id":"t1_n2cig59","score":2,"author_fullname":"t2_e6v0plyv","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Thank you!!!!\\nYou made a a lot of sense.\\nAre there models that are less strict on community guidelines? Ones that happily tell me some offending jokes or teach me how to rob a bank? (FBI it's for a joke please don't take me)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2ck1bc","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thank you!!!!\\nYou made a a lot of sense.\\nAre there models that are less strict on community guidelines? Ones that happily tell me some offending jokes or teach me how to rob a bank? (FBI it&amp;#39;s for a joke please don&amp;#39;t take me)&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lwafqm","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwafqm/running_local_llm_for_the_first_time/n2ck1bc/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752148681,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2d1fjp","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Red_Redditor_Reddit","can_mod_post":false,"created_utc":1752154765,"send_replies":true,"parent_id":"t1_n2cig59","score":0,"author_fullname":"t2_8eelmfjg","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"What's wrong with ollama? ","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2d1fjp","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;What&amp;#39;s wrong with ollama? &lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lwafqm","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwafqm/running_local_llm_for_the_first_time/n2d1fjp/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752154765,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2e3y28","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MaxKruse96","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2e1w32","score":1,"author_fullname":"t2_pfi81","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"if you load it, put the advanced settings checkbox in lmstudio and there will be a slider \\"GPU offload\\". All to the right = full gpu = faster, but needs more vram. always aim for full right.  \\nputting it on SSD will increase load time of the model, but not performance","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n2e3y28","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;if you load it, put the advanced settings checkbox in lmstudio and there will be a slider &amp;quot;GPU offload&amp;quot;. All to the right = full gpu = faster, but needs more vram. always aim for full right.&lt;br/&gt;\\nputting it on SSD will increase load time of the model, but not performance&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lwafqm","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwafqm/running_local_llm_for_the_first_time/n2e3y28/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752165783,"author_flair_text":null,"treatment_tags":[],"created_utc":1752165783,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":6,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2e1w32","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Routine_Author961","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2dwvdj","score":1,"author_fullname":"t2_e6v0plyv","approved_by":null,"mod_note":null,"all_awardings":[],"body":"I got you, running a smaller model like phi will improve running time? Can I tweak the settings so more of the GPU will be used?\\n\\n\\nBtw now noticed I'm running it from an HDD, will moving it to an SSD improve it much?","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n2e1w32","is_submitter":true,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I got you, running a smaller model like phi will improve running time? Can I tweak the settings so more of the GPU will be used?&lt;/p&gt;\\n\\n&lt;p&gt;Btw now noticed I&amp;#39;m running it from an HDD, will moving it to an SSD improve it much?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lwafqm","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwafqm/running_local_llm_for_the_first_time/n2e1w32/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752165196,"author_flair_text":null,"treatment_tags":[],"created_utc":1752165196,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2dwvdj","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MaxKruse96","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2dwg61","score":3,"author_fullname":"t2_pfi81","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"thats expected performance for your gpu. ways to make it faster, at any significant rate, is just better gpu.\\n\\nand yes, laptops can run llms, but laptop hardware is even weaker than normal pcs.","edited":false,"author_flair_css_class":null,"name":"t1_n2dwvdj","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;thats expected performance for your gpu. ways to make it faster, at any significant rate, is just better gpu.&lt;/p&gt;\\n\\n&lt;p&gt;and yes, laptops can run llms, but laptop hardware is even weaker than normal pcs.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lwafqm","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwafqm/running_local_llm_for_the_first_time/n2dwvdj/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752163777,"author_flair_text":null,"collapsed":false,"created_utc":1752163777,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n2dwg61","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Routine_Author961","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2dup2y","score":0,"author_fullname":"t2_e6v0plyv","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"It's not the newest but I thought it's fine. I have and old i5 6500, gtx 1070 with 8GB VRAM and CUDA. and 32GB RAM. does that sound right? is there a way to make it faster? can't you run models on laptop?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2dwg61","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It&amp;#39;s not the newest but I thought it&amp;#39;s fine. I have and old i5 6500, gtx 1070 with 8GB VRAM and CUDA. and 32GB RAM. does that sound right? is there a way to make it faster? can&amp;#39;t you run models on laptop?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lwafqm","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwafqm/running_local_llm_for_the_first_time/n2dwg61/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752163657,"author_flair_text":null,"treatment_tags":[],"created_utc":1752163657,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}},"user_reports":[],"saved":false,"id":"n2dup2y","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MaxKruse96","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2dtk9w","score":1,"author_fullname":"t2_pfi81","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"you havent said what your hardware is, so \\"slow\\" can be any number of things. A 7b model at 28 tokens/s sounds like ur on older hardware or a laptop, and in that case you wont get a \\"chatgpt at home\\".","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2dup2y","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;you havent said what your hardware is, so &amp;quot;slow&amp;quot; can be any number of things. A 7b model at 28 tokens/s sounds like ur on older hardware or a laptop, and in that case you wont get a &amp;quot;chatgpt at home&amp;quot;.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lwafqm","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwafqm/running_local_llm_for_the_first_time/n2dup2y/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752163162,"author_flair_text":null,"treatment_tags":[],"created_utc":1752163162,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2dtk9w","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Routine_Author961","can_mod_post":false,"created_utc":1752162846,"send_replies":true,"parent_id":"t1_n2cig59","score":0,"author_fullname":"t2_e6v0plyv","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Why is it so slow? I only tried 7B models, lm studio says it's fine but i get 26.69 tok/sec and a minute or so of thinking after i send each prompet","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2dtk9w","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Why is it so slow? I only tried 7B models, lm studio says it&amp;#39;s fine but i get 26.69 tok/sec and a minute or so of thinking after i send each prompet&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lwafqm","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwafqm/running_local_llm_for_the_first_time/n2dtk9w/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752162846,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}},"user_reports":[],"saved":false,"id":"n2cig59","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"MaxKruse96","can_mod_post":false,"created_utc":1752148051,"send_replies":true,"parent_id":"t3_1lwafqm","score":11,"author_fullname":"t2_pfi81","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"just as a headsup: if you manage to work with lmstudio, ollama is a step down for 99% of use cases. dont bother.\\n\\nModels only know what you tell them.   \\nModels cannot access the internet, they arent programs. You can, however, give the Model some tools (anything from web search, to access to your google mail etc)\\n\\nModels to use depend entirely on your VRAM. If you have a GPU with 12gb, Models of a size of 14b is the soft-limit of what makes sense.\\n\\nFor STEM: Qwen3 (8b or 14b)  \\nFor \\"World Knowledge\\" or Rephrasing of texts you want to write: Gemma3 12b\\n\\nThose are the 2 big ones, that come in smaller variants too if needed.\\n\\nIn terms of \\"streaming\\" your model, LMStudio offers an \\"OpenAI Compatible Server\\", its in the developer tab on the left. Once enabled, you can load a model and its available through [http://localhost:1234/v1/](http://localhost:1234/v1/)\\n\\nUI's to access \\"remote\\" (e.g. on another machine usually) LLMs include OpenwebUI and AnythingLLM. The setup for those is good to search in their docs.\\n\\nIn general, its good to know \\\\*how\\\\* to ask questions. Models \\\\*will\\\\* answer, but if you ask it something it cant possibly know, you get nonsense. See it as a very simple yet complicated \\"If this information would be accessible by an intern in a company, the LLM probably knows it\\".","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2cig59","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;just as a headsup: if you manage to work with lmstudio, ollama is a step down for 99% of use cases. dont bother.&lt;/p&gt;\\n\\n&lt;p&gt;Models only know what you tell them.&lt;br/&gt;\\nModels cannot access the internet, they arent programs. You can, however, give the Model some tools (anything from web search, to access to your google mail etc)&lt;/p&gt;\\n\\n&lt;p&gt;Models to use depend entirely on your VRAM. If you have a GPU with 12gb, Models of a size of 14b is the soft-limit of what makes sense.&lt;/p&gt;\\n\\n&lt;p&gt;For STEM: Qwen3 (8b or 14b)&lt;br/&gt;\\nFor &amp;quot;World Knowledge&amp;quot; or Rephrasing of texts you want to write: Gemma3 12b&lt;/p&gt;\\n\\n&lt;p&gt;Those are the 2 big ones, that come in smaller variants too if needed.&lt;/p&gt;\\n\\n&lt;p&gt;In terms of &amp;quot;streaming&amp;quot; your model, LMStudio offers an &amp;quot;OpenAI Compatible Server&amp;quot;, its in the developer tab on the left. Once enabled, you can load a model and its available through &lt;a href=\\"http://localhost:1234/v1/\\"&gt;http://localhost:1234/v1/&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;UI&amp;#39;s to access &amp;quot;remote&amp;quot; (e.g. on another machine usually) LLMs include OpenwebUI and AnythingLLM. The setup for those is good to search in their docs.&lt;/p&gt;\\n\\n&lt;p&gt;In general, its good to know *how* to ask questions. Models *will* answer, but if you ask it something it cant possibly know, you get nonsense. See it as a very simple yet complicated &amp;quot;If this information would be accessible by an intern in a company, the LLM probably knows it&amp;quot;.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwafqm/running_local_llm_for_the_first_time/n2cig59/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752148051,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lwafqm","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":11}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2cx1b8","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"jacek2023","can_mod_post":false,"created_utc":1752153348,"send_replies":true,"parent_id":"t3_1lwafqm","score":3,"author_fullname":"t2_vqgbql9w","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"The model is running on your local machine. \\n\\nModel can say many different things but it is limited by the way you run it, if tools are disabled it can't use Internet at all.\\n\\nHowever you must communicate somehow with the model, your browser may collect the data, your operating system may collect the data.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2cx1b8","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The model is running on your local machine. &lt;/p&gt;\\n\\n&lt;p&gt;Model can say many different things but it is limited by the way you run it, if tools are disabled it can&amp;#39;t use Internet at all.&lt;/p&gt;\\n\\n&lt;p&gt;However you must communicate somehow with the model, your browser may collect the data, your operating system may collect the data.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwafqm/running_local_llm_for_the_first_time/n2cx1b8/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752153348,"author_flair_text":"llama.cpp","treatment_tags":[],"link_id":"t3_1lwafqm","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2efokf","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ArsNeph","can_mod_post":false,"created_utc":1752168970,"send_replies":true,"parent_id":"t3_1lwafqm","score":3,"author_fullname":"t2_vt0xkv60d","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"The models are running on your VRAM and RAM, so they're not sending any data to the internet unless you explicitly give them web search or other functionality. That said, LM studio is closed source software, so they could be sending data to their servers and you wouldn't be able to tell. \\n\\nMistral 7B is positively ancient, try Qwen 3 8B or Gemma 3 12B, they are light years ahead of it.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2efokf","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The models are running on your VRAM and RAM, so they&amp;#39;re not sending any data to the internet unless you explicitly give them web search or other functionality. That said, LM studio is closed source software, so they could be sending data to their servers and you wouldn&amp;#39;t be able to tell. &lt;/p&gt;\\n\\n&lt;p&gt;Mistral 7B is positively ancient, try Qwen 3 8B or Gemma 3 12B, they are light years ahead of it.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwafqm/running_local_llm_for_the_first_time/n2efokf/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752168970,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lwafqm","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2hxud4","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"BidWestern1056","can_mod_post":false,"created_utc":1752210679,"send_replies":true,"parent_id":"t3_1lwafqm","score":1,"author_fullname":"t2_uzxql7po","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"no it cant collect. id recommend gemma3 models or qwen3. openwebui lets you do what youre asking for hosting. \\nalternatively is suggest my npc tools\\nhttps://github.com/NPC-Worldwide/npcsh (shell)\\nhttps://github.com/NPC-Worldwide/npc-studio (gui interface)\\nnpc tools organize your convos based on paths so you can find things relevant to your projects and they equip your local models with tools like claude code does, but no one is profiting off your information!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2hxud4","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;no it cant collect. id recommend gemma3 models or qwen3. openwebui lets you do what youre asking for hosting. \\nalternatively is suggest my npc tools\\n&lt;a href=\\"https://github.com/NPC-Worldwide/npcsh\\"&gt;https://github.com/NPC-Worldwide/npcsh&lt;/a&gt; (shell)\\n&lt;a href=\\"https://github.com/NPC-Worldwide/npc-studio\\"&gt;https://github.com/NPC-Worldwide/npc-studio&lt;/a&gt; (gui interface)\\nnpc tools organize your convos based on paths so you can find things relevant to your projects and they equip your local models with tools like claude code does, but no one is profiting off your information!&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwafqm/running_local_llm_for_the_first_time/n2hxud4/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752210679,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lwafqm","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),r=()=>e.jsx(l,{data:t});export{r as default};
