import{j as e}from"./index-xfnGEtuL.js";import{R as t}from"./RedditPostRenderer-DAZRIZZK.js";import"./index-BaNn5-RR.js";const l=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"I just tested the \`unsloth/Qwen3-235B-A22B-Instruct-2507-UD-Q3_K_XL.gguf\` model using \`llama.cpp\` on a Threadripper machine equiped with 128 GB RAM + 72 GB VRAM. \\n\\nBy selectively offloading MoE tensors to the CPU - aiming to maximize the VRAM usage - I managed to run the model at generation rate of 15 tokens/s and a context window of 32k tokens. This token generation speed is really great for a non-reasoning model. \\n  \\nHere is the full execution command I used:\\n\\n\`\`\`\\n./llama-server \\\\\\n--model downloaded_models/Qwen3-235B-A22B-Instruct-2507-UD-Q3_K_XL-00001-of-00003.gguf \\\\\\n--port 11433 \\\\\\n--host \\"0.0.0.0\\" \\\\\\n--verbose \\\\\\n--flash-attn \\\\\\n--cache-type-k q8_0 \\\\\\n--cache-type-v q8_0 \\\\\\n--n-gpu-layers 999 \\\\\\n-ot \\"blk\\\\.(?:[1-8]?[1379])\\\\.ffn_.*_exps\\\\.weight=CPU\\" \\\\\\n--prio 3 \\\\\\n--threads 32 \\\\\\n--ctx-size 32768 \\\\\\n--temp 0.6 \\\\\\n--min-p 0.0 \\\\\\n--top-p 0.95 \\\\\\n--top-k 20 \\\\\\n--repeat-penalty 1\\n\`\`\`\\n\\nI'm still new to \`llama.cpp\` and quantization, so any advice is welcome. I think Q4_K_XL might be too heavy for this machine, so I wonder how much quality I would lose by using Q3_K_XL instead.\\n\\n","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Running Qwen3 235B-A22B 2507 on a Threadripper 3970X + 3x RTX 3090 Machine at 15 tok/s","link_flair_richtext":[{"e":"text","t":"Discussion"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":105,"top_awarded_type":null,"hide_score":false,"name":"t3_1m7pqln","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.9,"author_flair_background_color":null,"ups":52,"total_awards_received":0,"media_embed":{"content":"&lt;iframe width=\\"356\\" height=\\"200\\" src=\\"https://www.youtube.com/embed/7HXCQ-4F_oQ?feature=oembed&amp;enablejsapi=1\\" frameborder=\\"0\\" allow=\\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\\" referrerpolicy=\\"strict-origin-when-cross-origin\\" allowfullscreen title=\\"Running Qwen 3 2507 UD-Q3_K_XL on a Threadripper 3970X + 3x RTX 3090 Machine\\"&gt;&lt;/iframe&gt;","width":356,"scrolling":false,"height":200},"thumbnail_width":140,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_14u3g9s5kx","secure_media":{"oembed":{"provider_url":"https://www.youtube.com/","title":"Running Qwen 3 2507 UD-Q3_K_XL on a Threadripper 3970X + 3x RTX 3090 Machine","html":"&lt;iframe width=\\"356\\" height=\\"200\\" src=\\"https://www.youtube.com/embed/7HXCQ-4F_oQ?feature=oembed&amp;enablejsapi=1\\" frameborder=\\"0\\" allow=\\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\\" referrerpolicy=\\"strict-origin-when-cross-origin\\" allowfullscreen title=\\"Running Qwen 3 2507 UD-Q3_K_XL on a Threadripper 3970X + 3x RTX 3090 Machine\\"&gt;&lt;/iframe&gt;","thumbnail_width":480,"height":200,"width":356,"version":"1.0","author_name":"Septerium","provider_name":"YouTube","thumbnail_url":"https://i.ytimg.com/vi/7HXCQ-4F_oQ/hqdefault.jpg","type":"video","thumbnail_height":360,"author_url":"https://www.youtube.com/@JohnnyGomezSn"},"type":"youtube.com"},"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{"content":"&lt;iframe width=\\"356\\" height=\\"200\\" src=\\"https://www.youtube.com/embed/7HXCQ-4F_oQ?feature=oembed&amp;enablejsapi=1\\" frameborder=\\"0\\" allow=\\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\\" referrerpolicy=\\"strict-origin-when-cross-origin\\" allowfullscreen title=\\"Running Qwen 3 2507 UD-Q3_K_XL on a Threadripper 3970X + 3x RTX 3090 Machine\\"&gt;&lt;/iframe&gt;","width":356,"scrolling":false,"media_domain_url":"https://www.redditmedia.com/mediaembed/1m7pqln","height":200},"link_flair_text":"Discussion","can_mod_post":false,"score":52,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"https://external-preview.redd.it/SJM7H0dEjQbZg7rpDS-XlIxBG6BcDeZN9RBYNbnkGWI.jpeg?width=140&amp;height=105&amp;crop=140:105,smart&amp;auto=webp&amp;s=1b8996779707c4a5f85298d6cf4e8395ec809c0d","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"post_hint":"rich:video","content_categories":null,"is_self":false,"subreddit_type":"public","created":1753316083,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"youtube.com","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;I just tested the &lt;code&gt;unsloth/Qwen3-235B-A22B-Instruct-2507-UD-Q3_K_XL.gguf&lt;/code&gt; model using &lt;code&gt;llama.cpp&lt;/code&gt; on a Threadripper machine equiped with 128 GB RAM + 72 GB VRAM. &lt;/p&gt;\\n\\n&lt;p&gt;By selectively offloading MoE tensors to the CPU - aiming to maximize the VRAM usage - I managed to run the model at generation rate of 15 tokens/s and a context window of 32k tokens. This token generation speed is really great for a non-reasoning model. &lt;/p&gt;\\n\\n&lt;p&gt;Here is the full execution command I used:&lt;/p&gt;\\n\\n&lt;p&gt;&lt;code&gt;\\n./llama-server \\\\\\n--model downloaded_models/Qwen3-235B-A22B-Instruct-2507-UD-Q3_K_XL-00001-of-00003.gguf \\\\\\n--port 11433 \\\\\\n--host &amp;quot;0.0.0.0&amp;quot; \\\\\\n--verbose \\\\\\n--flash-attn \\\\\\n--cache-type-k q8_0 \\\\\\n--cache-type-v q8_0 \\\\\\n--n-gpu-layers 999 \\\\\\n-ot &amp;quot;blk\\\\.(?:[1-8]?[1379])\\\\.ffn_.*_exps\\\\.weight=CPU&amp;quot; \\\\\\n--prio 3 \\\\\\n--threads 32 \\\\\\n--ctx-size 32768 \\\\\\n--temp 0.6 \\\\\\n--min-p 0.0 \\\\\\n--top-p 0.95 \\\\\\n--top-k 20 \\\\\\n--repeat-penalty 1\\n&lt;/code&gt;&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;m still new to &lt;code&gt;llama.cpp&lt;/code&gt; and quantization, so any advice is welcome. I think Q4_K_XL might be too heavy for this machine, so I wonder how much quality I would lose by using Q3_K_XL instead.&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"url_overridden_by_dest":"https://www.youtube.com/watch?v=7HXCQ-4F_oQ","view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"preview":{"images":[{"source":{"url":"https://external-preview.redd.it/SJM7H0dEjQbZg7rpDS-XlIxBG6BcDeZN9RBYNbnkGWI.jpeg?auto=webp&amp;s=fb78672ddcf654bd2c828f30bcdaede2ae00db46","width":480,"height":360},"resolutions":[{"url":"https://external-preview.redd.it/SJM7H0dEjQbZg7rpDS-XlIxBG6BcDeZN9RBYNbnkGWI.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=b68e4415698a411ba429105637449852662e35d9","width":108,"height":81},{"url":"https://external-preview.redd.it/SJM7H0dEjQbZg7rpDS-XlIxBG6BcDeZN9RBYNbnkGWI.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=b7a94209a8c4dae66ae50d2f66698b6671ae7897","width":216,"height":162},{"url":"https://external-preview.redd.it/SJM7H0dEjQbZg7rpDS-XlIxBG6BcDeZN9RBYNbnkGWI.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=b8cd0c77917208f92bbcf8528d34b5d0cb74b361","width":320,"height":240}],"variants":{},"id":"SJM7H0dEjQbZg7rpDS-XlIxBG6BcDeZN9RBYNbnkGWI"}],"enabled":false},"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"5f921ea4-c7bc-11ed-9c23-3a00622979b4","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"mod_note":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"num_reports":null,"removal_reason":null,"link_flair_background_color":"#646d73","id":"1m7pqln","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"FalseMap1582","discussion_type":null,"num_comments":17,"send_replies":true,"media":{"oembed":{"provider_url":"https://www.youtube.com/","title":"Running Qwen 3 2507 UD-Q3_K_XL on a Threadripper 3970X + 3x RTX 3090 Machine","html":"&lt;iframe width=\\"356\\" height=\\"200\\" src=\\"https://www.youtube.com/embed/7HXCQ-4F_oQ?feature=oembed&amp;enablejsapi=1\\" frameborder=\\"0\\" allow=\\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share\\" referrerpolicy=\\"strict-origin-when-cross-origin\\" allowfullscreen title=\\"Running Qwen 3 2507 UD-Q3_K_XL on a Threadripper 3970X + 3x RTX 3090 Machine\\"&gt;&lt;/iframe&gt;","thumbnail_width":480,"height":200,"width":356,"version":"1.0","author_name":"Septerium","provider_name":"YouTube","thumbnail_url":"https://i.ytimg.com/vi/7HXCQ-4F_oQ/hqdefault.jpg","type":"video","thumbnail_height":360,"author_url":"https://www.youtube.com/@JohnnyGomezSn"},"type":"youtube.com"},"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1m7pqln/running_qwen3_235ba22b_2507_on_a_threadripper/","stickied":false,"url":"https://www.youtube.com/watch?v=7HXCQ-4F_oQ","subreddit_subscribers":503759,"created_utc":1753316083,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4vn2bx","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"FullstackSensei","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4u2gsl","score":1,"author_fullname":"t2_17n3nqtj56","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"If you're using 1gb ethernet, that'll be your bottleneck. Maybe look into getting some old mellanox 56gb infiniband cards. They're very cheap at 12-15 a pop on ebay. Just make sure to get matching FDR cables for the links to operate at the full 56gbps. Copper cables are up to 3M long and ~25 each for 3M, 10-12 for 2M. Each card had two ports, so you can link up to 3 systems without needing an infiniband switch. They can do normal IP either using IPoIB, or depending on the model of the card you can switch it to work in ethernet mode (retaining the 56gbps speed). Only \\"issue\\" is that you need an X8 slot to get them 56gbps.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4vn2bx","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;If you&amp;#39;re using 1gb ethernet, that&amp;#39;ll be your bottleneck. Maybe look into getting some old mellanox 56gb infiniband cards. They&amp;#39;re very cheap at 12-15 a pop on ebay. Just make sure to get matching FDR cables for the links to operate at the full 56gbps. Copper cables are up to 3M long and ~25 each for 3M, 10-12 for 2M. Each card had two ports, so you can link up to 3 systems without needing an infiniband switch. They can do normal IP either using IPoIB, or depending on the model of the card you can switch it to work in ethernet mode (retaining the 56gbps speed). Only &amp;quot;issue&amp;quot; is that you need an X8 slot to get them 56gbps.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m7pqln","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7pqln/running_qwen3_235ba22b_2507_on_a_threadripper/n4vn2bx/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753354971,"author_flair_text":null,"treatment_tags":[],"created_utc":1753354971,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4uanze","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"plankalkul-z1","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4u9lpp","score":2,"author_fullname":"t2_w73n3yrsx","approved_by":null,"mod_note":null,"all_awardings":[],"body":"Thank you, will try.\\n\\nIn my experiments, llama.cpp gives me +10..15% of performance in tensor splitting mode (vs Ollama)... If tensor splitting will work with partial offloading, I should at least get *something* (even if manual offload config turns out to be not better than Ollama's).","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n4uanze","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thank you, will try.&lt;/p&gt;\\n\\n&lt;p&gt;In my experiments, llama.cpp gives me +10..15% of performance in tensor splitting mode (vs Ollama)... If tensor splitting will work with partial offloading, I should at least get &lt;em&gt;something&lt;/em&gt; (even if manual offload config turns out to be not better than Ollama&amp;#39;s).&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1m7pqln","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7pqln/running_qwen3_235ba22b_2507_on_a_threadripper/n4uanze/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753329712,"author_flair_text":null,"treatment_tags":[],"created_utc":1753329712,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n4u9lpp","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"segmond","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4u7f6k","score":3,"author_fullname":"t2_ah13x","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"sorry, I was thinking of qwen3-coder-430b.  Since you have less vram than ram, begin by offloading as much as you can to the gpu,.  \\n\\n\\nsomething like this will load 1-9 to first gpu, and 10-19 to second gpu,   \\n\\\\--override-tensor  \\"blk.(\\\\[1-9\\\\]).ffn\\\\_.\\\\*\\\\_exps.=CUDA0,,blk.(\\\\[1\\\\]|\\\\[0-9\\\\]).ffn\\\\_.\\\\*\\\\_exps.=CUDA1,ffn\\\\_.\\\\*\\\\_exps.=CPU\\"\\n\\nIt usually takes a few experiments for me to see what would work, say you are running out of ram, you can just trim it down to 0-5 and see how much memory it's using, let's say it uses 35gb, that tells you that each layer is about 7gb, and with you having  48gb, then you know you can put 1 more for a total of about 42gb with the rest for kv cache, so that would be 6 layers per GPU, then I'll do 0-6 for gpu0, 7-9|10-12 for gpu1.   If you find out you don't have enough context for your needs, you drop down to 5 layers for each.","edited":false,"author_flair_css_class":null,"name":"t1_n4u9lpp","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;sorry, I was thinking of qwen3-coder-430b.  Since you have less vram than ram, begin by offloading as much as you can to the gpu,.  &lt;/p&gt;\\n\\n&lt;p&gt;something like this will load 1-9 to first gpu, and 10-19 to second gpu,&lt;br/&gt;\\n--override-tensor  &amp;quot;blk.([1-9]).ffn_.*_exps.=CUDA0,,blk.([1]|[0-9]).ffn_.*_exps.=CUDA1,ffn_.*_exps.=CPU&amp;quot;&lt;/p&gt;\\n\\n&lt;p&gt;It usually takes a few experiments for me to see what would work, say you are running out of ram, you can just trim it down to 0-5 and see how much memory it&amp;#39;s using, let&amp;#39;s say it uses 35gb, that tells you that each layer is about 7gb, and with you having  48gb, then you know you can put 1 more for a total of about 42gb with the rest for kv cache, so that would be 6 layers per GPU, then I&amp;#39;ll do 0-6 for gpu0, 7-9|10-12 for gpu1.   If you find out you don&amp;#39;t have enough context for your needs, you drop down to 5 layers for each.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1m7pqln","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":"light","treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7pqln/running_qwen3_235ba22b_2507_on_a_threadripper/n4u9lpp/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753329253,"author_flair_text":"llama.cpp","collapsed":false,"created_utc":1753329253,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n4u7f6k","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"plankalkul-z1","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4u2gsl","score":1,"author_fullname":"t2_w73n3yrsx","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Thank you for the answer.\\n\\n\\n&gt; Isn't that a total of 192gb of ram?\\n\\n\\nYes, correct.\\n\\n\\n&gt; How are you able to load Q4?\\n\\n\\nOllama. You can throw at it anything that fits total memory (VRAM + RAM), and it just runs it, offloading as it sees fit. Q4 for Qwen3-235b requires about 118 Gb, plus context (8k bf16 in my case).\\n\\n\\nNow, I have no idea what exactly is offloaded, all I can check is GPU/CPU ratio with \`ollama ps\`, and I suspect that with fine-grained control I could get more than my 8 tps, hence my original question.\\n\\n\\nP.S. It's funny that almost as soon as I posted my message it got downvoted: I suspect that was a knee-jerk reaction to \\"Ollama\\"...\\n\\n\\nAn ideal inference engine should have fully automatic memory management (that's missing in all but Ollama). Once server starts, it should report what went where, AND provide options to fine-tune it on the next run (that's missing in Ollama). Unfortunately, with current attitude among users, we're not going to get that any time soon.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4u7f6k","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thank you for the answer.&lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;Isn&amp;#39;t that a total of 192gb of ram?&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;Yes, correct.&lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;How are you able to load Q4?&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;Ollama. You can throw at it anything that fits total memory (VRAM + RAM), and it just runs it, offloading as it sees fit. Q4 for Qwen3-235b requires about 118 Gb, plus context (8k bf16 in my case).&lt;/p&gt;\\n\\n&lt;p&gt;Now, I have no idea what exactly is offloaded, all I can check is GPU/CPU ratio with &lt;code&gt;ollama ps&lt;/code&gt;, and I suspect that with fine-grained control I could get more than my 8 tps, hence my original question.&lt;/p&gt;\\n\\n&lt;p&gt;P.S. It&amp;#39;s funny that almost as soon as I posted my message it got downvoted: I suspect that was a knee-jerk reaction to &amp;quot;Ollama&amp;quot;...&lt;/p&gt;\\n\\n&lt;p&gt;An ideal inference engine should have fully automatic memory management (that&amp;#39;s missing in all but Ollama). Once server starts, it should report what went where, AND provide options to fine-tune it on the next run (that&amp;#39;s missing in Ollama). Unfortunately, with current attitude among users, we&amp;#39;re not going to get that any time soon.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m7pqln","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7pqln/running_qwen3_235ba22b_2507_on_a_threadripper/n4u7f6k/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753328336,"author_flair_text":null,"treatment_tags":[],"created_utc":1753328336,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4u2gsl","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"segmond","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4tlzr4","score":3,"author_fullname":"t2_ah13x","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I have enough GPU to offload everything, however they are very ancient and basic GPUs, P40s, MI50s, 3060s, 3080ti, 3090s etc across 3 clusters.  It's super slow. :-D  Especially with RPC inference over the network, sometimes it's just faster to offload some to VRAM and rest to RAM.   I just wanted to see how this performs across GPU/network, then I'll slowly start removing GPU and seeing if I can get faster.   You have to figure out which layer to offload based on your GPU size, I have 12gb, 16gb and 24gb, so my offloading is all over the place.  Ada is not that fast, but with your DDR5 you should be faster for sure.  Isn't that a total of 192gb of ram?  How are you able to load Q4?","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n4u2gsl","is_submitter":false,"collapsed":false,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I have enough GPU to offload everything, however they are very ancient and basic GPUs, P40s, MI50s, 3060s, 3080ti, 3090s etc across 3 clusters.  It&amp;#39;s super slow. :-D  Especially with RPC inference over the network, sometimes it&amp;#39;s just faster to offload some to VRAM and rest to RAM.   I just wanted to see how this performs across GPU/network, then I&amp;#39;ll slowly start removing GPU and seeing if I can get faster.   You have to figure out which layer to offload based on your GPU size, I have 12gb, 16gb and 24gb, so my offloading is all over the place.  Ada is not that fast, but with your DDR5 you should be faster for sure.  Isn&amp;#39;t that a total of 192gb of ram?  How are you able to load Q4?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m7pqln","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7pqln/running_qwen3_235ba22b_2507_on_a_threadripper/n4u2gsl/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753326336,"author_flair_text":"llama.cpp","treatment_tags":[],"created_utc":1753326336,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4vkesb","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"FullstackSensei","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4tlzr4","score":1,"author_fullname":"t2_17n3nqtj56","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Have you tried using ik_llama.cpp?","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n4vkesb","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Have you tried using ik_llama.cpp?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m7pqln","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7pqln/running_qwen3_235ba22b_2507_on_a_threadripper/n4vkesb/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753353757,"author_flair_text":null,"treatment_tags":[],"created_utc":1753353757,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4tlzr4","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"plankalkul-z1","can_mod_post":false,"created_utc":1753320397,"send_replies":true,"parent_id":"t1_n4te81t","score":4,"author_fullname":"t2_w73n3yrsx","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"&gt; i'm running q4kxl at 60k tokens\\n\\n\\nWhat exact HW config you have? Esp. VRAM/RAM.\\n\\n\\nAlso, would appreciate \`llama-server\` command line: what layer offloading you use.\\n\\n\\nI run Q4 at 8 t/s using Ollama on 2x RTX6000 Adas and Ryzen 9950X with 96Gb of DDR5 6000 EXPO, and I suspect that's not the most I could get from it...","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4tlzr4","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;i&amp;#39;m running q4kxl at 60k tokens&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;What exact HW config you have? Esp. VRAM/RAM.&lt;/p&gt;\\n\\n&lt;p&gt;Also, would appreciate &lt;code&gt;llama-server&lt;/code&gt; command line: what layer offloading you use.&lt;/p&gt;\\n\\n&lt;p&gt;I run Q4 at 8 t/s using Ollama on 2x RTX6000 Adas and Ryzen 9950X with 96Gb of DDR5 6000 EXPO, and I suspect that&amp;#39;s not the most I could get from it...&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m7pqln","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7pqln/running_qwen3_235ba22b_2507_on_a_threadripper/n4tlzr4/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753320397,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}}],"before":null}},"user_reports":[],"saved":false,"id":"n4te81t","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"segmond","can_mod_post":false,"created_utc":1753317681,"send_replies":true,"parent_id":"t3_1m7pqln","score":6,"author_fullname":"t2_ah13x","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"context window of 32k, but how much actual data did you load?   i'm running q4kxl at 60k tokens, but slows to a crawl when I have 20k tokens, but this is an ancient celeron cpu with some mi50s","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4te81t","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;context window of 32k, but how much actual data did you load?   i&amp;#39;m running q4kxl at 60k tokens, but slows to a crawl when I have 20k tokens, but this is an ancient celeron cpu with some mi50s&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7pqln/running_qwen3_235ba22b_2507_on_a_threadripper/n4te81t/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753317681,"author_flair_text":"llama.cpp","treatment_tags":[],"link_id":"t3_1m7pqln","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4vq3o8","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"FalseMap1582","can_mod_post":false,"created_utc":1753356301,"send_replies":true,"parent_id":"t1_n4txlw5","score":1,"author_fullname":"t2_14u3g9s5kx","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I didn't have the time to properly testing it yet. But I intend to try it with aider this week","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4vq3o8","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I didn&amp;#39;t have the time to properly testing it yet. But I intend to try it with aider this week&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m7pqln","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7pqln/running_qwen3_235ba22b_2507_on_a_threadripper/n4vq3o8/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753356301,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4txlw5","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"x0xxin","can_mod_post":false,"created_utc":1753324505,"send_replies":true,"parent_id":"t3_1m7pqln","score":2,"author_fullname":"t2_btpe1","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Thanks for sharing! How is the UD-3Q-K_XL performing for you in terms of intelligence and/or specific use cases? What would you compare it to if anything? I'm super tempted to grab it now.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4txlw5","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thanks for sharing! How is the UD-3Q-K_XL performing for you in terms of intelligence and/or specific use cases? What would you compare it to if anything? I&amp;#39;m super tempted to grab it now.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7pqln/running_qwen3_235ba22b_2507_on_a_threadripper/n4txlw5/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753324505,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m7pqln","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4v3ol5","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"tarruda","can_mod_post":false,"created_utc":1753344556,"send_replies":true,"parent_id":"t3_1m7pqln","score":2,"author_fullname":"t2_dphk4","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"This IQ4_XS quant is on a Mac Studio M1 ultra with 128gb RAM (~$2.5k used on eBay)\\n\\n    $ llama-bench -m Qwen3-235B-A22B-Instruct-2507-IQ4_XS-00001-of-00003.gguf\\n    | model                          |       size |     params | backend    | threads |            test |                  t/s |\\n    | ------------------------------ | ---------: | ---------: | ---------- | ------: | --------------: | -------------------: |\\n    | qwen3moe 235B.A22B IQ4_XS - 4.25 bpw | 116.86 GiB |   235.09 B | Metal,BLAS |      16 |           pp512 |        147.18 Â± 0.65 |\\n    | qwen3moe 235B.A22B IQ4_XS - 4.25 bpw | 116.86 GiB |   235.09 B | Metal,BLAS |      16 |           tg128 |         17.75 Â± 0.00 |\\n\\nI tested and can load up to 40k context before MacOS starts swapping or crashing","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4v3ol5","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;This IQ4_XS quant is on a Mac Studio M1 ultra with 128gb RAM (~$2.5k used on eBay)&lt;/p&gt;\\n\\n&lt;pre&gt;&lt;code&gt;$ llama-bench -m Qwen3-235B-A22B-Instruct-2507-IQ4_XS-00001-of-00003.gguf\\n| model                          |       size |     params | backend    | threads |            test |                  t/s |\\n| ------------------------------ | ---------: | ---------: | ---------- | ------: | --------------: | -------------------: |\\n| qwen3moe 235B.A22B IQ4_XS - 4.25 bpw | 116.86 GiB |   235.09 B | Metal,BLAS |      16 |           pp512 |        147.18 Â± 0.65 |\\n| qwen3moe 235B.A22B IQ4_XS - 4.25 bpw | 116.86 GiB |   235.09 B | Metal,BLAS |      16 |           tg128 |         17.75 Â± 0.00 |\\n&lt;/code&gt;&lt;/pre&gt;\\n\\n&lt;p&gt;I tested and can load up to 40k context before MacOS starts swapping or crashing&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7pqln/running_qwen3_235ba22b_2507_on_a_threadripper/n4v3ol5/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753344556,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m7pqln","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4vlouk","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"FullstackSensei","can_mod_post":false,"created_utc":1753354355,"send_replies":true,"parent_id":"t1_n4uyqoi","score":1,"author_fullname":"t2_17n3nqtj56","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"While I agree that TR isn't the cheapest option, the reason why TR is great for LLM inference is memory bandwidth. Up to 3rd gen TR you get 8 DDR4 memory channels. With 2nd or 3rd gen TR and 3600 memory, you get 230GB/s theoretical max. No other desktop platform comes even close. A desktop with DDR5-6400 memory has less than half that at 102GB/s.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4vlouk","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;While I agree that TR isn&amp;#39;t the cheapest option, the reason why TR is great for LLM inference is memory bandwidth. Up to 3rd gen TR you get 8 DDR4 memory channels. With 2nd or 3rd gen TR and 3600 memory, you get 230GB/s theoretical max. No other desktop platform comes even close. A desktop with DDR5-6400 memory has less than half that at 102GB/s.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m7pqln","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7pqln/running_qwen3_235ba22b_2507_on_a_threadripper/n4vlouk/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753354355,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4vpnob","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"FalseMap1582","can_mod_post":false,"created_utc":1753356114,"send_replies":true,"parent_id":"t1_n4uyqoi","score":1,"author_fullname":"t2_14u3g9s5kx","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I've already had it since year 2021 ðŸ˜‰... I used it for scientific computing back then","edited":1753356721,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4vpnob","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;ve already had it since year 2021 ðŸ˜‰... I used it for scientific computing back then&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m7pqln","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7pqln/running_qwen3_235ba22b_2507_on_a_threadripper/n4vpnob/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753356114,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4w03qm","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"alanoo","can_mod_post":false,"created_utc":1753360205,"send_replies":true,"parent_id":"t1_n4uyqoi","score":1,"author_fullname":"t2_iiwf3","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"In fact you need a Threadripper Pro for that, non Pro are officially limited to 256 GB of RAM","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4w03qm","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;In fact you need a Threadripper Pro for that, non Pro are officially limited to 256 GB of RAM&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m7pqln","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7pqln/running_qwen3_235ba22b_2507_on_a_threadripper/n4w03qm/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753360205,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4uyqoi","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Highwaytothebeach","can_mod_post":false,"created_utc":1753341752,"send_replies":true,"parent_id":"t3_1m7pqln","score":1,"author_fullname":"t2_1qychuraq9","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Threadripper...I was thinking people were getting these because wanting up to 1 TB of RAM... with DDR6 those machines are promised to have UP to  8 TB of RAM.... Wonder why did you get it since having only 128 GB ? You can do the same with 4 core 3000 series  at about 20 times cheaper...","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4uyqoi","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Threadripper...I was thinking people were getting these because wanting up to 1 TB of RAM... with DDR6 those machines are promised to have UP to  8 TB of RAM.... Wonder why did you get it since having only 128 GB ? You can do the same with 4 core 3000 series  at about 20 times cheaper...&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7pqln/running_qwen3_235ba22b_2507_on_a_threadripper/n4uyqoi/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753341752,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m7pqln","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4usu9i","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"GPTrack_ai","can_mod_post":false,"created_utc":1753338546,"send_replies":true,"parent_id":"t3_1m7pqln","score":-1,"author_fullname":"t2_1tpuoj72sa","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"below Q4 is not good...","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4usu9i","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;below Q4 is not good...&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7pqln/running_qwen3_235ba22b_2507_on_a_threadripper/n4usu9i/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753338546,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m7pqln","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-1}}],"before":null}}]`),r=()=>e.jsx(t,{data:l});export{r as default};
