import{j as e}from"./index-BpC9hjVs.js";import{R as l}from"./RedditPostRenderer-BEo6AnSR.js";import"./index-DwkJHX1_.js";const t=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"So, here is the problem. I'm actually facing it as I'm writing this post.\\n\\nI use multiple LLM models (32b and 70b at Q4 or Q8, qwen, qwq, deepseek, llama, etc). I also use Open WebUI for prompting them. What I like the most is the ability to have a single prompt sent to multiple LLMs and get their outputs side by side. It's like asking multiple experts with various opinions before making a decision. \\n\\nI have a dual RTX 3090 setup (48gb vram total). Open Web UI is integrated with ollama and models are being loaded from local NVMe drive. I have posted photos of my setup some time ago. Nothing fancy, some older server/workstation grade build.\\n\\nThe problem is, the NVMe is just too slow. Because of limited amount of Vram, each model has to be run once at the time which means the whole model has to be reloaded from the NVMe to Vram again and again. I potentially could increase amount of memory (like 128GB) in my system (proxmox VM) to cache models in regular RAM but perhaps there are other solutions, some hardware etc?\\n\\nAny ideas anyone? Thanks.","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"NVMe for local LLM is too slow. Any ideas?","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lzx039","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.73,"author_flair_background_color":null,"subreddit_type":"public","ups":5,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_11ccns","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":5,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1752523597,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;So, here is the problem. I&amp;#39;m actually facing it as I&amp;#39;m writing this post.&lt;/p&gt;\\n\\n&lt;p&gt;I use multiple LLM models (32b and 70b at Q4 or Q8, qwen, qwq, deepseek, llama, etc). I also use Open WebUI for prompting them. What I like the most is the ability to have a single prompt sent to multiple LLMs and get their outputs side by side. It&amp;#39;s like asking multiple experts with various opinions before making a decision. &lt;/p&gt;\\n\\n&lt;p&gt;I have a dual RTX 3090 setup (48gb vram total). Open Web UI is integrated with ollama and models are being loaded from local NVMe drive. I have posted photos of my setup some time ago. Nothing fancy, some older server/workstation grade build.&lt;/p&gt;\\n\\n&lt;p&gt;The problem is, the NVMe is just too slow. Because of limited amount of Vram, each model has to be run once at the time which means the whole model has to be reloaded from the NVMe to Vram again and again. I potentially could increase amount of memory (like 128GB) in my system (proxmox VM) to cache models in regular RAM but perhaps there are other solutions, some hardware etc?&lt;/p&gt;\\n\\n&lt;p&gt;Any ideas anyone? Thanks.&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1lzx039","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"ChopSticksPlease","discussion_type":null,"num_comments":21,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lzx039/nvme_for_local_llm_is_too_slow_any_ideas/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lzx039/nvme_for_local_llm_is_too_slow_any_ideas/","subreddit_subscribers":499295,"created_utc":1752523597,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n35fn6u","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"HypnoDaddy4You","can_mod_post":false,"send_replies":true,"parent_id":"t1_n35aprr","score":5,"author_fullname":"t2_lb2n7mbsw","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"You can also just turn up the temperature and ask the same model multiple times.\\n\\nOr use a MoE model, this is how they're architected internally.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n35fn6u","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You can also just turn up the temperature and ask the same model multiple times.&lt;/p&gt;\\n\\n&lt;p&gt;Or use a MoE model, this is how they&amp;#39;re architected internally.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lzx039","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzx039/nvme_for_local_llm_is_too_slow_any_ideas/n35fn6u/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752528593,"author_flair_text":null,"treatment_tags":[],"created_utc":1752528593,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n35r4eb","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"No_Afternoon_4260","can_mod_post":false,"send_replies":true,"parent_id":"t1_n35aprr","score":1,"author_fullname":"t2_cj9kap4bx","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I agree with you.\\nLoading times I bottleneck by nvme and pcie lanes mostly.\\nIf you are on pcie 4.0 x8 or worst x4, probably a faster nvme or RAID won't change much.\\nSee what it cost you to get a RAID, knowingly that it won't get you more than half the loading time (without counting the overhead that'll stay the same.)\\nIf using llama.cpp you still load one gpu at a time. Not sure about other backends didn't measured/looked that closely.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n35r4eb","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I agree with you.\\nLoading times I bottleneck by nvme and pcie lanes mostly.\\nIf you are on pcie 4.0 x8 or worst x4, probably a faster nvme or RAID won&amp;#39;t change much.\\nSee what it cost you to get a RAID, knowingly that it won&amp;#39;t get you more than half the loading time (without counting the overhead that&amp;#39;ll stay the same.)\\nIf using llama.cpp you still load one gpu at a time. Not sure about other backends didn&amp;#39;t measured/looked that closely.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lzx039","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzx039/nvme_for_local_llm_is_too_slow_any_ideas/n35r4eb/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752532100,"author_flair_text":"llama.cpp","treatment_tags":[],"created_utc":1752532100,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n35aprr","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ChopSticksPlease","can_mod_post":false,"send_replies":true,"parent_id":"t1_n357i5i","score":2,"author_fullname":"t2_11ccns","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Because i get various outputs from models that were trained differently? Especially when I ask open questions. While the answers tend to be simmilar there are niuances that I find interesting. Try for yourself.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n35aprr","is_submitter":true,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Because i get various outputs from models that were trained differently? Especially when I ask open questions. While the answers tend to be simmilar there are niuances that I find interesting. Try for yourself.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lzx039","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzx039/nvme_for_local_llm_is_too_slow_any_ideas/n35aprr/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752527170,"author_flair_text":null,"treatment_tags":[],"created_utc":1752527170,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n357i5i","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"giant3","can_mod_post":false,"created_utc":1752526263,"send_replies":true,"parent_id":"t1_n34zt2r","score":5,"author_fullname":"t2_82esi","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I agree. I don't understand this desire to run multiple models unless OP is being paid for evaluating LLMs or using it as part of his job, it is just waste of time &amp; energy.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n357i5i","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I agree. I don&amp;#39;t understand this desire to run multiple models unless OP is being paid for evaluating LLMs or using it as part of his job, it is just waste of time &amp;amp; energy.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lzx039","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzx039/nvme_for_local_llm_is_too_slow_any_ideas/n357i5i/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752526263,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}}],"before":null}},"user_reports":[],"saved":false,"id":"n34zt2r","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"HypnoDaddy4You","can_mod_post":false,"created_utc":1752524115,"send_replies":true,"parent_id":"t3_1lzx039","score":19,"author_fullname":"t2_lb2n7mbsw","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"My idea?\\n\\nDon't do this. Loading the model will always take a non zero amount of time. Pick a model that's good for the task st hand and stick to it. If you simply must have multiple models running, use multiple hosts.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n34zt2r","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;My idea?&lt;/p&gt;\\n\\n&lt;p&gt;Don&amp;#39;t do this. Loading the model will always take a non zero amount of time. Pick a model that&amp;#39;s good for the task st hand and stick to it. If you simply must have multiple models running, use multiple hosts.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzx039/nvme_for_local_llm_is_too_slow_any_ideas/n34zt2r/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752524115,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lzx039","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":19}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n35c2ng","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"DeltaSqueezer","can_mod_post":false,"created_utc":1752527560,"send_replies":true,"parent_id":"t3_1lzx039","score":6,"author_fullname":"t2_8jqx3m14","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Just load it into RAM first. As you are inferencing with one model, load the next model into RAM so that it is ready.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n35c2ng","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Just load it into RAM first. As you are inferencing with one model, load the next model into RAM so that it is ready.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzx039/nvme_for_local_llm_is_too_slow_any_ideas/n35c2ng/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752527560,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lzx039","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n350fyr","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ChopSticksPlease","can_mod_post":false,"created_utc":1752524293,"send_replies":true,"parent_id":"t1_n35001t","score":3,"author_fullname":"t2_11ccns","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"256gb currently (16x16gb), extending it to say 512gb is possible but would need to replace the memory sticks.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n350fyr","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;256gb currently (16x16gb), extending it to say 512gb is possible but would need to replace the memory sticks.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lzx039","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzx039/nvme_for_local_llm_is_too_slow_any_ideas/n350fyr/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752524293,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n35001t","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"fizzy1242","can_mod_post":false,"created_utc":1752524169,"send_replies":true,"parent_id":"t3_1lzx039","score":2,"author_fullname":"t2_16zcsx","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"how much normal RAM do you have? it might be getting paged into disk if the file size exceeds your normal ram amount. Pretty sure model gets loaded onto ram first, then onto vram","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n35001t","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;how much normal RAM do you have? it might be getting paged into disk if the file size exceeds your normal ram amount. Pretty sure model gets loaded onto ram first, then onto vram&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzx039/nvme_for_local_llm_is_too_slow_any_ideas/n35001t/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752524169,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lzx039","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"50c36eba-fdca-11ee-9735-92a88d7e3b87","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"50c36eba-fdca-11ee-9735-92a88d7e3b87","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n35hxb8","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"remghoost7","can_mod_post":false,"send_replies":true,"parent_id":"t1_n355diu","score":1,"author_fullname":"t2_sejql","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I recently bought two of these [Crucial P310 1TB drives](https://www.amazon.com/dp/B0DC8VPSHV?th=1) and put one of them in a PCIe gen 4 slot.\\n\\nIt does around 7GB/s sequential and loads a 27B-Q4\\\\_K\\\\_L (just over 17GB) *functionally instantly*.  \\nRealistically, it takes around 2.5s, but that's way fast enough for my use-cases.\\n\\nTwo of these in RAID0 in PCIe gen 4 slots would get around 14GB/s (in theory).\\n\\n---\\n\\nAnd there are *much quicker* drives out there.\\n\\n[Kioxia CM7 drives](https://americas.kioxia.com/en-us/business/ssd/enterprise-ssd/cm7-r.html) come to mind, topping out at around 14GB/s.  \\nTwo of those in RAID0 would give you around 30GB/s.\\n\\nGranted, those will run you [around $600 for a 2TB drive.](https://www.newegg.com/p/0D9-00SE-00029?item=9SIA12KKCA4914&amp;nm_mc=knc-googleadwords&amp;cm_mmc=knc-googleadwords-_-solid%20state%20disk-_-kioxia-_-9SIA12KKCA4914&amp;source=region)  \\nNot including the price for the hardware necessary to actually push them to that limit.\\n\\n---\\n\\nBut yeah, I agree with you.  \\nJust find a model that you like and stick with it or use a RAM cache if you *really* want to.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n35hxb8","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I recently bought two of these &lt;a href=\\"https://www.amazon.com/dp/B0DC8VPSHV?th=1\\"&gt;Crucial P310 1TB drives&lt;/a&gt; and put one of them in a PCIe gen 4 slot.&lt;/p&gt;\\n\\n&lt;p&gt;It does around 7GB/s sequential and loads a 27B-Q4_K_L (just over 17GB) &lt;em&gt;functionally instantly&lt;/em&gt;.&lt;br/&gt;\\nRealistically, it takes around 2.5s, but that&amp;#39;s way fast enough for my use-cases.&lt;/p&gt;\\n\\n&lt;p&gt;Two of these in RAID0 in PCIe gen 4 slots would get around 14GB/s (in theory).&lt;/p&gt;\\n\\n&lt;hr/&gt;\\n\\n&lt;p&gt;And there are &lt;em&gt;much quicker&lt;/em&gt; drives out there.&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://americas.kioxia.com/en-us/business/ssd/enterprise-ssd/cm7-r.html\\"&gt;Kioxia CM7 drives&lt;/a&gt; come to mind, topping out at around 14GB/s.&lt;br/&gt;\\nTwo of those in RAID0 would give you around 30GB/s.&lt;/p&gt;\\n\\n&lt;p&gt;Granted, those will run you &lt;a href=\\"https://www.newegg.com/p/0D9-00SE-00029?item=9SIA12KKCA4914&amp;amp;nm_mc=knc-googleadwords&amp;amp;cm_mmc=knc-googleadwords-_-solid%20state%20disk-_-kioxia-_-9SIA12KKCA4914&amp;amp;source=region\\"&gt;around $600 for a 2TB drive.&lt;/a&gt;&lt;br/&gt;\\nNot including the price for the hardware necessary to actually push them to that limit.&lt;/p&gt;\\n\\n&lt;hr/&gt;\\n\\n&lt;p&gt;But yeah, I agree with you.&lt;br/&gt;\\nJust find a model that you like and stick with it or use a RAM cache if you &lt;em&gt;really&lt;/em&gt; want to.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lzx039","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzx039/nvme_for_local_llm_is_too_slow_any_ideas/n35hxb8/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752529262,"author_flair_text":null,"treatment_tags":[],"created_utc":1752529262,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n355diu","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Kirys79","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3531dx","score":2,"author_fullname":"t2_4o8ex3bu","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"os? can you do a read bench?\\n\\nassuming pcie gen 3 you have a max theoretical speed of 4GBytes/s from the NVME this means about 8s for each model minimum but the average ssd is less than that so I assume you have 15-20s for model.\\n\\nThe 3090 should have 16lanes (if you have them on your board) that means that from ram you could achieve 2-3 seconds for load.\\n\\nSo setting up a ramdisk could highly reduce the load times, but you need more than 128gb of ram according to your list of models.\\n\\nmaybe you should narrow the list to the ones most effective for the task.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n355diu","is_submitter":false,"collapsed":false,"author_flair_richtext":[{"e":"text","t":"Ollama"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;os? can you do a read bench?&lt;/p&gt;\\n\\n&lt;p&gt;assuming pcie gen 3 you have a max theoretical speed of 4GBytes/s from the NVME this means about 8s for each model minimum but the average ssd is less than that so I assume you have 15-20s for model.&lt;/p&gt;\\n\\n&lt;p&gt;The 3090 should have 16lanes (if you have them on your board) that means that from ram you could achieve 2-3 seconds for load.&lt;/p&gt;\\n\\n&lt;p&gt;So setting up a ramdisk could highly reduce the load times, but you need more than 128gb of ram according to your list of models.&lt;/p&gt;\\n\\n&lt;p&gt;maybe you should narrow the list to the ones most effective for the task.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lzx039","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzx039/nvme_for_local_llm_is_too_slow_any_ideas/n355diu/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752525665,"author_flair_text":"Ollama","treatment_tags":[],"created_utc":1752525665,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n3531dx","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ChopSticksPlease","can_mod_post":false,"created_utc":1752525009,"send_replies":true,"parent_id":"t1_n352s3w","score":1,"author_fullname":"t2_11ccns","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"yep, nvme passed directly to the vm, same as gpus","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3531dx","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;yep, nvme passed directly to the vm, same as gpus&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lzx039","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzx039/nvme_for_local_llm_is_too_slow_any_ideas/n3531dx/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752525009,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n352s3w","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Kirys79","can_mod_post":false,"created_utc":1752524938,"send_replies":true,"parent_id":"t3_1lzx039","score":2,"author_fullname":"t2_4o8ex3bu","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"The local NVME is directly attached to the VM or the virtual disk is on the NVME?\\n\\ndo a read speed test on a model file (using dd if you are on linux with odirect) to asses the actual read speed.\\n\\nIMO more ram for the VM could help only if the sum of all models size is less than the total ram.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n352s3w","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"Ollama"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The local NVME is directly attached to the VM or the virtual disk is on the NVME?&lt;/p&gt;\\n\\n&lt;p&gt;do a read speed test on a model file (using dd if you are on linux with odirect) to asses the actual read speed.&lt;/p&gt;\\n\\n&lt;p&gt;IMO more ram for the VM could help only if the sum of all models size is less than the total ram.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzx039/nvme_for_local_llm_is_too_slow_any_ideas/n352s3w/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752524938,"author_flair_text":"Ollama","treatment_tags":[],"link_id":"t3_1lzx039","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n35cd6j","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Andre4s11","can_mod_post":false,"send_replies":true,"parent_id":"t1_n35aauz","score":1,"author_fullname":"t2_uitx4","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"then it's a CPU-memory-disk bundle, I would look at the whole thing load avarage i/o top and other","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n35cd6j","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;then it&amp;#39;s a CPU-memory-disk bundle, I would look at the whole thing load avarage i/o top and other&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lzx039","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzx039/nvme_for_local_llm_is_too_slow_any_ideas/n35cd6j/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752527644,"author_flair_text":null,"treatment_tags":[],"created_utc":1752527644,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n35aauz","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ChopSticksPlease","can_mod_post":false,"created_utc":1752527050,"send_replies":true,"parent_id":"t1_n3591tv","score":1,"author_fullname":"t2_11ccns","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Inference speed has not been yet a problem, tps is at usable pace. Its more about the time to load the model from the disk before it even starts working.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n35aauz","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Inference speed has not been yet a problem, tps is at usable pace. Its more about the time to load the model from the disk before it even starts working.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lzx039","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzx039/nvme_for_local_llm_is_too_slow_any_ideas/n35aauz/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752527050,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3591tv","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Andre4s11","can_mod_post":false,"created_utc":1752526695,"send_replies":true,"parent_id":"t3_1lzx039","score":1,"author_fullname":"t2_uitx4","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I may be wrong!! but 3090 does not fully reveal itself in a pair on nvlink!! If you care not only about speeding up the exchange between cards, but also about having a single VRAM pool, you will have to look at professional or data-center lines (A100, H100) with NVSwitch. In the consumer segment, for LLM-Inference tasks and training large models, you can rely on model-parallel and offload strategies (DeepSpeed, ZeRO, FSDP, etc.), even with NVLink Bridge.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3591tv","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I may be wrong!! but 3090 does not fully reveal itself in a pair on nvlink!! If you care not only about speeding up the exchange between cards, but also about having a single VRAM pool, you will have to look at professional or data-center lines (A100, H100) with NVSwitch. In the consumer segment, for LLM-Inference tasks and training large models, you can rely on model-parallel and offload strategies (DeepSpeed, ZeRO, FSDP, etc.), even with NVLink Bridge.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzx039/nvme_for_local_llm_is_too_slow_any_ideas/n3591tv/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752526695,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lzx039","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n35fsp0","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"eloquentemu","can_mod_post":false,"created_utc":1752528638,"send_replies":true,"parent_id":"t3_1lzx039","score":1,"author_fullname":"t2_lpdsy","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"The problem is that this stuff is just big.  Don't be surprised if GIGAbytes take some time to shuffle around.  Like, a Gen4 NVMe is ~7-8GBps (for a good drive), one channel of DDR5 is ~40GBps, a 3090 is 32GBps at Gen4x16.\\n\\nOf course you don't say what your CPU is so maybe you're using a DDR4 and PCIe Gen3 system?  That would certainly make things a _lot_ worse.\\n\\n&gt; I potentially could increase amount of memory (like 128GB) in my system (proxmox VM) to cache models in regular RAM but perhaps there are other solutions, some hardware etc?\\n\\nThis is the best solution.  Make a ram disk, copy your models to the ram disk, and load from there.  You will still be limited to the banwidth of your RAM, PCIe connection (esp if Gen3 and/or x8) and CPU's I/O controller (though it's usually faster than the PCIe).\\n\\nSecondary options are to upgrade your storage.  If you have PCIe Gen5, there are now drives on the market that can hit ~14GBps but you... uh... pay for that ;).  You could also get a second NVMe drive and put it in a RAID0.  I've heard mixed performance results with that, but it might work depending on where your bottlenecks are.\\n\\nAssorted other tips:\\n\\n- ~~if you haven't, pass the NVMe (as a normal PCIe device like the GPUs) through to the VM.  The overhead of access through the hypervisor is often fairly devastating~~ (sounds like you did this)\\n- try using 1G hugepages for the VM (i.e. allocate 1G pages on the proxmox kernel command line and set the VM to use them).  I haven't benchmarked it extensively, but the one time recently I disabled them I saw a noticeable drop in I/O performance specifically around model load.  It might have been something else, IDK, but it's at least a small performance bump.\\n- Edit: Checking below it sounds like you have a dual CPU setup.  For best results make sure you limit stuff to one CPU as much as possible.  Reading the NVMe from across CPUs etc adds a decent amount of overhead","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n35fsp0","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The problem is that this stuff is just big.  Don&amp;#39;t be surprised if GIGAbytes take some time to shuffle around.  Like, a Gen4 NVMe is ~7-8GBps (for a good drive), one channel of DDR5 is ~40GBps, a 3090 is 32GBps at Gen4x16.&lt;/p&gt;\\n\\n&lt;p&gt;Of course you don&amp;#39;t say what your CPU is so maybe you&amp;#39;re using a DDR4 and PCIe Gen3 system?  That would certainly make things a &lt;em&gt;lot&lt;/em&gt; worse.&lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;I potentially could increase amount of memory (like 128GB) in my system (proxmox VM) to cache models in regular RAM but perhaps there are other solutions, some hardware etc?&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;This is the best solution.  Make a ram disk, copy your models to the ram disk, and load from there.  You will still be limited to the banwidth of your RAM, PCIe connection (esp if Gen3 and/or x8) and CPU&amp;#39;s I/O controller (though it&amp;#39;s usually faster than the PCIe).&lt;/p&gt;\\n\\n&lt;p&gt;Secondary options are to upgrade your storage.  If you have PCIe Gen5, there are now drives on the market that can hit ~14GBps but you... uh... pay for that ;).  You could also get a second NVMe drive and put it in a RAID0.  I&amp;#39;ve heard mixed performance results with that, but it might work depending on where your bottlenecks are.&lt;/p&gt;\\n\\n&lt;p&gt;Assorted other tips:&lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;&lt;del&gt;if you haven&amp;#39;t, pass the NVMe (as a normal PCIe device like the GPUs) through to the VM.  The overhead of access through the hypervisor is often fairly devastating&lt;/del&gt; (sounds like you did this)&lt;/li&gt;\\n&lt;li&gt;try using 1G hugepages for the VM (i.e. allocate 1G pages on the proxmox kernel command line and set the VM to use them).  I haven&amp;#39;t benchmarked it extensively, but the one time recently I disabled them I saw a noticeable drop in I/O performance specifically around model load.  It might have been something else, IDK, but it&amp;#39;s at least a small performance bump.&lt;/li&gt;\\n&lt;li&gt;Edit: Checking below it sounds like you have a dual CPU setup.  For best results make sure you limit stuff to one CPU as much as possible.  Reading the NVMe from across CPUs etc adds a decent amount of overhead&lt;/li&gt;\\n&lt;/ul&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzx039/nvme_for_local_llm_is_too_slow_any_ideas/n35fsp0/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752528638,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lzx039","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n35g5zz","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MelodicRecognition7","can_mod_post":false,"created_utc":1752528747,"send_replies":true,"parent_id":"t3_1lzx039","score":1,"author_fullname":"t2_1eex9ug5","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"check if Open WebUI has an option similar to llamacpp's \\"--no-mmap\\", maybe they use the same stupid memory mapping by default.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n35g5zz","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;check if Open WebUI has an option similar to llamacpp&amp;#39;s &amp;quot;--no-mmap&amp;quot;, maybe they use the same stupid memory mapping by default.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzx039/nvme_for_local_llm_is_too_slow_any_ideas/n35g5zz/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752528747,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lzx039","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n36gcs5","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"LostLakkris","can_mod_post":false,"created_utc":1752540457,"send_replies":true,"parent_id":"t3_1lzx039","score":1,"author_fullname":"t2_2xl44cmc","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Everyone else will probably give better tips, but raid0 multiple nvme?\\n\\nOtherwise probably avoid model swapping entirely.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n36gcs5","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Everyone else will probably give better tips, but raid0 multiple nvme?&lt;/p&gt;\\n\\n&lt;p&gt;Otherwise probably avoid model swapping entirely.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzx039/nvme_for_local_llm_is_too_slow_any_ideas/n36gcs5/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752540457,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lzx039","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n376bs3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"TensorThief","can_mod_post":false,"created_utc":1752549774,"send_replies":true,"parent_id":"t3_1lzx039","score":1,"author_fullname":"t2_1kt2k01uih","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"NVME is great for storing models you are not using right this minute.\\n\\nFor everything else, there is ram tempfs:\\n\\nroot@TURIN2D24G-2L-500W:\\\\~# fio --name=readtest --rw=read --bs=2M --ioengine=libaio --numjobs=8 --size=3G --direct=1 --filename=/ram/exl2/test\\n\\n... snip ...\\n\\nRun status group 0 (all jobs):\\n\\nREAD: bw=69.8GiB/s (74.9GB/s), 8930MiB/s-10.0GiB/s (9364MB/s-10.8GB/s), io=24.0GiB (25.8GB), run=299-344msec\\n\\nroot@TURIN2D24G-2L-500W:\\\\~# ls /ram/exl2/\\n\\nCydonia-v1.3-Magnum-v4-22B-8bpw-h8-exl2  Devstral-Small-2507-8bpw-exl3  Doctor-Shotgun\\\\_ML2-123B-Magnum-Diamond-5.0bpw-exl2\\n\\nHot-loading models into GPUs is possible if you have the right model storage.\\n\\nhttps://preview.redd.it/wrk2dty8jycf1.png?width=1018&amp;format=png&amp;auto=webp&amp;s=9120f9769eabf77614a6785c32ab7f5547fc3fa3\\n\\nEdit to add a pic from TabbyAPI, hot loading Devstral Q8 in just \\\\~4 seconds is fast enough requests from Cline or openwebui is fast enough most requests dont really notice.","edited":1752550165,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n376bs3","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;NVME is great for storing models you are not using right this minute.&lt;/p&gt;\\n\\n&lt;p&gt;For everything else, there is ram tempfs:&lt;/p&gt;\\n\\n&lt;p&gt;root@TURIN2D24G-2L-500W:~# fio --name=readtest --rw=read --bs=2M --ioengine=libaio --numjobs=8 --size=3G --direct=1 --filename=/ram/exl2/test&lt;/p&gt;\\n\\n&lt;p&gt;... snip ...&lt;/p&gt;\\n\\n&lt;p&gt;Run status group 0 (all jobs):&lt;/p&gt;\\n\\n&lt;p&gt;READ: bw=69.8GiB/s (74.9GB/s), 8930MiB/s-10.0GiB/s (9364MB/s-10.8GB/s), io=24.0GiB (25.8GB), run=299-344msec&lt;/p&gt;\\n\\n&lt;p&gt;root@TURIN2D24G-2L-500W:~# ls /ram/exl2/&lt;/p&gt;\\n\\n&lt;p&gt;Cydonia-v1.3-Magnum-v4-22B-8bpw-h8-exl2  Devstral-Small-2507-8bpw-exl3  Doctor-Shotgun_ML2-123B-Magnum-Diamond-5.0bpw-exl2&lt;/p&gt;\\n\\n&lt;p&gt;Hot-loading models into GPUs is possible if you have the right model storage.&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://preview.redd.it/wrk2dty8jycf1.png?width=1018&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9120f9769eabf77614a6785c32ab7f5547fc3fa3\\"&gt;https://preview.redd.it/wrk2dty8jycf1.png?width=1018&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=9120f9769eabf77614a6785c32ab7f5547fc3fa3&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;Edit to add a pic from TabbyAPI, hot loading Devstral Q8 in just ~4 seconds is fast enough requests from Cline or openwebui is fast enough most requests dont really notice.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzx039/nvme_for_local_llm_is_too_slow_any_ideas/n376bs3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752549774,"media_metadata":{"wrk2dty8jycf1":{"status":"valid","e":"Image","m":"image/png","p":[{"y":28,"x":108,"u":"https://preview.redd.it/wrk2dty8jycf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=f9083d4edf0794146dfab1508f7d22dfac3929f9"},{"y":57,"x":216,"u":"https://preview.redd.it/wrk2dty8jycf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=5054dd721dded66d21c38cac22a807ac680472cc"},{"y":84,"x":320,"u":"https://preview.redd.it/wrk2dty8jycf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=dadcf966f93f288335d66749d388ce4c982687a8"},{"y":169,"x":640,"u":"https://preview.redd.it/wrk2dty8jycf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=d3be4a34c28fb3d3c5626f8eba5d66fe49011971"},{"y":254,"x":960,"u":"https://preview.redd.it/wrk2dty8jycf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=b04711c6788a97a01b9a25dcbf3ff6e766c93d3e"}],"s":{"y":270,"x":1018,"u":"https://preview.redd.it/wrk2dty8jycf1.png?width=1018&amp;format=png&amp;auto=webp&amp;s=9120f9769eabf77614a6785c32ab7f5547fc3fa3"},"id":"wrk2dty8jycf1"}},"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lzx039","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n384id9","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"kaisurniwurer","can_mod_post":false,"created_utc":1752566712,"send_replies":true,"parent_id":"t3_1lzx039","score":1,"author_fullname":"t2_qafso","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Linus have some videos on making fast drive reads/writes.\\n\\nBut on the more sane note, You could stack some drives in RAID 0.\\n\\nOr you could run them from the drives with MMAP too, if time is not an issue.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n384id9","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Linus have some videos on making fast drive reads/writes.&lt;/p&gt;\\n\\n&lt;p&gt;But on the more sane note, You could stack some drives in RAID 0.&lt;/p&gt;\\n\\n&lt;p&gt;Or you could run them from the drives with MMAP too, if time is not an issue.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzx039/nvme_for_local_llm_is_too_slow_any_ideas/n384id9/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752566712,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lzx039","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n351vy7","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"false79","can_mod_post":false,"created_utc":1752524691,"send_replies":true,"parent_id":"t3_1lzx039","score":-3,"author_fullname":"t2_wn888","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Not a problem when you have 256GB or 512GB VRAM on a Mac Studio M3 so you almost never have to unload. Although with only 80 GPU cores, it's infers as fast as a 4070 I believe. Over the last 8 weeks, there was stock of refurb mac studios. They were selling slow do their price tag but eventually sold out, even the 16TB SSD one.\\n\\nIf you still want to use Windows, there is software out there that will convert your RAM into temporary storage so that you go from RAM to RAM instead of Disk to RAM. It would be 25x faster for the loading part into ram. But from RAM to GPU VRAM, that would be limited to your PCIe's available bandwidth.\\n\\nI like [https://www.softperfect.com/products/ramdisk/](https://www.softperfect.com/products/ramdisk/)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n351vy7","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Not a problem when you have 256GB or 512GB VRAM on a Mac Studio M3 so you almost never have to unload. Although with only 80 GPU cores, it&amp;#39;s infers as fast as a 4070 I believe. Over the last 8 weeks, there was stock of refurb mac studios. They were selling slow do their price tag but eventually sold out, even the 16TB SSD one.&lt;/p&gt;\\n\\n&lt;p&gt;If you still want to use Windows, there is software out there that will convert your RAM into temporary storage so that you go from RAM to RAM instead of Disk to RAM. It would be 25x faster for the loading part into ram. But from RAM to GPU VRAM, that would be limited to your PCIe&amp;#39;s available bandwidth.&lt;/p&gt;\\n\\n&lt;p&gt;I like &lt;a href=\\"https://www.softperfect.com/products/ramdisk/\\"&gt;https://www.softperfect.com/products/ramdisk/&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzx039/nvme_for_local_llm_is_too_slow_any_ideas/n351vy7/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752524691,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lzx039","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-3}}],"before":null}}]`),r=()=>e.jsx(l,{data:t});export{r as default};
