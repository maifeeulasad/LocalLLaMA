import{j as e}from"./index-BlGsFJYy.js";import{R as t}from"./RedditPostRenderer-B6uvq_Zl.js";import"./index-DDvVtNwD.js";const n=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Hi folks â€” hoping someone can help me finally crack this.\\n\\nIâ€™m trying to run Open WebUI (ghcr.io/open-webui/open-webui:main) via Docker on my Windows machine, connected to a locally running Ollama server, but the WebUI refuses to show up in the browser.\\n\\n\\n---\\n\\nðŸ› ï¸ Setup Details\\n\\nOS: Windows 11 using Docker Desktop (WSL2 backend)\\n\\nDocker version: 28.3.0\\n\\nGPU: NVIDIA RTX 5070 (12GB VRAM)\\n\\nOllama version: v0.9.6 (running fine locally)\\n\\n\\nContainer creation:\\n\\ndocker run -d ^\\n  --name open-webui ^\\n  -p 3000:3000 ^\\n  -e OLLAMA_API_BASE_URL=http://&lt;my-local-ip&gt;:11434 ^\\n  -v open-webui-data:/app/backend/data ^\\n  ghcr.io/open-webui/open-webui:main\\n\\n(I've replaced &lt;my-local-ip&gt; with the correct IPv4 address under vEthernet (WSL) adapter.)\\n\\n\\n---\\n\\nâœ… Whatâ€™s Working\\n\\nOllama is running fine on 127.0.0.1:11434\\n\\nDocker container starts with status healthy\\n\\ndocker logs shows:\\n\\nFetching 30 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| ...\\nINFO: Started server process [1]\\nINFO: Waiting for application startup.\\n\\nNo networking conflicts â€” port 3000 is clean\\n\\ndocker exec works fine â€” shell is responsive\\n\\nUsing either GUI or CLI to spin up containers results in same behavior\\n\\n\\n\\n---\\n\\nâŒ Whatâ€™s Not Working\\n\\nOpen WebUI never finishes startup\\nIt just hangs at Waiting for application startup forever.\\n\\nNothing loads in the browser â€” localhost:3000 and 127.0.0.1:3000 are dead\\n\\ncurl inside the container returns:\\n\\ncurl: (7) Failed to connect to host.docker.internal port 11434\\n\\nConfirmed no outbound firewall issues\\n\\nNo fatal container errors or restarts â€” just stalls\\n\\n\\n\\n---\\n\\nðŸ§ª What Iâ€™ve Tried\\n\\nRunning ollama serve before container spin-up âœ…\\n\\nUsing host.docker.internal vs direct IP âœ…\\n\\nRebuilt container from scratch (images, volumes reset) âœ…\\n\\nDocker Desktop GUI and CLI methods âœ…\\n\\nChecked for GPU resource bottlenecks â€” nothing out of ordinary\\n\\nSearched GitHub issues &amp; Discord â€” found similar stuck states but no resolution yet\\n\\n\\n\\n---\\n\\nâ“My Ask\\n\\nWhatâ€™s the cause of this startup stall?\\nIf the container is healthy, ports are exposed, and Ollama is live, why wonâ€™t Open WebUI move past initialization or respond at localhost:3000?\\n\\n----\\n\\nIâ€™ll happily provide logs, configs, or compose files if needed â€” thanks in advance!","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"ðŸš¨ Docker container stuck on â€œWaiting for application startupâ€ â€” Open WebUI wonâ€™t load in browser","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1m12ij7","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.5,"author_flair_background_color":null,"subreddit_type":"public","ups":0,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_s5bwwi1o","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":0,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1752637310,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Hi folks â€” hoping someone can help me finally crack this.&lt;/p&gt;\\n\\n&lt;p&gt;Iâ€™m trying to run Open WebUI (ghcr.io/open-webui/open-webui:main) via Docker on my Windows machine, connected to a locally running Ollama server, but the WebUI refuses to show up in the browser.&lt;/p&gt;\\n\\n&lt;hr/&gt;\\n\\n&lt;p&gt;ðŸ› ï¸ Setup Details&lt;/p&gt;\\n\\n&lt;p&gt;OS: Windows 11 using Docker Desktop (WSL2 backend)&lt;/p&gt;\\n\\n&lt;p&gt;Docker version: 28.3.0&lt;/p&gt;\\n\\n&lt;p&gt;GPU: NVIDIA RTX 5070 (12GB VRAM)&lt;/p&gt;\\n\\n&lt;p&gt;Ollama version: v0.9.6 (running fine locally)&lt;/p&gt;\\n\\n&lt;p&gt;Container creation:&lt;/p&gt;\\n\\n&lt;p&gt;docker run -d ^\\n  --name open-webui ^\\n  -p 3000:3000 ^\\n  -e OLLAMA_API_BASE_URL=http://&amp;lt;my-local-ip&amp;gt;:11434 ^\\n  -v open-webui-data:/app/backend/data ^\\n  ghcr.io/open-webui/open-webui:main&lt;/p&gt;\\n\\n&lt;p&gt;(I&amp;#39;ve replaced &amp;lt;my-local-ip&amp;gt; with the correct IPv4 address under vEthernet (WSL) adapter.)&lt;/p&gt;\\n\\n&lt;hr/&gt;\\n\\n&lt;p&gt;âœ… Whatâ€™s Working&lt;/p&gt;\\n\\n&lt;p&gt;Ollama is running fine on 127.0.0.1:11434&lt;/p&gt;\\n\\n&lt;p&gt;Docker container starts with status healthy&lt;/p&gt;\\n\\n&lt;p&gt;docker logs shows:&lt;/p&gt;\\n\\n&lt;p&gt;Fetching 30 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| ...\\nINFO: Started server process [1]\\nINFO: Waiting for application startup.&lt;/p&gt;\\n\\n&lt;p&gt;No networking conflicts â€” port 3000 is clean&lt;/p&gt;\\n\\n&lt;p&gt;docker exec works fine â€” shell is responsive&lt;/p&gt;\\n\\n&lt;p&gt;Using either GUI or CLI to spin up containers results in same behavior&lt;/p&gt;\\n\\n&lt;hr/&gt;\\n\\n&lt;p&gt;âŒ Whatâ€™s Not Working&lt;/p&gt;\\n\\n&lt;p&gt;Open WebUI never finishes startup\\nIt just hangs at Waiting for application startup forever.&lt;/p&gt;\\n\\n&lt;p&gt;Nothing loads in the browser â€” localhost:3000 and 127.0.0.1:3000 are dead&lt;/p&gt;\\n\\n&lt;p&gt;curl inside the container returns:&lt;/p&gt;\\n\\n&lt;p&gt;curl: (7) Failed to connect to host.docker.internal port 11434&lt;/p&gt;\\n\\n&lt;p&gt;Confirmed no outbound firewall issues&lt;/p&gt;\\n\\n&lt;p&gt;No fatal container errors or restarts â€” just stalls&lt;/p&gt;\\n\\n&lt;hr/&gt;\\n\\n&lt;p&gt;ðŸ§ª What Iâ€™ve Tried&lt;/p&gt;\\n\\n&lt;p&gt;Running ollama serve before container spin-up âœ…&lt;/p&gt;\\n\\n&lt;p&gt;Using host.docker.internal vs direct IP âœ…&lt;/p&gt;\\n\\n&lt;p&gt;Rebuilt container from scratch (images, volumes reset) âœ…&lt;/p&gt;\\n\\n&lt;p&gt;Docker Desktop GUI and CLI methods âœ…&lt;/p&gt;\\n\\n&lt;p&gt;Checked for GPU resource bottlenecks â€” nothing out of ordinary&lt;/p&gt;\\n\\n&lt;p&gt;Searched GitHub issues &amp;amp; Discord â€” found similar stuck states but no resolution yet&lt;/p&gt;\\n\\n&lt;hr/&gt;\\n\\n&lt;p&gt;â“My Ask&lt;/p&gt;\\n\\n&lt;p&gt;Whatâ€™s the cause of this startup stall?\\nIf the container is healthy, ports are exposed, and Ollama is live, why wonâ€™t Open WebUI move past initialization or respond at localhost:3000?&lt;/p&gt;\\n\\n&lt;hr/&gt;\\n\\n&lt;p&gt;Iâ€™ll happily provide logs, configs, or compose files if needed â€” thanks in advance!&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":true,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1m12ij7","is_robot_indexable":true,"num_duplicates":1,"report_reasons":null,"author":"0nlyAxeman","discussion_type":null,"num_comments":2,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1m12ij7/docker_container_stuck_on_waiting_for_application/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1m12ij7/docker_container_stuck_on_waiting_for_application/","subreddit_subscribers":499774,"created_utc":1752637310,"num_crossposts":1,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3es6pe","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"0nlyAxeman","can_mod_post":false,"created_utc":1752652904,"send_replies":true,"parent_id":"t1_n3eklou","score":1,"author_fullname":"t2_s5bwwi1o","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yep, definitely will skip the docker part it's been a pain in the back already. Thanks for the advice, I tried  fixing docker with Chatgpt but it's just not worth the time anymore. I'm gonna go your route next time. I'll update if it works","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3es6pe","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yep, definitely will skip the docker part it&amp;#39;s been a pain in the back already. Thanks for the advice, I tried  fixing docker with Chatgpt but it&amp;#39;s just not worth the time anymore. I&amp;#39;m gonna go your route next time. I&amp;#39;ll update if it works&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m12ij7","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m12ij7/docker_container_stuck_on_waiting_for_application/n3es6pe/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752652904,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3eklou","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Marksta","can_mod_post":false,"created_utc":1752648660,"send_replies":true,"parent_id":"t3_1m12ij7","score":0,"author_fullname":"t2_559a1","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Why docker? You can just run it directly on Windows. Clone the repo, use uv to create a new python env, pip install -r the requirements in backend folder, then they even put a bat file for you in there to run on windows. It's simpler than figuring out WSL Unix docker virtualization ip network routing to run a python program. Not even chatGPT can figure that non sense out.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3eklou","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Why docker? You can just run it directly on Windows. Clone the repo, use uv to create a new python env, pip install -r the requirements in backend folder, then they even put a bat file for you in there to run on windows. It&amp;#39;s simpler than figuring out WSL Unix docker virtualization ip network routing to run a python program. Not even chatGPT can figure that non sense out.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m12ij7/docker_container_stuck_on_waiting_for_application/n3eklou/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752648660,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m12ij7","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}}]`),r=()=>e.jsx(t,{data:n});export{r as default};
