import{j as l}from"./index-C9o7w-KS.js";import{R as e}from"./RedditPostRenderer-C4RcPiw4.js";import"./index-CvnyzE1l.js";const a=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Hi folks!\\n\\nSince the launch of Hunyuan-A13B, I’ve been struggling to get it running on an RTX 5090 with 32 GB of RAM. The official Docker images from Tencent don’t seem to be compatible with the Blackwell architecture. I even tried building vLLM from source via \`git clone\`, but no luck either.\\n\\nAny hints?","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"How to run Hunyuan-A13B on a RTX 5090 / Blackwell ?","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lohzzj","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.6,"author_flair_background_color":null,"subreddit_type":"public","ups":4,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_dyvrh","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":4,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1751314583,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Hi folks!&lt;/p&gt;\\n\\n&lt;p&gt;Since the launch of Hunyuan-A13B, I’ve been struggling to get it running on an RTX 5090 with 32 GB of RAM. The official Docker images from Tencent don’t seem to be compatible with the Blackwell architecture. I even tried building vLLM from source via &lt;code&gt;git clone&lt;/code&gt;, but no luck either.&lt;/p&gt;\\n\\n&lt;p&gt;Any hints?&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1lohzzj","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"celsowm","discussion_type":null,"num_comments":10,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lohzzj/how_to_run_hunyuana13b_on_a_rtx_5090_blackwell/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lohzzj/how_to_run_hunyuana13b_on_a_rtx_5090_blackwell/","subreddit_subscribers":493242,"created_utc":1751314583,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0n6oa7","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"celsowm","can_mod_post":false,"created_utc":1751316466,"send_replies":true,"parent_id":"t1_n0n620w","score":-1,"author_fullname":"t2_dyvrh","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I've gptq too but no luck","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0n6oa7","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;ve gptq too but no luck&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lohzzj","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lohzzj/how_to_run_hunyuana13b_on_a_rtx_5090_blackwell/n0n6oa7/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751316466,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0n620w","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Glittering-Bag-4662","can_mod_post":false,"created_utc":1751316286,"send_replies":true,"parent_id":"t3_1lohzzj","score":11,"author_fullname":"t2_10rvna3i1t","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I think you gotta wait for llama.cpp for support, esp cause the full model is still too large for a 5090 so q4 ggufs prob","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0n620w","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I think you gotta wait for llama.cpp for support, esp cause the full model is still too large for a 5090 so q4 ggufs prob&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lohzzj/how_to_run_hunyuana13b_on_a_rtx_5090_blackwell/n0n620w/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751316286,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lohzzj","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":11}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0n92nl","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"1ncehost","can_mod_post":false,"created_utc":1751317153,"send_replies":true,"parent_id":"t3_1lohzzj","score":5,"author_fullname":"t2_lrannsv","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"New architecture so you have to wait until its implemented.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0n92nl","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;New architecture so you have to wait until its implemented.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lohzzj/how_to_run_hunyuana13b_on_a_rtx_5090_blackwell/n0n92nl/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751317153,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lohzzj","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0o5q4u","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"FullOf_Bad_Ideas","can_mod_post":false,"created_utc":1751327545,"send_replies":true,"parent_id":"t3_1lohzzj","score":5,"author_fullname":"t2_9s7pmakgx","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"SGLang 0.4.8.post1 supports it, but I doubt you would squezze it into 32GB of VRAM, even with GPTQ quant.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0o5q4u","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;SGLang 0.4.8.post1 supports it, but I doubt you would squezze it into 32GB of VRAM, even with GPTQ quant.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lohzzj/how_to_run_hunyuana13b_on_a_rtx_5090_blackwell/n0o5q4u/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751327545,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lohzzj","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0p0qdd","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Double_Cause4609","can_mod_post":false,"created_utc":1751338604,"send_replies":true,"parent_id":"t3_1lohzzj","score":4,"author_fullname":"t2_1kubzxt2ww","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"...\\n\\nYou want to run an 80B parameter model...On a 32GB device?\\n\\nAt 4bit, you're looking at 40GB, without factoring in attention or KV caching.\\n\\nGranted, things like hybrid inference (using CPU for the experts, GPU for Attention) is possible, but you'd be looking at KTransformers or LlamaCPP for that, to the best of my knowledge. The performance would probably be fairly decent, too.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0p0qdd","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;...&lt;/p&gt;\\n\\n&lt;p&gt;You want to run an 80B parameter model...On a 32GB device?&lt;/p&gt;\\n\\n&lt;p&gt;At 4bit, you&amp;#39;re looking at 40GB, without factoring in attention or KV caching.&lt;/p&gt;\\n\\n&lt;p&gt;Granted, things like hybrid inference (using CPU for the experts, GPU for Attention) is possible, but you&amp;#39;d be looking at KTransformers or LlamaCPP for that, to the best of my knowledge. The performance would probably be fairly decent, too.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lohzzj/how_to_run_hunyuana13b_on_a_rtx_5090_blackwell/n0p0qdd/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751338604,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lohzzj","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0oy220","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"townofsalemfangay","can_mod_post":false,"created_utc":1751337599,"send_replies":true,"parent_id":"t3_1lohzzj","score":3,"author_fullname":"t2_122dsg","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Just change the docker YAML config so the device\\\\_map is set to \\"auto\\".. or skip it and run natively. You can definitely offload to RAM or CPU in Python; it works, but the performance is garbage.\\n\\nI’ve done it before: ran Hunyuan-large with DeepSpeed, parallelising across multiple CUDA nodes on my network. It technically works, just don’t expect miracles.\\n\\nAlternatively, wait for llamacpp to support it properly. That’ll be way cleaner.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0oy220","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Just change the docker YAML config so the device_map is set to &amp;quot;auto&amp;quot;.. or skip it and run natively. You can definitely offload to RAM or CPU in Python; it works, but the performance is garbage.&lt;/p&gt;\\n\\n&lt;p&gt;I’ve done it before: ran Hunyuan-large with DeepSpeed, parallelising across multiple CUDA nodes on my network. It technically works, just don’t expect miracles.&lt;/p&gt;\\n\\n&lt;p&gt;Alternatively, wait for llamacpp to support it properly. That’ll be way cleaner.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lohzzj/how_to_run_hunyuana13b_on_a_rtx_5090_blackwell/n0oy220/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751337599,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lohzzj","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0ntk8l","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"jacek2023","can_mod_post":false,"created_utc":1751323542,"send_replies":true,"parent_id":"t3_1lohzzj","score":2,"author_fullname":"t2_vqgbql9w","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"you can try WIP version\\n\\n[https://github.com/ggml-org/llama.cpp/issues/14415](https://github.com/ggml-org/llama.cpp/issues/14415)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0ntk8l","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;you can try WIP version&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://github.com/ggml-org/llama.cpp/issues/14415\\"&gt;https://github.com/ggml-org/llama.cpp/issues/14415&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lohzzj/how_to_run_hunyuana13b_on_a_rtx_5090_blackwell/n0ntk8l/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751323542,"author_flair_text":"llama.cpp","treatment_tags":[],"link_id":"t3_1lohzzj","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0njny7","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"terminoid_","can_mod_post":false,"created_utc":1751320331,"send_replies":true,"parent_id":"t3_1lohzzj","score":1,"author_fullname":"t2_1iu07dnz2i","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"read the model card, there's example inference code using transformers","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0njny7","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;read the model card, there&amp;#39;s example inference code using transformers&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lohzzj/how_to_run_hunyuana13b_on_a_rtx_5090_blackwell/n0njny7/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751320331,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lohzzj","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0o2sy1","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Spiritual_Tie_5574","can_mod_post":false,"created_utc":1751326581,"send_replies":true,"parent_id":"t3_1lohzzj","score":1,"author_fullname":"t2_h755hoap","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Use the nightly version of pytorch","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0o2sy1","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Use the nightly version of pytorch&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lohzzj/how_to_run_hunyuana13b_on_a_rtx_5090_blackwell/n0o2sy1/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751326581,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lohzzj","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0q0wnx","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"b3081a","can_mod_post":false,"created_utc":1751356334,"send_replies":true,"parent_id":"t3_1lohzzj","score":1,"author_fullname":"t2_17n5yh7l","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"llama.cpp has a near-completion PR pending merge now, so it should be rather soon. It's an MoE model so you'll be able to take advantage of \`-ot\` in llama.cpp to offload experts in a few layers to CPU without losing too much performance.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0q0wnx","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;llama.cpp has a near-completion PR pending merge now, so it should be rather soon. It&amp;#39;s an MoE model so you&amp;#39;ll be able to take advantage of &lt;code&gt;-ot&lt;/code&gt; in llama.cpp to offload experts in a few layers to CPU without losing too much performance.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lohzzj/how_to_run_hunyuana13b_on_a_rtx_5090_blackwell/n0q0wnx/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751356334,"author_flair_text":"llama.cpp","treatment_tags":[],"link_id":"t3_1lohzzj","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),n=()=>l.jsx(e,{data:a});export{n as default};
