import{j as l}from"./index-CqAPCjw5.js";import{R as e}from"./RedditPostRenderer-4oBDAtGr.js";import"./index-D3Sdy_Op.js";const t=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Hey r/LocalLLaMA!As a web dev tinkering with local AI, I created Local AI Monster: A React app using MLC's WebLLM and WebGPU to run quantized Instruct models (e.g., Llama-3-8B, Phi-3-mini-4k, Gemma-2-9B) entirely client-side. No installs, no serversâ€”just open in Chrome/Edge and chat.Key Features:\\n\\n* Auto-Detect VRAM &amp; Models: Estimates your GPU memory, picks the best fit from Hugging Face MLC models (fallbacks for low VRAM).\\n* Chat Perks: Multi-chats, local storage, temperature/max tokens controls, streaming responses with markdown and code highlighting (Shiki).\\n* Privacy: Fully local, no data outbound.\\n* Performance: Loads in \\\\~30-60s on mid-range GPUs, generates 15-30 tokens/sec depending on hardware.\\n\\nIdeal for quick tests or coding help without heavy tools.Get StartedOpen-source on GitHub: [https://github.com/ShadovvBeast/local-ai-monster](https://github.com/ShadovvBeast/local-ai-monster) (MITâ€”fork/PRs welcome!).\\n\\nYou're welcome to try it at [https://localai.monster/](https://localai.monster/)\\n\\nFeedback?\\n\\n* Runs on your setup? (Share VRAM/speed!)\\n* Model/feature ideas?\\n* Comparisons to your workflows?\\n\\nLet's make browser AI better!","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Introducing Local AI Monster: Run Powerful LLMs Right in Your Browser ðŸš€","link_flair_richtext":[{"e":"text","t":"Resources"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lxd7ki","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.83,"author_flair_background_color":null,"subreddit_type":"public","ups":4,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_erroa","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Resources","can_mod_post":false,"score":4,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1752255513,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Hey &lt;a href=\\"/r/LocalLLaMA\\"&gt;r/LocalLLaMA&lt;/a&gt;!As a web dev tinkering with local AI, I created Local AI Monster: A React app using MLC&amp;#39;s WebLLM and WebGPU to run quantized Instruct models (e.g., Llama-3-8B, Phi-3-mini-4k, Gemma-2-9B) entirely client-side. No installs, no serversâ€”just open in Chrome/Edge and chat.Key Features:&lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;Auto-Detect VRAM &amp;amp; Models: Estimates your GPU memory, picks the best fit from Hugging Face MLC models (fallbacks for low VRAM).&lt;/li&gt;\\n&lt;li&gt;Chat Perks: Multi-chats, local storage, temperature/max tokens controls, streaming responses with markdown and code highlighting (Shiki).&lt;/li&gt;\\n&lt;li&gt;Privacy: Fully local, no data outbound.&lt;/li&gt;\\n&lt;li&gt;Performance: Loads in ~30-60s on mid-range GPUs, generates 15-30 tokens/sec depending on hardware.&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;p&gt;Ideal for quick tests or coding help without heavy tools.Get StartedOpen-source on GitHub: &lt;a href=\\"https://github.com/ShadovvBeast/local-ai-monster\\"&gt;https://github.com/ShadovvBeast/local-ai-monster&lt;/a&gt; (MITâ€”fork/PRs welcome!).&lt;/p&gt;\\n\\n&lt;p&gt;You&amp;#39;re welcome to try it at &lt;a href=\\"https://localai.monster/\\"&gt;https://localai.monster/&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;Feedback?&lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;Runs on your setup? (Share VRAM/speed!)&lt;/li&gt;\\n&lt;li&gt;Model/feature ideas?&lt;/li&gt;\\n&lt;li&gt;Comparisons to your workflows?&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;p&gt;Let&amp;#39;s make browser AI better!&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#ccac2b","id":"1lxd7ki","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"ShadovvBeast","discussion_type":null,"num_comments":5,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lxd7ki/introducing_local_ai_monster_run_powerful_llms/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lxd7ki/introducing_local_ai_monster_run_powerful_llms/","subreddit_subscribers":497825,"created_utc":1752255513,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2lh3nl","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Dangerous-Yak3976","can_mod_post":false,"created_utc":1752258974,"send_replies":true,"parent_id":"t3_1lxd7ki","score":1,"author_fullname":"t2_5vmxq95xe","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I wouldn't call \\"AI Monster\\" something that can barely run Llama3-8B.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2lh3nl","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I wouldn&amp;#39;t call &amp;quot;AI Monster&amp;quot; something that can barely run Llama3-8B.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxd7ki/introducing_local_ai_monster_run_powerful_llms/n2lh3nl/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752258974,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lxd7ki","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2mcguw","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"un_passant","can_mod_post":false,"created_utc":1752268279,"send_replies":true,"parent_id":"t1_n2lzee5","score":1,"author_fullname":"t2_7rqtc","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Privacy Badger is triggered, btwâ€¦","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2mcguw","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Privacy Badger is triggered, btwâ€¦&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lxd7ki","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxd7ki/introducing_local_ai_monster_run_powerful_llms/n2mcguw/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752268279,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2lzee5","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Scott_Tx","can_mod_post":false,"created_utc":1752264408,"send_replies":true,"parent_id":"t3_1lxd7ki","score":1,"author_fullname":"t2_l07bb","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Why would I want to run it in my browser? Sounds like a good way to leak private data.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2lzee5","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Why would I want to run it in my browser? Sounds like a good way to leak private data.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxd7ki/introducing_local_ai_monster_run_powerful_llms/n2lzee5/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752264408,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lxd7ki","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2mw7lc","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ShadovvBeast","can_mod_post":false,"created_utc":1752274707,"send_replies":true,"parent_id":"t1_n2mcban","score":1,"author_fullname":"t2_erroa","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"What are you using to run it?  \\nIt depends on your GPU, it attempts to find the best fit for your GPU","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2mw7lc","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;What are you using to run it?&lt;br/&gt;\\nIt depends on your GPU, it attempts to find the best fit for your GPU&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lxd7ki","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxd7ki/introducing_local_ai_monster_run_powerful_llms/n2mw7lc/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752274707,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2mcban","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"un_passant","can_mod_post":false,"created_utc":1752268231,"send_replies":true,"parent_id":"t3_1lxd7ki","score":1,"author_fullname":"t2_7rqtc","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"For how long should I wait while it prints \\"Estimating VRAM...\\" ?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2mcban","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;For how long should I wait while it prints &amp;quot;Estimating VRAM...&amp;quot; ?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxd7ki/introducing_local_ai_monster_run_powerful_llms/n2mcban/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752268231,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lxd7ki","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),n=()=>l.jsx(e,{data:t});export{n as default};
