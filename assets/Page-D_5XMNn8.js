import{j as e}from"./index-BlGsFJYy.js";import{R as l}from"./RedditPostRenderer-B6uvq_Zl.js";import"./index-DDvVtNwD.js";const a=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Paying for Claude max and using cloud models makes me depressed, because I know there is zero privacy. \\n\\nOn the other hand, I'd have to pay $10k+ to get a slow version of q4 deepseek to run. So what choice do I have?\\n\\nIs there realistically any alternative?","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Do you use local LLMs for work over cloud models? Why/how?","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lw0138","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.33,"author_flair_background_color":null,"subreddit_type":"public","ups":0,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_1bl579qtd6","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":0,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1752110318,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Paying for Claude max and using cloud models makes me depressed, because I know there is zero privacy. &lt;/p&gt;\\n\\n&lt;p&gt;On the other hand, I&amp;#39;d have to pay $10k+ to get a slow version of q4 deepseek to run. So what choice do I have?&lt;/p&gt;\\n\\n&lt;p&gt;Is there realistically any alternative?&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":true,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1lw0138","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"TumbleweedDeep825","discussion_type":null,"num_comments":26,"send_replies":false,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lw0138/do_you_use_local_llms_for_work_over_cloud_models/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lw0138/do_you_use_local_llms_for_work_over_cloud_models/","subreddit_subscribers":497354,"created_utc":1752110318,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2b37jb","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"National_Meeting_749","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2axsxz","score":1,"author_fullname":"t2_drm5tg5d","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I don't code, and definitely prefer a GUI, though I will use CLI. \\nI run between different software for different things, but currently I'm running LMstudio backend and then AnythingLLM frontend. \\n\\nI use AI for text editing and creative writing/world building. \\nText editing and generation. \\nAre my main uses.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2b37jb","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I don&amp;#39;t code, and definitely prefer a GUI, though I will use CLI. \\nI run between different software for different things, but currently I&amp;#39;m running LMstudio backend and then AnythingLLM frontend. &lt;/p&gt;\\n\\n&lt;p&gt;I use AI for text editing and creative writing/world building. \\nText editing and generation. \\nAre my main uses.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lw0138","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lw0138/do_you_use_local_llms_for_work_over_cloud_models/n2b37jb/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752121581,"author_flair_text":null,"treatment_tags":[],"created_utc":1752121581,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2axsxz","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"eleqtriq","can_mod_post":false,"created_utc":1752119291,"send_replies":true,"parent_id":"t1_n2ae6gy","score":1,"author_fullname":"t2_66vte","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Curious on how you use local models?  IDE+extension, CLI?  Taking a small survey.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2axsxz","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Curious on how you use local models?  IDE+extension, CLI?  Taking a small survey.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lw0138","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lw0138/do_you_use_local_llms_for_work_over_cloud_models/n2axsxz/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752119291,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2deg4k","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ag789","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2cpsof","score":1,"author_fullname":"t2_13x892","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"the models are only as good as what they are made of, or they'd hallucinate or give nothing.  \\nand sometimes nothing is better","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2deg4k","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;the models are only as good as what they are made of, or they&amp;#39;d hallucinate or give nothing.&lt;br/&gt;\\nand sometimes nothing is better&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lw0138","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lw0138/do_you_use_local_llms_for_work_over_cloud_models/n2deg4k/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752158612,"author_flair_text":null,"treatment_tags":[],"created_utc":1752158612,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2cpsof","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"National_Meeting_749","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2bydp2","score":1,"author_fullname":"t2_drm5tg5d","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Oh. Trust me. \\nI have. \\n\\nI've tried the abliterated models too, I don't like them. \\nGive me a full uncensored or nothing.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2cpsof","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Oh. Trust me. \\nI have. &lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;ve tried the abliterated models too, I don&amp;#39;t like them. \\nGive me a full uncensored or nothing.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lw0138","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lw0138/do_you_use_local_llms_for_work_over_cloud_models/n2cpsof/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752150860,"author_flair_text":null,"treatment_tags":[],"created_utc":1752150860,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2bydp2","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ag789","can_mod_post":false,"created_utc":1752138009,"send_replies":true,"parent_id":"t1_n2ae6gy","score":1,"author_fullname":"t2_13x892","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"for 'spicy'  prompts, you can check the various (uncensored) models,\\n\\n[https://ollama.com/search](https://ollama.com/search)\\n\\nthey run locally","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2bydp2","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;for &amp;#39;spicy&amp;#39;  prompts, you can check the various (uncensored) models,&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://ollama.com/search\\"&gt;https://ollama.com/search&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;they run locally&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lw0138","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lw0138/do_you_use_local_llms_for_work_over_cloud_models/n2bydp2/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752138009,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2ghr5y","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"National_Meeting_749","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2f5qf7","score":1,"author_fullname":"t2_drm5tg5d","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"What size models are you looking for?","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2ghr5y","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;What size models are you looking for?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lw0138","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lw0138/do_you_use_local_llms_for_work_over_cloud_models/n2ghr5y/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752190962,"author_flair_text":null,"treatment_tags":[],"created_utc":1752190962,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2f5qf7","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Huge-Masterpiece-824","can_mod_post":false,"created_utc":1752176307,"send_replies":true,"parent_id":"t1_n2ae6gy","score":1,"author_fullname":"t2_b4iipfel","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I have a few local models running off ollama + sillytavern. Mostly using them for documentation and DnD atm. My group want to have the npc “spicier” do you have any recommendation to where to look? \\n\\nI navigate the normal space pretty well since I work with CV, but anything spicy is like forbidden land to me.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2f5qf7","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I have a few local models running off ollama + sillytavern. Mostly using them for documentation and DnD atm. My group want to have the npc “spicier” do you have any recommendation to where to look? &lt;/p&gt;\\n\\n&lt;p&gt;I navigate the normal space pretty well since I work with CV, but anything spicy is like forbidden land to me.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lw0138","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lw0138/do_you_use_local_llms_for_work_over_cloud_models/n2f5qf7/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752176307,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2ae6gy","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"National_Meeting_749","can_mod_post":false,"created_utc":1752111941,"send_replies":true,"parent_id":"t3_1lw0138","score":3,"author_fullname":"t2_drm5tg5d","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I want more control. Plus I don't want every spicy question I ask to be saved on openAI's server forever. \\n\\nI can use it unlimited for no monthly fee besides my power bill. \\n\\nI can try jailbreaks and not risk getting banned. \\n\\nSpicy content without being milked for money/risking a ban. \\n\\nVariety of models. \\n\\nYou also don't actually need 10k to get a decent quant of deepseek running at decent speed. If you have the right (mainly software) stack you can get deepseek running on a more realistic setup. \\n\\nA lot of what you do with an LLM can be done on MUCH smaller models than a full deepseek. On simple agentic tasks, a Qwen 3 4b is more than capable enough to","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2ae6gy","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I want more control. Plus I don&amp;#39;t want every spicy question I ask to be saved on openAI&amp;#39;s server forever. &lt;/p&gt;\\n\\n&lt;p&gt;I can use it unlimited for no monthly fee besides my power bill. &lt;/p&gt;\\n\\n&lt;p&gt;I can try jailbreaks and not risk getting banned. &lt;/p&gt;\\n\\n&lt;p&gt;Spicy content without being milked for money/risking a ban. &lt;/p&gt;\\n\\n&lt;p&gt;Variety of models. &lt;/p&gt;\\n\\n&lt;p&gt;You also don&amp;#39;t actually need 10k to get a decent quant of deepseek running at decent speed. If you have the right (mainly software) stack you can get deepseek running on a more realistic setup. &lt;/p&gt;\\n\\n&lt;p&gt;A lot of what you do with an LLM can be done on MUCH smaller models than a full deepseek. On simple agentic tasks, a Qwen 3 4b is more than capable enough to&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lw0138/do_you_use_local_llms_for_work_over_cloud_models/n2ae6gy/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752111941,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lw0138","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2brfgn","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Professional-Bear857","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2acc6p","score":1,"author_fullname":"t2_yrl9ztfsa","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"There's decent coding models that you can run locally on 24gb of vram, with occasional use of deepseek R1 or similar when needed. Models like qwq32b, qwen3 32b, and smaller 14b models can also work well depending on the level of complexity needed.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2brfgn","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;There&amp;#39;s decent coding models that you can run locally on 24gb of vram, with occasional use of deepseek R1 or similar when needed. Models like qwq32b, qwen3 32b, and smaller 14b models can also work well depending on the level of complexity needed.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lw0138","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lw0138/do_you_use_local_llms_for_work_over_cloud_models/n2brfgn/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752133957,"author_flair_text":null,"treatment_tags":[],"created_utc":1752133957,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2acc6p","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"TumbleweedDeep825","can_mod_post":false,"created_utc":1752111294,"send_replies":true,"parent_id":"t1_n2ac0dn","score":2,"author_fullname":"t2_1bl579qtd6","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"automate basic coding tasks with claude code.\\n\\ngenerate code / fixes that gets 80% of the way there. I manually review and fix the rest. \\n\\nspam commands / tasks in CC and it executes in sequential order.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2acc6p","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;automate basic coding tasks with claude code.&lt;/p&gt;\\n\\n&lt;p&gt;generate code / fixes that gets 80% of the way there. I manually review and fix the rest. &lt;/p&gt;\\n\\n&lt;p&gt;spam commands / tasks in CC and it executes in sequential order.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lw0138","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lw0138/do_you_use_local_llms_for_work_over_cloud_models/n2acc6p/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752111294,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n2ac0dn","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"jacek2023","can_mod_post":false,"created_utc":1752111180,"send_replies":true,"parent_id":"t3_1lw0138","score":2,"author_fullname":"t2_vqgbql9w","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"What is your use case? Why do you need LLM?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2ac0dn","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;What is your use case? Why do you need LLM?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lw0138/do_you_use_local_llms_for_work_over_cloud_models/n2ac0dn/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752111180,"author_flair_text":"llama.cpp","treatment_tags":[],"link_id":"t3_1lw0138","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2ar9fg","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ttkciar","can_mod_post":false,"created_utc":1752116667,"send_replies":true,"parent_id":"t3_1lw0138","score":2,"author_fullname":"t2_cpegz","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"You can always trade off speed for cost.\\n\\nI have some old dual-Xeon servers, originally bought for GEANT4 and Rocstar, which have 256GB of memory.\\n\\nThey only set me back $800 and will infer Tulu3-70B or Qwen2.5-VL-72B slowly (about 0.8 tokens/second), and Tulu3-405B ***very*** slowly (about 0.15 tokens/second).\\n\\nMost of the time, though, I use Gemma3-27B or Phi-4-25B, which will fit in my MI60's VRAM.\\n\\nAn MI60 will set you back about $500 (on eBay).","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2ar9fg","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You can always trade off speed for cost.&lt;/p&gt;\\n\\n&lt;p&gt;I have some old dual-Xeon servers, originally bought for GEANT4 and Rocstar, which have 256GB of memory.&lt;/p&gt;\\n\\n&lt;p&gt;They only set me back $800 and will infer Tulu3-70B or Qwen2.5-VL-72B slowly (about 0.8 tokens/second), and Tulu3-405B &lt;strong&gt;&lt;em&gt;very&lt;/em&gt;&lt;/strong&gt; slowly (about 0.15 tokens/second).&lt;/p&gt;\\n\\n&lt;p&gt;Most of the time, though, I use Gemma3-27B or Phi-4-25B, which will fit in my MI60&amp;#39;s VRAM.&lt;/p&gt;\\n\\n&lt;p&gt;An MI60 will set you back about $500 (on eBay).&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lw0138/do_you_use_local_llms_for_work_over_cloud_models/n2ar9fg/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752116667,"author_flair_text":"llama.cpp","treatment_tags":[],"link_id":"t3_1lw0138","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2as1d7","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ForsookComparison","can_mod_post":false,"created_utc":1752116972,"send_replies":true,"parent_id":"t3_1lw0138","score":2,"author_fullname":"t2_on5es7pe3","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"The LLMs yearn for my .env files","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2as1d7","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The LLMs yearn for my .env files&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lw0138/do_you_use_local_llms_for_work_over_cloud_models/n2as1d7/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752116972,"author_flair_text":"llama.cpp","treatment_tags":[],"link_id":"t3_1lw0138","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2brofw","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"zipperlein","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2ay4lq","score":2,"author_fullname":"t2_x3duw","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"If it does not focus on your task you can try tweaking the prompts a bit. RooCode is VERY customizable. I am just getting starting with it too. Don't have too much experience yet. One thing i can say is, that RooCode needs a decent context size. I am using 65000 tokens for now, could go higher, but i am also running Qwen3-4B-Embedning and Qwen3-4B-Reranker on the setup.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2brofw","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;If it does not focus on your task you can try tweaking the prompts a bit. RooCode is VERY customizable. I am just getting starting with it too. Don&amp;#39;t have too much experience yet. One thing i can say is, that RooCode needs a decent context size. I am using 65000 tokens for now, could go higher, but i am also running Qwen3-4B-Embedning and Qwen3-4B-Reranker on the setup.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lw0138","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lw0138/do_you_use_local_llms_for_work_over_cloud_models/n2brofw/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752134099,"author_flair_text":null,"treatment_tags":[],"created_utc":1752134099,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2brgly","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"zipperlein","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2b12yc","score":1,"author_fullname":"t2_x3duw","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I tried Cline, but it did get stuck everytime with local models. RooCode worked way better for me. Best thing in RooCode is that u can customize it a lot, I try to go down that route for now. Sourcegraph Cody worked best yet for me, but the extension for VSCode has no agent mode and u need a login for the local model (??). That's why I don't use that for now.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2brgly","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I tried Cline, but it did get stuck everytime with local models. RooCode worked way better for me. Best thing in RooCode is that u can customize it a lot, I try to go down that route for now. Sourcegraph Cody worked best yet for me, but the extension for VSCode has no agent mode and u need a login for the local model (??). That&amp;#39;s why I don&amp;#39;t use that for now.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lw0138","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lw0138/do_you_use_local_llms_for_work_over_cloud_models/n2brgly/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752133975,"author_flair_text":null,"treatment_tags":[],"created_utc":1752133975,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2b12yc","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"SillyLilBear","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2ay4lq","score":1,"author_fullname":"t2_wjjtz","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I hear Cline is a lot better, but I haven't used any local for coding as I haven't had good luck.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2b12yc","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I hear Cline is a lot better, but I haven&amp;#39;t used any local for coding as I haven&amp;#39;t had good luck.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lw0138","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lw0138/do_you_use_local_llms_for_work_over_cloud_models/n2b12yc/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752120658,"author_flair_text":null,"treatment_tags":[],"created_utc":1752120658,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2ay4lq","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"eleqtriq","can_mod_post":false,"created_utc":1752119424,"send_replies":true,"parent_id":"t1_n2au6tn","score":1,"author_fullname":"t2_66vte","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I tired to use RooCode with the same model recently it just kept going off the rails.   Seemed very much like a a RooCode problem, too. Any tips?  Model seems fine in a chat UI.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2ay4lq","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I tired to use RooCode with the same model recently it just kept going off the rails.   Seemed very much like a a RooCode problem, too. Any tips?  Model seems fine in a chat UI.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lw0138","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lw0138/do_you_use_local_llms_for_work_over_cloud_models/n2ay4lq/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752119424,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2au6tn","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"zipperlein","can_mod_post":false,"created_utc":1752117829,"send_replies":true,"parent_id":"t3_1lw0138","score":2,"author_fullname":"t2_x3duw","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I use Qwen32B for RooCode in VS Code. It's fast enough with 50 t/s with 4x3090. For harder things tasks I am mostly using manually the chat mode of different cloud providers. That's a compromise for me because I can control exactly which data I give to the cloud provider.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2au6tn","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I use Qwen32B for RooCode in VS Code. It&amp;#39;s fast enough with 50 t/s with 4x3090. For harder things tasks I am mostly using manually the chat mode of different cloud providers. That&amp;#39;s a compromise for me because I can control exactly which data I give to the cloud provider.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lw0138/do_you_use_local_llms_for_work_over_cloud_models/n2au6tn/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752117829,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lw0138","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2aahrn","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Pro-editor-1105","can_mod_post":false,"created_utc":1752110649,"send_replies":true,"parent_id":"t3_1lw0138","score":1,"author_fullname":"t2_uptissiz","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I think claude max is good value cause you are getting a lot more out of the 100 dollars or 200 a month compared to API. Although you could just use the deepseek API.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2aahrn","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I think claude max is good value cause you are getting a lot more out of the 100 dollars or 200 a month compared to API. Although you could just use the deepseek API.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lw0138/do_you_use_local_llms_for_work_over_cloud_models/n2aahrn/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752110649,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lw0138","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2ba8xu","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"cguy1234","can_mod_post":false,"created_utc":1752124812,"send_replies":true,"parent_id":"t1_n2azubo","score":1,"author_fullname":"t2_mhjuy","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"The other thing I think about is what local llm is even vaguely competitive with Claude Code, even if you had a huge budget? The proprietary LLM seems on a completely different level for coding.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2ba8xu","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The other thing I think about is what local llm is even vaguely competitive with Claude Code, even if you had a huge budget? The proprietary LLM seems on a completely different level for coding.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lw0138","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lw0138/do_you_use_local_llms_for_work_over_cloud_models/n2ba8xu/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752124812,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2bwzyn","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ag789","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2bufrs","score":1,"author_fullname":"t2_13x892","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"well, it isn't just compute, it is the ram just consider a 300 billion parameters model.  \\nthat cost at least 300x4 (bytes) \\\\~ 1200 GB \\\\~ 1.2 terabytes of ram and that alone hasn't count all the extras needed to keep that 300 billion parameters, plus the extras maybe 2-3 terabytes of ram.\\n\\nwell of course these days quantize that to 1 byte, but during training the full 32 bit worlds (4 bytes) is needed for each parameter.\\n\\nthen you count the compute, and perhaps Nvidia H100 gpus are needed\\n\\n[https://www.nvidia.com/en-sg/data-center/h100/](https://www.nvidia.com/en-sg/data-center/h100/)\\n\\nthat is probably what it takes to have the whole chatgpt or gemini in your little pc, it is hard to bottle up that genie.","edited":1752137811,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2bwzyn","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;well, it isn&amp;#39;t just compute, it is the ram just consider a 300 billion parameters model.&lt;br/&gt;\\nthat cost at least 300x4 (bytes) ~ 1200 GB ~ 1.2 terabytes of ram and that alone hasn&amp;#39;t count all the extras needed to keep that 300 billion parameters, plus the extras maybe 2-3 terabytes of ram.&lt;/p&gt;\\n\\n&lt;p&gt;well of course these days quantize that to 1 byte, but during training the full 32 bit worlds (4 bytes) is needed for each parameter.&lt;/p&gt;\\n\\n&lt;p&gt;then you count the compute, and perhaps Nvidia H100 gpus are needed&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://www.nvidia.com/en-sg/data-center/h100/\\"&gt;https://www.nvidia.com/en-sg/data-center/h100/&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;that is probably what it takes to have the whole chatgpt or gemini in your little pc, it is hard to bottle up that genie.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lw0138","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lw0138/do_you_use_local_llms_for_work_over_cloud_models/n2bwzyn/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752137210,"author_flair_text":null,"treatment_tags":[],"created_utc":1752137210,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2bufrs","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AppearanceHeavy6724","can_mod_post":false,"created_utc":1752135704,"send_replies":true,"parent_id":"t1_n2azubo","score":1,"author_fullname":"t2_uz37qfx5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"$25 -$50 for used mining card is affordable for everyone.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2bufrs","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;$25 -$50 for used mining card is affordable for everyone.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lw0138","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lw0138/do_you_use_local_llms_for_work_over_cloud_models/n2bufrs/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752135704,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2azubo","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Logical_Divide_3595","can_mod_post":false,"created_utc":1752120131,"send_replies":true,"parent_id":"t3_1lw0138","score":1,"author_fullname":"t2_18riberpl8","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Most people can not afford local hardware","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2azubo","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Most people can not afford local hardware&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lw0138/do_you_use_local_llms_for_work_over_cloud_models/n2azubo/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752120131,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lw0138","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2bar3r","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ag789","can_mod_post":false,"created_utc":1752125058,"send_replies":true,"parent_id":"t3_1lw0138","score":1,"author_fullname":"t2_13x892","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"if it is just for coding, I'd guess it may be feasible to use codellama\\n\\n[https://github.com/meta-llama/codellama](https://github.com/meta-llama/codellama)  \\nand I used ollama, due to local out of date libraries (ollama runs in docker containers and is somewhat insulated froim the issue)  \\n[https://ollama.com/](https://ollama.com/)\\n\\nand I think the larger models e.g. 13B, 34B, 70B would make a difference, just that for those it takes a lot of memory cpu (mainboard) and a good gpu with lots of vram (e.g. 8, 12 GB, preferably more) and is costly.\\n\\nIn the end i settled with s 'small' 8B model with codellama locally, and used github copilot in addition.  \\nlarge models is too costly in terms of ram and resources to run.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2bar3r","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;if it is just for coding, I&amp;#39;d guess it may be feasible to use codellama&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://github.com/meta-llama/codellama\\"&gt;https://github.com/meta-llama/codellama&lt;/a&gt;&lt;br/&gt;\\nand I used ollama, due to local out of date libraries (ollama runs in docker containers and is somewhat insulated froim the issue)&lt;br/&gt;\\n&lt;a href=\\"https://ollama.com/\\"&gt;https://ollama.com/&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;and I think the larger models e.g. 13B, 34B, 70B would make a difference, just that for those it takes a lot of memory cpu (mainboard) and a good gpu with lots of vram (e.g. 8, 12 GB, preferably more) and is costly.&lt;/p&gt;\\n\\n&lt;p&gt;In the end i settled with s &amp;#39;small&amp;#39; 8B model with codellama locally, and used github copilot in addition.&lt;br/&gt;\\nlarge models is too costly in terms of ram and resources to run.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lw0138/do_you_use_local_llms_for_work_over_cloud_models/n2bar3r/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752125058,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lw0138","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2gnua0","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"KernQ","can_mod_post":false,"created_utc":1752193055,"send_replies":true,"parent_id":"t3_1lw0138","score":1,"author_fullname":"t2_1pozn81kn1","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"My process is to size the model for the use-case and go from there. For coding I've struggled to find local options that compete with Sonnet/CC. \\n\\nDeepseek R1 &amp; R3 are candidates and 0528 is particularly good (but very wordy). I've used a 300GB quant of 0528 that I think is probably good enough, but not fast enough on my current local rig. \\n\\nI think we're really close though. It's going to cost you $40k+ (4x RTX PRO 6000) but I think that's going to get you an \\"API competitive\\" personal, local option for coding.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2gnua0","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;My process is to size the model for the use-case and go from there. For coding I&amp;#39;ve struggled to find local options that compete with Sonnet/CC. &lt;/p&gt;\\n\\n&lt;p&gt;Deepseek R1 &amp;amp; R3 are candidates and 0528 is particularly good (but very wordy). I&amp;#39;ve used a 300GB quant of 0528 that I think is probably good enough, but not fast enough on my current local rig. &lt;/p&gt;\\n\\n&lt;p&gt;I think we&amp;#39;re really close though. It&amp;#39;s going to cost you $40k+ (4x RTX PRO 6000) but I think that&amp;#39;s going to get you an &amp;quot;API competitive&amp;quot; personal, local option for coding.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lw0138/do_you_use_local_llms_for_work_over_cloud_models/n2gnua0/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752193055,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lw0138","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),n=()=>e.jsx(l,{data:a});export{n as default};
