import{j as e}from"./index-BgwOAK4-.js";import{R as t}from"./RedditPostRenderer-BOBjDTFu.js";import"./index-BL22wVg5.js";const l=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"If so, would appreciate some links to the simplest of them to get up and running.\\n\\nDiffusion language models will give us the next great performance leap in language/text generation right?","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Is there any open-weight'd diffusion based language models I can test right now on my own hardware?","link_flair_richtext":[{"e":"text","t":"Discussion"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lp8kzx","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.82,"author_flair_background_color":null,"subreddit_type":"public","ups":7,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_b74xb","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Discussion","can_mod_post":false,"score":7,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1751392673,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;If so, would appreciate some links to the simplest of them to get up and running.&lt;/p&gt;\\n\\n&lt;p&gt;Diffusion language models will give us the next great performance leap in language/text generation right?&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"5f921ea4-c7bc-11ed-9c23-3a00622979b4","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#646d73","id":"1lp8kzx","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"wh33t","discussion_type":null,"num_comments":2,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lp8kzx/is_there_any_openweightd_diffusion_based_language/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lp8kzx/is_there_any_openweightd_diffusion_based_language/","subreddit_subscribers":493457,"created_utc":1751392673,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0u0o74","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Accomplished_Mode170","can_mod_post":false,"created_utc":1751405475,"send_replies":true,"parent_id":"t1_n0t4ldx","score":1,"author_fullname":"t2_4hfmiefj","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Love this write-up ‚úçÔ∏è \\n\\nSimilarly, [Sparse Attention](https://www.tilderesearch.com/blog/sparse-attn) is (counterintuitively) more expressive; ‚Äò[underlying geometries](https://arxiv.org/html/2402.09099v5)‚Äô üôÉ üìä","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0u0o74","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Love this write-up ‚úçÔ∏è &lt;/p&gt;\\n\\n&lt;p&gt;Similarly, &lt;a href=\\"https://www.tilderesearch.com/blog/sparse-attn\\"&gt;Sparse Attention&lt;/a&gt; is (counterintuitively) more expressive; ‚Äò&lt;a href=\\"https://arxiv.org/html/2402.09099v5\\"&gt;underlying geometries&lt;/a&gt;‚Äô üôÉ üìä&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lp8kzx","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp8kzx/is_there_any_openweightd_diffusion_based_language/n0u0o74/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751405475,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0t4ldx","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"eloquentemu","can_mod_post":false,"created_utc":1751396228,"send_replies":true,"parent_id":"t3_1lp8kzx","score":3,"author_fullname":"t2_lpdsy","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"&gt; Diffusion language models will give us the next great performance leap in language/text generation right?\\n\\nPeople speculate that, but I don't think it's panning out like that.  It also depends on what you mean by performance.\\n\\nCompute performance?  Yes, I believe that's the big selling point: it's kind of like they're basically processing a batch of tokens similar to what speculative decoding gets you.  This reduces the dependency on memory bandwidth, which is a pretty big cost driver / limitation.\\n\\nIf you mean functional performance, that's less clear and under active research.  For example, [one report I read](https://wandb.ai/byyoung3/ml-news/reports/Block-Diffusion-Language-Models-Combining-autoregression-and-diffusion--VmlldzoxMTg3MjU2OQ) discussed how block diffusion loses perplexity at block size &gt;4 but compute performance improves to size ~128.  But, that was just one experiment.  I think it's much like how LLM output degrades with temp=0, diffusion doesn't allow for sampling in the diffusion loop (at least in the systems I've seen).  Really it's all \\"we'll see\\".  That said, people seem to view diffusion as a thing that, say, fixes hallucinations or something by letting models write a 'complete thought' without interference from the sampler.  I don't think that'll pan out: if you dig into how LLMs work e.g. [Athropic's circuits](https://www.anthropic.com/news/tracing-thoughts-language-model) and play with samplers / output weights, it's fairly obvious autoregressive LLMs are more than capable of driving a 'complete thought' and that the sampler tends to just keep it from being the same thought over and over :).\\n\\nThat said, it's a super active area of development and there isn't anything off-the-shelf like \`llama.cpp\`.  You could check out [LLaDA](https://github.com/ML-GSAI/LLaDA), for one, which has the code and links to the models on HF.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0t4ldx","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;Diffusion language models will give us the next great performance leap in language/text generation right?&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;People speculate that, but I don&amp;#39;t think it&amp;#39;s panning out like that.  It also depends on what you mean by performance.&lt;/p&gt;\\n\\n&lt;p&gt;Compute performance?  Yes, I believe that&amp;#39;s the big selling point: it&amp;#39;s kind of like they&amp;#39;re basically processing a batch of tokens similar to what speculative decoding gets you.  This reduces the dependency on memory bandwidth, which is a pretty big cost driver / limitation.&lt;/p&gt;\\n\\n&lt;p&gt;If you mean functional performance, that&amp;#39;s less clear and under active research.  For example, &lt;a href=\\"https://wandb.ai/byyoung3/ml-news/reports/Block-Diffusion-Language-Models-Combining-autoregression-and-diffusion--VmlldzoxMTg3MjU2OQ\\"&gt;one report I read&lt;/a&gt; discussed how block diffusion loses perplexity at block size &amp;gt;4 but compute performance improves to size ~128.  But, that was just one experiment.  I think it&amp;#39;s much like how LLM output degrades with temp=0, diffusion doesn&amp;#39;t allow for sampling in the diffusion loop (at least in the systems I&amp;#39;ve seen).  Really it&amp;#39;s all &amp;quot;we&amp;#39;ll see&amp;quot;.  That said, people seem to view diffusion as a thing that, say, fixes hallucinations or something by letting models write a &amp;#39;complete thought&amp;#39; without interference from the sampler.  I don&amp;#39;t think that&amp;#39;ll pan out: if you dig into how LLMs work e.g. &lt;a href=\\"https://www.anthropic.com/news/tracing-thoughts-language-model\\"&gt;Athropic&amp;#39;s circuits&lt;/a&gt; and play with samplers / output weights, it&amp;#39;s fairly obvious autoregressive LLMs are more than capable of driving a &amp;#39;complete thought&amp;#39; and that the sampler tends to just keep it from being the same thought over and over :).&lt;/p&gt;\\n\\n&lt;p&gt;That said, it&amp;#39;s a super active area of development and there isn&amp;#39;t anything off-the-shelf like &lt;code&gt;llama.cpp&lt;/code&gt;.  You could check out &lt;a href=\\"https://github.com/ML-GSAI/LLaDA\\"&gt;LLaDA&lt;/a&gt;, for one, which has the code and links to the models on HF.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lp8kzx/is_there_any_openweightd_diffusion_based_language/n0t4ldx/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751396228,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lp8kzx","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}}]`),s=()=>e.jsx(t,{data:l});export{s as default};
