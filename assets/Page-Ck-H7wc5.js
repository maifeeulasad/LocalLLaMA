import{j as e}from"./index-BrqAgJWx.js";import{R as t}from"./RedditPostRenderer-chN7TDMj.js";import"./index-DlMtF8rT.js";const l=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"I recently implemented Reinforcement Learning from Human Feedback (RLHF) fine-tuning, including Supervised Fine-Tuning (SFT), Reward Modeling, and Proximal Policy Optimization (PPO), using Hugging Face's GPT-2 model. The three steps are implemented in the three separate notebooks on GitHub: [https://github.com/ash80/RLHF\\\\_in\\\\_notebooks](https://github.com/ash80/RLHF_in_notebooks)\\n\\nI've also recorded a detailed video walkthrough (3+ hours) of the implementation on YouTube: [https://youtu.be/K1UBOodkqEk](https://youtu.be/K1UBOodkqEk)\\n\\nI hope this is helpful for anyone looking to explore RLHF. Feedback is welcome ðŸ˜Š","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"RLHF from scratch, step-by-step, in 3 Jupyter notebooks","link_flair_richtext":[{"e":"text","t":"Tutorial | Guide"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1ln1ij8","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.94,"author_flair_background_color":null,"subreddit_type":"public","ups":75,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_efd3j","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Tutorial | Guide","can_mod_post":false,"score":75,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"post_hint":"self","content_categories":null,"is_self":true,"mod_note":null,"created":1751156595,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;I recently implemented Reinforcement Learning from Human Feedback (RLHF) fine-tuning, including Supervised Fine-Tuning (SFT), Reward Modeling, and Proximal Policy Optimization (PPO), using Hugging Face&amp;#39;s GPT-2 model. The three steps are implemented in the three separate notebooks on GitHub: &lt;a href=\\"https://github.com/ash80/RLHF_in_notebooks\\"&gt;https://github.com/ash80/RLHF_in_notebooks&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;ve also recorded a detailed video walkthrough (3+ hours) of the implementation on YouTube: &lt;a href=\\"https://youtu.be/K1UBOodkqEk\\"&gt;https://youtu.be/K1UBOodkqEk&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;I hope this is helpful for anyone looking to explore RLHF. Feedback is welcome ðŸ˜Š&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"preview":{"images":[{"source":{"url":"https://external-preview.redd.it/7s3bdCbz00b-4oL43jM8ycVne6tu1R-3wHRvv8i_Qh0.png?auto=webp&amp;s=7b0f6053c57bacbe1e27d4d8bcbc6f4c6c1ce226","width":1200,"height":600},"resolutions":[{"url":"https://external-preview.redd.it/7s3bdCbz00b-4oL43jM8ycVne6tu1R-3wHRvv8i_Qh0.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=718311ce447b521327cf71519b6b340c7be06b4d","width":108,"height":54},{"url":"https://external-preview.redd.it/7s3bdCbz00b-4oL43jM8ycVne6tu1R-3wHRvv8i_Qh0.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=c633f63b0d7f88f15a452028569b8fb885d98b1d","width":216,"height":108},{"url":"https://external-preview.redd.it/7s3bdCbz00b-4oL43jM8ycVne6tu1R-3wHRvv8i_Qh0.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=ff663e48855cbc3d12205cde9314e49116bf55e3","width":320,"height":160},{"url":"https://external-preview.redd.it/7s3bdCbz00b-4oL43jM8ycVne6tu1R-3wHRvv8i_Qh0.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=90d8be02ea9156a26f5b3e5afe3afe59b1640e4a","width":640,"height":320},{"url":"https://external-preview.redd.it/7s3bdCbz00b-4oL43jM8ycVne6tu1R-3wHRvv8i_Qh0.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=b3ca59094a23ec3ea413a5851bc5483467162efe","width":960,"height":480},{"url":"https://external-preview.redd.it/7s3bdCbz00b-4oL43jM8ycVne6tu1R-3wHRvv8i_Qh0.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=4563fc0fd1cee369d887af2ef43821b8d252af69","width":1080,"height":540}],"variants":{},"id":"7s3bdCbz00b-4oL43jM8ycVne6tu1R-3wHRvv8i_Qh0"}],"enabled":false},"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"449b05a6-bf8e-11ed-b4bd-66961e47bd50","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#0079d3","id":"1ln1ij8","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"ashz8888","discussion_type":null,"num_comments":5,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1ln1ij8/rlhf_from_scratch_stepbystep_in_3_jupyter/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1ln1ij8/rlhf_from_scratch_stepbystep_in_3_jupyter/","subreddit_subscribers":492929,"created_utc":1751156595,"num_crossposts":1,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0d7rk4","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"hi87","can_mod_post":false,"created_utc":1751177601,"send_replies":true,"parent_id":"t3_1ln1ij8","score":5,"author_fullname":"t2_etrdb","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"This is amazing. Ive been going thru Building LLMs from Scratch and this is immensely helpful.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0d7rk4","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;This is amazing. Ive been going thru Building LLMs from Scratch and this is immensely helpful.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ln1ij8/rlhf_from_scratch_stepbystep_in_3_jupyter/n0d7rk4/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751177601,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1ln1ij8","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0jy87t","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ashz8888","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0g0yhg","score":1,"author_fullname":"t2_efd3j","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Makes more sense now. The main difference seems to be the loss calculation.\\n\\nRL uses the delayed reward and distributes it across the generated tokens. This token level reward is then converted into a loss.  \\n  \\nThis SFT approach doesn't seem to use the reward in the loss calculation at all. The loss is still calculated from the cross entropy between the logprobs from the model and the tokens from the generated response. Only the learning rate is scaled based on the reward.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0jy87t","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Makes more sense now. The main difference seems to be the loss calculation.&lt;/p&gt;\\n\\n&lt;p&gt;RL uses the delayed reward and distributes it across the generated tokens. This token level reward is then converted into a loss.  &lt;/p&gt;\\n\\n&lt;p&gt;This SFT approach doesn&amp;#39;t seem to use the reward in the loss calculation at all. The loss is still calculated from the cross entropy between the logprobs from the model and the tokens from the generated response. Only the learning rate is scaled based on the reward.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ln1ij8","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ln1ij8/rlhf_from_scratch_stepbystep_in_3_jupyter/n0jy87t/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751279056,"author_flair_text":null,"treatment_tags":[],"created_utc":1751279056,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0g0yhg","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"throwaway2676","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0fz0pp","score":1,"author_fullname":"t2_85xkr","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Here is what I mean:\\n\\n1) The model is given an input question.\\n\\n2) The model generates a candidate answer.\\n\\n3) The candidate answer is given a reward by the reward model.\\n\\n4) The input question + generated answer are used to run a normal teacher forcing step, just like in SFT.  The only difference is that the learning rate for this step is scaled by the reward.\\n\\nThis seems to me to be very similar to RL, but RL is never framed this way, so I wonder what the difference is.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0g0yhg","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Here is what I mean:&lt;/p&gt;\\n\\n&lt;p&gt;1) The model is given an input question.&lt;/p&gt;\\n\\n&lt;p&gt;2) The model generates a candidate answer.&lt;/p&gt;\\n\\n&lt;p&gt;3) The candidate answer is given a reward by the reward model.&lt;/p&gt;\\n\\n&lt;p&gt;4) The input question + generated answer are used to run a normal teacher forcing step, just like in SFT.  The only difference is that the learning rate for this step is scaled by the reward.&lt;/p&gt;\\n\\n&lt;p&gt;This seems to me to be very similar to RL, but RL is never framed this way, so I wonder what the difference is.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ln1ij8","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ln1ij8/rlhf_from_scratch_stepbystep_in_3_jupyter/n0g0yhg/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751220519,"author_flair_text":null,"treatment_tags":[],"created_utc":1751220519,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0fz0pp","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ashz8888","can_mod_post":false,"created_utc":1751219921,"send_replies":true,"parent_id":"t1_n0eo85n","score":1,"author_fullname":"t2_efd3j","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I'm not sure if I fully understand what this variation is. Do you have a link?\\n\\nSFT is typically done on a question answer dataset, where the model is fed both the question and the answer. No generation is involved.\\n\\nIn PPO, the last step of RLHF, the model alternates between the generation and training. So model is essentially generating a new dataset to be trained on via RL.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0fz0pp","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;m not sure if I fully understand what this variation is. Do you have a link?&lt;/p&gt;\\n\\n&lt;p&gt;SFT is typically done on a question answer dataset, where the model is fed both the question and the answer. No generation is involved.&lt;/p&gt;\\n\\n&lt;p&gt;In PPO, the last step of RLHF, the model alternates between the generation and training. So model is essentially generating a new dataset to be trained on via RL.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ln1ij8","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ln1ij8/rlhf_from_scratch_stepbystep_in_3_jupyter/n0fz0pp/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751219921,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0eo85n","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"throwaway2676","can_mod_post":false,"created_utc":1751204988,"send_replies":true,"parent_id":"t3_1ln1ij8","score":2,"author_fullname":"t2_85xkr","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"As someone who's only ever casually dabbled in RL, I'm curious if anyone can tell me the basic difference between RL and a variation on SFT where the model generates the output for the training sequence and then the reward controls the learning rate for the optimization step (e.g., big positive learning rate for big positive rewards and big negative learning rate for big negative rewards)","edited":1751208283,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0eo85n","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;As someone who&amp;#39;s only ever casually dabbled in RL, I&amp;#39;m curious if anyone can tell me the basic difference between RL and a variation on SFT where the model generates the output for the training sequence and then the reward controls the learning rate for the optimization step (e.g., big positive learning rate for big positive rewards and big negative learning rate for big negative rewards)&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ln1ij8/rlhf_from_scratch_stepbystep_in_3_jupyter/n0eo85n/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751204988,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1ln1ij8","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}}]`),s=()=>e.jsx(t,{data:l});export{s as default};
