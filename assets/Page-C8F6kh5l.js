import{j as e}from"./index-DQXiEb7D.js";import{R as t}from"./RedditPostRenderer-BjndLgq8.js";import"./index-B-ILyjT1.js";const a=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Hi folks, has anyone used LLMs specifically to evaluate translation quality rather than generate translations? I mean using them to catch issues like dropped meaning, inconsistent terminology, awkward phrasing, and so on.\\n\\nI’m on a team experimenting with LLMs (GPT-4, Claude, etc.) for automated translation QA. Not to create translations, but to score, flag problems, and suggest batch corrections. The tool we’re working on is called Alconost.MT/Evaluate, here's what it looks like: \\n\\nhttps://preview.redd.it/kgu312b80gdf1.jpg?width=1200&amp;format=pjpg&amp;auto=webp&amp;s=ef7e69dd35bdac400f972e9b58fb94487e39b0ef\\n\\nI’m curious: what kinds of metrics or output formats would actually be useful for you guys when comparing translation providers or assessing quality, especially when you can’t get a full human review? (I’m old-school enough to believe nothing beats a real linguist’s eyeballs, but hey, sometimes you gotta trust the bots… or at least let them do the heavy lifting before the humans jump in.)\\n\\nCheers!\\n\\n","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Anyone here experimenting with LLMs for translation QA — not rewriting, just evaluating?","link_flair_richtext":[{"e":"text","t":"Discussion"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":91,"top_awarded_type":null,"hide_score":false,"media_metadata":{"kgu312b80gdf1":{"status":"valid","e":"Image","m":"image/jpg","p":[{"y":70,"x":108,"u":"https://preview.redd.it/kgu312b80gdf1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=062ab242b9fa23826c68510acfa30795c3d27098"},{"y":140,"x":216,"u":"https://preview.redd.it/kgu312b80gdf1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=0e6a4d4f67a1034dcad50771e9a2a3fb1fae4f63"},{"y":208,"x":320,"u":"https://preview.redd.it/kgu312b80gdf1.jpg?width=320&amp;crop=smart&amp;auto=webp&amp;s=effcf5d923b9b8a753a0f6536a092edd3119e1b2"},{"y":416,"x":640,"u":"https://preview.redd.it/kgu312b80gdf1.jpg?width=640&amp;crop=smart&amp;auto=webp&amp;s=3c50d6fb06e36cae6b1fc8687f136b8a531ac38f"},{"y":624,"x":960,"u":"https://preview.redd.it/kgu312b80gdf1.jpg?width=960&amp;crop=smart&amp;auto=webp&amp;s=b6d8335190e28e807c9f16ab5e4ec9f0613774bc"},{"y":702,"x":1080,"u":"https://preview.redd.it/kgu312b80gdf1.jpg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=86efd8d5f65a56b78c2da1b1f61f37fef3d7cded"}],"s":{"y":780,"x":1200,"u":"https://preview.redd.it/kgu312b80gdf1.jpg?width=1200&amp;format=pjpg&amp;auto=webp&amp;s=ef7e69dd35bdac400f972e9b58fb94487e39b0ef"},"id":"kgu312b80gdf1"}},"name":"t3_1m28oqc","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.92,"author_flair_background_color":null,"subreddit_type":"public","ups":20,"total_awards_received":0,"media_embed":{},"thumbnail_width":140,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_b2kvjgv2n","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Discussion","can_mod_post":false,"score":20,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"https://b.thumbs.redditmedia.com/DOzsE3kDEUuQywlws06uZepPnsVOAqjt3dXV5HNeD7E.jpg","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1752761706,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Hi folks, has anyone used LLMs specifically to evaluate translation quality rather than generate translations? I mean using them to catch issues like dropped meaning, inconsistent terminology, awkward phrasing, and so on.&lt;/p&gt;\\n\\n&lt;p&gt;I’m on a team experimenting with LLMs (GPT-4, Claude, etc.) for automated translation QA. Not to create translations, but to score, flag problems, and suggest batch corrections. The tool we’re working on is called Alconost.MT/Evaluate, here&amp;#39;s what it looks like: &lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://preview.redd.it/kgu312b80gdf1.jpg?width=1200&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=ef7e69dd35bdac400f972e9b58fb94487e39b0ef\\"&gt;https://preview.redd.it/kgu312b80gdf1.jpg?width=1200&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=ef7e69dd35bdac400f972e9b58fb94487e39b0ef&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;I’m curious: what kinds of metrics or output formats would actually be useful for you guys when comparing translation providers or assessing quality, especially when you can’t get a full human review? (I’m old-school enough to believe nothing beats a real linguist’s eyeballs, but hey, sometimes you gotta trust the bots… or at least let them do the heavy lifting before the humans jump in.)&lt;/p&gt;\\n\\n&lt;p&gt;Cheers!&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"5f921ea4-c7bc-11ed-9c23-3a00622979b4","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#646d73","id":"1m28oqc","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"NataliaShu","discussion_type":null,"num_comments":11,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1m28oqc/anyone_here_experimenting_with_llms_for/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1m28oqc/anyone_here_experimenting_with_llms_for/","subreddit_subscribers":501077,"created_utc":1752761706,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3omrxd","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"denzzilla","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3n33x6","score":2,"author_fullname":"t2_rqdrzz7m8","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Context is always a good idea since it helps the model better understand the user's content. Right now, we let you add general context, but it’s not always perfect, especially when you mix it with a bunch of terms. We’re planning to split it into separate inputs for better performance.  \\n  \\nAnyways, if we see more demand for this app and get user requests, we’ll probably add it. Thanks for the suggestion!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3omrxd","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Context is always a good idea since it helps the model better understand the user&amp;#39;s content. Right now, we let you add general context, but it’s not always perfect, especially when you mix it with a bunch of terms. We’re planning to split it into separate inputs for better performance.  &lt;/p&gt;\\n\\n&lt;p&gt;Anyways, if we see more demand for this app and get user requests, we’ll probably add it. Thanks for the suggestion!&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m28oqc","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m28oqc/anyone_here_experimenting_with_llms_for/n3omrxd/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752779370,"author_flair_text":null,"treatment_tags":[],"created_utc":1752779370,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n3n33x6","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DeProgrammer99","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3n147n","score":2,"author_fullname":"t2_w4j8t","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"How about auto-including some surrounding content for context and instructing the model on which part to evaluate? I was thinking of doing that in Faxtract (to help reduce overly similar flash cards when the user asks for it to expand upon the given text rather than purely turning the input into flash cards).","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3n33x6","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;How about auto-including some surrounding content for context and instructing the model on which part to evaluate? I was thinking of doing that in Faxtract (to help reduce overly similar flash cards when the user asks for it to expand upon the given text rather than purely turning the input into flash cards).&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m28oqc","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m28oqc/anyone_here_experimenting_with_llms_for/n3n33x6/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752763921,"author_flair_text":null,"treatment_tags":[],"created_utc":1752763921,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n3n147n","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"NataliaShu","can_mod_post":false,"created_utc":1752763361,"send_replies":true,"parent_id":"t1_n3n02rz","score":1,"author_fullname":"t2_b2kvjgv2n","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Thank you! I think describing context is vital for a well-performing prompt. Our tool already supports custom guidelines, so I assume the more effort a user puts into filling out this section, the better evaluation results they may get. (Sure, the same job shall be done when translating, but our tool is specifically for translation quality *evaluation*). \\n\\nCheers!\\n\\nhttps://preview.redd.it/wwi6ij6z4gdf1.png?width=1110&amp;format=png&amp;auto=webp&amp;s=c88307447e5c5d25175d3bea798bbfb7081f6c3e","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3n147n","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thank you! I think describing context is vital for a well-performing prompt. Our tool already supports custom guidelines, so I assume the more effort a user puts into filling out this section, the better evaluation results they may get. (Sure, the same job shall be done when translating, but our tool is specifically for translation quality &lt;em&gt;evaluation&lt;/em&gt;). &lt;/p&gt;\\n\\n&lt;p&gt;Cheers!&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://preview.redd.it/wwi6ij6z4gdf1.png?width=1110&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c88307447e5c5d25175d3bea798bbfb7081f6c3e\\"&gt;https://preview.redd.it/wwi6ij6z4gdf1.png?width=1110&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=c88307447e5c5d25175d3bea798bbfb7081f6c3e&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m28oqc","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m28oqc/anyone_here_experimenting_with_llms_for/n3n147n/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752763361,"media_metadata":{"wwi6ij6z4gdf1":{"status":"valid","e":"Image","m":"image/png","p":[{"y":85,"x":108,"u":"https://preview.redd.it/wwi6ij6z4gdf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=b84b6a94f330ed475e0e8e500c48467d13a4031d"},{"y":171,"x":216,"u":"https://preview.redd.it/wwi6ij6z4gdf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=8648652724250bc23ff75d7dbbc532cb4201c43c"},{"y":253,"x":320,"u":"https://preview.redd.it/wwi6ij6z4gdf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=ec4aa31088b938f6e714062e9e5de34405307235"},{"y":507,"x":640,"u":"https://preview.redd.it/wwi6ij6z4gdf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=cda0b800fc0cf3309da7e43ad434a294e96221e7"},{"y":761,"x":960,"u":"https://preview.redd.it/wwi6ij6z4gdf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=8015f558d53e421e53f9a4ca5325ccd933a14744"},{"y":856,"x":1080,"u":"https://preview.redd.it/wwi6ij6z4gdf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=80333b56535179f4f90cc14ce97f27dbbd91364b"}],"s":{"y":880,"x":1110,"u":"https://preview.redd.it/wwi6ij6z4gdf1.png?width=1110&amp;format=png&amp;auto=webp&amp;s=c88307447e5c5d25175d3bea798bbfb7081f6c3e"},"id":"wwi6ij6z4gdf1"}},"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3n2khz","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Patentsmatter","can_mod_post":false,"created_utc":1752763770,"send_replies":true,"parent_id":"t1_n3n02rz","score":1,"author_fullname":"t2_cocl8roo","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I noticed the same. \\n\\nPlus: when translating legal texts, it is mandatory that the terminology is maintained, and that \\"Antrag\\" is, in context A, always translated as \\"request\\" and not suddenly journeys under \\"petition\\".","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3n2khz","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I noticed the same. &lt;/p&gt;\\n\\n&lt;p&gt;Plus: when translating legal texts, it is mandatory that the terminology is maintained, and that &amp;quot;Antrag&amp;quot; is, in context A, always translated as &amp;quot;request&amp;quot; and not suddenly journeys under &amp;quot;petition&amp;quot;.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m28oqc","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m28oqc/anyone_here_experimenting_with_llms_for/n3n2khz/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752763770,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3n02rz","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"muntaxitome","can_mod_post":false,"created_utc":1752763072,"send_replies":true,"parent_id":"t3_1m28oqc","score":7,"author_fullname":"t2_so28y","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I think the problem with these line by line translations is that they nearly always miss context. \\n\\nWhen we ask translators (or chatgpt for that matter) to translate language files they are basically always perfectly fine when taken without context. However in the app itself they can then be problematic because the translation doesn't make sense in that location. I think AI would be much better at determining if it fits if it has access to screenshots of the language use or the codebase.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3n02rz","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I think the problem with these line by line translations is that they nearly always miss context. &lt;/p&gt;\\n\\n&lt;p&gt;When we ask translators (or chatgpt for that matter) to translate language files they are basically always perfectly fine when taken without context. However in the app itself they can then be problematic because the translation doesn&amp;#39;t make sense in that location. I think AI would be much better at determining if it fits if it has access to screenshots of the language use or the codebase.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m28oqc/anyone_here_experimenting_with_llms_for/n3n02rz/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752763072,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m28oqc","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":7}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3omm36","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"denzzilla","can_mod_post":false,"created_utc":1752779324,"send_replies":true,"parent_id":"t1_n3n5dxb","score":2,"author_fullname":"t2_rqdrzz7m8","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Sure thing! Everyone’s doing the same thing these days, haha. We’re just helping out people who don’t want to build this themselves but are curious about how AI can evaluate in different languages.\\n\\nLLM evaluation accuracy varies with content and domain. It works well with technical or standard texts, but sometimes falls short with more creative content—unless you do some prompt engineering or custom tweaking. Based on our tests (depending on the model/content), LLM evaluation can correlate 70-80% with human judgments.\\n\\nIt’s definitely not a last instance, nor a replacement for professional human review, but it can be handy for QA or just comparing different MT outputs when you don’t have a human reference.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3omm36","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Sure thing! Everyone’s doing the same thing these days, haha. We’re just helping out people who don’t want to build this themselves but are curious about how AI can evaluate in different languages.&lt;/p&gt;\\n\\n&lt;p&gt;LLM evaluation accuracy varies with content and domain. It works well with technical or standard texts, but sometimes falls short with more creative content—unless you do some prompt engineering or custom tweaking. Based on our tests (depending on the model/content), LLM evaluation can correlate 70-80% with human judgments.&lt;/p&gt;\\n\\n&lt;p&gt;It’s definitely not a last instance, nor a replacement for professional human review, but it can be handy for QA or just comparing different MT outputs when you don’t have a human reference.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m28oqc","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m28oqc/anyone_here_experimenting_with_llms_for/n3omm36/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752779324,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n3n5dxb","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"xadiant","can_mod_post":false,"created_utc":1752764547,"send_replies":true,"parent_id":"t3_1m28oqc","score":3,"author_fullname":"t2_omgp6","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"XTRF currently offers pretty much what you are building, but QA is a meta process in which I definitely don't trust AI in. Each end product is very different in translation and AI sucks at finding what's missing compared to what's wrong.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3n5dxb","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;XTRF currently offers pretty much what you are building, but QA is a meta process in which I definitely don&amp;#39;t trust AI in. Each end product is very different in translation and AI sucks at finding what&amp;#39;s missing compared to what&amp;#39;s wrong.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m28oqc/anyone_here_experimenting_with_llms_for/n3n5dxb/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752764547,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m28oqc","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3s1y3n","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"denzzilla","can_mod_post":false,"created_utc":1752825020,"send_replies":true,"parent_id":"t1_n3pfv3r","score":1,"author_fullname":"t2_rqdrzz7m8","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"We’re expecting Gemini and Deepseek to be added sometime next week, so you’ll be able to check then : )","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3s1y3n","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;We’re expecting Gemini and Deepseek to be added sometime next week, so you’ll be able to check then : )&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m28oqc","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m28oqc/anyone_here_experimenting_with_llms_for/n3s1y3n/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752825020,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3pfv3r","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"LetterRip","can_mod_post":false,"created_utc":1752787738,"send_replies":true,"parent_id":"t3_1m28oqc","score":1,"author_fullname":"t2_3zb81","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Gemini can flag my wrong translation answers on Duolingo. So it can definitely catch 'easy' errors, but no idea on how it would do on more challenging things like idiomatic translation.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3pfv3r","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Gemini can flag my wrong translation answers on Duolingo. So it can definitely catch &amp;#39;easy&amp;#39; errors, but no idea on how it would do on more challenging things like idiomatic translation.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m28oqc/anyone_here_experimenting_with_llms_for/n3pfv3r/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752787738,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m28oqc","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3s2snr","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"denzzilla","can_mod_post":false,"created_utc":1752825490,"send_replies":true,"parent_id":"t1_n3qi3n8","score":1,"author_fullname":"t2_rqdrzz7m8","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Basically, you can try any content type and domain. If you're familiar with the target language, you can easily check the model's accuracy (from my experience sonnet-4 works a bit better than GPT-4.1). Give it a try (it's free)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3s2snr","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Basically, you can try any content type and domain. If you&amp;#39;re familiar with the target language, you can easily check the model&amp;#39;s accuracy (from my experience sonnet-4 works a bit better than GPT-4.1). Give it a try (it&amp;#39;s free)&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m28oqc","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m28oqc/anyone_here_experimenting_with_llms_for/n3s2snr/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752825490,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3qi3n8","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"msbeaute00000001","can_mod_post":false,"created_utc":1752800414,"send_replies":true,"parent_id":"t3_1m28oqc","score":1,"author_fullname":"t2_lplboqf2","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"what kind of translation are you working on? Documentation or film/music?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3qi3n8","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;what kind of translation are you working on? Documentation or film/music?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m28oqc/anyone_here_experimenting_with_llms_for/n3qi3n8/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752800414,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m28oqc","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),o=()=>e.jsx(t,{data:a});export{o as default};
