import{j as e}from"./index-BlGsFJYy.js";import{R as l}from"./RedditPostRenderer-B6uvq_Zl.js";import"./index-DDvVtNwD.js";const t=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"I have a ryzen AI 7h CPU (with 50 TOPS NPU) with 64gb DDR5 RAM or an RTX5070 with 8gb DDR7. Should I run inference off of GPU or CPU for better performance?","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Larger model on CPU or small model on GPU","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lsq2m3","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.55,"author_flair_background_color":null,"subreddit_type":"public","ups":1,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_wrb36r6lc","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":1,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1751767413,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;I have a ryzen AI 7h CPU (with 50 TOPS NPU) with 64gb DDR5 RAM or an RTX5070 with 8gb DDR7. Should I run inference off of GPU or CPU for better performance?&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1lsq2m3","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"No_Professional_582","discussion_type":null,"num_comments":12,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lsq2m3/larger_model_on_cpu_or_small_model_on_gpu/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lsq2m3/larger_model_on_cpu_or_small_model_on_gpu/","subreddit_subscribers":495396,"created_utc":1751767413,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1kxvwn","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ttkciar","can_mod_post":false,"created_utc":1751774416,"send_replies":true,"parent_id":"t3_1lsq2m3","score":3,"author_fullname":"t2_cpegz","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"For performance, use a small model that fits in your RTX5070's VRAM.\\n\\nFor highly competent inference (which you might not need, depending on your use-case), use a larger model that fits in main memory.  It will be slow as balls, but give you better answers.\\n\\nIf your use-case is smut or lolz, go with the small model in VRAM.  For serious work you're better off with a larger model, and just adopt a workflow which has you working on other tasks while waiting for inference to finish.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1kxvwn","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;For performance, use a small model that fits in your RTX5070&amp;#39;s VRAM.&lt;/p&gt;\\n\\n&lt;p&gt;For highly competent inference (which you might not need, depending on your use-case), use a larger model that fits in main memory.  It will be slow as balls, but give you better answers.&lt;/p&gt;\\n\\n&lt;p&gt;If your use-case is smut or lolz, go with the small model in VRAM.  For serious work you&amp;#39;re better off with a larger model, and just adopt a workflow which has you working on other tasks while waiting for inference to finish.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lsq2m3/larger_model_on_cpu_or_small_model_on_gpu/n1kxvwn/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751774416,"author_flair_text":"llama.cpp","treatment_tags":[],"link_id":"t3_1lsq2m3","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1ly4xz","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AppearanceHeavy6724","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1lxof5","score":2,"author_fullname":"t2_uz37qfx5","approved_by":null,"mod_note":null,"all_awardings":[],"body":"If someone find the deviations in q2 models tolerable they of course are free to use them. I tried but dumbing down and serious loss of instructions following was too much for my taste.","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n1ly4xz","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;If someone find the deviations in q2 models tolerable they of course are free to use them. I tried but dumbing down and serious loss of instructions following was too much for my taste.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lsq2m3","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lsq2m3/larger_model_on_cpu_or_small_model_on_gpu/n1ly4xz/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751794626,"author_flair_text":null,"treatment_tags":[],"created_utc":1751794626,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n1lxof5","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"My_Unbiased_Opinion","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1ltdw4","score":0,"author_fullname":"t2_esiyl0yb","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"According to unsloths documentation UD Q2KXL is the most efficient in terms of size to performance ratio. You can run a smaller model at a higher quant, but more parameters IMHO is overall better as long as the quant is reasonable which Q2KXL UD is. ","edited":false,"author_flair_css_class":null,"name":"t1_n1lxof5","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;According to unsloths documentation UD Q2KXL is the most efficient in terms of size to performance ratio. You can run a smaller model at a higher quant, but more parameters IMHO is overall better as long as the quant is reasonable which Q2KXL UD is. &lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lsq2m3","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lsq2m3/larger_model_on_cpu_or_small_model_on_gpu/n1lxof5/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751794342,"author_flair_text":null,"collapsed":false,"created_utc":1751794342,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}},"user_reports":[],"saved":false,"id":"n1ltdw4","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AppearanceHeavy6724","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1l7ydw","score":3,"author_fullname":"t2_uz37qfx5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I would not ever run q2, as it is going be misbehaving in very strange way. Even Q3 is too much for me.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1ltdw4","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I would not ever run q2, as it is going be misbehaving in very strange way. Even Q3 is too much for me.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lsq2m3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lsq2m3/larger_model_on_cpu_or_small_model_on_gpu/n1ltdw4/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751791696,"author_flair_text":null,"treatment_tags":[],"created_utc":1751791696,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n1l7ydw","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"My_Unbiased_Opinion","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1kjjo1","score":0,"author_fullname":"t2_esiyl0yb","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I would run Qwen 3 14B at UD Q2KXL. I find it much smarter than 8B even if the 14B is quite aggressively quantized. ","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n1l7ydw","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I would run Qwen 3 14B at UD Q2KXL. I find it much smarter than 8B even if the 14B is quite aggressively quantized. &lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lsq2m3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lsq2m3/larger_model_on_cpu_or_small_model_on_gpu/n1l7ydw/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751779355,"author_flair_text":null,"treatment_tags":[],"created_utc":1751779355,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}},"user_reports":[],"saved":false,"id":"n1kjjo1","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"No_Professional_582","can_mod_post":false,"created_utc":1751768195,"send_replies":true,"parent_id":"t1_n1kifqi","score":2,"author_fullname":"t2_wrb36r6lc","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Laptop version only has 8 GB.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1kjjo1","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Laptop version only has 8 GB.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lsq2m3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lsq2m3/larger_model_on_cpu_or_small_model_on_gpu/n1kjjo1/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751768195,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n1kifqi","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"My_Unbiased_Opinion","can_mod_post":false,"created_utc":1751767744,"send_replies":true,"parent_id":"t3_1lsq2m3","score":2,"author_fullname":"t2_esiyl0yb","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"5070 has 12GB of VRAM. You can run some solid models off that. Look at the unsloth dynamic quants and run something at least UD Q2KXL","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1kifqi","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;5070 has 12GB of VRAM. You can run some solid models off that. Look at the unsloth dynamic quants and run something at least UD Q2KXL&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lsq2m3/larger_model_on_cpu_or_small_model_on_gpu/n1kifqi/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751767744,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lsq2m3","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1l7lor","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Double_Cause4609","can_mod_post":false,"created_utc":1751779170,"send_replies":true,"parent_id":"t3_1lsq2m3","score":2,"author_fullname":"t2_1kubzxt2ww","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"8GB on the GPU is tight. It depends on exactly what you're doing though. There is certainly value to having just \\\\*a lot\\\\* of tokens per second, but they're not going to be great tokens. IMO it's most useful if you're a programmer and want to get into agents or something.\\n\\nOn CPU, you could run fairly large MoE models which will run pretty comfortably. Hunyuan A13B should be about the right size as and when it gets support in LlamaCPP (and you could offload just the KV Cache to GPU I believe), and while performance won't be amazing, it should be useable.\\n\\nQwen 3 30B MoE should also be fairly suitable if you need a fast but competent model on CPU.\\n\\nThe NPU probably won't see much use right now unless you want to use a custom server (I think AMD has the Lemonade server or something to that effect; I haven't experimented with it to any terrible degree).","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1l7lor","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;8GB on the GPU is tight. It depends on exactly what you&amp;#39;re doing though. There is certainly value to having just *a lot* of tokens per second, but they&amp;#39;re not going to be great tokens. IMO it&amp;#39;s most useful if you&amp;#39;re a programmer and want to get into agents or something.&lt;/p&gt;\\n\\n&lt;p&gt;On CPU, you could run fairly large MoE models which will run pretty comfortably. Hunyuan A13B should be about the right size as and when it gets support in LlamaCPP (and you could offload just the KV Cache to GPU I believe), and while performance won&amp;#39;t be amazing, it should be useable.&lt;/p&gt;\\n\\n&lt;p&gt;Qwen 3 30B MoE should also be fairly suitable if you need a fast but competent model on CPU.&lt;/p&gt;\\n\\n&lt;p&gt;The NPU probably won&amp;#39;t see much use right now unless you want to use a custom server (I think AMD has the Lemonade server or something to that effect; I haven&amp;#39;t experimented with it to any terrible degree).&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lsq2m3/larger_model_on_cpu_or_small_model_on_gpu/n1l7lor/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751779170,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lsq2m3","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1n3nbh","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"bjodah","can_mod_post":false,"created_utc":1751812918,"send_replies":true,"parent_id":"t3_1lsq2m3","score":1,"author_fullname":"t2_atvy2","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I would try Qwen 3 30B A3B, using llama.cpp's llama-serve with the flags:  \\n\`\`\`\\n        --hf-repo bartowski/Qwen_Qwen3-30B-A3B-GGUF:Q8_0\\n        --override-tensor '([0-9]+).ffn_.*_exps.=CPU'\\n        --n-gpu-layers 48\\n        --jinja\\n        --cache-type-k q8_0\\n        --ctx-size 32768\\n        --min-p 0.005\\n        --top-p 0.97\\n        --top-k 40\\n        --temp 0.7\\n        --dry-multiplier 0.7\\n        --dry-allowed-length 4\\n        --dry-penalty-last-n 2048\\n        --presence-penalty 0.05\\n        --frequency-penalty 0.005\\n        --repeat-penalty 1.01\\n        --repeat-last-n 16\\n\`\`\`\\nprompt processing and partial offload on the GPU should give a nice boost. For comparison you can try dropping the --n-gpu-layers &amp; --override-tensor flag, I'm interested to hear what tokens per seconds you get for pp and tg with and without your 5070 mobile.\\n\\nEDIT, I should mention I run with these environment variables also:\\n\`\`\`\\n      - LLAMA_ARG_THREADS=16\\n      - LLAMA_ARG_ENDPOINT_METRICS=1\\n      - LLAMA_ARG_ENDPOINT_SLOTS=1\\n      - LLAMA_API_KEY=sk-empty\\n      - LLAMA_ARG_NO_CONTEXT_SHIFT=1\\n\`\`\`\\nYou should match number of physical cores (or maybe 2x with SMT enabled). Documentation for llama.cpp's server is found here:\\nhttps://github.com/ggml-org/llama.cpp/tree/master/tools/server#llamacpp-http-server","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1n3nbh","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I would try Qwen 3 30B A3B, using llama.cpp&amp;#39;s llama-serve with the flags:&lt;br/&gt;\\n&lt;code&gt;\\n        --hf-repo bartowski/Qwen_Qwen3-30B-A3B-GGUF:Q8_0\\n        --override-tensor &amp;#39;([0-9]+).ffn_.*_exps.=CPU&amp;#39;\\n        --n-gpu-layers 48\\n        --jinja\\n        --cache-type-k q8_0\\n        --ctx-size 32768\\n        --min-p 0.005\\n        --top-p 0.97\\n        --top-k 40\\n        --temp 0.7\\n        --dry-multiplier 0.7\\n        --dry-allowed-length 4\\n        --dry-penalty-last-n 2048\\n        --presence-penalty 0.05\\n        --frequency-penalty 0.005\\n        --repeat-penalty 1.01\\n        --repeat-last-n 16\\n&lt;/code&gt;\\nprompt processing and partial offload on the GPU should give a nice boost. For comparison you can try dropping the --n-gpu-layers &amp;amp; --override-tensor flag, I&amp;#39;m interested to hear what tokens per seconds you get for pp and tg with and without your 5070 mobile.&lt;/p&gt;\\n\\n&lt;p&gt;EDIT, I should mention I run with these environment variables also:\\n&lt;code&gt;\\n      - LLAMA_ARG_THREADS=16\\n      - LLAMA_ARG_ENDPOINT_METRICS=1\\n      - LLAMA_ARG_ENDPOINT_SLOTS=1\\n      - LLAMA_API_KEY=sk-empty\\n      - LLAMA_ARG_NO_CONTEXT_SHIFT=1\\n&lt;/code&gt;\\nYou should match number of physical cores (or maybe 2x with SMT enabled). Documentation for llama.cpp&amp;#39;s server is found here:\\n&lt;a href=\\"https://github.com/ggml-org/llama.cpp/tree/master/tools/server#llamacpp-http-server\\"&gt;https://github.com/ggml-org/llama.cpp/tree/master/tools/server#llamacpp-http-server&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lsq2m3/larger_model_on_cpu_or_small_model_on_gpu/n1n3nbh/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751812918,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lsq2m3","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1kwvr0","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"admajic","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1kwgzx","score":1,"author_fullname":"t2_60b9farf","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Wow. Didnt know that. I'm playing with desktop 3090. Speed is good if you want to play locally. Plus DDR5 RAM.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n1kwvr0","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Wow. Didnt know that. I&amp;#39;m playing with desktop 3090. Speed is good if you want to play locally. Plus DDR5 RAM.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lsq2m3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lsq2m3/larger_model_on_cpu_or_small_model_on_gpu/n1kwvr0/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751773957,"author_flair_text":null,"treatment_tags":[],"created_utc":1751773957,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1kwgzx","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"juggarjew","can_mod_post":false,"created_utc":1751773770,"send_replies":true,"parent_id":"t1_n1kpvpn","score":1,"author_fullname":"t2_i2ox1","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"That information is based on the RTX 5070 desktop card, a much different and much more powerful chip than the laptop 5070. Double the performance practically.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1kwgzx","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;That information is based on the RTX 5070 desktop card, a much different and much more powerful chip than the laptop 5070. Double the performance practically.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lsq2m3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lsq2m3/larger_model_on_cpu_or_small_model_on_gpu/n1kwgzx/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751773770,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1kpvpn","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"admajic","can_mod_post":false,"created_utc":1751770853,"send_replies":true,"parent_id":"t3_1lsq2m3","score":0,"author_fullname":"t2_60b9farf","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I asked perplexity for you. \\n\\nhow many tflops in this system\\n\\nI have a ryzen AI 7h CPU (with 50 TOPS NPU) with 64gb DDR5 RAM or an RTX5070 with 8gb DDR7. Should I run inference off of GPU or CPU for better performance?\\n\\nBased on your system specs:\\n\\n- **CPU:** Ryzen AI 7h (Ryzen AI 7 350 series, Zen 5, XDNA 2 NPU, 50 TOPS INT8)\\n- **RAM:** 64GB DDR5\\n- **GPU:** RTX 5070 with 8GB GDDR7\\n\\n### Raw Compute Performance\\n\\n| Component            | AI/FP32 Performance                    |\\n|----------------------|----------------------------------------|\\n| **Ryzen AI 7 NPU**   | 50 TOPS (INT8)                         |\\n| **RTX 5070 GPU**     | ~30.9 TFLOPS (FP32, float)             |\\n|                      | 988 AI TOPS (Tensor Cores, INT8/AI)    |\\n\\n**Note:**  \\n- **TOPS** (Tera Operations Per Second) for NPUs/AI accelerators is typically measured using INT8 precision, which is common for AI inference.\\n- **TFLOPS** (Tera Floating Point Operations Per Second) for GPUs is usually measured in FP32 (single-precision float), the standard for many deep learning frameworks, but GPUs also support INT8/FP16 at much higher rates using Tensor Cores.\\n\\n### Which Should You Use for AI Inference?\\n\\n**For most AI inference tasks, especially with frameworks like PyTorch or TensorFlow:**\\n\\n- **The RTX 5070 GPU will deliver significantly higher performance** than both the CPU and the integrated NPU, especially for models using FP16/FP32 or leveraging NVIDIA's Tensor Cores for INT8/FP16 inference[1][2].\\n- The **NPU (50 TOPS)** is designed for efficient, low-power AI tasks (e.g., Copilot+ features, background AI processing), but is not as fast or flexible as the RTX 5070 for large models, high throughput, or custom AI workloads[3][4].\\n- The **CPU** itself is not optimized for high-throughput AI inference compared to modern GPUs or dedicated NPUs[5].\\n\\n### Recommendation\\n\\n- **Run inference on the RTX 5070 GPU** for best performance, especially for large models, batch processing, or any workload where speed is critical.\\n- Use the **NPU** only if you have a specific, well-optimized workflow that targets on-device, low-power AI (e.g., Windows Copilot, background vision/audio tasks), or if your workload is explicitly designed for the NPU[3][4].\\n\\n**Summary Table**\\n\\n| Task Type          | Best Hardware         |\\n|--------------------|----------------------|\\n| General AI Inference / Deep Learning | **RTX 5070 GPU** |\\n| Lightweight, On-device AI (Copilot, background tasks) | Ryzen AI NPU  |\\n\\n**In almost all custom or high-throughput AI inference scenarios, the RTX 5070 will be much faster and more versatile than the CPU or NPU.**","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1kpvpn","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I asked perplexity for you. &lt;/p&gt;\\n\\n&lt;p&gt;how many tflops in this system&lt;/p&gt;\\n\\n&lt;p&gt;I have a ryzen AI 7h CPU (with 50 TOPS NPU) with 64gb DDR5 RAM or an RTX5070 with 8gb DDR7. Should I run inference off of GPU or CPU for better performance?&lt;/p&gt;\\n\\n&lt;p&gt;Based on your system specs:&lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;&lt;strong&gt;CPU:&lt;/strong&gt; Ryzen AI 7h (Ryzen AI 7 350 series, Zen 5, XDNA 2 NPU, 50 TOPS INT8)&lt;/li&gt;\\n&lt;li&gt;&lt;strong&gt;RAM:&lt;/strong&gt; 64GB DDR5&lt;/li&gt;\\n&lt;li&gt;&lt;strong&gt;GPU:&lt;/strong&gt; RTX 5070 with 8GB GDDR7&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;h3&gt;Raw Compute Performance&lt;/h3&gt;\\n\\n&lt;table&gt;&lt;thead&gt;\\n&lt;tr&gt;\\n&lt;th&gt;Component&lt;/th&gt;\\n&lt;th&gt;AI/FP32 Performance&lt;/th&gt;\\n&lt;/tr&gt;\\n&lt;/thead&gt;&lt;tbody&gt;\\n&lt;tr&gt;\\n&lt;td&gt;&lt;strong&gt;Ryzen AI 7 NPU&lt;/strong&gt;&lt;/td&gt;\\n&lt;td&gt;50 TOPS (INT8)&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;tr&gt;\\n&lt;td&gt;&lt;strong&gt;RTX 5070 GPU&lt;/strong&gt;&lt;/td&gt;\\n&lt;td&gt;~30.9 TFLOPS (FP32, float)&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;tr&gt;\\n&lt;td&gt;&lt;/td&gt;\\n&lt;td&gt;988 AI TOPS (Tensor Cores, INT8/AI)&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;/tbody&gt;&lt;/table&gt;\\n\\n&lt;p&gt;&lt;strong&gt;Note:&lt;/strong&gt;&lt;br/&gt;\\n- &lt;strong&gt;TOPS&lt;/strong&gt; (Tera Operations Per Second) for NPUs/AI accelerators is typically measured using INT8 precision, which is common for AI inference.\\n- &lt;strong&gt;TFLOPS&lt;/strong&gt; (Tera Floating Point Operations Per Second) for GPUs is usually measured in FP32 (single-precision float), the standard for many deep learning frameworks, but GPUs also support INT8/FP16 at much higher rates using Tensor Cores.&lt;/p&gt;\\n\\n&lt;h3&gt;Which Should You Use for AI Inference?&lt;/h3&gt;\\n\\n&lt;p&gt;&lt;strong&gt;For most AI inference tasks, especially with frameworks like PyTorch or TensorFlow:&lt;/strong&gt;&lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;&lt;strong&gt;The RTX 5070 GPU will deliver significantly higher performance&lt;/strong&gt; than both the CPU and the integrated NPU, especially for models using FP16/FP32 or leveraging NVIDIA&amp;#39;s Tensor Cores for INT8/FP16 inference[1][2].&lt;/li&gt;\\n&lt;li&gt;The &lt;strong&gt;NPU (50 TOPS)&lt;/strong&gt; is designed for efficient, low-power AI tasks (e.g., Copilot+ features, background AI processing), but is not as fast or flexible as the RTX 5070 for large models, high throughput, or custom AI workloads[3][4].&lt;/li&gt;\\n&lt;li&gt;The &lt;strong&gt;CPU&lt;/strong&gt; itself is not optimized for high-throughput AI inference compared to modern GPUs or dedicated NPUs[5].&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;h3&gt;Recommendation&lt;/h3&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;&lt;strong&gt;Run inference on the RTX 5070 GPU&lt;/strong&gt; for best performance, especially for large models, batch processing, or any workload where speed is critical.&lt;/li&gt;\\n&lt;li&gt;Use the &lt;strong&gt;NPU&lt;/strong&gt; only if you have a specific, well-optimized workflow that targets on-device, low-power AI (e.g., Windows Copilot, background vision/audio tasks), or if your workload is explicitly designed for the NPU[3][4].&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;p&gt;&lt;strong&gt;Summary Table&lt;/strong&gt;&lt;/p&gt;\\n\\n&lt;table&gt;&lt;thead&gt;\\n&lt;tr&gt;\\n&lt;th&gt;Task Type&lt;/th&gt;\\n&lt;th&gt;Best Hardware&lt;/th&gt;\\n&lt;/tr&gt;\\n&lt;/thead&gt;&lt;tbody&gt;\\n&lt;tr&gt;\\n&lt;td&gt;General AI Inference / Deep Learning&lt;/td&gt;\\n&lt;td&gt;&lt;strong&gt;RTX 5070 GPU&lt;/strong&gt;&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;tr&gt;\\n&lt;td&gt;Lightweight, On-device AI (Copilot, background tasks)&lt;/td&gt;\\n&lt;td&gt;Ryzen AI NPU&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;/tbody&gt;&lt;/table&gt;\\n\\n&lt;p&gt;&lt;strong&gt;In almost all custom or high-throughput AI inference scenarios, the RTX 5070 will be much faster and more versatile than the CPU or NPU.&lt;/strong&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lsq2m3/larger_model_on_cpu_or_small_model_on_gpu/n1kpvpn/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751770853,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lsq2m3","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}}]`),n=()=>e.jsx(l,{data:t});export{n as default};
