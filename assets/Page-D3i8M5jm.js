import{j as l}from"./index-BpC9hjVs.js";import{R as e}from"./RedditPostRenderer-BEo6AnSR.js";import"./index-DwkJHX1_.js";const a=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Hi Everyone \\n\\nI am silent follower of all you wonderful folks. I have learnt to play around Ollama and tie it up with my application make AI Application \\n\\nNow, I am planning to move to Llama.cpp can someone suggest how should I approach it and what should be learning path\\n\\nTIA\\n","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Llama.cpp after Ollama for industry grade softwares","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lqo8q0","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.59,"author_flair_background_color":null,"subreddit_type":"public","ups":3,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_eabbhyzu","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":3,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1751545491,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Hi Everyone &lt;/p&gt;\\n\\n&lt;p&gt;I am silent follower of all you wonderful folks. I have learnt to play around Ollama and tie it up with my application make AI Application &lt;/p&gt;\\n\\n&lt;p&gt;Now, I am planning to move to Llama.cpp can someone suggest how should I approach it and what should be learning path&lt;/p&gt;\\n\\n&lt;p&gt;TIA&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1lqo8q0","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"bull_bear25","discussion_type":null,"num_comments":18,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lqo8q0/llamacpp_after_ollama_for_industry_grade_softwares/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lqo8q0/llamacpp_after_ollama_for_industry_grade_softwares/","subreddit_subscribers":494198,"created_utc":1751545491,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n14vilh","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Foreign-Beginning-49","can_mod_post":false,"send_replies":true,"parent_id":"t1_n14alyl","score":2,"author_fullname":"t2_83u2l6o4","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Plus two for llama swap ","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n14vilh","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Plus two for llama swap &lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lqo8q0","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqo8q0/llamacpp_after_ollama_for_industry_grade_softwares/n14vilh/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751553907,"author_flair_text":"llama.cpp","treatment_tags":[],"created_utc":1751553907,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n14alyl","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"IJOY94","can_mod_post":false,"send_replies":true,"parent_id":"t1_n147kbl","score":6,"author_fullname":"t2_cq3rs","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"[https://github.com/mostlygeek/llama-swap](https://github.com/mostlygeek/llama-swap)","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n14alyl","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;&lt;a href=\\"https://github.com/mostlygeek/llama-swap\\"&gt;https://github.com/mostlygeek/llama-swap&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lqo8q0","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqo8q0/llamacpp_after_ollama_for_industry_grade_softwares/n14alyl/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751547301,"author_flair_text":null,"treatment_tags":[],"created_utc":1751547301,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1497hx","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"j0holo","can_mod_post":false,"send_replies":true,"parent_id":"t1_n147kbl","score":2,"author_fullname":"t2_hodrk","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"[https://llama-cpp-python.readthedocs.io/en/latest/](https://llama-cpp-python.readthedocs.io/en/latest/)","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n1497hx","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;&lt;a href=\\"https://llama-cpp-python.readthedocs.io/en/latest/\\"&gt;https://llama-cpp-python.readthedocs.io/en/latest/&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lqo8q0","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqo8q0/llamacpp_after_ollama_for_industry_grade_softwares/n1497hx/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751546813,"author_flair_text":null,"treatment_tags":[],"created_utc":1751546813,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n14b2n6","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"dodo13333","can_mod_post":false,"send_replies":true,"parent_id":"t1_n147kbl","score":1,"author_fullname":"t2_fxyo2uvcw","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"As a starter, I you will need a lammacpp server. Then, for each endpoint, you open a separate terminal (llm, embedding, reranker, etc). Go through help on server options. Gemini can assist you well on this, if stuck go back to reddit, by that time, you will have a clear issue to solve or advice. People here always helped me in similar situations.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n14b2n6","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;As a starter, I you will need a lammacpp server. Then, for each endpoint, you open a separate terminal (llm, embedding, reranker, etc). Go through help on server options. Gemini can assist you well on this, if stuck go back to reddit, by that time, you will have a clear issue to solve or advice. People here always helped me in similar situations.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lqo8q0","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqo8q0/llamacpp_after_ollama_for_industry_grade_softwares/n14b2n6/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751547460,"author_flair_text":null,"treatment_tags":[],"created_utc":1751547460,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n147kbl","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"bull_bear25","can_mod_post":false,"created_utc":1751546225,"send_replies":true,"parent_id":"t1_n1478ty","score":-1,"author_fullname":"t2_eabbhyzu","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Ollama was easy to use.\\nLlama.cpp I am unable to find the starting point. How to begin and what next. I have made RAG system it is working well with Ollama but I require more performance from LLM. Thought of using Llama.cpp","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n147kbl","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Ollama was easy to use.\\nLlama.cpp I am unable to find the starting point. How to begin and what next. I have made RAG system it is working well with Ollama but I require more performance from LLM. Thought of using Llama.cpp&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lqo8q0","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqo8q0/llamacpp_after_ollama_for_industry_grade_softwares/n147kbl/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751546225,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1478ty","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"j0holo","can_mod_post":false,"created_utc":1751546108,"send_replies":true,"parent_id":"t3_1lqo8q0","score":6,"author_fullname":"t2_hodrk","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Both Ollama and Llama.cpp are OpenAI API compatible. If you want to reliably handle multiple API calls at once I would recommend vLLM instead. But guessing from your question that is maybe a step too large.\\n\\nWhat kind of problem are you facing with switching from Ollama to Llama.cpp?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1478ty","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Both Ollama and Llama.cpp are OpenAI API compatible. If you want to reliably handle multiple API calls at once I would recommend vLLM instead. But guessing from your question that is maybe a step too large.&lt;/p&gt;\\n\\n&lt;p&gt;What kind of problem are you facing with switching from Ollama to Llama.cpp?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqo8q0/llamacpp_after_ollama_for_industry_grade_softwares/n1478ty/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751546108,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lqo8q0","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n152kzw","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Ok-Pipe-5151","can_mod_post":false,"created_utc":1751555910,"send_replies":true,"parent_id":"t3_1lqo8q0","score":6,"author_fullname":"t2_uxbdufm8b","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Neither of llama.cpp and ollama (which is itself a wrapper server around llama.cpp) has anything to do with \\"industry grade\\", unless you are embedding llama.cpp into your application for some purpose like running a evaluation model. \\n\\n\\nThere are several industry grade model serving libraries/servers including vLLM, sglang, deepspeed inference, triton etc. Both vLLM and sglang are very easy to deploy with their official container images","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n152kzw","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Neither of llama.cpp and ollama (which is itself a wrapper server around llama.cpp) has anything to do with &amp;quot;industry grade&amp;quot;, unless you are embedding llama.cpp into your application for some purpose like running a evaluation model. &lt;/p&gt;\\n\\n&lt;p&gt;There are several industry grade model serving libraries/servers including vLLM, sglang, deepspeed inference, triton etc. Both vLLM and sglang are very easy to deploy with their official container images&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqo8q0/llamacpp_after_ollama_for_industry_grade_softwares/n152kzw/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751555910,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lqo8q0","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1533vx","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Ok-Pipe-5151","can_mod_post":false,"send_replies":true,"parent_id":"t1_n14rfbn","score":3,"author_fullname":"t2_uxbdufm8b","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Then run your models locally? It is not what industry grade mean. Industry grade means operating a service commercially, typically at a large scale with atleast thousands of requests per second ","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n1533vx","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Then run your models locally? It is not what industry grade mean. Industry grade means operating a service commercially, typically at a large scale with atleast thousands of requests per second &lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lqo8q0","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqo8q0/llamacpp_after_ollama_for_industry_grade_softwares/n1533vx/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751556058,"author_flair_text":null,"treatment_tags":[],"created_utc":1751556058,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n157o3b","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"bull_bear25","can_mod_post":false,"send_replies":true,"parent_id":"t1_n14t4hm","score":2,"author_fullname":"t2_eabbhyzu","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"My bad apologies for confusion","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n157o3b","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;My bad apologies for confusion&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lqo8q0","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqo8q0/llamacpp_after_ollama_for_industry_grade_softwares/n157o3b/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751557334,"author_flair_text":null,"treatment_tags":[],"created_utc":1751557334,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n14t4hm","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"gpupoor","can_mod_post":false,"send_replies":true,"parent_id":"t1_n14rfbn","score":1,"author_fullname":"t2_1hcyral852","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"but then why industry grade? as in, the fastest? kind of confusing.\\n\\n\\nanyhow, fair enough in this case 1 GPU will be fine of course, but if you want the fastest software for AI models it's better if you learn to use vLLM. check if your hardware supports it. https://docs.vllm.ai/en/stable/getting_started/installation/gpu.html\\n\\n\\nOllama is based on llama.cpp so you won't see a big improvement in performance.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n14t4hm","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;but then why industry grade? as in, the fastest? kind of confusing.&lt;/p&gt;\\n\\n&lt;p&gt;anyhow, fair enough in this case 1 GPU will be fine of course, but if you want the fastest software for AI models it&amp;#39;s better if you learn to use vLLM. check if your hardware supports it. &lt;a href=\\"https://docs.vllm.ai/en/stable/getting_started/installation/gpu.html\\"&gt;https://docs.vllm.ai/en/stable/getting_started/installation/gpu.html&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;Ollama is based on llama.cpp so you won&amp;#39;t see a big improvement in performance.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lqo8q0","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqo8q0/llamacpp_after_ollama_for_industry_grade_softwares/n14t4hm/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751553202,"author_flair_text":null,"treatment_tags":[],"created_utc":1751553202,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n14rfbn","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"bull_bear25","can_mod_post":false,"created_utc":1751552709,"send_replies":true,"parent_id":"t1_n14dbim","score":1,"author_fullname":"t2_eabbhyzu","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Not if you use it for developing the application further rather than paying for API usage","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n14rfbn","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Not if you use it for developing the application further rather than paying for API usage&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lqo8q0","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqo8q0/llamacpp_after_ollama_for_industry_grade_softwares/n14rfbn/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751552709,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n14dbim","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"gpupoor","can_mod_post":false,"created_utc":1751548238,"send_replies":true,"parent_id":"t3_1lqo8q0","score":4,"author_fullname":"t2_1hcyral852","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"industry grade and llama.cpp is kind of an oxymoron, if you're planning to serve an app to potentially hundreds of users, llama.cpp with 1 GPU won't work.","edited":1751552903,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n14dbim","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;industry grade and llama.cpp is kind of an oxymoron, if you&amp;#39;re planning to serve an app to potentially hundreds of users, llama.cpp with 1 GPU won&amp;#39;t work.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqo8q0/llamacpp_after_ollama_for_industry_grade_softwares/n14dbim/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751548238,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lqo8q0","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n15c1p0","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"No-Statement-0001","can_mod_post":false,"created_utc":1751558561,"send_replies":true,"parent_id":"t3_1lqo8q0","score":1,"author_fullname":"t2_11gh93nhos","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Here’s a path that I would recommend. \\n\\nI’m leaving out some details for your learning but feel free to ask questions if you get stuck. This should be a step up in complexity from Ollama but you will get closer to the hardware and more control. \\n\\n- download the latest release from llama.cpp. https://github.com/ggml-org/llama.cpp/tags\\n\\n- the release archive will include llama-server. It is what provides an OpenAI compatible API. The docs are here:  https://github.com/ggml-org/llama.cpp/tree/master/tools/server\\n\\n- download a gguf model from HuggingFace. Typically [bartowski’s](https://huggingface.co/bartowski) and [unsloth AIs](https://huggingface.co/unsloth) are a good place to start. They’re also active in this community and we appreciate their efforts. \\n\\n- update your app to use llama-server’s OpenAI endpoints. This will allow you to switch inference engines much easier. \\n\\n- I have examples for using llama-server to serve various models on the [llama-swap wiki](https://github.com/mostlygeek/llama-swap/wiki).","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n15c1p0","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Here’s a path that I would recommend. &lt;/p&gt;\\n\\n&lt;p&gt;I’m leaving out some details for your learning but feel free to ask questions if you get stuck. This should be a step up in complexity from Ollama but you will get closer to the hardware and more control. &lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;&lt;p&gt;download the latest release from llama.cpp. &lt;a href=\\"https://github.com/ggml-org/llama.cpp/tags\\"&gt;https://github.com/ggml-org/llama.cpp/tags&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;p&gt;the release archive will include llama-server. It is what provides an OpenAI compatible API. The docs are here:  &lt;a href=\\"https://github.com/ggml-org/llama.cpp/tree/master/tools/server\\"&gt;https://github.com/ggml-org/llama.cpp/tree/master/tools/server&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;p&gt;download a gguf model from HuggingFace. Typically &lt;a href=\\"https://huggingface.co/bartowski\\"&gt;bartowski’s&lt;/a&gt; and &lt;a href=\\"https://huggingface.co/unsloth\\"&gt;unsloth AIs&lt;/a&gt; are a good place to start. They’re also active in this community and we appreciate their efforts. &lt;/p&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;p&gt;update your app to use llama-server’s OpenAI endpoints. This will allow you to switch inference engines much easier. &lt;/p&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;p&gt;I have examples for using llama-server to serve various models on the &lt;a href=\\"https://github.com/mostlygeek/llama-swap/wiki\\"&gt;llama-swap wiki&lt;/a&gt;.&lt;/p&gt;&lt;/li&gt;\\n&lt;/ul&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqo8q0/llamacpp_after_ollama_for_industry_grade_softwares/n15c1p0/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751558561,"author_flair_text":"llama.cpp","treatment_tags":[],"link_id":"t3_1lqo8q0","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n14v5q2","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"LA_rent_Aficionado","can_mod_post":false,"send_replies":true,"parent_id":"t1_n14sdux","score":2,"author_fullname":"t2_t8zbiflk","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Lmstudio is just a watered down wrapper for llama.cpp with less performance.\\n\\nFrankly, llama.cpp is well documented online in their GitHub and every online AI like ChatGPT is familiar enough to get you started.\\n\\nI made a llama server launcher if you are fearful of command line","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n14v5q2","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Lmstudio is just a watered down wrapper for llama.cpp with less performance.&lt;/p&gt;\\n\\n&lt;p&gt;Frankly, llama.cpp is well documented online in their GitHub and every online AI like ChatGPT is familiar enough to get you started.&lt;/p&gt;\\n\\n&lt;p&gt;I made a llama server launcher if you are fearful of command line&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lqo8q0","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqo8q0/llamacpp_after_ollama_for_industry_grade_softwares/n14v5q2/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751553803,"author_flair_text":null,"treatment_tags":[],"created_utc":1751553803,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n14yltq","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Ok_Cow1976","can_mod_post":false,"send_replies":true,"parent_id":"t1_n14sdux","score":1,"author_fullname":"t2_3pwbsmdr","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Forgot to mention, in lm studio, turn off \\" try mmap\\" (unless you want to use your SSD for your model) and \\"keep model in memory\\". As long as the model size + kv cache size is smaller than your VRAM size, things should be fast.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n14yltq","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Forgot to mention, in lm studio, turn off &amp;quot; try mmap&amp;quot; (unless you want to use your SSD for your model) and &amp;quot;keep model in memory&amp;quot;. As long as the model size + kv cache size is smaller than your VRAM size, things should be fast.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lqo8q0","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqo8q0/llamacpp_after_ollama_for_industry_grade_softwares/n14yltq/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751554786,"author_flair_text":null,"treatment_tags":[],"created_utc":1751554786,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n153mtc","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Ok-Pipe-5151","can_mod_post":false,"send_replies":true,"parent_id":"t1_n14sdux","score":1,"author_fullname":"t2_uxbdufm8b","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Bruh if you don't need a gui, stick with llama server. LM studio on non mac devices uses llama.cpp as default runtime. There won't be any difference, other than consuming more CPU ram due to electron GUI","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n153mtc","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Bruh if you don&amp;#39;t need a gui, stick with llama server. LM studio on non mac devices uses llama.cpp as default runtime. There won&amp;#39;t be any difference, other than consuming more CPU ram due to electron GUI&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lqo8q0","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqo8q0/llamacpp_after_ollama_for_industry_grade_softwares/n153mtc/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751556206,"author_flair_text":null,"treatment_tags":[],"created_utc":1751556206,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n14sdux","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"bull_bear25","can_mod_post":false,"created_utc":1751552987,"send_replies":true,"parent_id":"t1_n149271","score":-1,"author_fullname":"t2_eabbhyzu","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Thanks alot.\\nLet me try LM Studio","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n14sdux","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thanks alot.\\nLet me try LM Studio&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lqo8q0","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqo8q0/llamacpp_after_ollama_for_industry_grade_softwares/n14sdux/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751552987,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-1}}],"before":null}},"user_reports":[],"saved":false,"id":"n149271","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Ok_Cow1976","can_mod_post":false,"created_utc":1751546762,"send_replies":true,"parent_id":"t3_1lqo8q0","score":1,"author_fullname":"t2_3pwbsmdr","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Are you using nvidia or amd card(s)? And windows or linux or mac? Anyway, you need cuda driver for nvidia and rocm for amd. But you could always use vulkan driver as substitute (performance is not far away). Or you could just use lm studio which provides openai api as well and it does not require driver installation (because it is contained in lm studio). If you don't need to do \\"override tensors\\", lm studio should be sufficient.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n149271","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Are you using nvidia or amd card(s)? And windows or linux or mac? Anyway, you need cuda driver for nvidia and rocm for amd. But you could always use vulkan driver as substitute (performance is not far away). Or you could just use lm studio which provides openai api as well and it does not require driver installation (because it is contained in lm studio). If you don&amp;#39;t need to do &amp;quot;override tensors&amp;quot;, lm studio should be sufficient.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqo8q0/llamacpp_after_ollama_for_industry_grade_softwares/n149271/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751546762,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lqo8q0","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),n=()=>l.jsx(e,{data:a});export{n as default};
