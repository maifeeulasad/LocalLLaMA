import{j as e}from"./index-DACS7Nh6.js";import{R as t}from"./RedditPostRenderer-Dqa1NZuX.js";import"./index-DiMIVQx4.js";const a=JSON.parse('[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Hi everyone,\\n\\nI’m working on a project where I need to switch seamlessly between a locally-hosted LLaMA (via llama.cpp or vLLM) and various cloud LLMs (OpenAI, Gemini, Mistral, etc.). Managing separate SDKs and handling retries/failovers has been a real pain.\\n\\n**Questions:**\\n\\n1. How are you handling multi-provider routing in your local LLaMA stacks? Any patterns or existing tools?\\n2. What strategies do you use for latency-based fallback between local vs. remote models?\\n3. Tips on keeping your code DRY when you have to hit multiple different APIs?\\n\\nFor context, we’ve open-sourced a lightweight middleware called **TensorBlock Forge** (MIT) that gives you a single OpenAI-compatible endpoint for both local and cloud models. It handles health checks, key encryption, routing policies, and you can self-host it via Docker/K8s. But I’m curious what the community is already using or would like to see improved.\\n\\n**Repo:** [https://github.com/TensorBlock/forge](https://github.com/TensorBlock/forge)  \\n**Docs:** [https://tensorblock.co/api-docs](https://tensorblock.co/api-docs)\\n\\nWould love to hear your workflows, pointers, or feature requests—thanks in advance!\\n\\nhttps://preview.redd.it/msvogipfelbf1.png?width=3308&amp;format=png&amp;auto=webp&amp;s=272da05cd839c4c78dbdfae7307c55daf5d51c60\\n\\n**P.S.** We just hit #1 on Product Hunt today! If you’ve tried Forge (or plan to), an upvote would mean a lot: [https://www.producthunt.com/posts/tensorblock-forge]()","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Seeking advice on unifying local LLaMA and cloud LLMs under one API","link_flair_richtext":[{"e":"text","t":"Discussion"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":69,"top_awarded_type":null,"hide_score":false,"media_metadata":{"msvogipfelbf1":{"status":"valid","e":"Image","m":"image/png","p":[{"y":53,"x":108,"u":"https://preview.redd.it/msvogipfelbf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=eda7601d0f780222ec42017f659be625ecbebf95"},{"y":106,"x":216,"u":"https://preview.redd.it/msvogipfelbf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=08f494474fb819ebdb0e2735bd5a631cf272d211"},{"y":157,"x":320,"u":"https://preview.redd.it/msvogipfelbf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=de4f0661fd0c31d9691470cd422376e6a8ebc8ff"},{"y":315,"x":640,"u":"https://preview.redd.it/msvogipfelbf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=596397a62bbd16caae0fd5cc6eb20290eeac8b79"},{"y":473,"x":960,"u":"https://preview.redd.it/msvogipfelbf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=17a75b2e55fe0b0058a53ddb5ff427bccb31dbac"},{"y":532,"x":1080,"u":"https://preview.redd.it/msvogipfelbf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=8c96b0855318ee5e3337aebffb9f8dc67189cfe1"}],"s":{"y":1632,"x":3308,"u":"https://preview.redd.it/msvogipfelbf1.png?width=3308&amp;format=png&amp;auto=webp&amp;s=272da05cd839c4c78dbdfae7307c55daf5d51c60"},"id":"msvogipfelbf1"}},"name":"t3_1luha71","quarantine":false,"link_flair_text_color":"light","upvote_ratio":1,"author_flair_background_color":null,"ups":1,"total_awards_received":0,"media_embed":{},"thumbnail_width":140,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_pm9xeqh3","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Discussion","can_mod_post":false,"score":1,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"https://b.thumbs.redditmedia.com/dZW6kmM410yo2Y2BwkbZ-bmLebPWhbyRwDBNE3Pmk9I.jpg","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"post_hint":"self","content_categories":null,"is_self":true,"subreddit_type":"public","created":1751955259,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;\\n\\n&lt;p&gt;I’m working on a project where I need to switch seamlessly between a locally-hosted LLaMA (via llama.cpp or vLLM) and various cloud LLMs (OpenAI, Gemini, Mistral, etc.). Managing separate SDKs and handling retries/failovers has been a real pain.&lt;/p&gt;\\n\\n&lt;p&gt;&lt;strong&gt;Questions:&lt;/strong&gt;&lt;/p&gt;\\n\\n&lt;ol&gt;\\n&lt;li&gt;How are you handling multi-provider routing in your local LLaMA stacks? Any patterns or existing tools?&lt;/li&gt;\\n&lt;li&gt;What strategies do you use for latency-based fallback between local vs. remote models?&lt;/li&gt;\\n&lt;li&gt;Tips on keeping your code DRY when you have to hit multiple different APIs?&lt;/li&gt;\\n&lt;/ol&gt;\\n\\n&lt;p&gt;For context, we’ve open-sourced a lightweight middleware called &lt;strong&gt;TensorBlock Forge&lt;/strong&gt; (MIT) that gives you a single OpenAI-compatible endpoint for both local and cloud models. It handles health checks, key encryption, routing policies, and you can self-host it via Docker/K8s. But I’m curious what the community is already using or would like to see improved.&lt;/p&gt;\\n\\n&lt;p&gt;&lt;strong&gt;Repo:&lt;/strong&gt; &lt;a href=\\"https://github.com/TensorBlock/forge\\"&gt;https://github.com/TensorBlock/forge&lt;/a&gt;&lt;br/&gt;\\n&lt;strong&gt;Docs:&lt;/strong&gt; &lt;a href=\\"https://tensorblock.co/api-docs\\"&gt;https://tensorblock.co/api-docs&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;Would love to hear your workflows, pointers, or feature requests—thanks in advance!&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://preview.redd.it/msvogipfelbf1.png?width=3308&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=272da05cd839c4c78dbdfae7307c55daf5d51c60\\"&gt;https://preview.redd.it/msvogipfelbf1.png?width=3308&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=272da05cd839c4c78dbdfae7307c55daf5d51c60&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;&lt;strong&gt;P.S.&lt;/strong&gt; We just hit #1 on Product Hunt today! If you’ve tried Forge (or plan to), an upvote would mean a lot: [&lt;a href=\\"https://www.producthunt.com/posts/tensorblock-forge%5D()\\"&gt;https://www.producthunt.com/posts/tensorblock-forge]()&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":true,"is_crosspostable":false,"pinned":false,"over_18":false,"preview":{"images":[{"source":{"url":"https://external-preview.redd.it/KtqF55T5R5O_Sm4YbjwusEgQYb6qaXbNg0Zl4TFGdZw.png?auto=webp&amp;s=ec04562ed0ed7e5ed1924a8e3eaef2c87f92dee7","width":1200,"height":600},"resolutions":[{"url":"https://external-preview.redd.it/KtqF55T5R5O_Sm4YbjwusEgQYb6qaXbNg0Zl4TFGdZw.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=3942302dc94c4b8597f12b5928b3d53e4f36a0d6","width":108,"height":54},{"url":"https://external-preview.redd.it/KtqF55T5R5O_Sm4YbjwusEgQYb6qaXbNg0Zl4TFGdZw.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=2298f3beab60ee723a2e7c5e2689a75b82eeb315","width":216,"height":108},{"url":"https://external-preview.redd.it/KtqF55T5R5O_Sm4YbjwusEgQYb6qaXbNg0Zl4TFGdZw.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=611fd993a6bd1d7deafb9d172d6a7f6340cc9a40","width":320,"height":160},{"url":"https://external-preview.redd.it/KtqF55T5R5O_Sm4YbjwusEgQYb6qaXbNg0Zl4TFGdZw.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=e11d1fe69433d3ffe8c92c9faa8130e1f55e55e3","width":640,"height":320},{"url":"https://external-preview.redd.it/KtqF55T5R5O_Sm4YbjwusEgQYb6qaXbNg0Zl4TFGdZw.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=8b4005c21b796965f6651053ed58b1dc040bc188","width":960,"height":480},{"url":"https://external-preview.redd.it/KtqF55T5R5O_Sm4YbjwusEgQYb6qaXbNg0Zl4TFGdZw.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=70f9220d7ec0a9d8c30aafee15bcf72237aa58b5","width":1080,"height":540}],"variants":{},"id":"KtqF55T5R5O_Sm4YbjwusEgQYb6qaXbNg0Zl4TFGdZw"}],"enabled":false},"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"5f921ea4-c7bc-11ed-9c23-3a00622979b4","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"mod_note":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"num_reports":null,"removal_reason":null,"link_flair_background_color":"#646d73","id":"1luha71","is_robot_indexable":true,"num_duplicates":2,"report_reasons":null,"author":"Status-Hearing-4084","discussion_type":null,"num_comments":0,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1luha71/seeking_advice_on_unifying_local_llama_and_cloud/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1luha71/seeking_advice_on_unifying_local_llama_and_cloud/","subreddit_subscribers":496036,"created_utc":1751955259,"num_crossposts":2,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[],"before":null}}]'),i=()=>e.jsx(t,{data:a});export{i as default};
