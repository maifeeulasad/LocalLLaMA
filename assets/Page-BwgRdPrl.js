import{j as e}from"./index-C_z07ZVC.js";import{R as l}from"./RedditPostRenderer-DPnSR41P.js";import"./index-DKzOAewW.js";const t=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"I’m normally the guy they call in to fix the IT stuff nobody else can fix. I’ll laser focus on whatever it is and figure it out probably 99% of the time. I’ve been in IT for over 28+ years. \\nI’ve been messing with AI stuff for nearly 2 years now. Getting my Masters in AI right now. All that being said, I’ve never encountered a more difficult software package to run than trying to get vLLM working in Docker. \\nI can run nearly anything else in Docker except for vLLM. I feel like I’m really close, but every time I think it’s going to run, BAM! some new error that i find very little information on.\\n- I’m running Ubuntu 24.04\\n- I have a 4090, 3090, and 64GB of RAM on AERO-D TRX50 motherboard. \\n- Yes I have the Nvidia runtime container working \\n- Yes I have the hugginface token generated \\nis there an easy button somewhere that I’m missing? \\n","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Struggling with vLLM. The instructions make it sound so simple to run, but it’s like my Kryptonite. I give up.","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1loo2u3","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.79,"author_flair_background_color":null,"subreddit_type":"public","ups":46,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_y35oj","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":46,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1751330122,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;I’m normally the guy they call in to fix the IT stuff nobody else can fix. I’ll laser focus on whatever it is and figure it out probably 99% of the time. I’ve been in IT for over 28+ years. \\nI’ve been messing with AI stuff for nearly 2 years now. Getting my Masters in AI right now. All that being said, I’ve never encountered a more difficult software package to run than trying to get vLLM working in Docker. \\nI can run nearly anything else in Docker except for vLLM. I feel like I’m really close, but every time I think it’s going to run, BAM! some new error that i find very little information on.\\n- I’m running Ubuntu 24.04\\n- I have a 4090, 3090, and 64GB of RAM on AERO-D TRX50 motherboard. \\n- Yes I have the Nvidia runtime container working \\n- Yes I have the hugginface token generated \\nis there an easy button somewhere that I’m missing? &lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1loo2u3","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"Porespellar","discussion_type":null,"num_comments":60,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1loo2u3/struggling_with_vllm_the_instructions_make_it/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1loo2u3/struggling_with_vllm_the_instructions_make_it/","subreddit_subscribers":493458,"created_utc":1751330122,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0p72dr","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"The_IT_Dude_","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0oz4e4","score":7,"author_fullname":"t2_ft1ez","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Are you using the latest version of the container with the correct versions of the cuda drivers? If not, get ready for it to complain about the kvcache being the incorrect shape and all kinds of wild stuff.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0p72dr","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Are you using the latest version of the container with the correct versions of the cuda drivers? If not, get ready for it to complain about the kvcache being the incorrect shape and all kinds of wild stuff.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1loo2u3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1loo2u3/struggling_with_vllm_the_instructions_make_it/n0p72dr/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751341093,"author_flair_text":null,"treatment_tags":[],"created_utc":1751341093,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":7}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0q2jhv","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"kmouratidis","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0q2hwn","score":13,"author_fullname":"t2_k6u7rfxb","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"(pt.2):\\n\\n* RuntimeError: No suitable kernel. h\\\\_in=16 h\\\\_out=2816 dtype=Float out\\\\_dtype=Half\\n* RuntimeError: No suitable kernel. h\\\\_in=16 h\\\\_out=2816 dtype=Float out\\\\_dtype=BFloat16\\n* decode out of memory happened\\n* \\\\[rank0\\\\]:\\\\[W CudaIPCTypes.cpp:16\\\\] Producer process has been terminated before all shared CUDA tensors released. See Note \\\\[Sharing CUDA tensors\\\\]\\n* torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate...\\n* ImportError: cannot import name '\\\\_set\\\\_default\\\\_torch\\\\_dtype' from 'vllm.model\\\\_executor.model\\\\_loader' (/usr/local/lib/python3.10/dist-packages/vllm/model\\\\_executor/model\\\\_loader/\\\\_\\\\_init\\\\_\\\\_.py)\\n* Token indices sequence length is longer than the specified maximum sequence length for this model (4822 &gt; 4098). Running this sequence through the model will result in indexing error\\n* aiohttp.client\\\\_exceptions.ClientPayloadError: Response payload is not completed: &lt;TransferEncodingError: 400, message='Not enough data for satisfy transfer length header.'&gt;\\n* vllm.engine.async\\\\_llm\\\\_engine.AsyncEngineDeadError: Background loop has errored already. ","edited":1751357549,"author_flair_css_class":null,"name":"t1_n0q2jhv","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;(pt.2):&lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;RuntimeError: No suitable kernel. h_in=16 h_out=2816 dtype=Float out_dtype=Half&lt;/li&gt;\\n&lt;li&gt;RuntimeError: No suitable kernel. h_in=16 h_out=2816 dtype=Float out_dtype=BFloat16&lt;/li&gt;\\n&lt;li&gt;decode out of memory happened&lt;/li&gt;\\n&lt;li&gt;[rank0]:[W CudaIPCTypes.cpp:16] Producer process has been terminated before all shared CUDA tensors released. See Note [Sharing CUDA tensors]&lt;/li&gt;\\n&lt;li&gt;torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate...&lt;/li&gt;\\n&lt;li&gt;ImportError: cannot import name &amp;#39;_set_default_torch_dtype&amp;#39; from &amp;#39;vllm.model_executor.model_loader&amp;#39; (/usr/local/lib/python3.10/dist-packages/vllm/model_executor/model_loader/__init__.py)&lt;/li&gt;\\n&lt;li&gt;Token indices sequence length is longer than the specified maximum sequence length for this model (4822 &amp;gt; 4098). Running this sequence through the model will result in indexing error&lt;/li&gt;\\n&lt;li&gt;aiohttp.client_exceptions.ClientPayloadError: Response payload is not completed: &amp;lt;TransferEncodingError: 400, message=&amp;#39;Not enough data for satisfy transfer length header.&amp;#39;&amp;gt;&lt;/li&gt;\\n&lt;li&gt;vllm.engine.async_llm_engine.AsyncEngineDeadError: Background loop has errored already. &lt;/li&gt;\\n&lt;/ul&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1loo2u3","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1loo2u3/struggling_with_vllm_the_instructions_make_it/n0q2jhv/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751357320,"author_flair_text":null,"collapsed":false,"created_utc":1751357320,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":13}}],"before":null}},"user_reports":[],"saved":false,"id":"n0q2hwn","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"kmouratidis","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0ptqcs","score":15,"author_fullname":"t2_k6u7rfxb","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"At work, every time I or one of our downstream users encountered an error, I wrote it down. Most of it was miconfiguration and OOMs (note, we're using AWS p3/g4/g5/g6(e) instances), but lots of it wasn't. From early 0.2.x versions up until a few months ago, I was adding stuff to the list. I do not include any bugs related to AWS or docker image compilation (the vLLM image was broken for a long while). Here you go (pt.1):\\n\\n* ValueError: Bfloat16 is only supported on GPUs with compute capability of at least 8.0. Your Tesla V100-SXM2-16GB GPU has compute capability 7.0. You can use float16 instead by explicitly setting the\\\\\`dtype\\\\\` flag in CLI, for example: --dtype=half.\\n* ValueError: No available memory for the cache blocks. Try increasing \\\\\`gpu\\\\_memory\\\\_utilization\\\\\` when initializing the engine.\\n* ValueError: The model's max seq len (32768) is larger than the maximum number of tokens that can be stored in KV cache (27872). Try increasing \\\\\`gpu\\\\_memory\\\\_utilization\\\\\` or decreasing \\\\\`max\\\\_model\\\\_len\\\\\` when initializing the engine.\\n* \\\\_get\\\\_exception\\\\_class.&lt;locals&gt;.Derived: Not enought memory. Please try to increase --mem-fraction-static.\\n* RuntimeError: CUDA **error**: no kernel image is available for execution on the device\\n* torch.distributed.DistBackendError: NCCL error in: ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1333, internal error - please report this issue to the NCCL developers, NCCL version 2.18.1\\n* terminate called after throwing an instance of 'std::runtime\\\\_error'  what():  \\\\[Rank 1\\\\] NCCL watchdog thread terminated with exception: CUDA error: device-side assert triggered\\n* /home/runner/work/vllm/vllm/csrc/quantization/awq/gemm\\\\_kernels.cu:46: void vllm::awq::gemm\\\\_forward\\\\_4bit\\\\_cuda\\\\_m16nXk32(int, int, \\\\_\\\\_half \\\\*, int \\\\*, \\\\_\\\\_half \\\\*, int \\\\*, int, int, int, \\\\_\\\\_half \\\\*) \\\\[with int N = 128\\\\]: block: \\\\[25,0,0\\\\], thread: \\\\[18,1,0\\\\] Assertion \\\\\`false\\\\\` failed.\\n* RuntimeError: probability tensor contains either \\\\\`inf\\\\\`, \\\\\`nan\\\\\` or element &lt; 0\\n* KV cache pool leak detected!\\n* \\\\*\\\\*\\\\* SIGBUS received at time=1711379973 on cpu 18 \\\\*\\\\*\\\\* PC: @     0x7f753929a84a  (unknown)  (unknown) ... Fatal Python error: Bus error","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0q2hwn","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;At work, every time I or one of our downstream users encountered an error, I wrote it down. Most of it was miconfiguration and OOMs (note, we&amp;#39;re using AWS p3/g4/g5/g6(e) instances), but lots of it wasn&amp;#39;t. From early 0.2.x versions up until a few months ago, I was adding stuff to the list. I do not include any bugs related to AWS or docker image compilation (the vLLM image was broken for a long while). Here you go (pt.1):&lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;ValueError: Bfloat16 is only supported on GPUs with compute capability of at least 8.0. Your Tesla V100-SXM2-16GB GPU has compute capability 7.0. You can use float16 instead by explicitly setting the\`dtype\` flag in CLI, for example: --dtype=half.&lt;/li&gt;\\n&lt;li&gt;ValueError: No available memory for the cache blocks. Try increasing \`gpu_memory_utilization\` when initializing the engine.&lt;/li&gt;\\n&lt;li&gt;ValueError: The model&amp;#39;s max seq len (32768) is larger than the maximum number of tokens that can be stored in KV cache (27872). Try increasing \`gpu_memory_utilization\` or decreasing \`max_model_len\` when initializing the engine.&lt;/li&gt;\\n&lt;li&gt;_get_exception_class.&amp;lt;locals&amp;gt;.Derived: Not enought memory. Please try to increase --mem-fraction-static.&lt;/li&gt;\\n&lt;li&gt;RuntimeError: CUDA &lt;strong&gt;error&lt;/strong&gt;: no kernel image is available for execution on the device&lt;/li&gt;\\n&lt;li&gt;torch.distributed.DistBackendError: NCCL error in: ../torch/csrc/distributed/c10d/ProcessGroupNCCL.cpp:1333, internal error - please report this issue to the NCCL developers, NCCL version 2.18.1&lt;/li&gt;\\n&lt;li&gt;terminate called after throwing an instance of &amp;#39;std::runtime_error&amp;#39;  what():  [Rank 1] NCCL watchdog thread terminated with exception: CUDA error: device-side assert triggered&lt;/li&gt;\\n&lt;li&gt;/home/runner/work/vllm/vllm/csrc/quantization/awq/gemm_kernels.cu:46: void vllm::awq::gemm_forward_4bit_cuda_m16nXk32(int, int, __half *, int *, __half *, int *, int, int, int, __half *) [with int N = 128]: block: [25,0,0], thread: [18,1,0] Assertion \`false\` failed.&lt;/li&gt;\\n&lt;li&gt;RuntimeError: probability tensor contains either \`inf\`, \`nan\` or element &amp;lt; 0&lt;/li&gt;\\n&lt;li&gt;KV cache pool leak detected!&lt;/li&gt;\\n&lt;li&gt;*** SIGBUS received at time=1711379973 on cpu 18 *** PC: @     0x7f753929a84a  (unknown)  (unknown) ... Fatal Python error: Bus error&lt;/li&gt;\\n&lt;/ul&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1loo2u3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1loo2u3/struggling_with_vllm_the_instructions_make_it/n0q2hwn/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751357294,"author_flair_text":null,"treatment_tags":[],"created_utc":1751357294,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":15}}],"before":null}},"user_reports":[],"saved":false,"id":"n0ptqcs","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"gjsmo","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0oz4e4","score":4,"author_fullname":"t2_mp8mx","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Post at least some of them, that's a start. vLLM is definitely not as easy as something like Ollama, but it's also legitimately much faster.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0ptqcs","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Post at least some of them, that&amp;#39;s a start. vLLM is definitely not as easy as something like Ollama, but it&amp;#39;s also legitimately much faster.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1loo2u3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1loo2u3/struggling_with_vllm_the_instructions_make_it/n0ptqcs/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751352091,"author_flair_text":null,"treatment_tags":[],"created_utc":1751352091,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0r39jo","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"mister2d","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0oz4e4","score":4,"author_fullname":"t2_7gnbe","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Could have spent another few seconds code pasting your vLLM argument list.\\n\\nIt's very likely not a docker issue.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0r39jo","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Could have spent another few seconds code pasting your vLLM argument list.&lt;/p&gt;\\n\\n&lt;p&gt;It&amp;#39;s very likely not a docker issue.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1loo2u3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1loo2u3/struggling_with_vllm_the_instructions_make_it/n0r39jo/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751375164,"author_flair_text":null,"treatment_tags":[],"created_utc":1751375164,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}}],"before":null}},"user_reports":[],"saved":false,"id":"n0oz4e4","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Porespellar","can_mod_post":false,"created_utc":1751337998,"send_replies":true,"parent_id":"t1_n0oekfw","score":15,"author_fullname":"t2_y35oj","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Bro, it’s like a new error every time. There’s been so many. I’m tired boss. I’ll try again in the morning, it’s been a whack-a-mole situation and my patience is thin right now. Claude has actually been really helpful with troubleshooting even more so than like Stack Overflow.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0oz4e4","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Bro, it’s like a new error every time. There’s been so many. I’m tired boss. I’ll try again in the morning, it’s been a whack-a-mole situation and my patience is thin right now. Claude has actually been really helpful with troubleshooting even more so than like Stack Overflow.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1loo2u3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1loo2u3/struggling_with_vllm_the_instructions_make_it/n0oz4e4/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751337998,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":15}}],"before":null}},"user_reports":[],"saved":false,"id":"n0oekfw","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"DinoAmino","can_mod_post":false,"created_utc":1751330643,"send_replies":true,"parent_id":"t3_1loo2u3","score":74,"author_fullname":"t2_j1v7f","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"In your 28 years did you ever hear the phrase \\"steps to reproduce\\"? Can't help you if you don't provide your configuration and the error you're encountering.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0oekfw","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;In your 28 years did you ever hear the phrase &amp;quot;steps to reproduce&amp;quot;? Can&amp;#39;t help you if you don&amp;#39;t provide your configuration and the error you&amp;#39;re encountering.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1loo2u3/struggling_with_vllm_the_instructions_make_it/n0oekfw/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751330643,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1loo2u3","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":74}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0pntgm","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Glittering-Call8746","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0p7ja0","score":1,"author_fullname":"t2_tqwl6sawb","approved_by":null,"mod_note":null,"all_awardings":[],"body":"Nah i get u. Nothing to do with card. I know there are .. issues..  with ktansformers.. too many to see.  But if you could possibly point me the open issues related to your setup I could get a headups before jumping in.. I would definitely appreciate it. Rocm been.. disappointing after a year in waiting.. just saying..","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n0pntgm","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Nah i get u. Nothing to do with card. I know there are .. issues..  with ktansformers.. too many to see.  But if you could possibly point me the open issues related to your setup I could get a headups before jumping in.. I would definitely appreciate it. Rocm been.. disappointing after a year in waiting.. just saying..&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1loo2u3","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1loo2u3/struggling_with_vllm_the_instructions_make_it/n0pntgm/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751348860,"author_flair_text":null,"treatment_tags":[],"created_utc":1751348860,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0p7ja0","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Direspark","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0p3bsz","score":1,"author_fullname":"t2_675qc","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"It has nothing to do with the card. These are issues with ktransformers itself.","edited":false,"author_flair_css_class":null,"name":"t1_n0p7ja0","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It has nothing to do with the card. These are issues with ktransformers itself.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1loo2u3","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1loo2u3/struggling_with_vllm_the_instructions_make_it/n0p7ja0/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751341285,"author_flair_text":null,"collapsed":false,"created_utc":1751341285,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0p3bsz","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Glittering-Call8746","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0oio1k","score":1,"author_fullname":"t2_tqwl6sawb","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"What's the github url for the open issues.. I was thinking of jumping from 7900xtx to rtx 3090 for ktransformers.. I didn't know there would be issues..","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0p3bsz","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;What&amp;#39;s the github url for the open issues.. I was thinking of jumping from 7900xtx to rtx 3090 for ktransformers.. I didn&amp;#39;t know there would be issues..&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1loo2u3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1loo2u3/struggling_with_vllm_the_instructions_make_it/n0p3bsz/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751339607,"author_flair_text":null,"treatment_tags":[],"created_utc":1751339607,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0q0pq3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Few-Yam9901","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0oio1k","score":1,"author_fullname":"t2_1rhlf3bcfk","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Give Aphrodite engine a spin. It’s just as fast as vLLM, it either uses it or uses fork of it but it was way simpler for me","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0q0pq3","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Give Aphrodite engine a spin. It’s just as fast as vLLM, it either uses it or uses fork of it but it was way simpler for me&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1loo2u3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1loo2u3/struggling_with_vllm_the_instructions_make_it/n0q0pq3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751356218,"author_flair_text":null,"treatment_tags":[],"created_utc":1751356218,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0q6kuu","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Umthrfcker","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0oio1k","score":1,"author_fullname":"t2_mtnfuaa7g","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Experiencing the same right now. Ktransformers is such a pain in the ass.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0q6kuu","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Experiencing the same right now. Ktransformers is such a pain in the ass.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1loo2u3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1loo2u3/struggling_with_vllm_the_instructions_make_it/n0q6kuu/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751359794,"author_flair_text":null,"treatment_tags":[],"created_utc":1751359794,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0oio1k","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Direspark","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0oh3gc","score":6,"author_fullname":"t2_675qc","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I've tried it with multiple machines. Main is an RTX 3090 + Xeon workstation with 64gb RAM. Though unlike OP the issues I end up hitting always are open issues which are being reported by multiple other people. Then I'll check back, see that it's fixed, pull, rebuild, hit another issue.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0oio1k","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;ve tried it with multiple machines. Main is an RTX 3090 + Xeon workstation with 64gb RAM. Though unlike OP the issues I end up hitting always are open issues which are being reported by multiple other people. Then I&amp;#39;ll check back, see that it&amp;#39;s fixed, pull, rebuild, hit another issue.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1loo2u3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1loo2u3/struggling_with_vllm_the_instructions_make_it/n0oio1k/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751332098,"author_flair_text":null,"treatment_tags":[],"created_utc":1751332098,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}}],"before":null}},"user_reports":[],"saved":false,"id":"n0oh3gc","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Glittering-Call8746","can_mod_post":false,"created_utc":1751331541,"send_replies":true,"parent_id":"t1_n0ofnh4","score":2,"author_fullname":"t2_tqwl6sawb","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"What's ur setup ?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0oh3gc","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;What&amp;#39;s ur setup ?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1loo2u3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1loo2u3/struggling_with_vllm_the_instructions_make_it/n0oh3gc/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751331541,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0sybmv","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Conscious_Cut_6144","can_mod_post":false,"created_utc":1751394458,"send_replies":true,"parent_id":"t1_n0ofnh4","score":1,"author_fullname":"t2_9hl4ymvj","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"To be fair ktransformers is a hot mess.\\n\\nBasically the only way I have been able to do it is following the instructions from [ubergarm](https://github.com/ubergarm) https://github.com/ubergarm/r1-ktransformers-guide","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0sybmv","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;To be fair ktransformers is a hot mess.&lt;/p&gt;\\n\\n&lt;p&gt;Basically the only way I have been able to do it is following the instructions from &lt;a href=\\"https://github.com/ubergarm\\"&gt;ubergarm&lt;/a&gt; &lt;a href=\\"https://github.com/ubergarm/r1-ktransformers-guide\\"&gt;https://github.com/ubergarm/r1-ktransformers-guide&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1loo2u3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1loo2u3/struggling_with_vllm_the_instructions_make_it/n0sybmv/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751394458,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0ofnh4","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Direspark","can_mod_post":false,"created_utc":1751331029,"send_replies":true,"parent_id":"t3_1loo2u3","score":8,"author_fullname":"t2_675qc","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Me with ktansformers","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0ofnh4","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Me with ktansformers&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1loo2u3/struggling_with_vllm_the_instructions_make_it/n0ofnh4/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751331029,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1loo2u3","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":8}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0p2ikg","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"DAlmighty","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0ozl22","score":5,"author_fullname":"t2_a04uj","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"True but OP is on Ada.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0p2ikg","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;True but OP is on Ada.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1loo2u3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1loo2u3/struggling_with_vllm_the_instructions_make_it/n0p2ikg/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751339293,"author_flair_text":null,"treatment_tags":[],"created_utc":1751339293,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}}],"before":null}},"user_reports":[],"saved":false,"id":"n0ozl22","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"DinoAmino","can_mod_post":false,"created_utc":1751338173,"send_replies":true,"parent_id":"t1_n0ou1b3","score":5,"author_fullname":"t2_j1v7f","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Cuda 12.8 minimum for Blackwell.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0ozl22","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Cuda 12.8 minimum for Blackwell.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1loo2u3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1loo2u3/struggling_with_vllm_the_instructions_make_it/n0ozl22/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751338173,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0pnpki","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"random-tomato","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0phkl7","score":3,"author_fullname":"t2_fmd6oq5v6","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yeah I'm 99% sure if you have CUDA 12.9.1 that won't work for 3090s/4090s. You can look up whichever version it is and make sure to download that one.","edited":false,"author_flair_css_class":null,"name":"t1_n0pnpki","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yeah I&amp;#39;m 99% sure if you have CUDA 12.9.1 that won&amp;#39;t work for 3090s/4090s. You can look up whichever version it is and make sure to download that one.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1loo2u3","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":"light","treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1loo2u3/struggling_with_vllm_the_instructions_make_it/n0pnpki/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751348802,"author_flair_text":"llama.cpp","collapsed":false,"created_utc":1751348802,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0pwy66","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Ylsid","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0phkl7","score":2,"author_fullname":"t2_6lmlc","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Using an old version of CUDA because the newer ones just don't work? That makes sense! 🤡 (I am making fun of the system not you)","edited":false,"author_flair_css_class":null,"name":"t1_n0pwy66","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Using an old version of CUDA because the newer ones just don&amp;#39;t work? That makes sense! 🤡 (I am making fun of the system not you)&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1loo2u3","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1loo2u3/struggling_with_vllm_the_instructions_make_it/n0pwy66/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751353949,"author_flair_text":null,"collapsed":false,"created_utc":1751353949,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0pjoyc","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"UnionCounty22","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0piums","score":1,"author_fullname":"t2_6di5yjsp","approved_by":null,"mod_note":null,"all_awardings":[],"body":"Haha thank you kind sir. Modern tools are an amazing blessing","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n0pjoyc","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Haha thank you kind sir. Modern tools are an amazing blessing&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1loo2u3","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1loo2u3/struggling_with_vllm_the_instructions_make_it/n0pjoyc/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751346756,"author_flair_text":null,"treatment_tags":[],"created_utc":1751346756,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0piums","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"GoldCompetition7722","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0phkl7","score":1,"author_fullname":"t2_banbmed5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"\\"used cline when I did vllm\\".. thats a progamer move, Sir. Hats off)","edited":false,"author_flair_css_class":null,"name":"t1_n0piums","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;&amp;quot;used cline when I did vllm&amp;quot;.. thats a progamer move, Sir. Hats off)&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1loo2u3","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1loo2u3/struggling_with_vllm_the_instructions_make_it/n0piums/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751346339,"author_flair_text":null,"collapsed":false,"created_utc":1751346339,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0phkl7","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"UnionCounty22","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0p5f8m","score":4,"author_fullname":"t2_6di5yjsp","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Oh ya you’re going to want 12.4 for 3090 &amp; 4090. I just hopped off for the night but I have vllm running in Ubuntu 24.04. No docker or anything just a good old conda environment. If I were you I would try to install it into a fresh environment. Then when you hit apt glib and libc errors paste that to gpt4o or 4.1 etc. It will give you correct versions from the errors.  I think I may have used cline when I did vllm so it auto fixed it and started it up.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0phkl7","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Oh ya you’re going to want 12.4 for 3090 &amp;amp; 4090. I just hopped off for the night but I have vllm running in Ubuntu 24.04. No docker or anything just a good old conda environment. If I were you I would try to install it into a fresh environment. Then when you hit apt glib and libc errors paste that to gpt4o or 4.1 etc. It will give you correct versions from the errors.  I think I may have used cline when I did vllm so it auto fixed it and started it up.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1loo2u3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1loo2u3/struggling_with_vllm_the_instructions_make_it/n0phkl7/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751345711,"author_flair_text":null,"treatment_tags":[],"created_utc":1751345711,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0qgorq","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Gubru","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0pyyip","score":2,"author_fullname":"t2_iaggx","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"If using 12.9 instead of 12.8 is a problem then the CUDA team severely fucked up. You only get to do breaking changes with major versions, that’s the basic tenet of semver.","edited":false,"author_flair_css_class":null,"name":"t1_n0qgorq","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;If using 12.9 instead of 12.8 is a problem then the CUDA team severely fucked up. You only get to do breaking changes with major versions, that’s the basic tenet of semver.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1loo2u3","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1loo2u3/struggling_with_vllm_the_instructions_make_it/n0qgorq/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751365639,"author_flair_text":null,"collapsed":false,"created_utc":1751365639,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n0pyyip","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"butsicle","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0p5f8m","score":2,"author_fullname":"t2_acbu3","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"This is likely the issue. Clean install Cuda 12.8.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0pyyip","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;This is likely the issue. Clean install Cuda 12.8.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1loo2u3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1loo2u3/struggling_with_vllm_the_instructions_make_it/n0pyyip/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751355151,"author_flair_text":null,"treatment_tags":[],"created_utc":1751355151,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n0p5f8m","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Porespellar","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0p1o3z","score":1,"author_fullname":"t2_y35oj","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I’m on 12.9.1","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0p5f8m","is_submitter":true,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I’m on 12.9.1&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1loo2u3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1loo2u3/struggling_with_vllm_the_instructions_make_it/n0p5f8m/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751340429,"author_flair_text":null,"treatment_tags":[],"created_utc":1751340429,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0p1o3z","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"butsicle","can_mod_post":false,"created_utc":1751338962,"send_replies":true,"parent_id":"t1_n0ou1b3","score":0,"author_fullname":"t2_acbu3","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Cuda 12.8 for the latest version of vLLM","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0p1o3z","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Cuda 12.8 for the latest version of vLLM&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1loo2u3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1loo2u3/struggling_with_vllm_the_instructions_make_it/n0p1o3z/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751338962,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}},"user_reports":[],"saved":false,"id":"n0ou1b3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"DAlmighty","can_mod_post":false,"created_utc":1751336121,"send_replies":true,"parent_id":"t3_1loo2u3","score":18,"author_fullname":"t2_a04uj","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"If you guys think getting vLLM to run on Ada hardware is tough, stay FAR AWAY from Blackwell. \\n\\nI have felt your pain with getting vLLM to run so off the top of my head here are some things to check:\\n1. Make sure you’re running at least CUDA 12.4(I think)\\n2. Insure are passing the NVIDIA driver and capabilities in the docker configs\\n3. Torch latest is safe. Not sure of the minimum. \\n4. Install flashInfer, it will make life easier later on. \\n\\nYou didn’t mention which docker container you were using or any error messages you’re seeing so getting real help will be tough.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0ou1b3","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;If you guys think getting vLLM to run on Ada hardware is tough, stay FAR AWAY from Blackwell. &lt;/p&gt;\\n\\n&lt;p&gt;I have felt your pain with getting vLLM to run so off the top of my head here are some things to check:\\n1. Make sure you’re running at least CUDA 12.4(I think)\\n2. Insure are passing the NVIDIA driver and capabilities in the docker configs\\n3. Torch latest is safe. Not sure of the minimum. \\n4. Install flashInfer, it will make life easier later on. &lt;/p&gt;\\n\\n&lt;p&gt;You didn’t mention which docker container you were using or any error messages you’re seeing so getting real help will be tough.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1loo2u3/struggling_with_vllm_the_instructions_make_it/n0ou1b3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751336121,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1loo2u3","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":18}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0ou87y","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"opi098514","can_mod_post":false,"created_utc":1751336188,"send_replies":true,"parent_id":"t3_1loo2u3","score":9,"author_fullname":"t2_51dfdxum","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Gunna need more than “it doesn’t work bro.” Like we need errors, what model you are running. Litterally anything more than “it’s hard to use”","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0ou87y","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Gunna need more than “it doesn’t work bro.” Like we need errors, what model you are running. Litterally anything more than “it’s hard to use”&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1loo2u3/struggling_with_vllm_the_instructions_make_it/n0ou87y/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751336188,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1loo2u3","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":9}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0oe9qj","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Few-Yam9901","can_mod_post":false,"created_utc":1751330538,"send_replies":true,"parent_id":"t3_1loo2u3","score":4,"author_fullname":"t2_1rhlf3bcfk","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Same almost always run into problem every now and again an awq model just works. But 9 times out of 10 I need to trouble shoot to get vLLM to work","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0oe9qj","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Same almost always run into problem every now and again an awq model just works. But 9 times out of 10 I need to trouble shoot to get vLLM to work&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1loo2u3/struggling_with_vllm_the_instructions_make_it/n0oe9qj/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751330538,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1loo2u3","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0pwn0o","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"kmouratidis","can_mod_post":false,"created_utc":1751353765,"send_replies":true,"parent_id":"t3_1loo2u3","score":4,"author_fullname":"t2_k6u7rfxb","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"You're not wrong. I've been working and testing LLM inference frameworks for the better part of the last 1.5-2 years, at work and at home. ALL frameworks suck, in their own unique way.\\n\\nvLLM is a pain to configure. For a long time their docker images were completely broken, so at work we ended up using a custom built image. The users of our service (mostly AI researchers and engineers) rarely get a working configuration, with the most common issue being OOMs. I wrote a guide with tips about all the frameworks and their quirks, but even I struggle with random bugs, misconfiguration, and OOMs.\\n\\nStrangely, sglang has been a relatively good experience lately. It was a bigger pain than vLLM a year ago, but it has improved a lot. It also has its issues, but at least it's not as VRAM hungry and it \\"auto-configures\\" itself (with caveats). It's what I use at home, along with TabbyAPI/llamacpp when something doesn't run on sglang.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0pwn0o","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You&amp;#39;re not wrong. I&amp;#39;ve been working and testing LLM inference frameworks for the better part of the last 1.5-2 years, at work and at home. ALL frameworks suck, in their own unique way.&lt;/p&gt;\\n\\n&lt;p&gt;vLLM is a pain to configure. For a long time their docker images were completely broken, so at work we ended up using a custom built image. The users of our service (mostly AI researchers and engineers) rarely get a working configuration, with the most common issue being OOMs. I wrote a guide with tips about all the frameworks and their quirks, but even I struggle with random bugs, misconfiguration, and OOMs.&lt;/p&gt;\\n\\n&lt;p&gt;Strangely, sglang has been a relatively good experience lately. It was a bigger pain than vLLM a year ago, but it has improved a lot. It also has its issues, but at least it&amp;#39;s not as VRAM hungry and it &amp;quot;auto-configures&amp;quot; itself (with caveats). It&amp;#39;s what I use at home, along with TabbyAPI/llamacpp when something doesn&amp;#39;t run on sglang.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1loo2u3/struggling_with_vllm_the_instructions_make_it/n0pwn0o/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751353765,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1loo2u3","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0oxxm1","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Guna1260","can_mod_post":false,"created_utc":1751337552,"send_replies":true,"parent_id":"t3_1loo2u3","score":7,"author_fullname":"t2_2u7552ej","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Frankly VLLM is often pain. You never know which version will break what. From python version to cuda version to flash infer to every thing needs to be lined properly to get things working. I had success with GPTQ and AWQ. Never with GGUF. As VLLM does not support multi file GGUF(atleast last time I tried). Frankly I could see your pain. Every often I kind of think about moving to something like llamacpp or even ollama in my 4x3090 setup.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0oxxm1","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Frankly VLLM is often pain. You never know which version will break what. From python version to cuda version to flash infer to every thing needs to be lined properly to get things working. I had success with GPTQ and AWQ. Never with GGUF. As VLLM does not support multi file GGUF(atleast last time I tried). Frankly I could see your pain. Every often I kind of think about moving to something like llamacpp or even ollama in my 4x3090 setup.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1loo2u3/struggling_with_vllm_the_instructions_make_it/n0oxxm1/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751337552,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1loo2u3","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":7}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0pnvbc","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"random-tomato","can_mod_post":false,"created_utc":1751348887,"send_replies":true,"parent_id":"t1_n0op7vn","score":2,"author_fullname":"t2_fmd6oq5v6","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"This! After you install cuda libraries, sometimes other programs still don't recognize it so restarting often (but not too often) is a good idea.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0pnvbc","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;This! After you install cuda libraries, sometimes other programs still don&amp;#39;t recognize it so restarting often (but not too often) is a good idea.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1loo2u3","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1loo2u3/struggling_with_vllm_the_instructions_make_it/n0pnvbc/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751348887,"author_flair_text":"llama.cpp","treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n0op7vn","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"HistorianPotential48","can_mod_post":false,"created_utc":1751334406,"send_replies":true,"parent_id":"t3_1loo2u3","score":10,"author_fullname":"t2_4dzthia7","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"have you tried rebooting your computer (usually the smaller button besides the power button)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0op7vn","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;have you tried rebooting your computer (usually the smaller button besides the power button)&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1loo2u3/struggling_with_vllm_the_instructions_make_it/n0op7vn/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751334406,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1loo2u3","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":10}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0pbi02","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"kevin_1994","can_mod_post":false,"created_utc":1751342934,"send_replies":true,"parent_id":"t3_1loo2u3","score":3,"author_fullname":"t2_o015g","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"My experience is vllm is a huge pain to use as a hobbyist. It feels like this tool was build to run the raw bf16 tensors on enterprise machines. Which to be fair, it probably was. \\n\\nFor example the other day I tried to run the new Hunyuan model. I explicitly passed cuda device 0,1 but somewhere in the pipeline it was trying to use CUDA0. Eventually solved this by containerizing the runtime in docker and only passing the appropriate gpus. Ok, next run... some error about marlin quantization or something. Eventually work through this. Another error about using the wrong engine and can't use quantization. Ok, eventually work through this. Finally the model loads, took 20 mins btw... Seg fault. \\n\\nI just gave up and built a basic openai compatible server using python and transformers lol.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0pbi02","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;My experience is vllm is a huge pain to use as a hobbyist. It feels like this tool was build to run the raw bf16 tensors on enterprise machines. Which to be fair, it probably was. &lt;/p&gt;\\n\\n&lt;p&gt;For example the other day I tried to run the new Hunyuan model. I explicitly passed cuda device 0,1 but somewhere in the pipeline it was trying to use CUDA0. Eventually solved this by containerizing the runtime in docker and only passing the appropriate gpus. Ok, next run... some error about marlin quantization or something. Eventually work through this. Another error about using the wrong engine and can&amp;#39;t use quantization. Ok, eventually work through this. Finally the model loads, took 20 mins btw... Seg fault. &lt;/p&gt;\\n\\n&lt;p&gt;I just gave up and built a basic openai compatible server using python and transformers lol.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":true,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1loo2u3/struggling_with_vllm_the_instructions_make_it/n0pbi02/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751342934,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1loo2u3","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0rgylj","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"croninsiglos","can_mod_post":false,"created_utc":1751379588,"send_replies":true,"parent_id":"t1_n0pq6mf","score":3,"author_fullname":"t2_v7qje","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Sometimes this is the best solution. I can even start ollama cold, load a model, and get inference done in less time than vllm takes to start up.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0rgylj","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Sometimes this is the best solution. I can even start ollama cold, load a model, and get inference done in less time than vllm takes to start up.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1loo2u3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1loo2u3/struggling_with_vllm_the_instructions_make_it/n0rgylj/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751379588,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0rzc2x","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Careful-State-854","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0r53ha","score":1,"author_fullname":"t2_1asrkt8j7v","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"H100 and 800 users is a very nice project\\n\\nNote: If one of these 800 users have agents, not just chat, you may need way more h100s :)","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0rzc2x","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;H100 and 800 users is a very nice project&lt;/p&gt;\\n\\n&lt;p&gt;Note: If one of these 800 users have agents, not just chat, you may need way more h100s :)&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1loo2u3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1loo2u3/struggling_with_vllm_the_instructions_make_it/n0rzc2x/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751384815,"author_flair_text":null,"treatment_tags":[],"created_utc":1751384815,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0r53ha","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Porespellar","can_mod_post":false,"created_utc":1751375788,"send_replies":true,"parent_id":"t1_n0pq6mf","score":1,"author_fullname":"t2_y35oj","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"That’s what I’m using now, but I’m about to have a bunch of H100s (at work) and want to use them at their full potential and need to support a user base of about 800 total users so I figured vLLM was probably going to be necessary for batching or whatever. Trying to run it at home first before I try it at work. Hoping maybe smoother experience with H100s? 🤷‍♂️","edited":1751376026,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0r53ha","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;That’s what I’m using now, but I’m about to have a bunch of H100s (at work) and want to use them at their full potential and need to support a user base of about 800 total users so I figured vLLM was probably going to be necessary for batching or whatever. Trying to run it at home first before I try it at work. Hoping maybe smoother experience with H100s? 🤷‍♂️&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1loo2u3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1loo2u3/struggling_with_vllm_the_instructions_make_it/n0r53ha/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751375788,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0pq6mf","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Careful-State-854","can_mod_post":false,"created_utc":1751350119,"send_replies":true,"parent_id":"t3_1loo2u3","score":4,"author_fullname":"t2_1asrkt8j7v","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Just use ollama, it should be the same speed for single requests, and up to 10% slower when it runs 50 requests at the same time \\n\\nBut the vllm propaganda team makes themselves sound like 7 trillion times faster, like they summon gpus from the other side 😀","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0pq6mf","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Just use ollama, it should be the same speed for single requests, and up to 10% slower when it runs 50 requests at the same time &lt;/p&gt;\\n\\n&lt;p&gt;But the vllm propaganda team makes themselves sound like 7 trillion times faster, like they summon gpus from the other side 😀&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1loo2u3/struggling_with_vllm_the_instructions_make_it/n0pq6mf/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751350119,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1loo2u3","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0osjdb","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Ok_Hope_4007","can_mod_post":false,"created_utc":1751335584,"send_replies":true,"parent_id":"t3_1loo2u3","score":2,"author_fullname":"t2_pjzish3n","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I found it VERY picky in regards to gpu architecture/driver/CUDA Version/quantization technology AND your multi gpu settings.\\n\\nSo i assume your journey is to find the vllm compatibility baseline for these two cards.\\n\\nIn the end you will probably also find out that your desired combination does not work with two different cards.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0osjdb","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I found it VERY picky in regards to gpu architecture/driver/CUDA Version/quantization technology AND your multi gpu settings.&lt;/p&gt;\\n\\n&lt;p&gt;So i assume your journey is to find the vllm compatibility baseline for these two cards.&lt;/p&gt;\\n\\n&lt;p&gt;In the end you will probably also find out that your desired combination does not work with two different cards.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1loo2u3/struggling_with_vllm_the_instructions_make_it/n0osjdb/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751335584,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1loo2u3","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0p4oe1","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"I-cant_even","can_mod_post":false,"created_utc":1751340133,"send_replies":true,"parent_id":"t3_1loo2u3","score":2,"author_fullname":"t2_i56bj","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Goto claude, describe what you're trying to do, paste your error, follow steps, paste next error, rinse repeat.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0p4oe1","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Goto claude, describe what you&amp;#39;re trying to do, paste your error, follow steps, paste next error, rinse repeat.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1loo2u3/struggling_with_vllm_the_instructions_make_it/n0p4oe1/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751340133,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1loo2u3","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0p9ida","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ZiggityZaggityZoopoo","can_mod_post":false,"created_utc":1751342093,"send_replies":true,"parent_id":"t3_1loo2u3","score":2,"author_fullname":"t2_2p12rvb7","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Just give up? That was my solution.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0p9ida","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Just give up? That was my solution.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1loo2u3/struggling_with_vllm_the_instructions_make_it/n0p9ida/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751342093,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1loo2u3","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0pa5w7","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Nepherpitu","can_mod_post":false,"created_utc":1751342365,"send_replies":true,"parent_id":"t3_1loo2u3","score":2,"author_fullname":"t2_plp1w","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Well, you have hard time because using two different architectures. Use CUDA_VISIBLE_DEVICES to place 3090 first in order, it helps me. Also, V0 engine is faster and a bit more easier to run, so disable V1. Provide cache directory where models already downloaded and pass path to model folder, do not use HF downloader. Use AWQ quants.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0pa5w7","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Well, you have hard time because using two different architectures. Use CUDA_VISIBLE_DEVICES to place 3090 first in order, it helps me. Also, V0 engine is faster and a bit more easier to run, so disable V1. Provide cache directory where models already downloaded and pass path to model folder, do not use HF downloader. Use AWQ quants.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1loo2u3/struggling_with_vllm_the_instructions_make_it/n0pa5w7/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751342365,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1loo2u3","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0pjqyj","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Marksta","can_mod_post":false,"created_utc":1751346782,"send_replies":true,"parent_id":"t3_1loo2u3","score":2,"author_fullname":"t2_559a1","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Yeah, it's what llama.cpp reigns so supreme. All the other solutions are a headache or worse to get running.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0pjqyj","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yeah, it&amp;#39;s what llama.cpp reigns so supreme. All the other solutions are a headache or worse to get running.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1loo2u3/struggling_with_vllm_the_instructions_make_it/n0pjqyj/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751346782,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1loo2u3","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0udvcp","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"kmouratidis","can_mod_post":false,"created_utc":1751409623,"send_replies":true,"parent_id":"t1_n0py56i","score":1,"author_fullname":"t2_k6u7rfxb","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"\`--quantization gptq\` might be slower than \`gptq_marlin\`, at least in theory (was not the case when I tested with 0.5.x or so on older GPUs). Also it might be worth trying torch compilation (I use it in sglang, but should be possible for vllm/tensorrt too). And are you sure it makes sense using \`--kv-cache-dtype fp8\` with an INT4 model? (although probably fp16/bf16 won't be better).","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0udvcp","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;&lt;code&gt;--quantization gptq&lt;/code&gt; might be slower than &lt;code&gt;gptq_marlin&lt;/code&gt;, at least in theory (was not the case when I tested with 0.5.x or so on older GPUs). Also it might be worth trying torch compilation (I use it in sglang, but should be possible for vllm/tensorrt too). And are you sure it makes sense using &lt;code&gt;--kv-cache-dtype fp8&lt;/code&gt; with an INT4 model? (although probably fp16/bf16 won&amp;#39;t be better).&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1loo2u3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1loo2u3/struggling_with_vllm_the_instructions_make_it/n0udvcp/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751409623,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0py56i","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"kaisurniwurer","can_mod_post":false,"created_utc":1751354658,"send_replies":true,"parent_id":"t3_1loo2u3","score":2,"author_fullname":"t2_qafso","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"It was the same for me, after a few tries over a few days, getting chat to help me diagnose the problems as they pop up. 90% of my problems was missing a parameter or an incorrect parameter.\\n\\nEnded up with:\\n\\n    export VLLM_ATTENTION_BACKEND=FLASHINFER\\n    export VLLM_USE_FLASHINFER_SAMPLER=1\\n    export CUDA_VISIBLE_DEVICES=0,1\\n\\n    vllm serve /home/xxx/AI/LLM/Qwen3-30B-A3B-GPTQ-Int4 --tensor-parallel-size 2 --enable-expert-parallel --host 127.0.0.1 --port 5001 --api-key xxx --dtype auto --quantization gptq --gpu-memory-utilization 0.95 --kv-cache-dtype fp8 --calculate-kv-scales --max-seq-len 65536 --trust-remote-code --rope-scaling '{\\"rope_type\\":\\"yarn\\",\\"factor\\":2.0,\\"original_max_position_embeddings\\":32768}' \\n\\nWhen it did finally launch, the speed was pretty much the same as with kobold. I'm sure* that I could make it work better, but it was unnecessary pain in the ass and dropped the topic for now.","edited":1751359644,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0py56i","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It was the same for me, after a few tries over a few days, getting chat to help me diagnose the problems as they pop up. 90% of my problems was missing a parameter or an incorrect parameter.&lt;/p&gt;\\n\\n&lt;p&gt;Ended up with:&lt;/p&gt;\\n\\n&lt;pre&gt;&lt;code&gt;export VLLM_ATTENTION_BACKEND=FLASHINFER\\nexport VLLM_USE_FLASHINFER_SAMPLER=1\\nexport CUDA_VISIBLE_DEVICES=0,1\\n\\nvllm serve /home/xxx/AI/LLM/Qwen3-30B-A3B-GPTQ-Int4 --tensor-parallel-size 2 --enable-expert-parallel --host 127.0.0.1 --port 5001 --api-key xxx --dtype auto --quantization gptq --gpu-memory-utilization 0.95 --kv-cache-dtype fp8 --calculate-kv-scales --max-seq-len 65536 --trust-remote-code --rope-scaling &amp;#39;{&amp;quot;rope_type&amp;quot;:&amp;quot;yarn&amp;quot;,&amp;quot;factor&amp;quot;:2.0,&amp;quot;original_max_position_embeddings&amp;quot;:32768}&amp;#39; \\n&lt;/code&gt;&lt;/pre&gt;\\n\\n&lt;p&gt;When it did finally launch, the speed was pretty much the same as with kobold. I&amp;#39;m sure* that I could make it work better, but it was unnecessary pain in the ass and dropped the topic for now.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1loo2u3/struggling_with_vllm_the_instructions_make_it/n0py56i/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751354658,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1loo2u3","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"d2642412-d9ce-11ed-ae30-32b11309f5bd","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0phxfa","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"UnionCounty22","can_mod_post":false,"created_utc":1751345885,"send_replies":true,"parent_id":"t1_n0oery7","score":2,"author_fullname":"t2_6di5yjsp","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I wonder if using uv pip install vllm would resolve dependencies smoothly? Gawd I love uv.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0phxfa","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I wonder if using uv pip install vllm would resolve dependencies smoothly? Gawd I love uv.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1loo2u3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1loo2u3/struggling_with_vllm_the_instructions_make_it/n0phxfa/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751345885,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n0oery7","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ortegaalfredo","can_mod_post":false,"created_utc":1751330717,"send_replies":true,"parent_id":"t3_1loo2u3","score":2,"author_fullname":"t2_g177e","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Mi experience is that it's super easy to run, but basically I just do \\"pip install vllm\\" and that's it. For flashinfer is a little harder, something like\\n\\npip install flashinfer-python --find-links [https://flashinfer.ai/whl/cu124/torch2.6/flashinfer-python](https://flashinfer.ai/whl/cu124/torch2.6/flashinfer-python)\\n\\nBut also usually works.\\n\\nThing is, not every combination of model, quantization and parallelism works. I just find the qwen3 support is great and mostly everything works with that, but other models are hit-and-miss. You might try sglang that is almost the same level of performance and even easier to install imho.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0oery7","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"Alpaca"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Mi experience is that it&amp;#39;s super easy to run, but basically I just do &amp;quot;pip install vllm&amp;quot; and that&amp;#39;s it. For flashinfer is a little harder, something like&lt;/p&gt;\\n\\n&lt;p&gt;pip install flashinfer-python --find-links &lt;a href=\\"https://flashinfer.ai/whl/cu124/torch2.6/flashinfer-python\\"&gt;https://flashinfer.ai/whl/cu124/torch2.6/flashinfer-python&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;But also usually works.&lt;/p&gt;\\n\\n&lt;p&gt;Thing is, not every combination of model, quantization and parallelism works. I just find the qwen3 support is great and mostly everything works with that, but other models are hit-and-miss. You might try sglang that is almost the same level of performance and even easier to install imho.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1loo2u3/struggling_with_vllm_the_instructions_make_it/n0oery7/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751330717,"author_flair_text":"Alpaca","treatment_tags":[],"link_id":"t3_1loo2u3","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bd9e9e","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0pu72w","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"audioen","can_mod_post":false,"created_utc":1751352357,"send_replies":true,"parent_id":"t3_1loo2u3","score":2,"author_fullname":"t2_gz6hs","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I personally dislike Python software for having all the hallmarks of Java code from early 2000s: strict version requirements, massive dependencies, and lack of reproducibility unless every version of every dependency is nailed down exactly. In a way, it is actually worse because with Java code we didn't talk about shipping the entire operating system to make it run, which seems to be commonplace with python &amp; docker.\\n\\nCombine those aspects with general low performance and high memory usage, and it really feels like the 2000s all over again...\\n\\nSeriously, disk usage measurement of pretty much every venv directory related to AI comes back like 2+ GB of garbage having got installed there. Most of it is the nvidia poo. I can't wait to get rid of it and just use Vulkan or anything else.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0pu72w","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I personally dislike Python software for having all the hallmarks of Java code from early 2000s: strict version requirements, massive dependencies, and lack of reproducibility unless every version of every dependency is nailed down exactly. In a way, it is actually worse because with Java code we didn&amp;#39;t talk about shipping the entire operating system to make it run, which seems to be commonplace with python &amp;amp; docker.&lt;/p&gt;\\n\\n&lt;p&gt;Combine those aspects with general low performance and high memory usage, and it really feels like the 2000s all over again...&lt;/p&gt;\\n\\n&lt;p&gt;Seriously, disk usage measurement of pretty much every venv directory related to AI comes back like 2+ GB of garbage having got installed there. Most of it is the nvidia poo. I can&amp;#39;t wait to get rid of it and just use Vulkan or anything else.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1loo2u3/struggling_with_vllm_the_instructions_make_it/n0pu72w/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751352357,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1loo2u3","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0oredn","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AutomataManifold","can_mod_post":false,"created_utc":1751335177,"send_replies":true,"parent_id":"t3_1loo2u3","score":1,"author_fullname":"t2_bfs5bk7y8","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"What parameters are you invoking the server with? What's the actual error? \\n\\n\\nI generally run it bare metal rather than in a docker container, just to reduce the pass through headaches and maximize performance. But that's on a dedicated machine. ","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0oredn","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;What parameters are you invoking the server with? What&amp;#39;s the actual error? &lt;/p&gt;\\n\\n&lt;p&gt;I generally run it bare metal rather than in a docker container, just to reduce the pass through headaches and maximize performance. But that&amp;#39;s on a dedicated machine. &lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1loo2u3/struggling_with_vllm_the_instructions_make_it/n0oredn/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751335177,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1loo2u3","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0pg7cd","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"mlta01","can_mod_post":false,"created_utc":1751345062,"send_replies":true,"parent_id":"t3_1loo2u3","score":1,"author_fullname":"t2_1107ed","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Have you tried the [vllm docker container](https://docs.vllm.ai/en/stable/deployment/docker.html) ? I tried the containers on Ampere systems and they work. Maybe you need to first manually download the model using huggingface-cli ? \\n\\n\\n    docker run      --runtime nvidia \\\\\\n                --gpus all \\\\\\n                --ipc=host \\\\\\n                --net=host \\\\\\n                --shm-size 8G \\\\\\n                -v ~/.cache/huggingface:/root/.cache/huggingface \\\\\\n                --env \\"HUGGING_FACE_HUB_TOKEN=&lt;blah&gt;\\" \\\\\\n                vllm/vllm-openai:latest \\\\\\n                --tensor-parallel-size 2 \\\\\\n                --model google/gemma-3-27b-it-qat-q4_0-unquantized\\n\\nLike this...?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0pg7cd","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Have you tried the &lt;a href=\\"https://docs.vllm.ai/en/stable/deployment/docker.html\\"&gt;vllm docker container&lt;/a&gt; ? I tried the containers on Ampere systems and they work. Maybe you need to first manually download the model using huggingface-cli ? &lt;/p&gt;\\n\\n&lt;pre&gt;&lt;code&gt;docker run      --runtime nvidia \\\\\\n            --gpus all \\\\\\n            --ipc=host \\\\\\n            --net=host \\\\\\n            --shm-size 8G \\\\\\n            -v ~/.cache/huggingface:/root/.cache/huggingface \\\\\\n            --env &amp;quot;HUGGING_FACE_HUB_TOKEN=&amp;lt;blah&amp;gt;&amp;quot; \\\\\\n            vllm/vllm-openai:latest \\\\\\n            --tensor-parallel-size 2 \\\\\\n            --model google/gemma-3-27b-it-qat-q4_0-unquantized\\n&lt;/code&gt;&lt;/pre&gt;\\n\\n&lt;p&gt;Like this...?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1loo2u3/struggling_with_vllm_the_instructions_make_it/n0pg7cd/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751345062,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1loo2u3","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0pj155","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"LinkSea8324","can_mod_post":false,"created_utc":1751346428,"send_replies":true,"parent_id":"t3_1loo2u3","score":1,"author_fullname":"t2_152zyn72n4","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"If you think it's hard to run on ADA, as another guy said, stay away from blackwell\\n\\nAnd don't even bother trying to run it with GRID nVidia driviers","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0pj155","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;If you think it&amp;#39;s hard to run on ADA, as another guy said, stay away from blackwell&lt;/p&gt;\\n\\n&lt;p&gt;And don&amp;#39;t even bother trying to run it with GRID nVidia driviers&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1loo2u3/struggling_with_vllm_the_instructions_make_it/n0pj155/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751346428,"author_flair_text":"llama.cpp","treatment_tags":[],"link_id":"t3_1loo2u3","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0pjum9","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"admajic","can_mod_post":false,"created_utc":1751346835,"send_replies":true,"parent_id":"t3_1loo2u3","score":1,"author_fullname":"t2_60b9farf","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Try this hope it helps\\n\\nhttps://www.perplexity.ai/search/step-by-step-instructions-to-g-sAESrb0aRB2XvaYzXWqGcQ","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0pjum9","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Try this hope it helps&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://www.perplexity.ai/search/step-by-step-instructions-to-g-sAESrb0aRB2XvaYzXWqGcQ\\"&gt;https://www.perplexity.ai/search/step-by-step-instructions-to-g-sAESrb0aRB2XvaYzXWqGcQ&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1loo2u3/struggling_with_vllm_the_instructions_make_it/n0pjum9/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751346835,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1loo2u3","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0pmxk9","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"noiserr","can_mod_post":false,"created_utc":1751348398,"send_replies":true,"parent_id":"t3_1loo2u3","score":1,"author_fullname":"t2_2khn0","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I gave up on it too. Will be giving SGlang a try.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0pmxk9","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I gave up on it too. Will be giving SGlang a try.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1loo2u3/struggling_with_vllm_the_instructions_make_it/n0pmxk9/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751348398,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1loo2u3","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0pykej","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"-Ziero-","can_mod_post":false,"created_utc":1751354911,"send_replies":true,"parent_id":"t3_1loo2u3","score":1,"author_fullname":"t2_1ez4hahhti","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Have you tried building from source? I like having my own containers with all the packages I need.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0pykej","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Have you tried building from source? I like having my own containers with all the packages I need.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1loo2u3/struggling_with_vllm_the_instructions_make_it/n0pykej/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751354911,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1loo2u3","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0q6kps","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"p4s2wd","can_mod_post":false,"created_utc":1751359792,"send_replies":true,"parent_id":"t3_1loo2u3","score":1,"author_fullname":"t2_3tplbw0t","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Why not try sglang, it's more easy to run, or you can try llama.cpp.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0q6kps","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Why not try sglang, it&amp;#39;s more easy to run, or you can try llama.cpp.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1loo2u3/struggling_with_vllm_the_instructions_make_it/n0q6kps/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751359792,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1loo2u3","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0q6tow","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Excel_Document","can_mod_post":false,"created_utc":1751359943,"send_replies":true,"parent_id":"t3_1loo2u3","score":1,"author_fullname":"t2_dy04lp1x","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"there are working dockerfiles for vllm and i can also provide mine\\n\\n- you can also ask preplexity with deep research to make one for you (chat gpt/gemini keep including conflicting versions)\\n\\n- due to dependency hell it took me quite a while to get it working by myself , preplexity version worked immediately\\n\\n-","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0q6tow","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;there are working dockerfiles for vllm and i can also provide mine&lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;&lt;p&gt;you can also ask preplexity with deep research to make one for you (chat gpt/gemini keep including conflicting versions)&lt;/p&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;p&gt;due to dependency hell it took me quite a while to get it working by myself , preplexity version worked immediately&lt;/p&gt;&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;h2&gt;&lt;/h2&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1loo2u3/struggling_with_vllm_the_instructions_make_it/n0q6tow/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751359943,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1loo2u3","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0sxvkf","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Conscious_Cut_6144","can_mod_post":false,"created_utc":1751394332,"send_replies":true,"parent_id":"t3_1loo2u3","score":1,"author_fullname":"t2_9hl4ymvj","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Just don't use docker.  \\n  \\nmkdir vllm  \\ncd vllm  \\npython3 -m venv myenv  \\nsource myenv/bin/activate  \\npip install vllm  \\nvllm serve Qwen/Qwen3-32B-AWQ --max-model-len 8000 --tensor-parallel-size 2","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0sxvkf","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Just don&amp;#39;t use docker.  &lt;/p&gt;\\n\\n&lt;p&gt;mkdir vllm&lt;br/&gt;\\ncd vllm&lt;br/&gt;\\npython3 -m venv myenv&lt;br/&gt;\\nsource myenv/bin/activate&lt;br/&gt;\\npip install vllm&lt;br/&gt;\\nvllm serve Qwen/Qwen3-32B-AWQ --max-model-len 8000 --tensor-parallel-size 2&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1loo2u3/struggling_with_vllm_the_instructions_make_it/n0sxvkf/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751394332,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1loo2u3","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0vqua0","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"caetydid","can_mod_post":false,"created_utc":1751426847,"send_replies":true,"parent_id":"t3_1loo2u3","score":1,"author_fullname":"t2_2b6dk0nt","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I was just using vllm on a single RTX 4090 and was surprised how hard it is to not break anything when testing different models. Using two different GPUs seems like you are calling for the pain.\\n\\nI honestly don't get why vllm is recommended for production grade setups. Maybe have a look at [https://github.com/containers/ramalama](https://github.com/containers/ramalama) \\\\- i am just waiting until they come up with proper vllm engine support","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0vqua0","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I was just using vllm on a single RTX 4090 and was surprised how hard it is to not break anything when testing different models. Using two different GPUs seems like you are calling for the pain.&lt;/p&gt;\\n\\n&lt;p&gt;I honestly don&amp;#39;t get why vllm is recommended for production grade setups. Maybe have a look at &lt;a href=\\"https://github.com/containers/ramalama\\"&gt;https://github.com/containers/ramalama&lt;/a&gt; - i am just waiting until they come up with proper vllm engine support&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1loo2u3/struggling_with_vllm_the_instructions_make_it/n0vqua0/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751426847,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1loo2u3","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),n=()=>e.jsx(l,{data:t});export{n as default};
