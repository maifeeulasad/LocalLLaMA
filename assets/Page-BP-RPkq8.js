import{j as e}from"./index-DLSqWzaI.js";import{R as l}from"./RedditPostRenderer-CysRo2D_.js";import"./index-COXiL3Lo.js";const a=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"I've been using [NyxKrage's VRAM Calculator](https://huggingface.co/spaces/NyxKrage/LLM-Model-VRAM-Calculator) for a while, but I find sometimes I want to calculate this stuff without an internet connection or using a webpage. I also needed to calculate how much VRAM was needed for specific quants or for a lot of models. \\n\\nSo, I smacked together a cpp version of the calculator in a few hours. \\n\\nThere are two modes:\\n\\nCall the executable and supply all needed parameters with it as command-line arguments for JSON-formatted data perfect for workflows, or call the executable normally and input each argument manually.\\n\\nI'm planning to add functionality like calculating parameters, letting you use it without a \\\\\`config.json\\\\\`, etc. If you want anything added, add a Github Issue or feel free to fork it.\\n\\n[Link Here](https://github.com/71cj34/llmcalculator)","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Made a local C++ utility to calculate RAM needed to fit a quantized model","link_flair_richtext":[{"e":"text","t":"Resources"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1m4djo6","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.88,"author_flair_background_color":null,"subreddit_type":"public","ups":41,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_1e392klobf","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Resources","can_mod_post":false,"score":41,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"post_hint":"self","content_categories":null,"is_self":true,"mod_note":null,"created":1752977752,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;ve been using &lt;a href=\\"https://huggingface.co/spaces/NyxKrage/LLM-Model-VRAM-Calculator\\"&gt;NyxKrage&amp;#39;s VRAM Calculator&lt;/a&gt; for a while, but I find sometimes I want to calculate this stuff without an internet connection or using a webpage. I also needed to calculate how much VRAM was needed for specific quants or for a lot of models. &lt;/p&gt;\\n\\n&lt;p&gt;So, I smacked together a cpp version of the calculator in a few hours. &lt;/p&gt;\\n\\n&lt;p&gt;There are two modes:&lt;/p&gt;\\n\\n&lt;p&gt;Call the executable and supply all needed parameters with it as command-line arguments for JSON-formatted data perfect for workflows, or call the executable normally and input each argument manually.&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;m planning to add functionality like calculating parameters, letting you use it without a \`config.json\`, etc. If you want anything added, add a Github Issue or feel free to fork it.&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://github.com/71cj34/llmcalculator\\"&gt;Link Here&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"preview":{"images":[{"source":{"url":"https://external-preview.redd.it/CVC3UCXWBZbCWfBKaMAbb_pHJ3k1bjsiW1It_67lGVY.png?auto=webp&amp;s=3954f306637b48462dfbce5f7db7c79685858088","width":1200,"height":648},"resolutions":[{"url":"https://external-preview.redd.it/CVC3UCXWBZbCWfBKaMAbb_pHJ3k1bjsiW1It_67lGVY.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=dab6a28dd0587077a356703d53799748a71b3d4c","width":108,"height":58},{"url":"https://external-preview.redd.it/CVC3UCXWBZbCWfBKaMAbb_pHJ3k1bjsiW1It_67lGVY.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=0d78410b2ff71ed283d47b779a9b2110015782ee","width":216,"height":116},{"url":"https://external-preview.redd.it/CVC3UCXWBZbCWfBKaMAbb_pHJ3k1bjsiW1It_67lGVY.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=0ef9b2c4152dc5a850df1c49798c53736aa5db4a","width":320,"height":172},{"url":"https://external-preview.redd.it/CVC3UCXWBZbCWfBKaMAbb_pHJ3k1bjsiW1It_67lGVY.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=7d0e9c1ae5fa992910c2ee3bd14f2d87ae72e5cd","width":640,"height":345},{"url":"https://external-preview.redd.it/CVC3UCXWBZbCWfBKaMAbb_pHJ3k1bjsiW1It_67lGVY.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=e15e9744e01b7fabc83b056fd7bef010c2860e32","width":960,"height":518},{"url":"https://external-preview.redd.it/CVC3UCXWBZbCWfBKaMAbb_pHJ3k1bjsiW1It_67lGVY.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=b4343d18b58d772009825227390e2d2072d4bd48","width":1080,"height":583}],"variants":{},"id":"CVC3UCXWBZbCWfBKaMAbb_pHJ3k1bjsiW1It_67lGVY"}],"enabled":false},"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#ccac2b","id":"1m4djo6","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"philetairus_socius","discussion_type":null,"num_comments":23,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1m4djo6/made_a_local_c_utility_to_calculate_ram_needed_to/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1m4djo6/made_a_local_c_utility_to_calculate_ram_needed_to/","subreddit_subscribers":502274,"created_utc":1752977752,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n44z3p2","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Capable-Ad-7494","can_mod_post":false,"send_replies":true,"parent_id":"t1_n44n1hj","score":2,"author_fullname":"t2_9so78ol2","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"That sounds like it wouldn’t scale super well between different model architectures at longer context lengths though. Great for a ballpark though","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n44z3p2","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;That sounds like it wouldn’t scale super well between different model architectures at longer context lengths though. Great for a ballpark though&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m4djo6","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4djo6/made_a_local_c_utility_to_calculate_ram_needed_to/n44z3p2/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753002517,"author_flair_text":null,"treatment_tags":[],"created_utc":1753002517,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n44n1hj","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"triynizzles1","can_mod_post":false,"send_replies":true,"parent_id":"t1_n44kw3t","score":4,"author_fullname":"t2_zr0g49ixt","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I entered the example typed in to the vram calculator listed in the post and it gave me 19.5gb with 6k context and my napkin math was 19gb. No app needed.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n44n1hj","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I entered the example typed in to the vram calculator listed in the post and it gave me 19.5gb with 6k context and my napkin math was 19gb. No app needed.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m4djo6","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4djo6/made_a_local_c_utility_to_calculate_ram_needed_to/n44n1hj/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752995553,"author_flair_text":null,"treatment_tags":[],"created_utc":1752995553,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}}],"before":null}},"user_reports":[],"saved":false,"id":"n44kw3t","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"MelodicRecognition7","can_mod_post":false,"created_utc":1752994374,"send_replies":true,"parent_id":"t1_n4402q5","score":4,"author_fullname":"t2_1eex9ug5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"ew man, do not undercomplicate things, that's oppression!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n44kw3t","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;ew man, do not undercomplicate things, that&amp;#39;s oppression!&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m4djo6","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4djo6/made_a_local_c_utility_to_calculate_ram_needed_to/n44kw3t/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752994374,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n44yv16","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"tmvr","can_mod_post":false,"created_utc":1753002373,"send_replies":true,"parent_id":"t1_n4402q5","score":1,"author_fullname":"t2_11qlhv","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Roughly this, but the Q4 is a bit more, I go with 5/8 there as it is closer to the real 4.85 bpw of \\"mainstream\\" accepted quality/speed compromise of Q4\\\\_K\\\\_M.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n44yv16","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Roughly this, but the Q4 is a bit more, I go with 5/8 there as it is closer to the real 4.85 bpw of &amp;quot;mainstream&amp;quot; accepted quality/speed compromise of Q4_K_M.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m4djo6","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4djo6/made_a_local_c_utility_to_calculate_ram_needed_to/n44yv16/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753002373,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n48jhds","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"philetairus_socius","can_mod_post":false,"send_replies":true,"parent_id":"t1_n47j67d","score":1,"author_fullname":"t2_1e392klobf","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"yeah, i tried it last year, there are just too many variables for a perfect answer, if i can get within +- 5 tps I'll be happy","edited":false,"author_flair_css_class":null,"name":"t1_n48jhds","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;yeah, i tried it last year, there are just too many variables for a perfect answer, if i can get within +- 5 tps I&amp;#39;ll be happy&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1m4djo6","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4djo6/made_a_local_c_utility_to_calculate_ram_needed_to/n48jhds/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753048062,"author_flair_text":null,"collapsed":false,"created_utc":1753048062,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n47j67d","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"LongjumpingSpray8205","can_mod_post":false,"send_replies":true,"parent_id":"t1_n471dz1","score":1,"author_fullname":"t2_5nl88n9h","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"There are lots of variables on this one, I was trying to isolate impact of several variables( from pcie gen and lane burification [1x over usb, 4x ribbon, 8x ribbon or bios configured, 1 gpu at 16x vs 2 at 8x, combinations of nvlink and sli bridges, etc], to isolating gpu's by using integrated graphics or adding... yet another gpu,  I have a spreadsheet of experienced impact and my options on causes.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n47j67d","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;There are lots of variables on this one, I was trying to isolate impact of several variables( from pcie gen and lane burification [1x over usb, 4x ribbon, 8x ribbon or bios configured, 1 gpu at 16x vs 2 at 8x, combinations of nvlink and sli bridges, etc], to isolating gpu&amp;#39;s by using integrated graphics or adding... yet another gpu,  I have a spreadsheet of experienced impact and my options on causes.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m4djo6","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4djo6/made_a_local_c_utility_to_calculate_ram_needed_to/n47j67d/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753036699,"author_flair_text":null,"treatment_tags":[],"created_utc":1753036699,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n471dz1","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"philetairus_socius","can_mod_post":false,"send_replies":true,"parent_id":"t1_n46tjlp","score":1,"author_fullname":"t2_1e392klobf","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"If someone knows about this please tune me in, I seriously have no idea how you would calculate this and I'd love to know","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n471dz1","is_submitter":true,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;If someone knows about this please tune me in, I seriously have no idea how you would calculate this and I&amp;#39;d love to know&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m4djo6","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4djo6/made_a_local_c_utility_to_calculate_ram_needed_to/n471dz1/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753031414,"author_flair_text":null,"treatment_tags":[],"created_utc":1753031414,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n46tjlp","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ilhud9s","can_mod_post":false,"created_utc":1753029081,"send_replies":true,"parent_id":"t1_n4402q5","score":1,"author_fullname":"t2_3szjkcbx","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Is there a similar rough estimate for inference speed (token/sec)? I was thinking (memory bandwidth [GB]) / (model size [GB]) would be okay-ish but that has not been the case in my tests with Qwen3 0.6B vs 4B (all conditions, such as quant, context size, inference software etc are the same, except the param count). I was expecting 0.6B is roughly 6-7x faster than 4B but in reality the margin was much smaller.","edited":1753034701,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n46tjlp","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Is there a similar rough estimate for inference speed (token/sec)? I was thinking (memory bandwidth [GB]) / (model size [GB]) would be okay-ish but that has not been the case in my tests with Qwen3 0.6B vs 4B (all conditions, such as quant, context size, inference software etc are the same, except the param count). I was expecting 0.6B is roughly 6-7x faster than 4B but in reality the margin was much smaller.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m4djo6","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4djo6/made_a_local_c_utility_to_calculate_ram_needed_to/n46tjlp/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753029081,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4717l1","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"philetairus_socius","can_mod_post":false,"created_utc":1753031362,"send_replies":true,"parent_id":"t1_n4402q5","score":1,"author_fullname":"t2_1e392klobf","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yeah, I'm making exl2 quants but i want to be able to get some more precision programmatically so I made an integrated systems version of the website too i mentioned.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4717l1","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yeah, I&amp;#39;m making exl2 quants but i want to be able to get some more precision programmatically so I made an integrated systems version of the website too i mentioned.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m4djo6","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4djo6/made_a_local_c_utility_to_calculate_ram_needed_to/n4717l1/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753031362,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4402q5","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"triynizzles1","can_mod_post":false,"created_utc":1752984059,"send_replies":true,"parent_id":"t3_1m4djo6","score":21,"author_fullname":"t2_zr0g49ixt","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Fp16: Parameter size * 2 = GB of memory needed\\nQ8: parameter size * 1 = GB of memory needed.\\nQ4: parameter size * 0.5 = GB of memory needed\\n\\nAnd then plus 20% or so for useable context window.\\n\\nEx: 32b Q4 will take up 16gb and need another 3gb for a few thousand tokens context window.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4402q5","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Fp16: Parameter size * 2 = GB of memory needed\\nQ8: parameter size * 1 = GB of memory needed.\\nQ4: parameter size * 0.5 = GB of memory needed&lt;/p&gt;\\n\\n&lt;p&gt;And then plus 20% or so for useable context window.&lt;/p&gt;\\n\\n&lt;p&gt;Ex: 32b Q4 will take up 16gb and need another 3gb for a few thousand tokens context window.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4djo6/made_a_local_c_utility_to_calculate_ram_needed_to/n4402q5/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752984059,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m4djo6","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":21}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4759de","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"philetairus_socius","can_mod_post":false,"created_utc":1753032545,"send_replies":true,"parent_id":"t1_n43tthy","score":2,"author_fullname":"t2_1e392klobf","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"This looks very useful!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4759de","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;This looks very useful!&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m4djo6","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4djo6/made_a_local_c_utility_to_calculate_ram_needed_to/n4759de/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753032545,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n43tthy","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"DeProgrammer99","can_mod_post":false,"created_utc":1752981374,"send_replies":true,"parent_id":"t3_1m4djo6","score":5,"author_fullname":"t2_w4j8t","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Adding to this... I threw this together at some point for viewing GGUF metadata, but I recently also made it calculate KV cache per token. It has both C# and JavaScript versions. Doesn't account for the RAM for storing intermediate values, though.\\nhttps://github.com/dpmm99/GGUFDump\\n\\nZero dependencies other than either a browser or .NET runtime. Just drop a GGUF on the EXE or into the web page.","edited":1752981624,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n43tthy","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Adding to this... I threw this together at some point for viewing GGUF metadata, but I recently also made it calculate KV cache per token. It has both C# and JavaScript versions. Doesn&amp;#39;t account for the RAM for storing intermediate values, though.\\n&lt;a href=\\"https://github.com/dpmm99/GGUFDump\\"&gt;https://github.com/dpmm99/GGUFDump&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;Zero dependencies other than either a browser or .NET runtime. Just drop a GGUF on the EXE or into the web page.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4djo6/made_a_local_c_utility_to_calculate_ram_needed_to/n43tthy/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752981374,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m4djo6","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4armyb","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"philetairus_socius","can_mod_post":false,"send_replies":true,"parent_id":"t1_n49mkfy","score":2,"author_fullname":"t2_1e392klobf","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Alright I can do that, I actually didn't know that was a thing you could do- usually keeping them both the same is best, but since only keys are the ones getting dot producted and softmaxed I can see that you can probably get away with quantizing those ones lower. I'll work on it once I wake up. Thanks for telling me","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n4armyb","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Alright I can do that, I actually didn&amp;#39;t know that was a thing you could do- usually keeping them both the same is best, but since only keys are the ones getting dot producted and softmaxed I can see that you can probably get away with quantizing those ones lower. I&amp;#39;ll work on it once I wake up. Thanks for telling me&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1m4djo6","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4djo6/made_a_local_c_utility_to_calculate_ram_needed_to/n4armyb/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753080177,"author_flair_text":null,"treatment_tags":[],"created_utc":1753080177,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":6,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n49mkfy","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"admajic","can_mod_post":false,"send_replies":true,"parent_id":"t1_n498awc","score":1,"author_fullname":"t2_60b9farf","approved_by":null,"mod_note":null,"all_awardings":[],"body":"No you would need to add another section with drop downs for both k and v and user can choose\\n\\nhttps://www.perplexity.ai/search/how-does-the-deep-down-cache-k-_P5wnWq5QA.i97RbNeGtew#0","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n49mkfy","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;No you would need to add another section with drop downs for both k and v and user can choose&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://www.perplexity.ai/search/how-does-the-deep-down-cache-k-_P5wnWq5QA.i97RbNeGtew#0\\"&gt;https://www.perplexity.ai/search/how-does-the-deep-down-cache-k-_P5wnWq5QA.i97RbNeGtew#0&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1m4djo6","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4djo6/made_a_local_c_utility_to_calculate_ram_needed_to/n49mkfy/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753061996,"author_flair_text":null,"treatment_tags":[],"created_utc":1753061996,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n498awc","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"philetairus_socius","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4930ym","score":2,"author_fullname":"t2_1e392klobf","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"New release uploaded now. To be honest I'm not actually sure if I implemented it correctly, so please let me know if you see any issues.","edited":false,"author_flair_css_class":null,"name":"t1_n498awc","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;New release uploaded now. To be honest I&amp;#39;m not actually sure if I implemented it correctly, so please let me know if you see any issues.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1m4djo6","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4djo6/made_a_local_c_utility_to_calculate_ram_needed_to/n498awc/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753056672,"author_flair_text":null,"collapsed":false,"created_utc":1753056672,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n4930ym","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"philetairus_socius","can_mod_post":false,"send_replies":true,"parent_id":"t1_n48uk6b","score":1,"author_fullname":"t2_1e392klobf","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Ah, I see what you mean. I'm working on implementing it now. As far as I know, KV cache quantization should only slash context-size memory, not model loading, and you can only set it to fp16/fp8/fp4 so all that needs to be done is dividing context memory reqs by 2 or 4. Thanks for letting me know","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4930ym","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Ah, I see what you mean. I&amp;#39;m working on implementing it now. As far as I know, KV cache quantization should only slash context-size memory, not model loading, and you can only set it to fp16/fp8/fp4 so all that needs to be done is dividing context memory reqs by 2 or 4. Thanks for letting me know&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m4djo6","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4djo6/made_a_local_c_utility_to_calculate_ram_needed_to/n4930ym/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753054778,"author_flair_text":null,"treatment_tags":[],"created_utc":1753054778,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n48uk6b","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"admajic","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4755n5","score":1,"author_fullname":"t2_60b9farf","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"In lmstudio there is an extra option to load k and v cache quantasiation in q4 or q8 fp16 etc","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n48uk6b","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;In lmstudio there is an extra option to load k and v cache quantasiation in q4 or q8 fp16 etc&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m4djo6","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4djo6/made_a_local_c_utility_to_calculate_ram_needed_to/n48uk6b/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753051845,"author_flair_text":null,"treatment_tags":[],"created_utc":1753051845,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4755n5","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"philetairus_socius","can_mod_post":false,"created_utc":1753032516,"send_replies":true,"parent_id":"t1_n44usvq","score":1,"author_fullname":"t2_1e392klobf","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I'd like to know more about this issue? It should support calculation of kv cache currently and I can't replicate a 300gb ram result with a qwen 14b config.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4755n5","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;d like to know more about this issue? It should support calculation of kv cache currently and I can&amp;#39;t replicate a 300gb ram result with a qwen 14b config.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m4djo6","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4djo6/made_a_local_c_utility_to_calculate_ram_needed_to/n4755n5/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753032516,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n44usvq","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"admajic","can_mod_post":false,"created_utc":1752999971,"send_replies":true,"parent_id":"t3_1m4djo6","score":2,"author_fullname":"t2_60b9farf","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Can you add k and v cache then I can try 128k context. Says a 14gb model need 300gb ram without k cache cache","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n44usvq","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Can you add k and v cache then I can try 128k context. Says a 14gb model need 300gb ram without k cache cache&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4djo6/made_a_local_c_utility_to_calculate_ram_needed_to/n44usvq/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752999971,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m4djo6","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n44fv98","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"NickCanCode","can_mod_post":false,"created_utc":1752991652,"send_replies":true,"parent_id":"t1_n44a47l","score":5,"author_fullname":"t2_srx5k6vf","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"EXE = An EXEcutable, a compiled application ready to run on WIndows. Unlike webpage hosted on a web server and safe guarded by your browser, you run it directly on your PC and it can contains malicious code, virus, ransomware so it is recommended to only run 3rd party EXE from trusted source. Even if a exe is from an open source project, the compiled exe can still be unsafe and can be totally different from the source. That's why application distributed in Linux is mostly delivered by source code and compiled directly on end user's machine to reduce the risk.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n44fv98","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;EXE = An EXEcutable, a compiled application ready to run on WIndows. Unlike webpage hosted on a web server and safe guarded by your browser, you run it directly on your PC and it can contains malicious code, virus, ransomware so it is recommended to only run 3rd party EXE from trusted source. Even if a exe is from an open source project, the compiled exe can still be unsafe and can be totally different from the source. That&amp;#39;s why application distributed in Linux is mostly delivered by source code and compiled directly on end user&amp;#39;s machine to reduce the risk.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m4djo6","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4djo6/made_a_local_c_utility_to_calculate_ram_needed_to/n44fv98/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752991652,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n474m3x","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"philetairus_socius","can_mod_post":false,"created_utc":1753032358,"send_replies":true,"parent_id":"t1_n44a47l","score":1,"author_fullname":"t2_1e392klobf","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I would make a Mac version but I don't have a Mac so I don't have xcode and the rest of the mac development environment... I'll leave it to someone else :P","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n474m3x","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I would make a Mac version but I don&amp;#39;t have a Mac so I don&amp;#39;t have xcode and the rest of the mac development environment... I&amp;#39;ll leave it to someone else :P&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m4djo6","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4djo6/made_a_local_c_utility_to_calculate_ram_needed_to/n474m3x/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753032358,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n44a47l","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":"LOW_SCORE","no_follow":true,"author":"tronathan","can_mod_post":false,"created_utc":1752988733,"send_replies":true,"parent_id":"t3_1m4djo6","score":-7,"author_fullname":"t2_3aqn7","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":true,"body":"What’s an EXE?\\n\\nSrsly though, having the equivalent of Mac’s “Get Info…” as a command line utility with llama cpp would be handy.  Maybe even integrated? Kinda scope-creepy.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n44a47l","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;What’s an EXE?&lt;/p&gt;\\n\\n&lt;p&gt;Srsly though, having the equivalent of Mac’s “Get Info…” as a command line utility with llama cpp would be handy.  Maybe even integrated? Kinda scope-creepy.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":"comment score below threshold","distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4djo6/made_a_local_c_utility_to_calculate_ram_needed_to/n44a47l/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752988733,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m4djo6","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-7}}],"before":null}}]`),n=()=>e.jsx(l,{data:a});export{n as default};
