import{j as e}from"./index-xfnGEtuL.js";import{R as t}from"./RedditPostRenderer-DAZRIZZK.js";import"./index-BaNn5-RR.js";const a=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"I am curious to understand if models tend to perform better if you treat it like a tin-can, structuring the prompt as imperative commands. I suspect it would perform better with STEM-type tasks since those are how many of the problems in open datasets are written. But what about non-STEM tasks like creative writing or data retrieval?\\n\\nIn my experience, the \\"prompt optimizers\\" provided by e.g. OpenAI suck.\\n\\nIs there a way to identify what techniques for each model the best? Could one train small models to style transfer your prompts into specific different styles and autonomously see which works best for a particular task for a given model? I recall a similar strategy being used for jailbreaking models.","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Has there been research into prompting strategies for models?","link_flair_richtext":[{"e":"text","t":"Discussion"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1luycyq","quarantine":false,"link_flair_text_color":"light","upvote_ratio":1,"author_flair_background_color":null,"subreddit_type":"public","ups":4,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_101haj","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Discussion","can_mod_post":false,"score":4,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1752004095,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;I am curious to understand if models tend to perform better if you treat it like a tin-can, structuring the prompt as imperative commands. I suspect it would perform better with STEM-type tasks since those are how many of the problems in open datasets are written. But what about non-STEM tasks like creative writing or data retrieval?&lt;/p&gt;\\n\\n&lt;p&gt;In my experience, the &amp;quot;prompt optimizers&amp;quot; provided by e.g. OpenAI suck.&lt;/p&gt;\\n\\n&lt;p&gt;Is there a way to identify what techniques for each model the best? Could one train small models to style transfer your prompts into specific different styles and autonomously see which works best for a particular task for a given model? I recall a similar strategy being used for jailbreaking models.&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"5f921ea4-c7bc-11ed-9c23-3a00622979b4","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#646d73","id":"1luycyq","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"TheRealMasonMac","discussion_type":null,"num_comments":3,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1luycyq/has_there_been_research_into_prompting_strategies/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1luycyq/has_there_been_research_into_prompting_strategies/","subreddit_subscribers":496591,"created_utc":1752004095,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n21mq80","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ii_social","can_mod_post":false,"created_utc":1752004237,"send_replies":true,"parent_id":"t3_1luycyq","score":2,"author_fullname":"t2_tohvxz80x","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Hey yes, this is called evaluations, you can create a list of questions and get back answers across models and see which ones preform best, you can create variations of your tests to see how this plays out, I used to work on a startup that made this kind of software. \\n\\nThere’s free options out there!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n21mq80","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Hey yes, this is called evaluations, you can create a list of questions and get back answers across models and see which ones preform best, you can create variations of your tests to see how this plays out, I used to work on a startup that made this kind of software. &lt;/p&gt;\\n\\n&lt;p&gt;There’s free options out there!&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1luycyq/has_there_been_research_into_prompting_strategies/n21mq80/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752004237,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1luycyq","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n21nlul","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Double_Cause4609","can_mod_post":false,"created_utc":1752004478,"send_replies":true,"parent_id":"t3_1luycyq","score":2,"author_fullname":"t2_1kubzxt2ww","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"There's an incredible breadth of research into prompting. It's almost its own field of study.\\n\\nAs for imperative commands... Yes. This is actually a huge problem in east Asian languages, because they're not used to providing direct imperative commands (a lot of the time those will come out as suggestions) which makes it difficult to use these systems.\\n\\nAs for which techniques work best...It seems that there are scale variant techniques, where models of a sufficient size tend to have a few things in common, and there are model family dependent techniques that tend to work within a given model family (so like, most Gemma 3 models like XYZ in the prompt, or Llama 3.1 models like ABC, etc).\\n\\nThere are also techniques that seem to work in specific scenarios (math, reasoning, logic, creative writing, etc) in a general sense.\\n\\nAs for specifics? Google. There's so many that it's not immediately clear where you start. To give you an idea, there's prompting techniques specific to RAG, to inductive biases (RNNs like when the prompt and backround information are repeated, for instance), prompts specific to...Well...Everything.\\n\\nSome of them are closer to workflows than just individual prompt (things like Graph of Thought, or Forest of Thought, etc).\\n\\nSmolAgents and things like Program of thought found that giving LLMs access to a Python interpreter helps quite a bit, too.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n21nlul","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;There&amp;#39;s an incredible breadth of research into prompting. It&amp;#39;s almost its own field of study.&lt;/p&gt;\\n\\n&lt;p&gt;As for imperative commands... Yes. This is actually a huge problem in east Asian languages, because they&amp;#39;re not used to providing direct imperative commands (a lot of the time those will come out as suggestions) which makes it difficult to use these systems.&lt;/p&gt;\\n\\n&lt;p&gt;As for which techniques work best...It seems that there are scale variant techniques, where models of a sufficient size tend to have a few things in common, and there are model family dependent techniques that tend to work within a given model family (so like, most Gemma 3 models like XYZ in the prompt, or Llama 3.1 models like ABC, etc).&lt;/p&gt;\\n\\n&lt;p&gt;There are also techniques that seem to work in specific scenarios (math, reasoning, logic, creative writing, etc) in a general sense.&lt;/p&gt;\\n\\n&lt;p&gt;As for specifics? Google. There&amp;#39;s so many that it&amp;#39;s not immediately clear where you start. To give you an idea, there&amp;#39;s prompting techniques specific to RAG, to inductive biases (RNNs like when the prompt and backround information are repeated, for instance), prompts specific to...Well...Everything.&lt;/p&gt;\\n\\n&lt;p&gt;Some of them are closer to workflows than just individual prompt (things like Graph of Thought, or Forest of Thought, etc).&lt;/p&gt;\\n\\n&lt;p&gt;SmolAgents and things like Program of thought found that giving LLMs access to a Python interpreter helps quite a bit, too.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1luycyq/has_there_been_research_into_prompting_strategies/n21nlul/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752004478,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1luycyq","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n226k1x","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DinoAmino","can_mod_post":false,"created_utc":1752009671,"send_replies":true,"parent_id":"t3_1luycyq","score":2,"author_fullname":"t2_j1v7f","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Checkout OptiLLM. It has a dozen well known inferencing techniques to choose from based on published papers.\\n\\nhttps://github.com/codelion/optillm","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n226k1x","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Checkout OptiLLM. It has a dozen well known inferencing techniques to choose from based on published papers.&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://github.com/codelion/optillm\\"&gt;https://github.com/codelion/optillm&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1luycyq/has_there_been_research_into_prompting_strategies/n226k1x/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752009671,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1luycyq","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}}]`),r=()=>e.jsx(t,{data:a});export{r as default};
