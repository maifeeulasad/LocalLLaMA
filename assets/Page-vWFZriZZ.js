import{j as e}from"./index-CNyNkRpk.js";import{R as t}from"./RedditPostRenderer-Dza0u9i2.js";import"./index-BUchu_-K.js";const l=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"I'm trying to classify a social media dataset (about 5k social media posts - all text) using an LLM hosted via Ollama. \\n\\nFirst, I ran a sample of 200 posts on gemma3-27b-it via the Gemini API and tried out different prompts with temperature set to 0.1. Once I got a satisfactory result, I ran the sample on gemma3-27b-it-fp16 via Ollama running on an H-100 with the same prompt and temperature and got very similar results. \\n\\nHowever, when I run the entire dataset, the accuracy drops drastically. Even the posts that were classified earlier in the sample are incorrect this time around. The only difference is that I'm running 3 client nodes in parallel and making requests to the Ollama h100 server when I'm running the entire dataset. Is it possible that Ollama is taking some measures to ensure concurrency, which may downgrade the output quality? Or is there something that I might be missing? \\n\\nTIA. ","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Difference in output from Gemma3 running on Ollama.","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lw6u69","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.5,"author_flair_background_color":null,"subreddit_type":"public","ups":0,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_2yx8j17d","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":0,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1752133475,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;m trying to classify a social media dataset (about 5k social media posts - all text) using an LLM hosted via Ollama. &lt;/p&gt;\\n\\n&lt;p&gt;First, I ran a sample of 200 posts on gemma3-27b-it via the Gemini API and tried out different prompts with temperature set to 0.1. Once I got a satisfactory result, I ran the sample on gemma3-27b-it-fp16 via Ollama running on an H-100 with the same prompt and temperature and got very similar results. &lt;/p&gt;\\n\\n&lt;p&gt;However, when I run the entire dataset, the accuracy drops drastically. Even the posts that were classified earlier in the sample are incorrect this time around. The only difference is that I&amp;#39;m running 3 client nodes in parallel and making requests to the Ollama h100 server when I&amp;#39;m running the entire dataset. Is it possible that Ollama is taking some measures to ensure concurrency, which may downgrade the output quality? Or is there something that I might be missing? &lt;/p&gt;\\n\\n&lt;p&gt;TIA. &lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":true,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1lw6u69","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"HolidayPressure","discussion_type":null,"num_comments":4,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lw6u69/difference_in_output_from_gemma3_running_on_ollama/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lw6u69/difference_in_output_from_gemma3_running_on_ollama/","subreddit_subscribers":497354,"created_utc":1752133475,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2cezbq","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Icy_Bid6597","can_mod_post":false,"created_utc":1752146586,"send_replies":true,"parent_id":"t3_1lw6u69","score":3,"author_fullname":"t2_trc4foci4","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"It may be Ollama fault, but in general LLM are not deterministic in a manner that we used to. Same prompt executed in sequence and in parallel may give a little bit different results.\\n\\nIt is caused by multiple factors, from one is the choice of matrix multiplication kernel. LLM runner (or PyTorch, or CUDA) may choose other more optimal strategy to run a matrix multiplication depending on amount of parallel tokens to process. Mathematically all of them should be equal, but since we are working with floating points with limited precision even accumulation error may diverge results a little bit.\\n\\nIn some cases it is fine, in some it breaks the logic. In example if in your prompt you ask just for a single word answer like \\"answer with only yes or no\\" both of the tokens may be pretty close in probability and accumulation error may cause some instability.\\n\\nIn comparison if you will prompt your LLM to first give an explanation (like \\"chain of thought\\") and then assign a value, these seems to be less prone to that accumulation errors. Wording may be a little bit different, but general conclusion should be kept.\\n\\nWelcome to an LLM era. Nothing is as easy as it seems and nothing is predictable","edited":1752146876,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2cezbq","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It may be Ollama fault, but in general LLM are not deterministic in a manner that we used to. Same prompt executed in sequence and in parallel may give a little bit different results.&lt;/p&gt;\\n\\n&lt;p&gt;It is caused by multiple factors, from one is the choice of matrix multiplication kernel. LLM runner (or PyTorch, or CUDA) may choose other more optimal strategy to run a matrix multiplication depending on amount of parallel tokens to process. Mathematically all of them should be equal, but since we are working with floating points with limited precision even accumulation error may diverge results a little bit.&lt;/p&gt;\\n\\n&lt;p&gt;In some cases it is fine, in some it breaks the logic. In example if in your prompt you ask just for a single word answer like &amp;quot;answer with only yes or no&amp;quot; both of the tokens may be pretty close in probability and accumulation error may cause some instability.&lt;/p&gt;\\n\\n&lt;p&gt;In comparison if you will prompt your LLM to first give an explanation (like &amp;quot;chain of thought&amp;quot;) and then assign a value, these seems to be less prone to that accumulation errors. Wording may be a little bit different, but general conclusion should be kept.&lt;/p&gt;\\n\\n&lt;p&gt;Welcome to an LLM era. Nothing is as easy as it seems and nothing is predictable&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lw6u69/difference_in_output_from_gemma3_running_on_ollama/n2cezbq/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752146586,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lw6u69","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"50c36eba-fdca-11ee-9735-92a88d7e3b87","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2ggm55","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Glittering_Mouse_883","can_mod_post":false,"created_utc":1752190583,"send_replies":true,"parent_id":"t1_n2g2y3d","score":1,"author_fullname":"t2_sad822hq9","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I think it's the context length too. The default context length in ollama is short, did you remember to set it longer?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2ggm55","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"Ollama"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I think it&amp;#39;s the context length too. The default context length in ollama is short, did you remember to set it longer?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lw6u69","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lw6u69/difference_in_output_from_gemma3_running_on_ollama/n2ggm55/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752190583,"author_flair_text":"Ollama","treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2g2y3d","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MrMisterShin","can_mod_post":false,"created_utc":1752186138,"send_replies":true,"parent_id":"t3_1lw6u69","score":2,"author_fullname":"t2_3dhighjq","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Have you increased the context length in Ollama?\\n\\nThat would be the first step to eliminate.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2g2y3d","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Have you increased the context length in Ollama?&lt;/p&gt;\\n\\n&lt;p&gt;That would be the first step to eliminate.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lw6u69/difference_in_output_from_gemma3_running_on_ollama/n2g2y3d/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752186138,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lw6u69","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2c8uyk","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AppearanceHeavy6724","can_mod_post":false,"created_utc":1752143758,"send_replies":true,"parent_id":"t3_1lw6u69","score":0,"author_fullname":"t2_uz37qfx5","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Ollama is a wrong tool for that. try vllm.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2c8uyk","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Ollama is a wrong tool for that. try vllm.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lw6u69/difference_in_output_from_gemma3_running_on_ollama/n2c8uyk/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752143758,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lw6u69","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}}]`),o=()=>e.jsx(t,{data:l});export{o as default};
