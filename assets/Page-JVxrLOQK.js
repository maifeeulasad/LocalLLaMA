import{j as e}from"./index-BlGsFJYy.js";import{R as t}from"./RedditPostRenderer-B6uvq_Zl.js";import"./index-DDvVtNwD.js";const a=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Similiar in principle to the '[6x GPU Build. 4x RTX 3090 and 2x MI60. Epyc 7002](https://www.reddit.com/r/LocalLLaMA/comments/1g6ixae/6x_gpu_build_4x_rtx_3090_and_2x_mi60_epyc_7002/)' someone posted several months ago\\n\\nBut that thing is ginormous (dual PSU on either side of motherboard) so likely the 636mm wide version.\\n\\nI have a 12U (19inch) rack I want to install this into. The total rack width is 520mm BUT with the T-nut mounting posts on either side the opening narrows down to approx 480mm.\\n\\nI've found some frames but they are all 500mm wide. Which could 'fit' inside the rack but then wont be able to slide in and out on the shelf I will mount it on (for easy maintenance or fiddling)\\n\\nCan anyone point me to a 480mm wide dual layer frame that can slide into a 19inch rack shelf?\\n\\nThe only other solution I can think of is to hack a dual stack from two of these:\\n\\nhttps://preview.redd.it/qa3ttf30tpcf1.jpg?width=220&amp;format=pjpg&amp;auto=webp&amp;s=64aa05e469107d24131f92557562e0ae80c5f44a\\n\\nPS this build is just for fun/learning/tinkering with LLMs (and also replace one of my ex-office-PC proxmox nodes, Has nothing to do with work/career/business so I have already gone massively over on the 'just for fun'' budget by deciding on a Threadripper Pro 3945WX on a Gigabyte MC62-G40 board with 128GB RAM. And cant justify a more professional chassis solution\\n\\nThe MC62 &amp; TR 3945WX seemed the most cost effective way to get multiple PICE4.0 x16s for when I finally figure out how and why I would train models myself. I plan on selling my old RTX 4090 and pour that into some 3090's for the cheaper VRAM increase.\\n\\n\\\\---------------------------------------------------------------------------------------------------\\n\\n\\\\[unrelated to LLM's - also wondering about running a VM/LXC for PCVR with my RTX 5090 passed through and ditch the windows machine altogether? So if someone here has experience of VM/LXC linux &gt; Monado &gt; WiVRn versus Win11 &gt; Virtual Desktop performance for Quest3 simracing. Could you talk me through the benefit/downside of virtualising my PCVR rig so I could use the 5090 for VR AND LLMs. So eventually rather than fitting out multiple 3090's add another 5090 with the 4090+7800X3D+32GBDDR5+B650i mobo sale proceeds?","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"480mm wide multi GPU frame - can only find 500+mm frames","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":140,"top_awarded_type":null,"hide_score":false,"media_metadata":{"qa3ttf30tpcf1":{"status":"valid","e":"Image","m":"image/jpg","p":[{"y":108,"x":108,"u":"https://preview.redd.it/qa3ttf30tpcf1.jpg?width=108&amp;crop=smart&amp;auto=webp&amp;s=53d3b00e8f63b48c9790997de4baf9df85d2adbb"},{"y":216,"x":216,"u":"https://preview.redd.it/qa3ttf30tpcf1.jpg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c4452c2305b9bd0e32bcf1b3533b6a20957368b2"}],"s":{"y":220,"x":220,"u":"https://preview.redd.it/qa3ttf30tpcf1.jpg?width=220&amp;format=pjpg&amp;auto=webp&amp;s=64aa05e469107d24131f92557562e0ae80c5f44a"},"id":"qa3ttf30tpcf1"}},"name":"t3_1lz5cwa","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":1,"author_flair_background_color":null,"subreddit_type":"public","ups":2,"total_awards_received":0,"media_embed":{},"thumbnail_width":140,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_if95iuzc","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":2,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"https://b.thumbs.redditmedia.com/vcAl6hE0vLfYguTcveTyE8ouQSXIpZ8trLcegu0T6lk.jpg","edited":1752482948,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1752444529,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Similiar in principle to the &amp;#39;&lt;a href=\\"https://www.reddit.com/r/LocalLLaMA/comments/1g6ixae/6x_gpu_build_4x_rtx_3090_and_2x_mi60_epyc_7002/\\"&gt;6x GPU Build. 4x RTX 3090 and 2x MI60. Epyc 7002&lt;/a&gt;&amp;#39; someone posted several months ago&lt;/p&gt;\\n\\n&lt;p&gt;But that thing is ginormous (dual PSU on either side of motherboard) so likely the 636mm wide version.&lt;/p&gt;\\n\\n&lt;p&gt;I have a 12U (19inch) rack I want to install this into. The total rack width is 520mm BUT with the T-nut mounting posts on either side the opening narrows down to approx 480mm.&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;ve found some frames but they are all 500mm wide. Which could &amp;#39;fit&amp;#39; inside the rack but then wont be able to slide in and out on the shelf I will mount it on (for easy maintenance or fiddling)&lt;/p&gt;\\n\\n&lt;p&gt;Can anyone point me to a 480mm wide dual layer frame that can slide into a 19inch rack shelf?&lt;/p&gt;\\n\\n&lt;p&gt;The only other solution I can think of is to hack a dual stack from two of these:&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://preview.redd.it/qa3ttf30tpcf1.jpg?width=220&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=64aa05e469107d24131f92557562e0ae80c5f44a\\"&gt;https://preview.redd.it/qa3ttf30tpcf1.jpg?width=220&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=64aa05e469107d24131f92557562e0ae80c5f44a&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;PS this build is just for fun/learning/tinkering with LLMs (and also replace one of my ex-office-PC proxmox nodes, Has nothing to do with work/career/business so I have already gone massively over on the &amp;#39;just for fun&amp;#39;&amp;#39; budget by deciding on a Threadripper Pro 3945WX on a Gigabyte MC62-G40 board with 128GB RAM. And cant justify a more professional chassis solution&lt;/p&gt;\\n\\n&lt;p&gt;The MC62 &amp;amp; TR 3945WX seemed the most cost effective way to get multiple PICE4.0 x16s for when I finally figure out how and why I would train models myself. I plan on selling my old RTX 4090 and pour that into some 3090&amp;#39;s for the cheaper VRAM increase.&lt;/p&gt;\\n\\n&lt;p&gt;---------------------------------------------------------------------------------------------------&lt;/p&gt;\\n\\n&lt;p&gt;[unrelated to LLM&amp;#39;s - also wondering about running a VM/LXC for PCVR with my RTX 5090 passed through and ditch the windows machine altogether? So if someone here has experience of VM/LXC linux &amp;gt; Monado &amp;gt; WiVRn versus Win11 &amp;gt; Virtual Desktop performance for Quest3 simracing. Could you talk me through the benefit/downside of virtualising my PCVR rig so I could use the 5090 for VR AND LLMs. So eventually rather than fitting out multiple 3090&amp;#39;s add another 5090 with the 4090+7800X3D+32GBDDR5+B650i mobo sale proceeds?&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":true,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1lz5cwa","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"munkiemagik","discussion_type":null,"num_comments":0,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lz5cwa/480mm_wide_multi_gpu_frame_can_only_find_500mm/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lz5cwa/480mm_wide_multi_gpu_frame_can_only_find_500mm/","subreddit_subscribers":498850,"created_utc":1752444529,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[],"before":null}}]`),l=()=>e.jsx(t,{data:a});export{l as default};
