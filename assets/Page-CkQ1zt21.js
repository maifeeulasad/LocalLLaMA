import{j as e}from"./index-BgwOAK4-.js";import{R as l}from"./RedditPostRenderer-BOBjDTFu.js";import"./index-BL22wVg5.js";const t=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Hi everyone,\\n\\nI've worked on some projects with LLMs that are not local and I feel it's time for the switch. I have a few questions which I hope some of you wouldn't mind answering. I'm specifically interested in running LLMs on small low spec devices which is why I've always used the API method. \\n\\n1. How many parameter model can I realistically run on either android smartphones or raspberry pis? And any metrics on battery life consumption?\\n\\n2. Are the 7b or 0.5b models (like Qwen) good nowadays for general daily use? \\n\\n3. Do you think it's better to run an API to my self hosted local llama model than on the device itself and if so, could I do it on a really old machine (like an old desktop as I'd prefer not running an insane machine as a server).\\n\\n4. Do you have any model recommendations - especially for tool-calling?\\n\\n\\nThank you so much! ","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Advice on switching to LLM","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lvo6ae","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":1,"author_flair_background_color":null,"subreddit_type":"public","ups":1,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_dxn7c7h2","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":1,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1752080688,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;ve worked on some projects with LLMs that are not local and I feel it&amp;#39;s time for the switch. I have a few questions which I hope some of you wouldn&amp;#39;t mind answering. I&amp;#39;m specifically interested in running LLMs on small low spec devices which is why I&amp;#39;ve always used the API method. &lt;/p&gt;\\n\\n&lt;ol&gt;\\n&lt;li&gt;&lt;p&gt;How many parameter model can I realistically run on either android smartphones or raspberry pis? And any metrics on battery life consumption?&lt;/p&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;p&gt;Are the 7b or 0.5b models (like Qwen) good nowadays for general daily use? &lt;/p&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;p&gt;Do you think it&amp;#39;s better to run an API to my self hosted local llama model than on the device itself and if so, could I do it on a really old machine (like an old desktop as I&amp;#39;d prefer not running an insane machine as a server).&lt;/p&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;p&gt;Do you have any model recommendations - especially for tool-calling?&lt;/p&gt;&lt;/li&gt;\\n&lt;/ol&gt;\\n\\n&lt;p&gt;Thank you so much! &lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":true,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1lvo6ae","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"infinity123248","discussion_type":null,"num_comments":5,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lvo6ae/advice_on_switching_to_llm/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lvo6ae/advice_on_switching_to_llm/","subreddit_subscribers":497024,"created_utc":1752080688,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n28uyzl","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"BumbleSlob","can_mod_post":false,"send_replies":true,"parent_id":"t1_n283pa5","score":1,"author_fullname":"t2_1j7fhlcqkp","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yup. I just navigate in my iPhone Safari to https://macbook-pro:3000 and boom open WebUI. Then can turn that into a PWA.\\n\\nThe exact name you type in depends on how you register your device in Tailscale \\n\\nBut for anyone intimidated by this, I really can’t emphasize enough Tailscale is incredibly easy to setup. Less then 30 seconds setup. ","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n28uyzl","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yup. I just navigate in my iPhone Safari to https://macbook-pro:3000 and boom open WebUI. Then can turn that into a PWA.&lt;/p&gt;\\n\\n&lt;p&gt;The exact name you type in depends on how you register your device in Tailscale &lt;/p&gt;\\n\\n&lt;p&gt;But for anyone intimidated by this, I really can’t emphasize enough Tailscale is incredibly easy to setup. Less then 30 seconds setup. &lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lvo6ae","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lvo6ae/advice_on_switching_to_llm/n28uyzl/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752094205,"author_flair_text":null,"treatment_tags":[],"created_utc":1752094205,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n283pa5","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AlphaTechBro","can_mod_post":false,"created_utc":1752086681,"send_replies":true,"parent_id":"t1_n27nz5u","score":2,"author_fullname":"t2_aje19qfs","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"This is a great idea, and I'm interested in building my own.\\n\\nI already have an Open WebUI setup on a desktop, and can learn Tailscale, no problem. However I wanted to ask you how did you establish it as a PWA on your phone? Is that pretty straightforward as well?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n283pa5","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;This is a great idea, and I&amp;#39;m interested in building my own.&lt;/p&gt;\\n\\n&lt;p&gt;I already have an Open WebUI setup on a desktop, and can learn Tailscale, no problem. However I wanted to ask you how did you establish it as a PWA on your phone? Is that pretty straightforward as well?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lvo6ae","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lvo6ae/advice_on_switching_to_llm/n283pa5/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752086681,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n27nz5u","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"BumbleSlob","can_mod_post":false,"created_utc":1752082376,"send_replies":true,"parent_id":"t3_1lvo6ae","score":2,"author_fullname":"t2_1j7fhlcqkp","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"You really shouldn’t run LLMs on battery device. They get hot and chew through battery or are just generally really stupid cuz of low param counts. A MUCH better approach is running on some device and then interacting with them through your phone. \\n\\nEx: I have Open WebUI and Ollama running on my laptop. I can connect to my laptop from anywhere easily with Tailscale. I have Open WebUI setup as a PWA on my phone and tablet.\\n\\nCompletely secure and private, no battery issues, can pop in a complex question and pop my phone back in to my pocket without a care since it runs elsewhere, can run much larger models since I use the 64Gb of unified memory, much longer context lengths, etc. \\n\\nIt’s all the upsides, none of the downsides. ","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n27nz5u","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You really shouldn’t run LLMs on battery device. They get hot and chew through battery or are just generally really stupid cuz of low param counts. A MUCH better approach is running on some device and then interacting with them through your phone. &lt;/p&gt;\\n\\n&lt;p&gt;Ex: I have Open WebUI and Ollama running on my laptop. I can connect to my laptop from anywhere easily with Tailscale. I have Open WebUI setup as a PWA on my phone and tablet.&lt;/p&gt;\\n\\n&lt;p&gt;Completely secure and private, no battery issues, can pop in a complex question and pop my phone back in to my pocket without a care since it runs elsewhere, can run much larger models since I use the 64Gb of unified memory, much longer context lengths, etc. &lt;/p&gt;\\n\\n&lt;p&gt;It’s all the upsides, none of the downsides. &lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lvo6ae/advice_on_switching_to_llm/n27nz5u/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752082376,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lvo6ae","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2812yr","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AppearanceHeavy6724","can_mod_post":false,"created_utc":1752085952,"send_replies":true,"parent_id":"t1_n27q4q8","score":1,"author_fullname":"t2_uz37qfx5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"&gt; The smaller 4B models are only taking ~8-9GB of RAM.\\n\\nYeah if you run in silly BF16 quant. Normally Q8 4b should take 5-6B together with context.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2812yr","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;The smaller 4B models are only taking ~8-9GB of RAM.&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;Yeah if you run in silly BF16 quant. Normally Q8 4b should take 5-6B together with context.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lvo6ae","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lvo6ae/advice_on_switching_to_llm/n2812yr/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752085952,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n27q4q8","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"SM8085","can_mod_post":false,"created_utc":1752082950,"send_replies":true,"parent_id":"t3_1lvo6ae","score":2,"author_fullname":"t2_14vikjao97","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"&gt;Do you have any model recommendations - especially for tool-calling?\\n\\nThe [Berkeley Leaderboard](https://gorilla.cs.berkeley.edu/leaderboard.html) is good resource to check.  Qwen consistently scores high for tool-calling.  Qwen3 4B does so well for its size that it's my go-to for now.\\n\\nAround rank 50 and below make me nervous handing them tools.  Which includes most of the Gemma3 &amp; Llama models.  \\n\\n&gt;Do you think it's better to run an API to my self hosted local llama model than on the device itself and if so, could I do it on a really old machine (like an old desktop as I'd prefer not running an insane machine as a server).\\n\\nI love my old workstation I bought as an LLM rig.  I started playing with LLM on my regular PC but I got tired of it hogging all the resources.  Now the workstation sits on my LAN and I send everything through the API.\\n\\nScreenshot running some models via CPU+RAM:\\n\\nhttps://preview.redd.it/39d9cp4dxvbf1.png?width=1902&amp;format=png&amp;auto=webp&amp;s=1dd9fb5dd9d4555d2c10267e2b8d4aead6f3a0a3\\n\\nThe smaller 4B models are *only* taking \\\\~8-9GB of RAM.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n27q4q8","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;Do you have any model recommendations - especially for tool-calling?&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;The &lt;a href=\\"https://gorilla.cs.berkeley.edu/leaderboard.html\\"&gt;Berkeley Leaderboard&lt;/a&gt; is good resource to check.  Qwen consistently scores high for tool-calling.  Qwen3 4B does so well for its size that it&amp;#39;s my go-to for now.&lt;/p&gt;\\n\\n&lt;p&gt;Around rank 50 and below make me nervous handing them tools.  Which includes most of the Gemma3 &amp;amp; Llama models.  &lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;Do you think it&amp;#39;s better to run an API to my self hosted local llama model than on the device itself and if so, could I do it on a really old machine (like an old desktop as I&amp;#39;d prefer not running an insane machine as a server).&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;I love my old workstation I bought as an LLM rig.  I started playing with LLM on my regular PC but I got tired of it hogging all the resources.  Now the workstation sits on my LAN and I send everything through the API.&lt;/p&gt;\\n\\n&lt;p&gt;Screenshot running some models via CPU+RAM:&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://preview.redd.it/39d9cp4dxvbf1.png?width=1902&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1dd9fb5dd9d4555d2c10267e2b8d4aead6f3a0a3\\"&gt;https://preview.redd.it/39d9cp4dxvbf1.png?width=1902&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=1dd9fb5dd9d4555d2c10267e2b8d4aead6f3a0a3&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;The smaller 4B models are &lt;em&gt;only&lt;/em&gt; taking ~8-9GB of RAM.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lvo6ae/advice_on_switching_to_llm/n27q4q8/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752082950,"media_metadata":{"39d9cp4dxvbf1":{"status":"valid","e":"Image","m":"image/png","p":[{"y":6,"x":108,"u":"https://preview.redd.it/39d9cp4dxvbf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=27235cc5b65e5934862ddee6812d6f38cec17cf2"},{"y":13,"x":216,"u":"https://preview.redd.it/39d9cp4dxvbf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=32332d41e045c16e8a9658af21f4252f0ae4d326"},{"y":20,"x":320,"u":"https://preview.redd.it/39d9cp4dxvbf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=67e49860cce3ea881a75fb442efcce46c7acc402"},{"y":40,"x":640,"u":"https://preview.redd.it/39d9cp4dxvbf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=5d6f8b14e5dc11c7913515fcc0291858722e07a7"},{"y":60,"x":960,"u":"https://preview.redd.it/39d9cp4dxvbf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=f30c0c95cc0f9b8cb86ad9f3f46fc4430fc0b1c8"},{"y":67,"x":1080,"u":"https://preview.redd.it/39d9cp4dxvbf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=739078d235ec873b6b665b4b67963a66617f4053"}],"s":{"y":119,"x":1902,"u":"https://preview.redd.it/39d9cp4dxvbf1.png?width=1902&amp;format=png&amp;auto=webp&amp;s=1dd9fb5dd9d4555d2c10267e2b8d4aead6f3a0a3"},"id":"39d9cp4dxvbf1"}},"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lvo6ae","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}}]`),r=()=>e.jsx(l,{data:t});export{r as default};
