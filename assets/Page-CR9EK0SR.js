import{j as e}from"./index-CWmJdUH_.js";import{R as l}from"./RedditPostRenderer-D2iunoQ9.js";import"./index-BCg9RP6g.js";const a=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Hi :)\\n\\nI'm a little concerned about the potential foolishness of feeding forever remembering cloud AIs with my thoughts every day, even if I don't say anything very personal or sensitive. \\n\\nI have an rtx 5090 (32 gb)\\n\\nWhat are the best local models I can run? \\n\\nThanks","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"What can I run on my 5090?","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1m5w8yl","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.29,"author_flair_background_color":null,"subreddit_type":"public","ups":0,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_3231b","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":0,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1753134845,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Hi :)&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;m a little concerned about the potential foolishness of feeding forever remembering cloud AIs with my thoughts every day, even if I don&amp;#39;t say anything very personal or sensitive. &lt;/p&gt;\\n\\n&lt;p&gt;I have an rtx 5090 (32 gb)&lt;/p&gt;\\n\\n&lt;p&gt;What are the best local models I can run? &lt;/p&gt;\\n\\n&lt;p&gt;Thanks&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":true,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1m5w8yl","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"hurfery","discussion_type":null,"num_comments":22,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1m5w8yl/what_can_i_run_on_my_5090/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1m5w8yl/what_can_i_run_on_my_5090/","subreddit_subscribers":502981,"created_utc":1753134845,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4gz6ar","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ShengrenR","can_mod_post":false,"created_utc":1753159202,"send_replies":true,"parent_id":"t1_n4f73sp","score":2,"author_fullname":"t2_ji4n4","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"if you grab exl3 you can run 3bpw and it'll still be solidly coherent - more room for context and other goodies, like STT/TTS etc.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4gz6ar","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;if you grab exl3 you can run 3bpw and it&amp;#39;ll still be solidly coherent - more room for context and other goodies, like STT/TTS etc.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m5w8yl","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5w8yl/what_can_i_run_on_my_5090/n4gz6ar/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753159202,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4hqt3j","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Environmental-Metal9","can_mod_post":false,"created_utc":1753174049,"send_replies":true,"parent_id":"t1_n4hpkqj","score":1,"author_fullname":"t2_6x9o42az","approved_by":null,"mod_note":null,"all_awardings":[],"body":"Most of the models Iâ€™ve seen requiring this were older LLMs (llama2 finetunes) and a large number of diffusion and audio (stt/tts) models.\\n\\nMy understanding of how quantized models are run matches with what you described, the engine implements the reference model itself, and as such thereâ€™s no need to remotely execute anything, and at this point the trust model shifts from trusting a random model maker (in the case of Joe blow AGI super cool new model) to trusting a massive org trusted by many titans in the industry (better positioning!). The only reason why I mentioned anything is because context is important here!","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n4hqt3j","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Most of the models Iâ€™ve seen requiring this were older LLMs (llama2 finetunes) and a large number of diffusion and audio (stt/tts) models.&lt;/p&gt;\\n\\n&lt;p&gt;My understanding of how quantized models are run matches with what you described, the engine implements the reference model itself, and as such thereâ€™s no need to remotely execute anything, and at this point the trust model shifts from trusting a random model maker (in the case of Joe blow AGI super cool new model) to trusting a massive org trusted by many titans in the industry (better positioning!). The only reason why I mentioned anything is because context is important here!&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1m5w8yl","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5w8yl/what_can_i_run_on_my_5090/n4hqt3j/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753174049,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":7,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4hpkqj","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"LagOps91","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4hpb3y","score":1,"author_fullname":"t2_3wi6j7vwh","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"what kinds of models require this? that's news to me! personally, i only run gguf and as far as i know, those don't have any code that comes with them. it's up to llama.cpp to run the model and every model familly has it's own support for this.","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n4hpkqj","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;what kinds of models require this? that&amp;#39;s news to me! personally, i only run gguf and as far as i know, those don&amp;#39;t have any code that comes with them. it&amp;#39;s up to llama.cpp to run the model and every model familly has it&amp;#39;s own support for this.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1m5w8yl","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5w8yl/what_can_i_run_on_my_5090/n4hpkqj/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753173326,"author_flair_text":null,"treatment_tags":[],"created_utc":1753173326,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":6,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4hpb3y","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Environmental-Metal9","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4hmvky","score":2,"author_fullname":"t2_6x9o42az","approved_by":null,"mod_note":null,"all_awardings":[],"body":"I donâ€™t mean to be pedantic, but the model weights are just data that you download like a picture, but the models themselves are code, and depending on how youâ€™re executing the model, you have the option to pass \`trust_remote_code=True\` which some models require to work, and that means the model is definitely downloading extra code from the HF repo and running it. There are plenty of reasons for this, but it isnâ€™t accurate to say that models canâ€™t run code. The weights canâ€™t.\\n\\nThis is primarily thinking about the Transformers library, and how you find most model reference implementations, but quantized weights tend to be run in the model implementation of the engine running the weights, so your statement might be more accurate in llama.cpp/gguf world","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n4hpb3y","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I donâ€™t mean to be pedantic, but the model weights are just data that you download like a picture, but the models themselves are code, and depending on how youâ€™re executing the model, you have the option to pass &lt;code&gt;trust_remote_code=True&lt;/code&gt; which some models require to work, and that means the model is definitely downloading extra code from the HF repo and running it. There are plenty of reasons for this, but it isnâ€™t accurate to say that models canâ€™t run code. The weights canâ€™t.&lt;/p&gt;\\n\\n&lt;p&gt;This is primarily thinking about the Transformers library, and how you find most model reference implementations, but quantized weights tend to be run in the model implementation of the engine running the weights, so your statement might be more accurate in llama.cpp/gguf world&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1m5w8yl","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5w8yl/what_can_i_run_on_my_5090/n4hpb3y/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753173169,"author_flair_text":null,"treatment_tags":[],"created_utc":1753173169,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n4hmvky","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"LagOps91","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4hm35b","score":1,"author_fullname":"t2_3wi6j7vwh","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"effectively none at all. the models themselves are just a bunch of data, they can't execute anything by themselves. treat it like downloading a picture from the internet.","edited":false,"author_flair_css_class":null,"name":"t1_n4hmvky","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;effectively none at all. the models themselves are just a bunch of data, they can&amp;#39;t execute anything by themselves. treat it like downloading a picture from the internet.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1m5w8yl","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5w8yl/what_can_i_run_on_my_5090/n4hmvky/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753171738,"author_flair_text":null,"collapsed":false,"created_utc":1753171738,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4hn5mm","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"LagOps91","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4hm35b","score":1,"author_fullname":"t2_3wi6j7vwh","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"in terms of apps, most of them are fully open source. i personally use kobold.cpp since it has good support, is easy to use (i say easy to use, but there certainly still is a learning curve), needs no installation and comes with a ui frontend out of the box.","edited":1753172122,"author_flair_css_class":null,"name":"t1_n4hn5mm","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;in terms of apps, most of them are fully open source. i personally use kobold.cpp since it has good support, is easy to use (i say easy to use, but there certainly still is a learning curve), needs no installation and comes with a ui frontend out of the box.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1m5w8yl","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5w8yl/what_can_i_run_on_my_5090/n4hn5mm/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753171901,"author_flair_text":null,"collapsed":false,"created_utc":1753171901,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4hm35b","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"hurfery","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4hlvd3","score":1,"author_fullname":"t2_3231b","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Okay. Cool. Thanks. :)\\n\\nBtw... are there any privacy/data collecting concerns with these \\"offline\\" models/apps? ðŸ¤”","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4hm35b","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Okay. Cool. Thanks. :)&lt;/p&gt;\\n\\n&lt;p&gt;Btw... are there any privacy/data collecting concerns with these &amp;quot;offline&amp;quot; models/apps? ðŸ¤”&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m5w8yl","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5w8yl/what_can_i_run_on_my_5090/n4hm35b/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753171284,"author_flair_text":null,"treatment_tags":[],"created_utc":1753171284,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4hlvd3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"LagOps91","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4hj57l","score":1,"author_fullname":"t2_3wi6j7vwh","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"yes, it's possible. you will get slower speeds, but MoE models are faster to run and still fast enough to get good speeds if you offload parts to ram. you will likely need to do some custom tensor offloading strategies to get the most out of the model.\\n\\nwith 32gb vram and 64gb ram, you can run dots.llm1 and should get usable speed out of it. won't be as fast as nemotron super, but likely is a bit smarter.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n4hlvd3","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;yes, it&amp;#39;s possible. you will get slower speeds, but MoE models are faster to run and still fast enough to get good speeds if you offload parts to ram. you will likely need to do some custom tensor offloading strategies to get the most out of the model.&lt;/p&gt;\\n\\n&lt;p&gt;with 32gb vram and 64gb ram, you can run dots.llm1 and should get usable speed out of it. won&amp;#39;t be as fast as nemotron super, but likely is a bit smarter.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m5w8yl","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5w8yl/what_can_i_run_on_my_5090/n4hlvd3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753171159,"author_flair_text":null,"treatment_tags":[],"created_utc":1753171159,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4hj57l","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"hurfery","can_mod_post":false,"created_utc":1753169601,"send_replies":true,"parent_id":"t1_n4f73sp","score":1,"author_fullname":"t2_3231b","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I have 64 gb of system ram. How does that interact with the GPU for model usage? Can you run a model on vram + ram?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4hj57l","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I have 64 gb of system ram. How does that interact with the GPU for model usage? Can you run a model on vram + ram?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m5w8yl","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5w8yl/what_can_i_run_on_my_5090/n4hj57l/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753169601,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4f73sp","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"LagOps91","can_mod_post":false,"created_utc":1753136060,"send_replies":true,"parent_id":"t3_1m5w8yl","score":3,"author_fullname":"t2_3wi6j7vwh","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"You can run Nemotron Super 49b at Q4 on your system. if you got a lot of ram, large MoE models like dots.llm1 could also be a good option.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4f73sp","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You can run Nemotron Super 49b at Q4 on your system. if you got a lot of ram, large MoE models like dots.llm1 could also be a good option.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5w8yl/what_can_i_run_on_my_5090/n4f73sp/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753136060,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m5w8yl","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4gaxib","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Expensive-Apricot-25","can_mod_post":false,"created_utc":1753149540,"send_replies":true,"parent_id":"t1_n4fxwrx","score":1,"author_fullname":"t2_idqkwio0","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Out of curiosity, What quant of llama 70b do u use and whatâ€™s the speed?\\n\\nI have nothing close to a 5090, just curious lol","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4gaxib","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Out of curiosity, What quant of llama 70b do u use and whatâ€™s the speed?&lt;/p&gt;\\n\\n&lt;p&gt;I have nothing close to a 5090, just curious lol&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m5w8yl","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5w8yl/what_can_i_run_on_my_5090/n4gaxib/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753149540,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4fxwrx","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"RiskyBizz216","can_mod_post":false,"created_utc":1753145005,"send_replies":true,"parent_id":"t3_1m5w8yl","score":2,"author_fullname":"t2_4eu8nupk","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"fellow 5090 owner.\\n\\nI run some smaller quants of llama 70B which is the best imo\\n\\nfor speed + quality use devstral Q6 or bf16\\n\\ni play around with the qwen2.5 32B finetunes, theyre pretty good too\\n\\nI'm testing Kimi Dev","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4fxwrx","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;fellow 5090 owner.&lt;/p&gt;\\n\\n&lt;p&gt;I run some smaller quants of llama 70B which is the best imo&lt;/p&gt;\\n\\n&lt;p&gt;for speed + quality use devstral Q6 or bf16&lt;/p&gt;\\n\\n&lt;p&gt;i play around with the qwen2.5 32B finetunes, theyre pretty good too&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;m testing Kimi Dev&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5w8yl/what_can_i_run_on_my_5090/n4fxwrx/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753145005,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m5w8yl","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4gbpv8","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ForsookComparison","can_mod_post":false,"created_utc":1753149816,"send_replies":true,"parent_id":"t3_1m5w8yl","score":2,"author_fullname":"t2_on5es7pe3","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"- nemotron super 49b iq4\\n\\n- llama 3.3 70b iq3\\n\\n- qwen3 32b q6","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4gbpv8","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;ul&gt;\\n&lt;li&gt;&lt;p&gt;nemotron super 49b iq4&lt;/p&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;p&gt;llama 3.3 70b iq3&lt;/p&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;p&gt;qwen3 32b q6&lt;/p&gt;&lt;/li&gt;\\n&lt;/ul&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5w8yl/what_can_i_run_on_my_5090/n4gbpv8/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753149816,"author_flair_text":"llama.cpp","treatment_tags":[],"link_id":"t3_1m5w8yl","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4fdxhc","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ThinkExtension2328","can_mod_post":false,"created_utc":1753138289,"send_replies":true,"parent_id":"t3_1m5w8yl","score":2,"author_fullname":"t2_8eneodlk","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Definitely crysis! But thatâ€™s probably not what youâ€™re asking. \\n\\nYouâ€™re good for most 32b models with headroom for context windows. You will struggle with anything larger.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4fdxhc","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Definitely crysis! But thatâ€™s probably not what youâ€™re asking. &lt;/p&gt;\\n\\n&lt;p&gt;Youâ€™re good for most 32b models with headroom for context windows. You will struggle with anything larger.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5w8yl/what_can_i_run_on_my_5090/n4fdxhc/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753138289,"author_flair_text":"llama.cpp","treatment_tags":[],"link_id":"t3_1m5w8yl","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4f5avr","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Tbhmaximillian","can_mod_post":false,"created_utc":1753135473,"send_replies":true,"parent_id":"t3_1m5w8yl","score":1,"author_fullname":"t2_b5a3coy7","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"get yourself LM studio and search for models like Kimi. You will get an overview of all the versions on huggingface that are quantisized and work based your GPU and you can download and start them. Get a feeling what is working with some QWEN versions and then you can use that knowledge for your local ollama models.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4f5avr","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;get yourself LM studio and search for models like Kimi. You will get an overview of all the versions on huggingface that are quantisized and work based your GPU and you can download and start them. Get a feeling what is working with some QWEN versions and then you can use that knowledge for your local ollama models.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5w8yl/what_can_i_run_on_my_5090/n4f5avr/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753135473,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m5w8yl","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4fgfm1","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Weary-Wing-6806","can_mod_post":false,"created_utc":1753139105,"send_replies":true,"parent_id":"t3_1m5w8yl","score":1,"author_fullname":"t2_1t2xvghrcr","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Nice, youâ€™re chilling. You can run basically anything under 70B dense, and all the big MoE models like Mixtral and DeepSeek fly. 27B and 33B dense models run smooth, and you can even dabble in multimodal stuff like BakLLaVA or MedGemma if you want. Local whisper, TTS, whatever shouldn't be an issues. Youâ€™re not limited by hardware, just pick your model and go go go.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4fgfm1","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Nice, youâ€™re chilling. You can run basically anything under 70B dense, and all the big MoE models like Mixtral and DeepSeek fly. 27B and 33B dense models run smooth, and you can even dabble in multimodal stuff like BakLLaVA or MedGemma if you want. Local whisper, TTS, whatever shouldn&amp;#39;t be an issues. Youâ€™re not limited by hardware, just pick your model and go go go.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5w8yl/what_can_i_run_on_my_5090/n4fgfm1/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753139105,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m5w8yl","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4kj06w","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Expensive-Apricot-25","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4hjdvb","score":1,"author_fullname":"t2_idqkwio0","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"no, it is a lower parameter model, and it is a different type of model (moe model) that makes it signifigantly faster, but worse than a dense model.\\n\\nHonestly, you wont really be able to tell the difference unless you are doing hard things with them, or having them do actual work and using tools.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n4kj06w","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;no, it is a lower parameter model, and it is a different type of model (moe model) that makes it signifigantly faster, but worse than a dense model.&lt;/p&gt;\\n\\n&lt;p&gt;Honestly, you wont really be able to tell the difference unless you are doing hard things with them, or having them do actual work and using tools.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m5w8yl","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5w8yl/what_can_i_run_on_my_5090/n4kj06w/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753208340,"author_flair_text":null,"treatment_tags":[],"created_utc":1753208340,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4hjdvb","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"hurfery","can_mod_post":false,"created_utc":1753169740,"send_replies":true,"parent_id":"t1_n4gasa4","score":1,"author_fullname":"t2_3231b","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"What sort of tradeoff do you get from choosing a much faster model? Do they come across as lower IQ?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4hjdvb","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;What sort of tradeoff do you get from choosing a much faster model? Do they come across as lower IQ?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m5w8yl","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5w8yl/what_can_i_run_on_my_5090/n4hjdvb/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753169740,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4gasa4","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Expensive-Apricot-25","can_mod_post":false,"created_utc":1753149490,"send_replies":true,"parent_id":"t3_1m5w8yl","score":1,"author_fullname":"t2_idqkwio0","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Try qwen3 32b (best you can probably get)\\n\\nOr try qwen3 30b (3b active) not as good as 32b, but waaaaaaay faster.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4gasa4","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Try qwen3 32b (best you can probably get)&lt;/p&gt;\\n\\n&lt;p&gt;Or try qwen3 30b (3b active) not as good as 32b, but waaaaaaay faster.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5w8yl/what_can_i_run_on_my_5090/n4gasa4/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753149490,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m5w8yl","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4h3rue","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"jacek2023","can_mod_post":false,"created_utc":1753161371,"send_replies":true,"parent_id":"t3_1m5w8yl","score":1,"author_fullname":"t2_vqgbql9w","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"you can run plenty of 32B models in Q5/Q6, you can run 24B models in Q8 and probably gemma3 27B in Q8 or Q6, if you have also some RAM it's a good idea to try MoE models for example dots or huyuan or jamba or llama 4 scout","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4h3rue","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;you can run plenty of 32B models in Q5/Q6, you can run 24B models in Q8 and probably gemma3 27B in Q8 or Q6, if you have also some RAM it&amp;#39;s a good idea to try MoE models for example dots or huyuan or jamba or llama 4 scout&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m5w8yl/what_can_i_run_on_my_5090/n4h3rue/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753161371,"author_flair_text":"llama.cpp","treatment_tags":[],"link_id":"t3_1m5w8yl","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),n=()=>e.jsx(l,{data:a});export{n as default};
