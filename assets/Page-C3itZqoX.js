import{j as e}from"./index-CeRg6Q3f.js";import{R as t}from"./RedditPostRenderer-D7n1g-D8.js";import"./index-DPToWe3n.js";const a=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Hi, \\n\\nI'm currently facing a weird issue.   \\nI was testing different embedding models, with the goal being to integrate the best local one in a django application. \\n\\nArchitecture is as follows : \\n\\n\\\\- One Mac Book air running LMStudio, acting as a local server for llm and embedding operations\\n\\n\\\\- My PC for the django application, running the codebase \\n\\nI use CosineDistance to test the models. The functionality is a semantic search. \\n\\nI noticed the following : \\n\\n\\\\- Using the text-embedding-3-large model, (OAI API) gives great results  \\n\\\\- Using Nomic embedding model gives great results also  \\n\\\\- Using Qwen embedding models give very bad results, as if the encoding wouldn't make any sense. \\n\\ni'm using a aembed() method to call the embedding models, and I declare them using : \\n\\n    OpenAIEmbeddings(\\n    Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  model=model_name,\\n    Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  check_embedding_ctx_length=False,\\n    Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  base_url=base_url,\\n    Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  api_key=api_key,\\n    Â  Â  Â  Â  Â  Â  Â  Â  )\\n\\nAs LM studio provides an OpenAI-like API. Here are the values of the different tests I ran.\\n\\n[OpenAI cosine distance test results](https://preview.redd.it/cagenh3bs8cf1.png?width=1398&amp;format=png&amp;auto=webp&amp;s=412891b809b68d181ce304b2c56a5a5e71078b2b)\\n\\n[LM Studio Nomic cosine distance test](https://preview.redd.it/y5a8rizcs8cf1.png?width=1402&amp;format=png&amp;auto=webp&amp;s=0d1ffc2028a3ec98ef6c4ba02dcb184cb0d0999e)\\n\\n[LM Studio Qwen 3 cosine distance test ](https://preview.redd.it/ddyrcixfs8cf1.png?width=1396&amp;format=png&amp;auto=webp&amp;s=d870df0374cc82865c3fdcc8ad5f3735e7ca8742)\\n\\n\\n\\nI just can't figure out what's going on. Qwen 3 is supposed to be among the best models.   \\nCan someone give advice ?","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Issues with Qwen 3 Embedding models (4B and 0.6B)","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":20,"top_awarded_type":null,"hide_score":false,"media_metadata":{"ddyrcixfs8cf1":{"status":"valid","e":"Image","m":"image/png","p":[{"y":18,"x":108,"u":"https://preview.redd.it/ddyrcixfs8cf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=8281c3de83dfd9dd7fc4ecc485d574cb97977c63"},{"y":36,"x":216,"u":"https://preview.redd.it/ddyrcixfs8cf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=8735314d3421b279d123739bb2c90bbea57fd77d"},{"y":53,"x":320,"u":"https://preview.redd.it/ddyrcixfs8cf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=d718e00df6175fee45c3cb7b15aa85bb2b699579"},{"y":107,"x":640,"u":"https://preview.redd.it/ddyrcixfs8cf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=1e0960946a6784ad308bb826916cd041c18eb19d"},{"y":161,"x":960,"u":"https://preview.redd.it/ddyrcixfs8cf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=1fd7f7c4bab348a0ffeff88296bafa55f2b2ccec"},{"y":181,"x":1080,"u":"https://preview.redd.it/ddyrcixfs8cf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=dc07ac196afdb6c7702e5972386c9921b07cd5d6"}],"s":{"y":235,"x":1396,"u":"https://preview.redd.it/ddyrcixfs8cf1.png?width=1396&amp;format=png&amp;auto=webp&amp;s=d870df0374cc82865c3fdcc8ad5f3735e7ca8742"},"id":"ddyrcixfs8cf1"},"cagenh3bs8cf1":{"status":"valid","e":"Image","m":"image/png","p":[{"y":16,"x":108,"u":"https://preview.redd.it/cagenh3bs8cf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=016b9033e5b711fb8ec955dfbdaf35f9e4a02de1"},{"y":32,"x":216,"u":"https://preview.redd.it/cagenh3bs8cf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=e76892d526bb46541a8664d15dc126ae1cae8134"},{"y":47,"x":320,"u":"https://preview.redd.it/cagenh3bs8cf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=65a51cf2b6d2a786b3652b8b82d4c8b5065b9328"},{"y":95,"x":640,"u":"https://preview.redd.it/cagenh3bs8cf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=2ac95c684c39c3753abbebebb0f671244f29edff"},{"y":142,"x":960,"u":"https://preview.redd.it/cagenh3bs8cf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=23fd04ac4d7f7d739b24c307a537ece4a8b500d9"},{"y":160,"x":1080,"u":"https://preview.redd.it/cagenh3bs8cf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=50022bb4d86c6461de71fff4b60bd45c662acd18"}],"s":{"y":208,"x":1398,"u":"https://preview.redd.it/cagenh3bs8cf1.png?width=1398&amp;format=png&amp;auto=webp&amp;s=412891b809b68d181ce304b2c56a5a5e71078b2b"},"id":"cagenh3bs8cf1"},"y5a8rizcs8cf1":{"status":"valid","e":"Image","m":"image/png","p":[{"y":19,"x":108,"u":"https://preview.redd.it/y5a8rizcs8cf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=44cfe2f2e542ebe49c6baad2a3be43cc630a6366"},{"y":38,"x":216,"u":"https://preview.redd.it/y5a8rizcs8cf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=999c2133fe33ab1a6239795385b5aa42c9e98ca4"},{"y":57,"x":320,"u":"https://preview.redd.it/y5a8rizcs8cf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=f33f85fa6337dcba27c86e13a80abdd12158174e"},{"y":115,"x":640,"u":"https://preview.redd.it/y5a8rizcs8cf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=61854d235c844410648bdb272a1361342178db54"},{"y":173,"x":960,"u":"https://preview.redd.it/y5a8rizcs8cf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=991192bd1d9b4375a96f844110fd6419831c5808"},{"y":194,"x":1080,"u":"https://preview.redd.it/y5a8rizcs8cf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f70edd6cf0e0d3e2aad11e748a1ab88758ae4e9c"}],"s":{"y":253,"x":1402,"u":"https://preview.redd.it/y5a8rizcs8cf1.png?width=1402&amp;format=png&amp;auto=webp&amp;s=0d1ffc2028a3ec98ef6c4ba02dcb184cb0d0999e"},"id":"y5a8rizcs8cf1"}},"name":"t3_1lx66on","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":1,"author_flair_background_color":null,"subreddit_type":"public","ups":16,"total_awards_received":0,"media_embed":{},"thumbnail_width":140,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_1pagh8551x","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":16,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"https://b.thumbs.redditmedia.com/YgiW1tXulPbYxegiQD_ZWMXklDrQz0iXGKWh4ebIFjE.jpg","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1752238565,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Hi, &lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;m currently facing a weird issue.&lt;br/&gt;\\nI was testing different embedding models, with the goal being to integrate the best local one in a django application. &lt;/p&gt;\\n\\n&lt;p&gt;Architecture is as follows : &lt;/p&gt;\\n\\n&lt;p&gt;- One Mac Book air running LMStudio, acting as a local server for llm and embedding operations&lt;/p&gt;\\n\\n&lt;p&gt;- My PC for the django application, running the codebase &lt;/p&gt;\\n\\n&lt;p&gt;I use CosineDistance to test the models. The functionality is a semantic search. &lt;/p&gt;\\n\\n&lt;p&gt;I noticed the following : &lt;/p&gt;\\n\\n&lt;p&gt;- Using the text-embedding-3-large model, (OAI API) gives great results&lt;br/&gt;\\n- Using Nomic embedding model gives great results also&lt;br/&gt;\\n- Using Qwen embedding models give very bad results, as if the encoding wouldn&amp;#39;t make any sense. &lt;/p&gt;\\n\\n&lt;p&gt;i&amp;#39;m using a aembed() method to call the embedding models, and I declare them using : &lt;/p&gt;\\n\\n&lt;pre&gt;&lt;code&gt;OpenAIEmbeddings(\\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  model=model_name,\\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  check_embedding_ctx_length=False,\\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  base_url=base_url,\\nÂ  Â  Â  Â  Â  Â  Â  Â  Â  Â  api_key=api_key,\\nÂ  Â  Â  Â  Â  Â  Â  Â  )\\n&lt;/code&gt;&lt;/pre&gt;\\n\\n&lt;p&gt;As LM studio provides an OpenAI-like API. Here are the values of the different tests I ran.&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://preview.redd.it/cagenh3bs8cf1.png?width=1398&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=412891b809b68d181ce304b2c56a5a5e71078b2b\\"&gt;OpenAI cosine distance test results&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://preview.redd.it/y5a8rizcs8cf1.png?width=1402&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=0d1ffc2028a3ec98ef6c4ba02dcb184cb0d0999e\\"&gt;LM Studio Nomic cosine distance test&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://preview.redd.it/ddyrcixfs8cf1.png?width=1396&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d870df0374cc82865c3fdcc8ad5f3735e7ca8742\\"&gt;LM Studio Qwen 3 cosine distance test &lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;I just can&amp;#39;t figure out what&amp;#39;s going on. Qwen 3 is supposed to be among the best models.&lt;br/&gt;\\nCan someone give advice ?&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1lx66on","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"IndependentApart5556","discussion_type":null,"num_comments":14,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lx66on/issues_with_qwen_3_embedding_models_4b_and_06b/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lx66on/issues_with_qwen_3_embedding_models_4b_and_06b/","subreddit_subscribers":497826,"created_utc":1752238565,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2jtx7l","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Loose_Race908","can_mod_post":false,"created_utc":1752242215,"send_replies":true,"parent_id":"t1_n2jptx0","score":1,"author_fullname":"t2_bnej4s5r","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yup, ðŸ‘ this is effectively the same as my working CFG for loading the Qwen3 4B embedding and reranking models, it took a bit of troubleshooting to get them to work correctly but once they do they are superb.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2jtx7l","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yup, ðŸ‘ this is effectively the same as my working CFG for loading the Qwen3 4B embedding and reranking models, it took a bit of troubleshooting to get them to work correctly but once they do they are superb.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lx66on","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lx66on/issues_with_qwen_3_embedding_models_4b_and_06b/n2jtx7l/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752242215,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2jz5ph","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"atineiatte","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2jyhwd","score":1,"author_fullname":"t2_4x2qt","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Congratulations, you are now an advanced user! It's time to start using transformers via Python scripts :)","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2jz5ph","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Congratulations, you are now an advanced user! It&amp;#39;s time to start using transformers via Python scripts :)&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lx66on","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lx66on/issues_with_qwen_3_embedding_models_4b_and_06b/n2jz5ph/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752243780,"author_flair_text":null,"treatment_tags":[],"created_utc":1752243780,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2jyhwd","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"IndependentApart5556","can_mod_post":false,"created_utc":1752243589,"send_replies":true,"parent_id":"t1_n2jptx0","score":1,"author_fullname":"t2_1pagh8551x","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I'm not sure if I have access to such configuration options when using LM Studio on Mac","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2jyhwd","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;m not sure if I have access to such configuration options when using LM Studio on Mac&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lx66on","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lx66on/issues_with_qwen_3_embedding_models_4b_and_06b/n2jyhwd/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752243589,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2k83w0","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"SkyFeistyLlama8","can_mod_post":false,"created_utc":1752246344,"send_replies":true,"parent_id":"t1_n2jptx0","score":2,"author_fullname":"t2_1hgbaqgbnq","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I was using GGUF q8 quants of the 4B and 0.6B but I still got nonsense results. Cosine similarity only worked when query and target strings were very close to each other. I might try the f16 versions to see if there's any difference.\\n\\nEdit: no difference. Maybe something is wrong with how llama.cpp handles Qwen3 embedding models. IBM's granite-embedding-125m-english-f16 GGUF by Bartowski works fine, is much more accurate and runs a lot faster.","edited":1752248817,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2k83w0","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I was using GGUF q8 quants of the 4B and 0.6B but I still got nonsense results. Cosine similarity only worked when query and target strings were very close to each other. I might try the f16 versions to see if there&amp;#39;s any difference.&lt;/p&gt;\\n\\n&lt;p&gt;Edit: no difference. Maybe something is wrong with how llama.cpp handles Qwen3 embedding models. IBM&amp;#39;s granite-embedding-125m-english-f16 GGUF by Bartowski works fine, is much more accurate and runs a lot faster.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lx66on","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lx66on/issues_with_qwen_3_embedding_models_4b_and_06b/n2k83w0/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752246344,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2mf2b9","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Gregory-Wolf","can_mod_post":false,"created_utc":1752269079,"send_replies":true,"parent_id":"t1_n2jptx0","score":1,"author_fullname":"t2_gethr3mh","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"That's for embedding, right?  \\nAnd what's the prompt for retrieving?\\n\\nThanks!\\n\\nBtw, how does qwen3 compare to nomic's code embedder (it's a 7b model based on qwen 2.5 if I didn't miss anything).","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2mf2b9","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;That&amp;#39;s for embedding, right?&lt;br/&gt;\\nAnd what&amp;#39;s the prompt for retrieving?&lt;/p&gt;\\n\\n&lt;p&gt;Thanks!&lt;/p&gt;\\n\\n&lt;p&gt;Btw, how does qwen3 compare to nomic&amp;#39;s code embedder (it&amp;#39;s a 7b model based on qwen 2.5 if I didn&amp;#39;t miss anything).&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lx66on","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lx66on/issues_with_qwen_3_embedding_models_4b_and_06b/n2mf2b9/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752269079,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2jptx0","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"atineiatte","can_mod_post":false,"created_utc":1752240958,"send_replies":true,"parent_id":"t3_1lx66on","score":7,"author_fullname":"t2_4x2qt","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"    \\"embedding_model_name\\": \\"Qwen/Qwen3-Embedding-4B\\",\\n    \\"max_context_tokens\\": 32768,\\n    \\"embedding_dimension\\": 2560,\\n\\n        self.tokenizer = AutoTokenizer.from_pretrained(CONFIG[\\"embedding_model_name\\"], padding_side='left')\\n        self.model = AutoModel.from_pretrained(CONFIG[\\"embedding_model_name\\"])\\n        self.model.to(self.device)\\n        if self.device == \\"cuda\\":\\n            self.model = self.model.half()  # Convert to float16\\n        \\n        self.max_length = CONFIG[\\"max_context_tokens\\"]\\n        \\n        # Task description from pair embedding generator\\n        self.task_description = 'Given this project documentation, create a comprehensive embedding that focuses on project purpose and scope of work, technical details and implementation, and domain-specific information'\\n        instruction_template = f'Instruct: {self.task_description}\\\\nQuery:'\\n        instruction_tokens = len(self.tokenizer.encode(instruction_template))\\n        self.effective_max_tokens = self.max_length - instruction_tokens\\n\\nThese are the relevant parts of an embedding script I use and I get fantastic results","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2jptx0","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;pre&gt;&lt;code&gt;&amp;quot;embedding_model_name&amp;quot;: &amp;quot;Qwen/Qwen3-Embedding-4B&amp;quot;,\\n&amp;quot;max_context_tokens&amp;quot;: 32768,\\n&amp;quot;embedding_dimension&amp;quot;: 2560,\\n\\n    self.tokenizer = AutoTokenizer.from_pretrained(CONFIG[&amp;quot;embedding_model_name&amp;quot;], padding_side=&amp;#39;left&amp;#39;)\\n    self.model = AutoModel.from_pretrained(CONFIG[&amp;quot;embedding_model_name&amp;quot;])\\n    self.model.to(self.device)\\n    if self.device == &amp;quot;cuda&amp;quot;:\\n        self.model = self.model.half()  # Convert to float16\\n\\n    self.max_length = CONFIG[&amp;quot;max_context_tokens&amp;quot;]\\n\\n    # Task description from pair embedding generator\\n    self.task_description = &amp;#39;Given this project documentation, create a comprehensive embedding that focuses on project purpose and scope of work, technical details and implementation, and domain-specific information&amp;#39;\\n    instruction_template = f&amp;#39;Instruct: {self.task_description}\\\\nQuery:&amp;#39;\\n    instruction_tokens = len(self.tokenizer.encode(instruction_template))\\n    self.effective_max_tokens = self.max_length - instruction_tokens\\n&lt;/code&gt;&lt;/pre&gt;\\n\\n&lt;p&gt;These are the relevant parts of an embedding script I use and I get fantastic results&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lx66on/issues_with_qwen_3_embedding_models_4b_and_06b/n2jptx0/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752240958,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lx66on","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":7}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2k48qh","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"PaceZealousideal6091","can_mod_post":false,"created_utc":1752245250,"send_replies":true,"parent_id":"t1_n2jtxi1","score":1,"author_fullname":"t2_bpkdm1tk","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"No wonder! I have been  scratching my head bald! Thanks for the headsup.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2k48qh","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;No wonder! I have been  scratching my head bald! Thanks for the headsup.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lx66on","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lx66on/issues_with_qwen_3_embedding_models_4b_and_06b/n2k48qh/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752245250,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2mr4q3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"WaveCut","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2kpayw","score":1,"author_fullname":"t2_9sufu","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Have you seen any decent example?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2mr4q3","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Have you seen any decent example?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lx66on","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lx66on/issues_with_qwen_3_embedding_models_4b_and_06b/n2mr4q3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752272965,"author_flair_text":null,"treatment_tags":[],"created_utc":1752272965,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2n4ovm","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Ok_Warning2146","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2kpayw","score":1,"author_fullname":"t2_s6sfw4yy","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"How to format? It seems to me it is the same as others as far as the sentence transformer example given by the official [README.md](http://README.md)\\n\\n[https://huggingface.co/Qwen/Qwen3-Embedding-0.6B](https://huggingface.co/Qwen/Qwen3-Embedding-0.6B)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2n4ovm","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;How to format? It seems to me it is the same as others as far as the sentence transformer example given by the official &lt;a href=\\"http://README.md\\"&gt;README.md&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://huggingface.co/Qwen/Qwen3-Embedding-0.6B\\"&gt;https://huggingface.co/Qwen/Qwen3-Embedding-0.6B&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lx66on","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lx66on/issues_with_qwen_3_embedding_models_4b_and_06b/n2n4ovm/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752277620,"author_flair_text":null,"treatment_tags":[],"created_utc":1752277620,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2kpayw","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"matteogeniaccio","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2kh1np","score":1,"author_fullname":"t2_hoxc8","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Are you properly formatting the query? The query and the documents must be formatted differently in qwen3","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2kpayw","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Are you properly formatting the query? The query and the documents must be formatted differently in qwen3&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lx66on","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lx66on/issues_with_qwen_3_embedding_models_4b_and_06b/n2kpayw/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752251173,"author_flair_text":null,"treatment_tags":[],"created_utc":1752251173,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2kh1np","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Ok_Warning2146","can_mod_post":false,"created_utc":1752248826,"send_replies":true,"parent_id":"t1_n2jtxi1","score":1,"author_fullname":"t2_s6sfw4yy","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I was using sentence transformer but I still get bad results","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2kh1np","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I was using sentence transformer but I still get bad results&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lx66on","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lx66on/issues_with_qwen_3_embedding_models_4b_and_06b/n2kh1np/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752248826,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2jtxi1","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"matteogeniaccio","can_mod_post":false,"created_utc":1752242218,"send_replies":true,"parent_id":"t3_1lx66on","score":6,"author_fullname":"t2_hoxc8","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Qwen3 embedding is currently broken until this is merged:Â https://github.com/ggml-org/llama.cpp/pull/14029\\n\\n\\nOther engines like vllm give the correct results","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2jtxi1","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Qwen3 embedding is currently broken until this is merged:Â &lt;a href=\\"https://github.com/ggml-org/llama.cpp/pull/14029\\"&gt;https://github.com/ggml-org/llama.cpp/pull/14029&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;Other engines like vllm give the correct results&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lx66on/issues_with_qwen_3_embedding_models_4b_and_06b/n2jtxi1/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752242218,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lx66on","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2jkqge","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"techmago","can_mod_post":false,"created_utc":1752239301,"send_replies":true,"parent_id":"t3_1lx66on","score":1,"author_fullname":"t2_azy5rpp","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"i read somewhere that Qwen 3 Embedding need some very specific params. If you don't use them, it will perform porly.\\n\\n(AKA: i have the same issue)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2jkqge","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;i read somewhere that Qwen 3 Embedding need some very specific params. If you don&amp;#39;t use them, it will perform porly.&lt;/p&gt;\\n\\n&lt;p&gt;(AKA: i have the same issue)&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lx66on/issues_with_qwen_3_embedding_models_4b_and_06b/n2jkqge/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752239301,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lx66on","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2kgozl","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Ok_Warning2146","can_mod_post":false,"created_utc":1752248729,"send_replies":true,"parent_id":"t3_1lx66on","score":1,"author_fullname":"t2_s6sfw4yy","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Same experience here. I find other 150m models outperforming it.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2kgozl","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Same experience here. I find other 150m models outperforming it.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lx66on/issues_with_qwen_3_embedding_models_4b_and_06b/n2kgozl/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752248729,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lx66on","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),s=()=>e.jsx(t,{data:a});export{s as default};
