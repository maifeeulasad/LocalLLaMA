import{j as e}from"./index-CNyNkRpk.js";import{R as l}from"./RedditPostRenderer-Dza0u9i2.js";import"./index-BUchu_-K.js";const a=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Hi everyone,\\n\\nI'm looking to upgrade my hardware for serving a 24b to 30b language model (LLM) to around 50 concurrent users, and I'm trying to decide between two NVIDIA GPU configurations:\\n\\n1. **NVIDIA H200 (2x GPUs)**\\n   * Dual GPU setup\\n   * 141 VRAM per GPU (for a total of 282GB VRAM)\\n2. **NVIDIA L40S (8x GPUs)**\\n   * 8 GPUs in total\\n   * 24GB VRAM per GPU (for a total of 192GB VRAM)\\n\\nI’m leaning towards a setup that offers the best performance in terms of both memory bandwidth and raw computational power, as I’ll be handling complex queries and large models. My primary concern is whether the 2x GPUs with more memory (H200) will be able to handle the 24b-30b LLM load better, or if I should opt for the L40S with more GPUs but less memory per GPU.\\n\\nHas anyone had experience with serving large models on either of these setups, and which would you recommend for optimal performance with 50 concurrent users?\\n\\nAppreciate any insights!\\n\\nEdit: H200 VRAM","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Help Deciding Between NVIDIA H200 (2x GPUs) vs NVIDIA L40S (8x GPUs) for Serving 24b-30b LLM to 50 Concurrent Users","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1m285sn","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.75,"author_flair_background_color":null,"subreddit_type":"public","ups":6,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_lzvqlpc","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":6,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":1752762507,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1752760444,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;m looking to upgrade my hardware for serving a 24b to 30b language model (LLM) to around 50 concurrent users, and I&amp;#39;m trying to decide between two NVIDIA GPU configurations:&lt;/p&gt;\\n\\n&lt;ol&gt;\\n&lt;li&gt;&lt;strong&gt;NVIDIA H200 (2x GPUs)&lt;/strong&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;Dual GPU setup&lt;/li&gt;\\n&lt;li&gt;141 VRAM per GPU (for a total of 282GB VRAM)&lt;/li&gt;\\n&lt;/ul&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;strong&gt;NVIDIA L40S (8x GPUs)&lt;/strong&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;8 GPUs in total&lt;/li&gt;\\n&lt;li&gt;24GB VRAM per GPU (for a total of 192GB VRAM)&lt;/li&gt;\\n&lt;/ul&gt;&lt;/li&gt;\\n&lt;/ol&gt;\\n\\n&lt;p&gt;I’m leaning towards a setup that offers the best performance in terms of both memory bandwidth and raw computational power, as I’ll be handling complex queries and large models. My primary concern is whether the 2x GPUs with more memory (H200) will be able to handle the 24b-30b LLM load better, or if I should opt for the L40S with more GPUs but less memory per GPU.&lt;/p&gt;\\n\\n&lt;p&gt;Has anyone had experience with serving large models on either of these setups, and which would you recommend for optimal performance with 50 concurrent users?&lt;/p&gt;\\n\\n&lt;p&gt;Appreciate any insights!&lt;/p&gt;\\n\\n&lt;p&gt;Edit: H200 VRAM&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1m285sn","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"beratcmn","discussion_type":null,"num_comments":30,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1m285sn/help_deciding_between_nvidia_h200_2x_gpus_vs/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1m285sn/help_deciding_between_nvidia_h200_2x_gpus_vs/","subreddit_subscribers":500897,"created_utc":1752760444,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3olulw","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ButThatsMyRamSlot","can_mod_post":false,"created_utc":1752779104,"send_replies":true,"parent_id":"t1_n3n0j7x","score":1,"author_fullname":"t2_uei1i14w","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I’ve seen both of these configurations on vast.ai before","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3olulw","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I’ve seen both of these configurations on vast.ai before&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m285sn","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m285sn/help_deciding_between_nvidia_h200_2x_gpus_vs/n3olulw/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752779104,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3n0j7x","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"tomz17","can_mod_post":false,"created_utc":1752763201,"send_replies":true,"parent_id":"t3_1m285sn","score":11,"author_fullname":"t2_1mhx5","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"rent both configs and benchmark your use case?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3n0j7x","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;rent both configs and benchmark your use case?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m285sn/help_deciding_between_nvidia_h200_2x_gpus_vs/n3n0j7x/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752763201,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m285sn","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":11}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3nfa97","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Conscious_Cut_6144","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3my0ib","score":1,"author_fullname":"t2_9hl4ymvj","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"He halfway fixed it, L40’s are still listed as 1/2 their actual.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3nfa97","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;He halfway fixed it, L40’s are still listed as 1/2 their actual.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m285sn","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m285sn/help_deciding_between_nvidia_h200_2x_gpus_vs/n3nfa97/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752767296,"author_flair_text":null,"treatment_tags":[],"created_utc":1752767296,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3my0ib","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"No_Afternoon_4260","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3mxhp2","score":2,"author_fullname":"t2_cj9kap4bx","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yeah exactly what did op wrote","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3my0ib","is_submitter":false,"collapsed":false,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yeah exactly what did op wrote&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m285sn","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m285sn/help_deciding_between_nvidia_h200_2x_gpus_vs/n3my0ib/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752762490,"author_flair_text":"llama.cpp","treatment_tags":[],"created_utc":1752762490,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n3mxhp2","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Conscious_Cut_6144","can_mod_post":false,"created_utc":1752762341,"send_replies":true,"parent_id":"t1_n3mvzno","score":2,"author_fullname":"t2_9hl4ymvj","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"They aren’t 24 or 80…\\n141GB each\\n\\nAnd L40s is 48GB each","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3mxhp2","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;They aren’t 24 or 80…\\n141GB each&lt;/p&gt;\\n\\n&lt;p&gt;And L40s is 48GB each&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m285sn","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m285sn/help_deciding_between_nvidia_h200_2x_gpus_vs/n3mxhp2/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752762341,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3n7t5r","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"beratcmn","can_mod_post":false,"created_utc":1752765227,"send_replies":true,"parent_id":"t1_n3mvzno","score":1,"author_fullname":"t2_lzvqlpc","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Ahh, my bad. I edited to post to better reflect the real VRAM for H200s. Thank you!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3n7t5r","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Ahh, my bad. I edited to post to better reflect the real VRAM for H200s. Thank you!&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m285sn","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m285sn/help_deciding_between_nvidia_h200_2x_gpus_vs/n3n7t5r/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752765227,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3mvzno","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"No_Afternoon_4260","can_mod_post":false,"created_utc":1752761914,"send_replies":true,"parent_id":"t3_1m285sn","score":4,"author_fullname":"t2_cj9kap4bx","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Are you sure h200 aren't 80 gb a pop?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3mvzno","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Are you sure h200 aren&amp;#39;t 80 gb a pop?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m285sn/help_deciding_between_nvidia_h200_2x_gpus_vs/n3mvzno/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752761914,"author_flair_text":"llama.cpp","treatment_tags":[],"link_id":"t3_1m285sn","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3n76z0","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"beratcmn","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3n0nyn","score":2,"author_fullname":"t2_lzvqlpc","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"This tools looks amazing! When I inputted the Mistral Small 24B these are the results I get for 8x L40s GPUs. \\n\\nhttps://preview.redd.it/a1yska42agdf1.png?width=437&amp;format=png&amp;auto=webp&amp;s=d450edd0916c543622620e5c62bde4ea3f108113","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3n76z0","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;This tools looks amazing! When I inputted the Mistral Small 24B these are the results I get for 8x L40s GPUs. &lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://preview.redd.it/a1yska42agdf1.png?width=437&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d450edd0916c543622620e5c62bde4ea3f108113\\"&gt;https://preview.redd.it/a1yska42agdf1.png?width=437&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=d450edd0916c543622620e5c62bde4ea3f108113&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m285sn","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m285sn/help_deciding_between_nvidia_h200_2x_gpus_vs/n3n76z0/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752765055,"media_metadata":{"a1yska42agdf1":{"status":"valid","e":"Image","m":"image/png","p":[{"y":210,"x":108,"u":"https://preview.redd.it/a1yska42agdf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=623ed0450934722d8dd8070156dea4566cae3975"},{"y":420,"x":216,"u":"https://preview.redd.it/a1yska42agdf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=f553c5f0c713fae1242594e4725a8889145928a9"},{"y":622,"x":320,"u":"https://preview.redd.it/a1yska42agdf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=2139e776a50d7fd50b6ccb7b7a7fcf8c4c5dc1ba"}],"s":{"y":850,"x":437,"u":"https://preview.redd.it/a1yska42agdf1.png?width=437&amp;format=png&amp;auto=webp&amp;s=d450edd0916c543622620e5c62bde4ea3f108113"},"id":"a1yska42agdf1"}},"author_flair_text":null,"treatment_tags":[],"created_utc":1752765055,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3ob4sd","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"EmilPi","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3n0nyn","score":2,"author_fullname":"t2_jti45lwl","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"[https://apxml.com/tools/vram-calculator](https://apxml.com/tools/vram-calculator)  \\nMaybe you know some details how this calculator works. What is the assumed RAM speed if I add CPU offload to settings?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3ob4sd","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;&lt;a href=\\"https://apxml.com/tools/vram-calculator\\"&gt;https://apxml.com/tools/vram-calculator&lt;/a&gt;&lt;br/&gt;\\nMaybe you know some details how this calculator works. What is the assumed RAM speed if I add CPU offload to settings?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m285sn","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m285sn/help_deciding_between_nvidia_h200_2x_gpus_vs/n3ob4sd/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752776054,"author_flair_text":null,"treatment_tags":[],"created_utc":1752776054,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3nn2pv","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Candid_Payment_4094","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3n7ilo","score":3,"author_fullname":"t2_anbsqj0od","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I think it's not accurate in that sense. The bottleneck is not compute, it's clearly the VRAM and VRAM bandwidth. As the inter GPU communication has to go through the motherboard, it's way slower than H200.","edited":false,"author_flair_css_class":null,"name":"t1_n3nn2pv","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I think it&amp;#39;s not accurate in that sense. The bottleneck is not compute, it&amp;#39;s clearly the VRAM and VRAM bandwidth. As the inter GPU communication has to go through the motherboard, it&amp;#39;s way slower than H200.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1m285sn","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m285sn/help_deciding_between_nvidia_h200_2x_gpus_vs/n3nn2pv/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752769473,"author_flair_text":null,"collapsed":false,"created_utc":1752769473,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n3n7ilo","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"beratcmn","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3n0nyn","score":1,"author_fullname":"t2_lzvqlpc","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"https://preview.redd.it/jpg1plpnagdf1.png?width=432&amp;format=png&amp;auto=webp&amp;s=a0b7d7af03a8443399d59a1efaddea06ac9f9fa2\\n\\nBut numbers are worse when I switch the GPUs to 2xH200. Per-user token/s speed is almost half of what I get from 8xL40s setup.  \\n\\n\\nBased on your experience do you think this artificial benchmark is close to the real world?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3n7ilo","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;&lt;a href=\\"https://preview.redd.it/jpg1plpnagdf1.png?width=432&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a0b7d7af03a8443399d59a1efaddea06ac9f9fa2\\"&gt;https://preview.redd.it/jpg1plpnagdf1.png?width=432&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=a0b7d7af03a8443399d59a1efaddea06ac9f9fa2&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;But numbers are worse when I switch the GPUs to 2xH200. Per-user token/s speed is almost half of what I get from 8xL40s setup.  &lt;/p&gt;\\n\\n&lt;p&gt;Based on your experience do you think this artificial benchmark is close to the real world?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m285sn","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m285sn/help_deciding_between_nvidia_h200_2x_gpus_vs/n3n7ilo/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752765144,"media_metadata":{"jpg1plpnagdf1":{"status":"valid","e":"Image","m":"image/png","p":[{"y":212,"x":108,"u":"https://preview.redd.it/jpg1plpnagdf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=ff2b9c8f6eff75fd3bfaf3f49fae2690ee1db3f6"},{"y":425,"x":216,"u":"https://preview.redd.it/jpg1plpnagdf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=6f45f047c0c5e20a4849a3fad9054fb6402c076b"},{"y":630,"x":320,"u":"https://preview.redd.it/jpg1plpnagdf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=e65dd6cd434f0837614ba3ee4214ae4f0752d8f4"}],"s":{"y":851,"x":432,"u":"https://preview.redd.it/jpg1plpnagdf1.png?width=432&amp;format=png&amp;auto=webp&amp;s=a0b7d7af03a8443399d59a1efaddea06ac9f9fa2"},"id":"jpg1plpnagdf1"}},"author_flair_text":null,"treatment_tags":[],"created_utc":1752765144,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3n0nyn","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Candid_Payment_4094","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3mz24e","score":8,"author_fullname":"t2_anbsqj0od","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I am a sr machine learning engineer. I have experience with multiple H100s in production setting (using vLLM).  \\nFor concurrency it's better to assign the vLLM instance all the available GPUs (unless you have like 8 H200s or something) so that there is plenty of KVCache (for concurrency), rather than having the model weights duplicated across.\\n\\nSince L40S doesn't have NVLINK, making tensor parallelism difficult/slow. So I advice you to have H200.\\n\\nYou can test out different scenarios at: [https://apxml.com/tools/vram-calculator](https://apxml.com/tools/vram-calculator)\\n\\nWith 2 H200, you can probably serve more like 150-200 users (NOT active users) with Gemma-3 27b at full precision (16BFLOAT)","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3n0nyn","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I am a sr machine learning engineer. I have experience with multiple H100s in production setting (using vLLM).&lt;br/&gt;\\nFor concurrency it&amp;#39;s better to assign the vLLM instance all the available GPUs (unless you have like 8 H200s or something) so that there is plenty of KVCache (for concurrency), rather than having the model weights duplicated across.&lt;/p&gt;\\n\\n&lt;p&gt;Since L40S doesn&amp;#39;t have NVLINK, making tensor parallelism difficult/slow. So I advice you to have H200.&lt;/p&gt;\\n\\n&lt;p&gt;You can test out different scenarios at: &lt;a href=\\"https://apxml.com/tools/vram-calculator\\"&gt;https://apxml.com/tools/vram-calculator&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;With 2 H200, you can probably serve more like 150-200 users (NOT active users) with Gemma-3 27b at full precision (16BFLOAT)&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m285sn","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m285sn/help_deciding_between_nvidia_h200_2x_gpus_vs/n3n0nyn/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752763237,"author_flair_text":null,"treatment_tags":[],"created_utc":1752763237,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":8}}],"before":null}},"user_reports":[],"saved":false,"id":"n3mz24e","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"beratcmn","can_mod_post":false,"created_utc":1752762786,"send_replies":true,"parent_id":"t1_n3mxrwj","score":1,"author_fullname":"t2_lzvqlpc","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"ah sorry my bad, i edited the post","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3mz24e","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;ah sorry my bad, i edited the post&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m285sn","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m285sn/help_deciding_between_nvidia_h200_2x_gpus_vs/n3mz24e/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752762786,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3mxrwj","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Candid_Payment_4094","can_mod_post":false,"created_utc":1752762422,"send_replies":true,"parent_id":"t3_1m285sn","score":4,"author_fullname":"t2_anbsqj0od","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"H200 has 141GB of VRAM.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3mxrwj","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;H200 has 141GB of VRAM.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m285sn/help_deciding_between_nvidia_h200_2x_gpus_vs/n3mxrwj/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752762422,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m285sn","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3prxsh","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"NeuralNakama","can_mod_post":false,"created_utc":1752791486,"send_replies":true,"parent_id":"t3_1m285sn","score":1,"author_fullname":"t2_1oj5msrcxf","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"try on [vast.ai](http://vast.ai) or [runpod.io](http://runpod.io) use vllm and sglang probably sglang better. I don't think there will be much difference in performance for 50 people, but if you do more concurrent transactions, I think h200 will make a difference in speed. and definitely try with sglang","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3prxsh","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;try on &lt;a href=\\"http://vast.ai\\"&gt;vast.ai&lt;/a&gt; or &lt;a href=\\"http://runpod.io\\"&gt;runpod.io&lt;/a&gt; use vllm and sglang probably sglang better. I don&amp;#39;t think there will be much difference in performance for 50 people, but if you do more concurrent transactions, I think h200 will make a difference in speed. and definitely try with sglang&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m285sn/help_deciding_between_nvidia_h200_2x_gpus_vs/n3prxsh/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752791486,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m285sn","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3q37x3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"NoVibeCoding","can_mod_post":false,"created_utc":1752795207,"send_replies":true,"parent_id":"t3_1m285sn","score":1,"author_fullname":"t2_1neapdttam","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"We primarily focus on GPU rental, but occasionally, we build L40s / H200, etc., servers for our customers. Nowadays, I always recommend going with the Pro6000, unless you need NVLink for training or require the absolute best performance, and money is no object. The Pro6000 is a better deal overall.\\n\\nSome people worry about availability - we've some in stock and can obtain more through NVIDIA Inception, allowing us to build a server for you. The cost will be the same as building it yourself, as the build cost is offset by the discounts we receive from manufacturers.\\n\\nYou can also try a VM Pro6000 on the platform and see if it works for you: [https://www.cloudrift.ai/](https://www.cloudrift.ai/)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3q37x3","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;We primarily focus on GPU rental, but occasionally, we build L40s / H200, etc., servers for our customers. Nowadays, I always recommend going with the Pro6000, unless you need NVLink for training or require the absolute best performance, and money is no object. The Pro6000 is a better deal overall.&lt;/p&gt;\\n\\n&lt;p&gt;Some people worry about availability - we&amp;#39;ve some in stock and can obtain more through NVIDIA Inception, allowing us to build a server for you. The cost will be the same as building it yourself, as the build cost is offset by the discounts we receive from manufacturers.&lt;/p&gt;\\n\\n&lt;p&gt;You can also try a VM Pro6000 on the platform and see if it works for you: &lt;a href=\\"https://www.cloudrift.ai/\\"&gt;https://www.cloudrift.ai/&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":true,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m285sn/help_deciding_between_nvidia_h200_2x_gpus_vs/n3q37x3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752795207,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m285sn","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3rjgn0","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"GPTrack_ai","can_mod_post":false,"created_utc":1752815283,"send_replies":true,"parent_id":"t3_1m285sn","score":1,"author_fullname":"t2_1tpuoj72sa","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Neither, PCIe connected cards are outdated (too slow). Because of that Nvidia does not offer B200 and B300 as PCIe any more. Nowadays you need to go for SXM or superchip. Examples: GH200 624GB, DGX Station GB300 784GB.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3rjgn0","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Neither, PCIe connected cards are outdated (too slow). Because of that Nvidia does not offer B200 and B300 as PCIe any more. Nowadays you need to go for SXM or superchip. Examples: GH200 624GB, DGX Station GB300 784GB.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m285sn/help_deciding_between_nvidia_h200_2x_gpus_vs/n3rjgn0/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752815283,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m285sn","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3u1bku","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Sureshkk_15","can_mod_post":false,"created_utc":1752853072,"send_replies":true,"parent_id":"t3_1m285sn","score":1,"author_fullname":"t2_5mgndyut","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"You will get higher throughout on L40s also you should deploy 2 instance of model on L40s and 1 if on h200","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3u1bku","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You will get higher throughout on L40s also you should deploy 2 instance of model on L40s and 1 if on h200&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m285sn/help_deciding_between_nvidia_h200_2x_gpus_vs/n3u1bku/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752853072,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m285sn","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3n9fov","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Expensive_Ad_1945","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3n839f","score":2,"author_fullname":"t2_9pk85ihx","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"From my experience, more gpu in a single machine will reduce the speed by alot, better go with 2xH200, you'll get better latency and serving 50 users wouldn't be a problem at all with fp8. I wouldn't recommend quantizing your kv as the model performance can dropped alot especially on long context scenario. Then use super optimized serving engine like TensorRT LLM + Triton Inference.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3n9fov","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;From my experience, more gpu in a single machine will reduce the speed by alot, better go with 2xH200, you&amp;#39;ll get better latency and serving 50 users wouldn&amp;#39;t be a problem at all with fp8. I wouldn&amp;#39;t recommend quantizing your kv as the model performance can dropped alot especially on long context scenario. Then use super optimized serving engine like TensorRT LLM + Triton Inference.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m285sn","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m285sn/help_deciding_between_nvidia_h200_2x_gpus_vs/n3n9fov/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752765683,"author_flair_text":null,"treatment_tags":[],"created_utc":1752765683,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n3n839f","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"beratcmn","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3mst3h","score":1,"author_fullname":"t2_lzvqlpc","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yes, nvlink is the most confusing part for me. In theory more vram should mean more concurrency but H200 has a lot more memory bandwidth compared to L40s. In general I am quite confused tbh.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3n839f","is_submitter":true,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yes, nvlink is the most confusing part for me. In theory more vram should mean more concurrency but H200 has a lot more memory bandwidth compared to L40s. In general I am quite confused tbh.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m285sn","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m285sn/help_deciding_between_nvidia_h200_2x_gpus_vs/n3n839f/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752765304,"author_flair_text":null,"treatment_tags":[],"created_utc":1752765304,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3mst3h","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Expensive_Ad_1945","can_mod_post":false,"created_utc":1752760984,"send_replies":true,"parent_id":"t1_n3msj8p","score":2,"author_fullname":"t2_9pk85ihx","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Especially, L40 doesn't support nvlink as far as i'm concerned.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3mst3h","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Especially, L40 doesn&amp;#39;t support nvlink as far as i&amp;#39;m concerned.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m285sn","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m285sn/help_deciding_between_nvidia_h200_2x_gpus_vs/n3mst3h/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752760984,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n3msj8p","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Expensive_Ad_1945","can_mod_post":false,"created_utc":1752760902,"send_replies":true,"parent_id":"t3_1m285sn","score":2,"author_fullname":"t2_9pk85ihx","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"If your setup is a single server with multiple GPUs, the less number of gpus that have the better compute will be faster as the memory bandwidth when deploying model in multigpu setup will be greater than the gain. With the 8 L40 you'll get better total throughput, means more batch of user handled concurrently, with 2 H200 you'll get better latency. But with only 50 users, i think 2xH200 will suit you better.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3msj8p","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;If your setup is a single server with multiple GPUs, the less number of gpus that have the better compute will be faster as the memory bandwidth when deploying model in multigpu setup will be greater than the gain. With the 8 L40 you&amp;#39;ll get better total throughput, means more batch of user handled concurrently, with 2 H200 you&amp;#39;ll get better latency. But with only 50 users, i think 2xH200 will suit you better.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m285sn/help_deciding_between_nvidia_h200_2x_gpus_vs/n3msj8p/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752760902,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m285sn","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3n6odq","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Horsemen208","can_mod_post":false,"created_utc":1752764910,"send_replies":true,"parent_id":"t3_1m285sn","score":0,"author_fullname":"t2_ufi4srp63","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I would go with 8 L40s since you can distribute users on different GPUs for better efficiency. For training large models, you may want 2 H200s.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3n6odq","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I would go with 8 L40s since you can distribute users on different GPUs for better efficiency. For training large models, you may want 2 H200s.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m285sn/help_deciding_between_nvidia_h200_2x_gpus_vs/n3n6odq/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752764910,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m285sn","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3netfo","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Conscious_Cut_6144","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3n49bl","score":0,"author_fullname":"t2_9hl4ymvj","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Ya pro 6000’s are just coming out and will be hard to get for a while. Especially if you are on a deadline and outside the US.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3netfo","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Ya pro 6000’s are just coming out and will be hard to get for a while. Especially if you are on a deadline and outside the US.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m285sn","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m285sn/help_deciding_between_nvidia_h200_2x_gpus_vs/n3netfo/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752767165,"author_flair_text":null,"treatment_tags":[],"created_utc":1752767165,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}},"user_reports":[],"saved":false,"id":"n3n49bl","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"beratcmn","can_mod_post":false,"created_utc":1752764236,"send_replies":true,"parent_id":"t1_n3myd90","score":2,"author_fullname":"t2_lzvqlpc","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Unfortunately it's really hard to find 6000 series here for some reason. It's easier to find A and L series and H series.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3n49bl","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Unfortunately it&amp;#39;s really hard to find 6000 series here for some reason. It&amp;#39;s easier to find A and L series and H series.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m285sn","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m285sn/help_deciding_between_nvidia_h200_2x_gpus_vs/n3n49bl/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752764236,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n3myd90","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Conscious_Cut_6144","can_mod_post":false,"created_utc":1752762590,"send_replies":true,"parent_id":"t3_1m285sn","score":0,"author_fullname":"t2_9hl4ymvj","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Your specs are way off\\n384GB vram for the 8x l40. \\n282GB for the 2x H200’s\\n\\nA lot of small models won’t support TP8,\\nSo I’d probably go with the h200’s for 32b and smaller models.\\n\\nThat said, I would probably actually go for the L40’s and qwen 235B instead.\\n\\nOh and one more thing…\\nYou should really be looking into Pro 6000’s if that’s an option.","edited":1752762890,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3myd90","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Your specs are way off\\n384GB vram for the 8x l40. \\n282GB for the 2x H200’s&lt;/p&gt;\\n\\n&lt;p&gt;A lot of small models won’t support TP8,\\nSo I’d probably go with the h200’s for 32b and smaller models.&lt;/p&gt;\\n\\n&lt;p&gt;That said, I would probably actually go for the L40’s and qwen 235B instead.&lt;/p&gt;\\n\\n&lt;p&gt;Oh and one more thing…\\nYou should really be looking into Pro 6000’s if that’s an option.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m285sn/help_deciding_between_nvidia_h200_2x_gpus_vs/n3myd90/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752762590,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m285sn","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3nf7h0","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"beratcmn","can_mod_post":false,"created_utc":1752767275,"send_replies":true,"parent_id":"t1_n3neukb","score":1,"author_fullname":"t2_lzvqlpc","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Wdym by that?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3nf7h0","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Wdym by that?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m285sn","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m285sn/help_deciding_between_nvidia_h200_2x_gpus_vs/n3nf7h0/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752767275,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3neukb","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Barry_22","can_mod_post":false,"created_utc":1752767174,"send_replies":true,"parent_id":"t3_1m285sn","score":-1,"author_fullname":"t2_pctbl","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"That's... an overkill.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3neukb","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;That&amp;#39;s... an overkill.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m285sn/help_deciding_between_nvidia_h200_2x_gpus_vs/n3neukb/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752767174,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m285sn","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3nn17f","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Candid_Payment_4094","can_mod_post":false,"created_utc":1752769461,"send_replies":true,"parent_id":"t1_n3nirqi","score":1,"author_fullname":"t2_anbsqj0od","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Your calculations are WAAAAAY off. Gemma-3 27b at full precision barely runs on a single H100. How can you possibly fit a 32b model WITH 50 concurrent users on an RTX 6000 PRO blackwell? Keep in mind that you also want a sequence length that is at least like 16k or even 32k","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3nn17f","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Your calculations are WAAAAAY off. Gemma-3 27b at full precision barely runs on a single H100. How can you possibly fit a 32b model WITH 50 concurrent users on an RTX 6000 PRO blackwell? Keep in mind that you also want a sequence length that is at least like 16k or even 32k&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m285sn","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m285sn/help_deciding_between_nvidia_h200_2x_gpus_vs/n3nn17f/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752769461,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3nirqi","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Accurate-Material275","can_mod_post":false,"created_utc":1752768271,"send_replies":true,"parent_id":"t3_1m285sn","score":-1,"author_fullname":"t2_bm2zctumk","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"What your company really needs is someone that has even the smallest amount of knowledge in relation to AI infrastructure.\\n\\nIf you are looking to run a 32b model for 50 concurrent users you need a single RTX 6000 Pro Blackwell, as stated by others on this thread. Even running on PCIE Gen 4 it still is more than you need for your requirements. At most you could get two and run parallel instances, routed via something like liteLLM to balance requests against multiple instances. However I doubt you would need it.\\n\\n\\n\\nYou say 50 users. How often? What tasks? Speed expectation?\\n\\n  \\n\\n\\nFor reference, QwQ can being be served in fp16 via VLLM on a single 6000 Pro, results shown for a benchmark running with 16384 input tokens and 512 output tokens per request.\\n\\nThroughput: 0.10 requests/s, 1697.30 total tokens/s, 51.43 output tokens/s \\n\\nTotal num prompt tokens:  1638400 Total num output tokens:  51200\\n\\n\\n\\nFor a more reasonable user focussed setup ( i.e smaller input context ), below results are QwQ fp16 with 1024 input and 512 output.\\n\\nThroughput: 1.09 requests/s, 1670.47 total tokens/s, 557.77 output tokens/s \\n\\nTotal num prompt tokens:  102140 Total num output tokens:  51200\\n\\n\\n\\nAgain, this is a SINGLE card running the full half precision fp16 weights without any quantization of either weights or KV cache. I highly doubt your 50 users will all be hammering at the same time, unless they are agents and not human users, and again if so just add an additional unit and balance requests against two instances.\\n\\n\\n\\nYou will unlikely to be able to source two H200, and are likely to be ripped off by vendors looking to offload their L40S stock. The L40S card was never a real contender for LLM hosting anyway due to its nerfed memory bandwidth, and although there are some semi-attractive offers on units hitting the market, none are below the Pro 6000 mark nor offer anywhere near the capabilities and future proofing of the 6000 Pros.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3nirqi","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;What your company really needs is someone that has even the smallest amount of knowledge in relation to AI infrastructure.&lt;/p&gt;\\n\\n&lt;p&gt;If you are looking to run a 32b model for 50 concurrent users you need a single RTX 6000 Pro Blackwell, as stated by others on this thread. Even running on PCIE Gen 4 it still is more than you need for your requirements. At most you could get two and run parallel instances, routed via something like liteLLM to balance requests against multiple instances. However I doubt you would need it.&lt;/p&gt;\\n\\n&lt;p&gt;You say 50 users. How often? What tasks? Speed expectation?&lt;/p&gt;\\n\\n&lt;p&gt;For reference, QwQ can being be served in fp16 via VLLM on a single 6000 Pro, results shown for a benchmark running with 16384 input tokens and 512 output tokens per request.&lt;/p&gt;\\n\\n&lt;p&gt;Throughput: 0.10 requests/s, 1697.30 total tokens/s, 51.43 output tokens/s &lt;/p&gt;\\n\\n&lt;p&gt;Total num prompt tokens:  1638400 Total num output tokens:  51200&lt;/p&gt;\\n\\n&lt;p&gt;For a more reasonable user focussed setup ( i.e smaller input context ), below results are QwQ fp16 with 1024 input and 512 output.&lt;/p&gt;\\n\\n&lt;p&gt;Throughput: 1.09 requests/s, 1670.47 total tokens/s, 557.77 output tokens/s &lt;/p&gt;\\n\\n&lt;p&gt;Total num prompt tokens:  102140 Total num output tokens:  51200&lt;/p&gt;\\n\\n&lt;p&gt;Again, this is a SINGLE card running the full half precision fp16 weights without any quantization of either weights or KV cache. I highly doubt your 50 users will all be hammering at the same time, unless they are agents and not human users, and again if so just add an additional unit and balance requests against two instances.&lt;/p&gt;\\n\\n&lt;p&gt;You will unlikely to be able to source two H200, and are likely to be ripped off by vendors looking to offload their L40S stock. The L40S card was never a real contender for LLM hosting anyway due to its nerfed memory bandwidth, and although there are some semi-attractive offers on units hitting the market, none are below the Pro 6000 mark nor offer anywhere near the capabilities and future proofing of the 6000 Pros.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m285sn/help_deciding_between_nvidia_h200_2x_gpus_vs/n3nirqi/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752768271,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m285sn","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-1}}],"before":null}}]`),o=()=>e.jsx(l,{data:a});export{o as default};
