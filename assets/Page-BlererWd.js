import{j as t}from"./index-B8Dr9DfQ.js";import{R as e}from"./RedditPostRenderer-BvcEF1oP.js";import"./index-BohwUeOq.js";const l=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"**(US$ first, ₹ in brackets, used ChatGPT to capture the requirements)** \\n\\n**Constraints**\\n\\n|**Item**|**Limit**|\\n|:-|:-|\\n|**Total budget**|**US $ 6 000** (₹ 5 16 000) — can stretch to **₹ 6 00 000** if the spec really earns it|\\n|**Workload**|Mostly LLM **inference** on 7 B → 70 B models, occasional fine-tune (FP16/BF16 + Q-formats)|\\n|**Have already**|30 TB SATA array • Cooler Master 1 000 W PSU (fine for ≤2 GPUs)|\\n\\nCurrency used: **US $ at ₹ 86 = $ 1** (July ’25 Indian street pricing).\\n\\n\\n\\n\\n\\n\\n\\n# Four build options (all totals include GPU + CPU + board + 128 GB RAM + PSU upgrade if needed + case)**\\n\\n\\n\\n|**ID**|**GPUs**|**CPU • Board**|**PSU need**|**Total cost US $ (₹)**|**Why it tempts me / worries me**|\\n|:-|:-|:-|:-|:-|:-|\\n|**A – “One-and-done”**|**1 × W7900 48 GB** – $ 2 950 (₹ 2 54 k)|Ryzen 9 7950X • mid-tier X670E|1 000 W (reuse)|**$ 4 700** (₹ 4 04 k)|✅ Single card, ECC, 48 GB holds 70 B FP16❌ Lower raw FP16 vs XTX, ROCm quirks?|\\n|**B – “Dual XTX”**|**2 × 7900 XTX 24 GB** – $ 2 160 (₹ 1 86 k)|Ryzen 9 7950X • X670E|1 000 W (reuse)|**$ 3 900** (₹ 3 36 k)|✅ Best $/TFLOP, huge community tips   ✅ Big headroom for better RAM / NVMe❌ Split VRAM – no single-GPU 70 B|\\n|**C – “Mixed bag”**|**1 × W7900 + 1 × 7900 XTX** – $ 4 035 (₹ 3 47 k)|Ryzen 9 7950X • X670E|1 200 W upgrade (+$ 140 / ₹ 12 k)|**$ 5 920** (₹ 5 09 k)|✅ 72 GB total, still one 48 GB card❌ Any ROCm pain mixing Pro + consumer drivers?|\\n|**D – “Three XTX”**|**3 × 7900 XTX 24 GB** – $ 3 240 (₹ 2 79 k)|*Used* Threadripper 3970X • TRX40|1 600 W Platinum (+$ 523 / ₹ 45 k)|**$ 6 520** (₹ 5 61 k) ← still under 6 L|✅ 72 GB &amp; tensor-parallel across 3 cards  ✅ Lots of PCIe lanes❌ Old platform, higher power/heat|\\n\\n\\\\* Assumptions per build\\n\\n\\n\\n* **RAM** — 128 GB (4 × 32 GB) DDR5-5600 for AM5 builds ($ 523 / ₹ 45 k) or DDR4-3200 ECC for TRX40 ($ 372 / ₹ 32 k)\\n* **Case + fans** — roomy SSI-EEB or XL-ATX chassis (\\\\~$ 233 / ₹ 20 k)\\n* Storage reused; NVMe OS drive not factored (≲$ 100).\\n\\n**What I need from you**\\n\\n1. **Is one 48 GB W7900 (Option A) really simpler** for &gt;40 B models, or do clever tensor-parallel tricks make dual/triple XTX just as usable?\\n2. **ROCm stability** – any real-life driver differences between W-class and XTX?  Horror stories mixing them (Option C)?\\n3. **Multi-XTX on PCIe 4 ×8** (Options B &amp; D): do you actually hit bottlenecks in inference?\\n4. If you’ve built **TRX40/3970X rigs recently** (Option D), are they still worth it in 2025 vs a fresh AM5 or TRX50?\\n5. Any smarter combo that sneaks 48 GB+ of contiguous VRAM into the box while still living under **₹ 6 00 000 / US $ 7 k**?\\n\\n\\n\\n\\n\\nBenchmarks, thermals, power-draw screenshots, “don’t do it!” tales—everything helps.\\n\\n**Help me burn (or save) this budget wisely—thanks!**","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Please gut-check these W7900 vs 7900 XTX server builds","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lu75js","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.2,"author_flair_background_color":null,"subreddit_type":"public","ups":0,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_7bb63l85","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":0,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1751925018,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;&lt;strong&gt;(US$ first, ₹ in brackets, used ChatGPT to capture the requirements)&lt;/strong&gt; &lt;/p&gt;\\n\\n&lt;p&gt;&lt;strong&gt;Constraints&lt;/strong&gt;&lt;/p&gt;\\n\\n&lt;table&gt;&lt;thead&gt;\\n&lt;tr&gt;\\n&lt;th align=\\"left\\"&gt;&lt;strong&gt;Item&lt;/strong&gt;&lt;/th&gt;\\n&lt;th align=\\"left\\"&gt;&lt;strong&gt;Limit&lt;/strong&gt;&lt;/th&gt;\\n&lt;/tr&gt;\\n&lt;/thead&gt;&lt;tbody&gt;\\n&lt;tr&gt;\\n&lt;td align=\\"left\\"&gt;&lt;strong&gt;Total budget&lt;/strong&gt;&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;&lt;strong&gt;US $ 6 000&lt;/strong&gt; (₹ 5 16 000) — can stretch to &lt;strong&gt;₹ 6 00 000&lt;/strong&gt; if the spec really earns it&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;tr&gt;\\n&lt;td align=\\"left\\"&gt;&lt;strong&gt;Workload&lt;/strong&gt;&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;Mostly LLM &lt;strong&gt;inference&lt;/strong&gt; on 7 B → 70 B models, occasional fine-tune (FP16/BF16 + Q-formats)&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;tr&gt;\\n&lt;td align=\\"left\\"&gt;&lt;strong&gt;Have already&lt;/strong&gt;&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;30 TB SATA array • Cooler Master 1 000 W PSU (fine for ≤2 GPUs)&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;/tbody&gt;&lt;/table&gt;\\n\\n&lt;p&gt;Currency used: &lt;strong&gt;US $ at ₹ 86 = $ 1&lt;/strong&gt; (July ’25 Indian street pricing).&lt;/p&gt;\\n\\n&lt;h1&gt;Four build options (all totals include GPU + CPU + board + 128 GB RAM + PSU upgrade if needed + case)**&lt;/h1&gt;\\n\\n&lt;table&gt;&lt;thead&gt;\\n&lt;tr&gt;\\n&lt;th align=\\"left\\"&gt;&lt;strong&gt;ID&lt;/strong&gt;&lt;/th&gt;\\n&lt;th align=\\"left\\"&gt;&lt;strong&gt;GPUs&lt;/strong&gt;&lt;/th&gt;\\n&lt;th align=\\"left\\"&gt;&lt;strong&gt;CPU • Board&lt;/strong&gt;&lt;/th&gt;\\n&lt;th align=\\"left\\"&gt;&lt;strong&gt;PSU need&lt;/strong&gt;&lt;/th&gt;\\n&lt;th align=\\"left\\"&gt;&lt;strong&gt;Total cost US $ (₹)&lt;/strong&gt;&lt;/th&gt;\\n&lt;th align=\\"left\\"&gt;&lt;strong&gt;Why it tempts me / worries me&lt;/strong&gt;&lt;/th&gt;\\n&lt;/tr&gt;\\n&lt;/thead&gt;&lt;tbody&gt;\\n&lt;tr&gt;\\n&lt;td align=\\"left\\"&gt;&lt;strong&gt;A – “One-and-done”&lt;/strong&gt;&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;&lt;strong&gt;1 × W7900 48 GB&lt;/strong&gt; – $ 2 950 (₹ 2 54 k)&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;Ryzen 9 7950X • mid-tier X670E&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;1 000 W (reuse)&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;&lt;strong&gt;$ 4 700&lt;/strong&gt; (₹ 4 04 k)&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;✅ Single card, ECC, 48 GB holds 70 B FP16❌ Lower raw FP16 vs XTX, ROCm quirks?&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;tr&gt;\\n&lt;td align=\\"left\\"&gt;&lt;strong&gt;B – “Dual XTX”&lt;/strong&gt;&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;&lt;strong&gt;2 × 7900 XTX 24 GB&lt;/strong&gt; – $ 2 160 (₹ 1 86 k)&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;Ryzen 9 7950X • X670E&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;1 000 W (reuse)&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;&lt;strong&gt;$ 3 900&lt;/strong&gt; (₹ 3 36 k)&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;✅ Best $/TFLOP, huge community tips   ✅ Big headroom for better RAM / NVMe❌ Split VRAM – no single-GPU 70 B&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;tr&gt;\\n&lt;td align=\\"left\\"&gt;&lt;strong&gt;C – “Mixed bag”&lt;/strong&gt;&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;&lt;strong&gt;1 × W7900 + 1 × 7900 XTX&lt;/strong&gt; – $ 4 035 (₹ 3 47 k)&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;Ryzen 9 7950X • X670E&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;1 200 W upgrade (+$ 140 / ₹ 12 k)&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;&lt;strong&gt;$ 5 920&lt;/strong&gt; (₹ 5 09 k)&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;✅ 72 GB total, still one 48 GB card❌ Any ROCm pain mixing Pro + consumer drivers?&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;tr&gt;\\n&lt;td align=\\"left\\"&gt;&lt;strong&gt;D – “Three XTX”&lt;/strong&gt;&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;&lt;strong&gt;3 × 7900 XTX 24 GB&lt;/strong&gt; – $ 3 240 (₹ 2 79 k)&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;&lt;em&gt;Used&lt;/em&gt; Threadripper 3970X • TRX40&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;1 600 W Platinum (+$ 523 / ₹ 45 k)&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;&lt;strong&gt;$ 6 520&lt;/strong&gt; (₹ 5 61 k) ← still under 6 L&lt;/td&gt;\\n&lt;td align=\\"left\\"&gt;✅ 72 GB &amp;amp; tensor-parallel across 3 cards  ✅ Lots of PCIe lanes❌ Old platform, higher power/heat&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;/tbody&gt;&lt;/table&gt;\\n\\n&lt;p&gt;* Assumptions per build&lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;&lt;strong&gt;RAM&lt;/strong&gt; — 128 GB (4 × 32 GB) DDR5-5600 for AM5 builds ($ 523 / ₹ 45 k) or DDR4-3200 ECC for TRX40 ($ 372 / ₹ 32 k)&lt;/li&gt;\\n&lt;li&gt;&lt;strong&gt;Case + fans&lt;/strong&gt; — roomy SSI-EEB or XL-ATX chassis (~$ 233 / ₹ 20 k)&lt;/li&gt;\\n&lt;li&gt;Storage reused; NVMe OS drive not factored (≲$ 100).&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;p&gt;&lt;strong&gt;What I need from you&lt;/strong&gt;&lt;/p&gt;\\n\\n&lt;ol&gt;\\n&lt;li&gt;&lt;strong&gt;Is one 48 GB W7900 (Option A) really simpler&lt;/strong&gt; for &amp;gt;40 B models, or do clever tensor-parallel tricks make dual/triple XTX just as usable?&lt;/li&gt;\\n&lt;li&gt;&lt;strong&gt;ROCm stability&lt;/strong&gt; – any real-life driver differences between W-class and XTX?  Horror stories mixing them (Option C)?&lt;/li&gt;\\n&lt;li&gt;&lt;strong&gt;Multi-XTX on PCIe 4 ×8&lt;/strong&gt; (Options B &amp;amp; D): do you actually hit bottlenecks in inference?&lt;/li&gt;\\n&lt;li&gt;If you’ve built &lt;strong&gt;TRX40/3970X rigs recently&lt;/strong&gt; (Option D), are they still worth it in 2025 vs a fresh AM5 or TRX50?&lt;/li&gt;\\n&lt;li&gt;Any smarter combo that sneaks 48 GB+ of contiguous VRAM into the box while still living under &lt;strong&gt;₹ 6 00 000 / US $ 7 k&lt;/strong&gt;?&lt;/li&gt;\\n&lt;/ol&gt;\\n\\n&lt;p&gt;Benchmarks, thermals, power-draw screenshots, “don’t do it!” tales—everything helps.&lt;/p&gt;\\n\\n&lt;p&gt;&lt;strong&gt;Help me burn (or save) this budget wisely—thanks!&lt;/strong&gt;&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":true,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1lu75js","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"rgroadie2707","discussion_type":null,"num_comments":3,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lu75js/please_gutcheck_these_w7900_vs_7900_xtx_server/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lu75js/please_gutcheck_these_w7900_vs_7900_xtx_server/","subreddit_subscribers":496034,"created_utc":1751925018,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1vslkj","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MikeRoz","can_mod_post":false,"created_utc":1751926662,"send_replies":true,"parent_id":"t3_1lu75js","score":3,"author_fullname":"t2_ht2fg","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"No, you can't fit 70B parameter models in 48 GB at FP16. Unless you mean a 4-bit quant with FP16 cache or something?\\n\\nI am a huge fan of exllamav2's tensor parallelism. Hopefully exllamav3 gets it soon. Full disclosure, I have not tried exl2 TP on AMD, and exl3 doesn't have AMD support yet.\\n\\nThat said, even without tensor parallelism, splitting models across cards isn't terrible. The experience is roughly equivalent to if you had a single GPU with lots of VRAM - no performance gain, increased power consumption, but less power consumption than if each GPU was being fully utilized. But, again, I'm speaking from experience in CUDAland, I'd listen more closely to what people with AMD GPUs have to say.\\n\\nFinally, if you have any expectation that you'll want to grow beyond two or maybe three cards, moving to Threadripper, especially Threadripper Pro, is like pay-to-win. You can save a lot of money by bifrucating slots and converting m.2 to slots, but it's going to be more labor- and research-intensive, and use cases like training will suffer in performance.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1vslkj","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;No, you can&amp;#39;t fit 70B parameter models in 48 GB at FP16. Unless you mean a 4-bit quant with FP16 cache or something?&lt;/p&gt;\\n\\n&lt;p&gt;I am a huge fan of exllamav2&amp;#39;s tensor parallelism. Hopefully exllamav3 gets it soon. Full disclosure, I have not tried exl2 TP on AMD, and exl3 doesn&amp;#39;t have AMD support yet.&lt;/p&gt;\\n\\n&lt;p&gt;That said, even without tensor parallelism, splitting models across cards isn&amp;#39;t terrible. The experience is roughly equivalent to if you had a single GPU with lots of VRAM - no performance gain, increased power consumption, but less power consumption than if each GPU was being fully utilized. But, again, I&amp;#39;m speaking from experience in CUDAland, I&amp;#39;d listen more closely to what people with AMD GPUs have to say.&lt;/p&gt;\\n\\n&lt;p&gt;Finally, if you have any expectation that you&amp;#39;ll want to grow beyond two or maybe three cards, moving to Threadripper, especially Threadripper Pro, is like pay-to-win. You can save a lot of money by bifrucating slots and converting m.2 to slots, but it&amp;#39;s going to be more labor- and research-intensive, and use cases like training will suffer in performance.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lu75js/please_gutcheck_these_w7900_vs_7900_xtx_server/n1vslkj/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751926662,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lu75js","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1vr42j","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Highwaytothebeach","can_mod_post":false,"created_utc":1751926192,"send_replies":true,"parent_id":"t3_1lu75js","score":2,"author_fullname":"t2_1qychuraq9","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Why all people who build servers fail at very first step.?  Hello, it is a time of all models, including MOE models. First get 512 - 768 gb RAM , then stick everything else.on the top That is why you have thredripper right? To get more than 256 GB of ram ordinary people can with their little PC boards...\\n\\n  \\nOr you just look for a barebone like this ,[https://www.reddit.com/r/LocalLLaMA/comments/1l6awvn/gigabyte\\\\_aitop500trx50/](https://www.reddit.com/r/LocalLLaMA/comments/1l6awvn/gigabyte_aitop500trx50/) which should save you a lot of time ....","edited":1751926775,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1vr42j","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Why all people who build servers fail at very first step.?  Hello, it is a time of all models, including MOE models. First get 512 - 768 gb RAM , then stick everything else.on the top That is why you have thredripper right? To get more than 256 GB of ram ordinary people can with their little PC boards...&lt;/p&gt;\\n\\n&lt;p&gt;Or you just look for a barebone like this ,&lt;a href=\\"https://www.reddit.com/r/LocalLLaMA/comments/1l6awvn/gigabyte_aitop500trx50/\\"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1l6awvn/gigabyte_aitop500trx50/&lt;/a&gt; which should save you a lot of time ....&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lu75js/please_gutcheck_these_w7900_vs_7900_xtx_server/n1vr42j/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751926192,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lu75js","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1wzz7o","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"triynizzles1","can_mod_post":false,"created_utc":1751941160,"send_replies":true,"parent_id":"t3_1lu75js","score":1,"author_fullname":"t2_zr0g49ixt","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Would there be any other restrictions besides cost like preference for AMD, can only buy new parts, parts availability for you location?\\n\\nIf you are in the price range to go with a single GPU with lots of memory like w7900, going with a single card is best because you won’t need a special case, power supply, ventilation, etc. you are also less likely to run into unforeseen issues that can come up with multiple GPUs.\\n\\nAs others have said, you won’t be able to run fp16 for a 70 billion parameter model. You will need at least 140 GB for the weights and then another 20 GB for KV cache. However, at Q4, you can run a 70 B model on a single 48 GB GPU with 6k context window.\\n\\nIf you’re going to be fine-tuning, you might need more than 48 GB. I have a RTX 8000 and every so often it gives me a Cuda out of memory error. However, if I fine-tune using unsloth i don’t have this issue and can fine-tune much larger models\\n\\nAlso worth noting is your operating system. PyTorch on Windows does not support ROCm, It is supported on the Linux. This could lead to some issues with AMD GPUs and certain tasks on windows.\\n\\nSome other alternatives worth considering if you can buy used and at a good price:\\n\\nAMD MI210. 64gb of fast hbm2 memory. Its a passive cooler so you would need to sort out a custom cooling solution. ~$4500\\n\\n2x Amd w7800 32gb and ~$1500 each. 64 gb total for about $3-4k","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1wzz7o","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Would there be any other restrictions besides cost like preference for AMD, can only buy new parts, parts availability for you location?&lt;/p&gt;\\n\\n&lt;p&gt;If you are in the price range to go with a single GPU with lots of memory like w7900, going with a single card is best because you won’t need a special case, power supply, ventilation, etc. you are also less likely to run into unforeseen issues that can come up with multiple GPUs.&lt;/p&gt;\\n\\n&lt;p&gt;As others have said, you won’t be able to run fp16 for a 70 billion parameter model. You will need at least 140 GB for the weights and then another 20 GB for KV cache. However, at Q4, you can run a 70 B model on a single 48 GB GPU with 6k context window.&lt;/p&gt;\\n\\n&lt;p&gt;If you’re going to be fine-tuning, you might need more than 48 GB. I have a RTX 8000 and every so often it gives me a Cuda out of memory error. However, if I fine-tune using unsloth i don’t have this issue and can fine-tune much larger models&lt;/p&gt;\\n\\n&lt;p&gt;Also worth noting is your operating system. PyTorch on Windows does not support ROCm, It is supported on the Linux. This could lead to some issues with AMD GPUs and certain tasks on windows.&lt;/p&gt;\\n\\n&lt;p&gt;Some other alternatives worth considering if you can buy used and at a good price:&lt;/p&gt;\\n\\n&lt;p&gt;AMD MI210. 64gb of fast hbm2 memory. Its a passive cooler so you would need to sort out a custom cooling solution. ~$4500&lt;/p&gt;\\n\\n&lt;p&gt;2x Amd w7800 32gb and ~$1500 each. 64 gb total for about $3-4k&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lu75js/please_gutcheck_these_w7900_vs_7900_xtx_server/n1wzz7o/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751941160,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lu75js","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),r=()=>t.jsx(e,{data:l});export{r as default};
