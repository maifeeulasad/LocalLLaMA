import{j as e}from"./index-DAwUrOQb.js";import{R as t}from"./RedditPostRenderer-DESUWA8s.js";import"./index-CA_ihzEz.js";const a=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"EDIT: I made this post before remembering that LLMs store their reasoning traces in the KV cache so my idea won't work, it would be the same as using the no\\\\_think mode or a non-reasoning model. Hey, the more you learn, huh?\\n\\nI've been wondering about something with reasoning models like DeepSeek R1. We know that &lt;think&gt; tags help performance, and we know that for some models no\\\\_think prompting gets worse results. But what if there's a third option we haven't tested?\\n\\n**The experiment:** Use abliteration techniques (like uncensoring methods) to surgically remove the model's ability to generate &lt;think&gt; content, BUT make the model believe it has already completed its reasoning process. Then compare three scenarios:\\n\\n1. **Normal &lt;think&gt; mode** \\\\- Model reasons step by step\\n2. **no\\\\_think mode** \\\\- Model knows it's giving direct answers\\n3. **\\"reasoning amnesia\\" mode** \\\\- Model thinks it reasoned but actually didn't\\n\\nThis would test whether the thinking process itself improves outputs, or if just believing you've reasoned is enough. Since distilled models were trained on reasoning traces, they learned both to generate AND consume reasoning - this experiment could separate which part actually drives performance.\\n\\n**Why this matters:** If performance stays high in mode 3, it suggests reasoning might be more about internal state/expectations than actual step-by-step processing. If it drops significantly, it proves the thinking process genuinely adds value beyond pattern matching.\\n\\nHas anyone tried this specific approach? It seems like it could reveal something fundamental about how reasoning works in these models, especially for math, coding, and logic problems.","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"What if we remove reasoning models' &lt;think&gt; process but make them believe they already reasoned?","link_flair_richtext":[{"e":"text","t":"Discussion"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lm5a05","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.37,"author_flair_background_color":null,"subreddit_type":"public","ups":0,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_a1x7xr5v","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Discussion","can_mod_post":false,"score":0,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":1751071383,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1751058753,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;EDIT: I made this post before remembering that LLMs store their reasoning traces in the KV cache so my idea won&amp;#39;t work, it would be the same as using the no_think mode or a non-reasoning model. Hey, the more you learn, huh?&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;ve been wondering about something with reasoning models like DeepSeek R1. We know that &amp;lt;think&amp;gt; tags help performance, and we know that for some models no_think prompting gets worse results. But what if there&amp;#39;s a third option we haven&amp;#39;t tested?&lt;/p&gt;\\n\\n&lt;p&gt;&lt;strong&gt;The experiment:&lt;/strong&gt; Use abliteration techniques (like uncensoring methods) to surgically remove the model&amp;#39;s ability to generate &amp;lt;think&amp;gt; content, BUT make the model believe it has already completed its reasoning process. Then compare three scenarios:&lt;/p&gt;\\n\\n&lt;ol&gt;\\n&lt;li&gt;&lt;strong&gt;Normal &amp;lt;think&amp;gt; mode&lt;/strong&gt; - Model reasons step by step&lt;/li&gt;\\n&lt;li&gt;&lt;strong&gt;no_think mode&lt;/strong&gt; - Model knows it&amp;#39;s giving direct answers&lt;/li&gt;\\n&lt;li&gt;&lt;strong&gt;&amp;quot;reasoning amnesia&amp;quot; mode&lt;/strong&gt; - Model thinks it reasoned but actually didn&amp;#39;t&lt;/li&gt;\\n&lt;/ol&gt;\\n\\n&lt;p&gt;This would test whether the thinking process itself improves outputs, or if just believing you&amp;#39;ve reasoned is enough. Since distilled models were trained on reasoning traces, they learned both to generate AND consume reasoning - this experiment could separate which part actually drives performance.&lt;/p&gt;\\n\\n&lt;p&gt;&lt;strong&gt;Why this matters:&lt;/strong&gt; If performance stays high in mode 3, it suggests reasoning might be more about internal state/expectations than actual step-by-step processing. If it drops significantly, it proves the thinking process genuinely adds value beyond pattern matching.&lt;/p&gt;\\n\\n&lt;p&gt;Has anyone tried this specific approach? It seems like it could reveal something fundamental about how reasoning works in these models, especially for math, coding, and logic problems.&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":true,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"5f921ea4-c7bc-11ed-9c23-3a00622979b4","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#646d73","id":"1lm5a05","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"DistractedSentient","discussion_type":null,"num_comments":42,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lm5a05/what_if_we_remove_reasoning_models_think_process/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lm5a05/what_if_we_remove_reasoning_models_think_process/","subreddit_subscribers":492572,"created_utc":1751058753,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n06p8yr","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"llmentry","can_mod_post":false,"created_utc":1751082651,"send_replies":true,"parent_id":"t1_n05ci71","score":2,"author_fullname":"t2_1lufy6yx6z","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"&gt;\\"Reasoning tokens\\" is one of the dumbest marketing tricks that OpenAI pulled. There's nothing special about training. It is just added context that the model is creating itself rather than getting it from an external source.\\n\\nIt's slightly more than just \\"more of the same\\" context.  We don't know how the closed models are doing this, but DeepSeek achieved it as part of the instruction training from a normal model base.  For R1-zero, they simply used this as their instruction template for training, [as they describe](https://arxiv.org/abs/2501.12948):\\n\\n&gt;A conversation between User and Assistant. The user asks a question, and the Assistant solves it. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within &lt;think&gt; &lt;/think&gt; and &lt;answer&gt; &lt;/answer&gt; tags, respectively, i.e., &lt;think&gt; reasoning process here &lt;/think&gt; &lt;answer&gt; answer here &lt;/answer&gt;. User: prompt. Assistant:\\n\\nThey then used RL, rewarding for both a correct response, and for the use of the &lt;/think&gt; tags.\\n\\nThis essentially rewarded a model that could work through multiple avenues to address a problem without being constrained to immediately give a single \\"best-guess\\" answer.  Very neatly, without any special guidance as to how to solve this process, this naturally led to a self-questioning internal monologue within the &lt;think&gt; tags.  \\n\\nThere were some issues with this process, though, so for R1 itself (the model most people are now using), they first fine-tuned V3 on pre-generated reasoning CoT (this is why OpenAI accused them of stealing their process), and then also included some additional CoT filters for an aesthetically-pleasing output.  (This included removing thing such as mixed language reasoning, interestingly -- DeepSeek actually say that this led to slightly worse reasoning outcomes, but they did it so that the reasoning output \\"aligns with human preferences, making it more readable\\".)\\n\\nSo, it's *useful* context that is only generated because of the freedom allowed through the reasoning process.  But, yes, there's nothing inherently special about it.  The model doesn't \\"know\\" that it's \\"thought\\"!\\n\\nIt's funny how the marketing narratives have led people to think that \\"reasoning\\" comes from something like a different model architecture.\\n\\n(Also, I keep wondering how much better DeepSeek would have done with better instruct prompting! \\"The assistant first thinks about the reasoning process **in the mind**\\" ... what on earth does \\"in the mind\\" mean to a model??)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n06p8yr","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;&amp;quot;Reasoning tokens&amp;quot; is one of the dumbest marketing tricks that OpenAI pulled. There&amp;#39;s nothing special about training. It is just added context that the model is creating itself rather than getting it from an external source.&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;It&amp;#39;s slightly more than just &amp;quot;more of the same&amp;quot; context.  We don&amp;#39;t know how the closed models are doing this, but DeepSeek achieved it as part of the instruction training from a normal model base.  For R1-zero, they simply used this as their instruction template for training, &lt;a href=\\"https://arxiv.org/abs/2501.12948\\"&gt;as they describe&lt;/a&gt;:&lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;A conversation between User and Assistant. The user asks a question, and the Assistant solves it. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. The reasoning process and answer are enclosed within &amp;lt;think&amp;gt; &amp;lt;/think&amp;gt; and &amp;lt;answer&amp;gt; &amp;lt;/answer&amp;gt; tags, respectively, i.e., &amp;lt;think&amp;gt; reasoning process here &amp;lt;/think&amp;gt; &amp;lt;answer&amp;gt; answer here &amp;lt;/answer&amp;gt;. User: prompt. Assistant:&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;They then used RL, rewarding for both a correct response, and for the use of the &amp;lt;/think&amp;gt; tags.&lt;/p&gt;\\n\\n&lt;p&gt;This essentially rewarded a model that could work through multiple avenues to address a problem without being constrained to immediately give a single &amp;quot;best-guess&amp;quot; answer.  Very neatly, without any special guidance as to how to solve this process, this naturally led to a self-questioning internal monologue within the &amp;lt;think&amp;gt; tags.  &lt;/p&gt;\\n\\n&lt;p&gt;There were some issues with this process, though, so for R1 itself (the model most people are now using), they first fine-tuned V3 on pre-generated reasoning CoT (this is why OpenAI accused them of stealing their process), and then also included some additional CoT filters for an aesthetically-pleasing output.  (This included removing thing such as mixed language reasoning, interestingly -- DeepSeek actually say that this led to slightly worse reasoning outcomes, but they did it so that the reasoning output &amp;quot;aligns with human preferences, making it more readable&amp;quot;.)&lt;/p&gt;\\n\\n&lt;p&gt;So, it&amp;#39;s &lt;em&gt;useful&lt;/em&gt; context that is only generated because of the freedom allowed through the reasoning process.  But, yes, there&amp;#39;s nothing inherently special about it.  The model doesn&amp;#39;t &amp;quot;know&amp;quot; that it&amp;#39;s &amp;quot;thought&amp;quot;!&lt;/p&gt;\\n\\n&lt;p&gt;It&amp;#39;s funny how the marketing narratives have led people to think that &amp;quot;reasoning&amp;quot; comes from something like a different model architecture.&lt;/p&gt;\\n\\n&lt;p&gt;(Also, I keep wondering how much better DeepSeek would have done with better instruct prompting! &amp;quot;The assistant first thinks about the reasoning process &lt;strong&gt;in the mind&lt;/strong&gt;&amp;quot; ... what on earth does &amp;quot;in the mind&amp;quot; mean to a model??)&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lm5a05","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lm5a05/what_if_we_remove_reasoning_models_think_process/n06p8yr/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751082651,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n05ci71","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"binge-worthy-gamer","can_mod_post":false,"created_utc":1751064212,"send_replies":true,"parent_id":"t3_1lm5a05","score":12,"author_fullname":"t2_1r68ddklgz","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"\\"Reasoning tokens\\" is one of the dumbest marketing tricks that OpenAI pulled. There's nothing special about training. It is just added context that the model is creating itself rather than getting it from an external source. If you remove all of it then the context is no longer present and any benefits of \\"reasoning\\" will not be had. \\"Belief\\" that a thought has occurred wouldn't do shit.\\n\\n\\nYou could create the context yourself and add that in the template of the reasoning tokens though. But at that point there's no point of expressing it as reasoning.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n05ci71","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;&amp;quot;Reasoning tokens&amp;quot; is one of the dumbest marketing tricks that OpenAI pulled. There&amp;#39;s nothing special about training. It is just added context that the model is creating itself rather than getting it from an external source. If you remove all of it then the context is no longer present and any benefits of &amp;quot;reasoning&amp;quot; will not be had. &amp;quot;Belief&amp;quot; that a thought has occurred wouldn&amp;#39;t do shit.&lt;/p&gt;\\n\\n&lt;p&gt;You could create the context yourself and add that in the template of the reasoning tokens though. But at that point there&amp;#39;s no point of expressing it as reasoning.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lm5a05/what_if_we_remove_reasoning_models_think_process/n05ci71/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751064212,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lm5a05","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":12}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0dbs1f","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DistractedSentient","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0d8i41","score":1,"author_fullname":"t2_a1x7xr5v","approved_by":null,"mod_note":null,"all_awardings":[],"body":"Got it.","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n0dbs1f","is_submitter":true,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Got it.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lm5a05","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lm5a05/what_if_we_remove_reasoning_models_think_process/n0dbs1f/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751179855,"author_flair_text":null,"treatment_tags":[],"created_utc":1751179855,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0d8i41","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AppearanceHeavy6724","can_mod_post":false,"send_replies":true,"parent_id":"t1_n088luz","score":2,"author_fullname":"t2_uz37qfx5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"yes.","edited":false,"author_flair_css_class":null,"name":"t1_n0d8i41","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;yes.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lm5a05","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lm5a05/what_if_we_remove_reasoning_models_think_process/n0d8i41/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751178007,"author_flair_text":null,"collapsed":false,"created_utc":1751178007,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n088luz","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DistractedSentient","can_mod_post":false,"send_replies":true,"parent_id":"t1_n06ad88","score":0,"author_fullname":"t2_a1x7xr5v","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Right, that makes sense too. So when the user asks another question, the model will start reasoning again which will be filled into the KV cache and clears the reasoning trace after the model completes its output including the answer. This repeats over and over again, right?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n088luz","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Right, that makes sense too. So when the user asks another question, the model will start reasoning again which will be filled into the KV cache and clears the reasoning trace after the model completes its output including the answer. This repeats over and over again, right?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lm5a05","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lm5a05/what_if_we_remove_reasoning_models_think_process/n088luz/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751112260,"author_flair_text":null,"treatment_tags":[],"created_utc":1751112260,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}},"user_reports":[],"saved":false,"id":"n06ad88","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AppearanceHeavy6724","can_mod_post":false,"send_replies":true,"parent_id":"t1_n05se5l","score":2,"author_fullname":"t2_uz37qfx5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Llama.cpp removes reasoning traces from kv cache once final answer is produced ","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n06ad88","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Llama.cpp removes reasoning traces from kv cache once final answer is produced &lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lm5a05","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lm5a05/what_if_we_remove_reasoning_models_think_process/n06ad88/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751076487,"author_flair_text":null,"treatment_tags":[],"created_utc":1751076487,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n05se5l","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DistractedSentient","can_mod_post":false,"created_utc":1751069751,"send_replies":true,"parent_id":"t1_n05qof2","score":1,"author_fullname":"t2_a1x7xr5v","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Makes sense. But I'm not directly letting the model pretend it already reasoned step by step, yk, it's more like it would just assume it did the reasoning process and proceed to give an answer. But of course this was before I remember caching exists for the GPT models and that the reasoning traces also get stored in the KV cache. I should've done a bit more research before posting this, my bad. I talked to Claude Sonnet 4 just to see what it says about it and it just told me to post it here, ironically.  \\n  \\nMy original idea was that maybe the answers that these models give out don't need a reasoning trace to back them up. Usually R1 for example keeps trying to \\"make sure\\" the answer is correct even though it had already come to the correct conclusion and loops for a while. So I thought what if we tricked the model into assuming it finished its reasoning and straight up gave the answer.\\n\\nBut of course, KV cache lol.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n05se5l","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Makes sense. But I&amp;#39;m not directly letting the model pretend it already reasoned step by step, yk, it&amp;#39;s more like it would just assume it did the reasoning process and proceed to give an answer. But of course this was before I remember caching exists for the GPT models and that the reasoning traces also get stored in the KV cache. I should&amp;#39;ve done a bit more research before posting this, my bad. I talked to Claude Sonnet 4 just to see what it says about it and it just told me to post it here, ironically.  &lt;/p&gt;\\n\\n&lt;p&gt;My original idea was that maybe the answers that these models give out don&amp;#39;t need a reasoning trace to back them up. Usually R1 for example keeps trying to &amp;quot;make sure&amp;quot; the answer is correct even though it had already come to the correct conclusion and loops for a while. So I thought what if we tricked the model into assuming it finished its reasoning and straight up gave the answer.&lt;/p&gt;\\n\\n&lt;p&gt;But of course, KV cache lol.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lm5a05","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lm5a05/what_if_we_remove_reasoning_models_think_process/n05se5l/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751069751,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n05qof2","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MDT-49","can_mod_post":false,"created_utc":1751069126,"send_replies":true,"parent_id":"t3_1lm5a05","score":3,"author_fullname":"t2_h8yrica5","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I don't see how this could possibly turn out well. The most likely result, I think, is more hallucinations (LLM incorrectly mentioning that they've reasoned) and a more \\"confident tone,\\" even when wrong.\\n\\nIn the best-case scenario, this could improve the response because their might be some connection between phrases like \\"I've thought about it\\" and quality in the LLM. However, I think this is a bit of a stretch.\\n\\nIt's a bit like telling a student on a math exam not to solve an equation step by step (chain of thought/reasoning), but to just pretend they already did and answer whatever comes to mind.","edited":1751069401,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n05qof2","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I don&amp;#39;t see how this could possibly turn out well. The most likely result, I think, is more hallucinations (LLM incorrectly mentioning that they&amp;#39;ve reasoned) and a more &amp;quot;confident tone,&amp;quot; even when wrong.&lt;/p&gt;\\n\\n&lt;p&gt;In the best-case scenario, this could improve the response because their might be some connection between phrases like &amp;quot;I&amp;#39;ve thought about it&amp;quot; and quality in the LLM. However, I think this is a bit of a stretch.&lt;/p&gt;\\n\\n&lt;p&gt;It&amp;#39;s a bit like telling a student on a math exam not to solve an equation step by step (chain of thought/reasoning), but to just pretend they already did and answer whatever comes to mind.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lm5a05/what_if_we_remove_reasoning_models_think_process/n05qof2/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751069126,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lm5a05","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n05wi91","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DistractedSentient","can_mod_post":false,"created_utc":1751071270,"send_replies":true,"parent_id":"t1_n05vfiz","score":1,"author_fullname":"t2_a1x7xr5v","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"You're right. I forgot that LLMs store their reasoning traces in the KV cache before creating this post, so no\\\\_think acts the same as non-reasoning. I don't know if it's the sleeping pills I'm taking but lately it's been nothing but shorthand memory loss lol. It's about the same as the models not being able to see their reasoning traces in the context for a given chat right?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n05wi91","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You&amp;#39;re right. I forgot that LLMs store their reasoning traces in the KV cache before creating this post, so no_think acts the same as non-reasoning. I don&amp;#39;t know if it&amp;#39;s the sleeping pills I&amp;#39;m taking but lately it&amp;#39;s been nothing but shorthand memory loss lol. It&amp;#39;s about the same as the models not being able to see their reasoning traces in the context for a given chat right?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lm5a05","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lm5a05/what_if_we_remove_reasoning_models_think_process/n05wi91/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751071270,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n05vfiz","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Interesting8547","can_mod_post":false,"created_utc":1751070871,"send_replies":true,"parent_id":"t3_1lm5a05","score":2,"author_fullname":"t2_d82aa036","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I think some of the Qwen models can be used like that, with the \\"think\\" part being turned off. As far as I know they become dumber.  They can't be really tricked, I mean they can be, but the answer would not be better than a reasoning model.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n05vfiz","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I think some of the Qwen models can be used like that, with the &amp;quot;think&amp;quot; part being turned off. As far as I know they become dumber.  They can&amp;#39;t be really tricked, I mean they can be, but the answer would not be better than a reasoning model.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lm5a05/what_if_we_remove_reasoning_models_think_process/n05vfiz/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751070871,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lm5a05","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ff54b802-c910-11ed-be9d-ea867d8afa86","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ff54b802-c910-11ed-be9d-ea867d8afa86","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ff54b802-c910-11ed-be9d-ea867d8afa86","distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n08sq7q","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Original_Finding2212","can_mod_post":false,"send_replies":true,"parent_id":"t1_n08g8kt","score":2,"author_fullname":"t2_78ipric6","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Example of where you can take thinking \\n\\n&gt; 事实1: 爱因斯坦的狭义相对论提出时空统一与质能等价；事实2: 质能等价解释了太阳的核聚变与放射性裂变；事实3: 斯比拉德提出链式反应并促成曼哈顿计划；事实4: 帖文强调科学发现既能造福也能毁灭；事实5: 作者呼吁选择合作与超越生存斗争 --&gt; 结论1: 帖文通过原子弹话题阐释相对论的深层含义 &amp; 结论2: 文章点明道德责任与人类选择的细腻张力","edited":false,"author_flair_css_class":null,"name":"t1_n08sq7q","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"Llama 33B"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Example of where you can take thinking &lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;事实1: 爱因斯坦的狭义相对论提出时空统一与质能等价；事实2: 质能等价解释了太阳的核聚变与放射性裂变；事实3: 斯比拉德提出链式反应并促成曼哈顿计划；事实4: 帖文强调科学发现既能造福也能毁灭；事实5: 作者呼吁选择合作与超越生存斗争 --&amp;gt; 结论1: 帖文通过原子弹话题阐释相对论的深层含义 &amp;amp; 结论2: 文章点明道德责任与人类选择的细腻张力&lt;/p&gt;\\n&lt;/blockquote&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lm5a05","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":"light","treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lm5a05/what_if_we_remove_reasoning_models_think_process/n08sq7q/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751120045,"author_flair_text":"Llama 33B","collapsed":false,"created_utc":1751120045,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n08g8kt","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DistractedSentient","can_mod_post":false,"send_replies":true,"parent_id":"t1_n08bbbo","score":2,"author_fullname":"t2_a1x7xr5v","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Got it, will try","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n08g8kt","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Got it, will try&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lm5a05","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lm5a05/what_if_we_remove_reasoning_models_think_process/n08g8kt/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751115453,"author_flair_text":null,"treatment_tags":[],"created_utc":1751115453,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n08bbbo","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Original_Finding2212","can_mod_post":false,"send_replies":true,"parent_id":"t1_n088rkr","score":2,"author_fullname":"t2_78ipric6","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Let it do thinking, but add instructions and get “guided thinking”","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n08bbbo","is_submitter":false,"collapsed":false,"author_flair_richtext":[{"e":"text","t":"Llama 33B"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Let it do thinking, but add instructions and get “guided thinking”&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lm5a05","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lm5a05/what_if_we_remove_reasoning_models_think_process/n08bbbo/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751113444,"author_flair_text":"Llama 33B","treatment_tags":[],"created_utc":1751113444,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n088rkr","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DistractedSentient","can_mod_post":false,"created_utc":1751112332,"send_replies":true,"parent_id":"t1_n06cywh","score":1,"author_fullname":"t2_a1x7xr5v","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Interesting, I also feel the less a model thinks the faster it will arrive on an answer. R1 for example just keeps on looping \\"just to make sure\\" even though it already arrived on the right track....","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n088rkr","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Interesting, I also feel the less a model thinks the faster it will arrive on an answer. R1 for example just keeps on looping &amp;quot;just to make sure&amp;quot; even though it already arrived on the right track....&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lm5a05","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lm5a05/what_if_we_remove_reasoning_models_think_process/n088rkr/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751112332,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n06cywh","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Original_Finding2212","can_mod_post":false,"created_utc":1751077493,"send_replies":true,"parent_id":"t3_1lm5a05","score":2,"author_fullname":"t2_78ipric6","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I have added “short-hand” reasoning with “logic jumps” that reduced generation but improved results.\\n\\nThis was for Amazon’s Nova Pro model.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n06cywh","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"Llama 33B"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I have added “short-hand” reasoning with “logic jumps” that reduced generation but improved results.&lt;/p&gt;\\n\\n&lt;p&gt;This was for Amazon’s Nova Pro model.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lm5a05/what_if_we_remove_reasoning_models_think_process/n06cywh/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751077493,"author_flair_text":"Llama 33B","treatment_tags":[],"link_id":"t3_1lm5a05","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"7d1f04e6-4920-11ef-b2e1-2e580594e1a1","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n08904h","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DistractedSentient","can_mod_post":false,"created_utc":1751112437,"send_replies":true,"parent_id":"t1_n06jcz0","score":0,"author_fullname":"t2_a1x7xr5v","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Thanks for your thoughts, will definitely check out the paper! I agree that the longer a model reasons the higher the probability that the answer will be incorrect. We need reasoning models to be as efficient as possible for sure...","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n08904h","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thanks for your thoughts, will definitely check out the paper! I agree that the longer a model reasons the higher the probability that the answer will be incorrect. We need reasoning models to be as efficient as possible for sure...&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lm5a05","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lm5a05/what_if_we_remove_reasoning_models_think_process/n08904h/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751112437,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}},"user_reports":[],"saved":false,"id":"n06jcz0","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"asankhs","can_mod_post":false,"created_utc":1751080088,"send_replies":true,"parent_id":"t3_1lm5a05","score":2,"author_fullname":"t2_e0bph","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I think there is a lot of work in this area now, the tokens inside the &lt;think&gt;&lt;/think&gt; tags are just regular tokens we can parse them, intervenue, stop them in the middle, extend them by adding filler words like \\"wait\\" etc. In our recent paper we showed how you can reduce the \\"reasoning\\" tokens by half while maintaining accuracy. I am not sure if removing it altogether would work except for the simplest of the queries.   \\n  \\nPaper: AutoThink: efficient inference for reasoning LLMs - [https://papers.ssrn.com/sol3/papers.cfm?abstract\\\\_id=5253327](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5253327)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n06jcz0","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"Llama 3.1"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I think there is a lot of work in this area now, the tokens inside the &amp;lt;think&amp;gt;&amp;lt;/think&amp;gt; tags are just regular tokens we can parse them, intervenue, stop them in the middle, extend them by adding filler words like &amp;quot;wait&amp;quot; etc. In our recent paper we showed how you can reduce the &amp;quot;reasoning&amp;quot; tokens by half while maintaining accuracy. I am not sure if removing it altogether would work except for the simplest of the queries.   &lt;/p&gt;\\n\\n&lt;p&gt;Paper: AutoThink: efficient inference for reasoning LLMs - &lt;a href=\\"https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5253327\\"&gt;https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5253327&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lm5a05/what_if_we_remove_reasoning_models_think_process/n06jcz0/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751080088,"author_flair_text":"Llama 3.1","treatment_tags":[],"link_id":"t3_1lm5a05","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#93b1ba","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0896ph","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DistractedSentient","can_mod_post":false,"created_utc":1751112518,"send_replies":true,"parent_id":"t1_n0730wj","score":1,"author_fullname":"t2_a1x7xr5v","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Oh, that seems like exactly what I'm looking for. I'll try to find it, thanks!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0896ph","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Oh, that seems like exactly what I&amp;#39;m looking for. I&amp;#39;ll try to find it, thanks!&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lm5a05","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lm5a05/what_if_we_remove_reasoning_models_think_process/n0896ph/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751112518,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0730wj","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"hapliniste","can_mod_post":false,"created_utc":1751089375,"send_replies":true,"parent_id":"t3_1lm5a05","score":2,"author_fullname":"t2_fc7rd","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"There was a paper that tried this and it works if I remember. Start the response by \\"now that I reasons,...\\"\\n\\nNo one in this comment section has any idea what they're talking about","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0730wj","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;There was a paper that tried this and it works if I remember. Start the response by &amp;quot;now that I reasons,...&amp;quot;&lt;/p&gt;\\n\\n&lt;p&gt;No one in this comment section has any idea what they&amp;#39;re talking about&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lm5a05/what_if_we_remove_reasoning_models_think_process/n0730wj/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751089375,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lm5a05","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n05wng0","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DistractedSentient","can_mod_post":false,"created_utc":1751071323,"send_replies":true,"parent_id":"t1_n05w4uc","score":3,"author_fullname":"t2_a1x7xr5v","approved_by":null,"mod_note":null,"all_awardings":[],"body":"Thanks man! Appreciate it haha.","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n05wng0","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thanks man! Appreciate it haha.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lm5a05","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lm5a05/what_if_we_remove_reasoning_models_think_process/n05wng0/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751071323,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":7,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n05w4uc","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"__JockY__","can_mod_post":false,"send_replies":true,"parent_id":"t1_n05vg9j","score":5,"author_fullname":"t2_qf8h7ka8","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"For the love of all that's holy, leave it up. It's great to see folks just riding with the \\"huh, i was wrong\\" train instead of getting all butt-hurt.\\n\\nYour attitude is to your credit, let the masses see you are not immune to new data and the power of reasonable persuasion.","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n05w4uc","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;For the love of all that&amp;#39;s holy, leave it up. It&amp;#39;s great to see folks just riding with the &amp;quot;huh, i was wrong&amp;quot; train instead of getting all butt-hurt.&lt;/p&gt;\\n\\n&lt;p&gt;Your attitude is to your credit, let the masses see you are not immune to new data and the power of reasonable persuasion.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lm5a05","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lm5a05/what_if_we_remove_reasoning_models_think_process/n05w4uc/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751071132,"author_flair_text":null,"treatment_tags":[],"created_utc":1751071132,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":6,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}}],"before":null}},"user_reports":[],"saved":false,"id":"n05vg9j","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DistractedSentient","can_mod_post":false,"send_replies":true,"parent_id":"t1_n05uqv0","score":3,"author_fullname":"t2_a1x7xr5v","approved_by":null,"mod_note":null,"all_awardings":[],"body":"Exactly, I forgot that LLMs cache before making this post. I should've done a bit more... thinking, ironically. So this must be why I got the downvotes. LLMs store their reasoning traces in the KV cache as well so this cannot work as you said. Should I just delete my post or keep it running now that I realized my mistake?","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n05vg9j","is_submitter":true,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Exactly, I forgot that LLMs cache before making this post. I should&amp;#39;ve done a bit more... thinking, ironically. So this must be why I got the downvotes. LLMs store their reasoning traces in the KV cache as well so this cannot work as you said. Should I just delete my post or keep it running now that I realized my mistake?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lm5a05","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lm5a05/what_if_we_remove_reasoning_models_think_process/n05vg9j/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751070879,"author_flair_text":null,"treatment_tags":[],"created_utc":1751070879,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n05uqv0","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"__JockY__","can_mod_post":false,"send_replies":true,"parent_id":"t1_n05r0qx","score":3,"author_fullname":"t2_qf8h7ka8","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"By what mechanism could it possibly work? In thinking mode the LLM is generating context that is converted to vectors stored in the KV cache. If that context is missing, so are the vectors; if the vectors are missing, what is the LLM going to process? Only what you provide in the prompt.\\n\\nIt can't work.","edited":false,"author_flair_css_class":null,"name":"t1_n05uqv0","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;By what mechanism could it possibly work? In thinking mode the LLM is generating context that is converted to vectors stored in the KV cache. If that context is missing, so are the vectors; if the vectors are missing, what is the LLM going to process? Only what you provide in the prompt.&lt;/p&gt;\\n\\n&lt;p&gt;It can&amp;#39;t work.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lm5a05","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lm5a05/what_if_we_remove_reasoning_models_think_process/n05uqv0/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751070617,"author_flair_text":null,"collapsed":false,"created_utc":1751070617,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n05r0qx","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DistractedSentient","can_mod_post":false,"send_replies":true,"parent_id":"t1_n05q0yd","score":1,"author_fullname":"t2_a1x7xr5v","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Ironically, Claude Sonnet 4 told me to post it here lol. But yeah, I could talk to the SOTA models and see what they say, but since there aren't any published articles about this or people talking about it, I don't know about the factuality, you know? That's the main reason I posted it here, hoping to see if people can prove me wrong and let me know why it will/won't work...","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n05r0qx","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Ironically, Claude Sonnet 4 told me to post it here lol. But yeah, I could talk to the SOTA models and see what they say, but since there aren&amp;#39;t any published articles about this or people talking about it, I don&amp;#39;t know about the factuality, you know? That&amp;#39;s the main reason I posted it here, hoping to see if people can prove me wrong and let me know why it will/won&amp;#39;t work...&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lm5a05","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lm5a05/what_if_we_remove_reasoning_models_think_process/n05r0qx/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751069251,"author_flair_text":null,"treatment_tags":[],"created_utc":1751069251,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n06lhxw","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"llmentry","can_mod_post":false,"send_replies":true,"parent_id":"t1_n05q0yd","score":0,"author_fullname":"t2_1lufy6yx6z","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"To be fair, most reasoning work happened after the knowledge cut-off dates of even frontier models.  And there would potentially be guardrails preventing discussion of \\"trade secrets\\" (even facile ones) as well.\\n\\nI did a quick try with GPT-4.1 and Gemini 2.5, and both gave rubbish, confused and misinformed responses.  Either they don't know, or can't say, or both.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n06lhxw","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;To be fair, most reasoning work happened after the knowledge cut-off dates of even frontier models.  And there would potentially be guardrails preventing discussion of &amp;quot;trade secrets&amp;quot; (even facile ones) as well.&lt;/p&gt;\\n\\n&lt;p&gt;I did a quick try with GPT-4.1 and Gemini 2.5, and both gave rubbish, confused and misinformed responses.  Either they don&amp;#39;t know, or can&amp;#39;t say, or both.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lm5a05","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lm5a05/what_if_we_remove_reasoning_models_think_process/n06lhxw/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751080997,"author_flair_text":null,"treatment_tags":[],"created_utc":1751080997,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}},"user_reports":[],"saved":false,"id":"n05q0yd","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"__JockY__","can_mod_post":false,"send_replies":true,"parent_id":"t1_n04z0e9","score":5,"author_fullname":"t2_qf8h7ka8","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"At the risk of stating the bleedin’ obvious, this is a conversation you should be having with a frontier model, which will explain things conversationally and can answer all your questions.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n05q0yd","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;At the risk of stating the bleedin’ obvious, this is a conversation you should be having with a frontier model, which will explain things conversationally and can answer all your questions.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lm5a05","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lm5a05/what_if_we_remove_reasoning_models_think_process/n05q0yd/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751068888,"author_flair_text":null,"treatment_tags":[],"created_utc":1751068888,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n052r6n","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"eloquentemu","can_mod_post":false,"send_replies":true,"parent_id":"t1_n04z0e9","score":3,"author_fullname":"t2_lpdsy","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"An LLM basically just processes a block of text and produces the next token (word, character, etc).  That is, every new token is basically just reprocessing the whole document again, but the KV cache caches the redundant work.  All the chat stuff you see is just layered on that: a couple of keywords (key tokens) that the model and text view agree to mean that the user or model said something.\\n\\nThat is to say, the input to an LLM on every cycle is just a block of text with some \`&lt;user&gt;\` \`&lt;model&gt;\` \`&lt;thinking&gt;\` type delimiters.  You can totally go into that blob and instead of doing \\"&lt;user&gt;hi&lt;/user&gt;\\" and having the model append \`&lt;user&gt;hi&lt;/user&gt;&lt;thinking&gt;Okay,\` you can just feed it \`&lt;user&gt;hi&lt;/user&gt;&lt;thinking&gt;&lt;/thinking&gt;&lt;model&gt;\` or \`&lt;user&gt;hi&lt;/user&gt;&lt;model&gt;\` and have it start generation from there.  The model will have no idea that the empty or missing \`&lt;thinking&gt;\` wasn't its doing.\\n\\nThe problem with this, that you may notice now, is there there is no internal state and nothing to really fool.  Thinking models are just trained that conversations look like \`&lt;user&gt;hi&lt;/user&gt;&lt;thinking&gt;&lt;/thinking&gt;&lt;model&gt;&lt;/model&gt;&lt;user&gt;...\` so if you give it \`&lt;/user&gt;\` it'll generate \`&lt;/thinking&gt;\` because that's what the training data showed it.  So if you give it \`&lt;/thinking&gt;\` it'll generate \`&lt;model&gt;\`.  It won't really be tricked or anything, but it will get a bit messed up because it was probably trained to 'look' at the text in \`&lt;thinking&gt;&lt;/thinking&gt;\` when generating text in \`&lt;model&gt;&lt;/model&gt;\`.  So of like how you might rely on training wheels when riding a bike if you've only ever used them.  Suddenly take them away and you'll probably crash.  It's not that it matters whether you believe they are there or not, it's that your brain was trained with them there and so it learned to use them.\\n\\nIf you want to mess around learn more about this, I recommend https://github.com/lmg-anon/mikupad","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n052r6n","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;An LLM basically just processes a block of text and produces the next token (word, character, etc).  That is, every new token is basically just reprocessing the whole document again, but the KV cache caches the redundant work.  All the chat stuff you see is just layered on that: a couple of keywords (key tokens) that the model and text view agree to mean that the user or model said something.&lt;/p&gt;\\n\\n&lt;p&gt;That is to say, the input to an LLM on every cycle is just a block of text with some &lt;code&gt;&amp;lt;user&amp;gt;&lt;/code&gt; &lt;code&gt;&amp;lt;model&amp;gt;&lt;/code&gt; &lt;code&gt;&amp;lt;thinking&amp;gt;&lt;/code&gt; type delimiters.  You can totally go into that blob and instead of doing &amp;quot;&amp;lt;user&amp;gt;hi&amp;lt;/user&amp;gt;&amp;quot; and having the model append &lt;code&gt;&amp;lt;user&amp;gt;hi&amp;lt;/user&amp;gt;&amp;lt;thinking&amp;gt;Okay,&lt;/code&gt; you can just feed it &lt;code&gt;&amp;lt;user&amp;gt;hi&amp;lt;/user&amp;gt;&amp;lt;thinking&amp;gt;&amp;lt;/thinking&amp;gt;&amp;lt;model&amp;gt;&lt;/code&gt; or &lt;code&gt;&amp;lt;user&amp;gt;hi&amp;lt;/user&amp;gt;&amp;lt;model&amp;gt;&lt;/code&gt; and have it start generation from there.  The model will have no idea that the empty or missing &lt;code&gt;&amp;lt;thinking&amp;gt;&lt;/code&gt; wasn&amp;#39;t its doing.&lt;/p&gt;\\n\\n&lt;p&gt;The problem with this, that you may notice now, is there there is no internal state and nothing to really fool.  Thinking models are just trained that conversations look like &lt;code&gt;&amp;lt;user&amp;gt;hi&amp;lt;/user&amp;gt;&amp;lt;thinking&amp;gt;&amp;lt;/thinking&amp;gt;&amp;lt;model&amp;gt;&amp;lt;/model&amp;gt;&amp;lt;user&amp;gt;...&lt;/code&gt; so if you give it &lt;code&gt;&amp;lt;/user&amp;gt;&lt;/code&gt; it&amp;#39;ll generate &lt;code&gt;&amp;lt;/thinking&amp;gt;&lt;/code&gt; because that&amp;#39;s what the training data showed it.  So if you give it &lt;code&gt;&amp;lt;/thinking&amp;gt;&lt;/code&gt; it&amp;#39;ll generate &lt;code&gt;&amp;lt;model&amp;gt;&lt;/code&gt;.  It won&amp;#39;t really be tricked or anything, but it will get a bit messed up because it was probably trained to &amp;#39;look&amp;#39; at the text in &lt;code&gt;&amp;lt;thinking&amp;gt;&amp;lt;/thinking&amp;gt;&lt;/code&gt; when generating text in &lt;code&gt;&amp;lt;model&amp;gt;&amp;lt;/model&amp;gt;&lt;/code&gt;.  So of like how you might rely on training wheels when riding a bike if you&amp;#39;ve only ever used them.  Suddenly take them away and you&amp;#39;ll probably crash.  It&amp;#39;s not that it matters whether you believe they are there or not, it&amp;#39;s that your brain was trained with them there and so it learned to use them.&lt;/p&gt;\\n\\n&lt;p&gt;If you want to mess around learn more about this, I recommend &lt;a href=\\"https://github.com/lmg-anon/mikupad\\"&gt;https://github.com/lmg-anon/mikupad&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lm5a05","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lm5a05/what_if_we_remove_reasoning_models_think_process/n052r6n/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751060925,"author_flair_text":null,"treatment_tags":[],"created_utc":1751060925,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n054whl","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DistractedSentient","can_mod_post":false,"send_replies":true,"parent_id":"t1_n053jx6","score":1,"author_fullname":"t2_a1x7xr5v","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"This makes a lot of sense, thanks for the detailed comment!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n054whl","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;This makes a lot of sense, thanks for the detailed comment!&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lm5a05","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lm5a05/what_if_we_remove_reasoning_models_think_process/n054whl/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751061632,"author_flair_text":null,"treatment_tags":[],"created_utc":1751061632,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n05qown","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DistractedSentient","can_mod_post":false,"send_replies":true,"parent_id":"t1_n05qavr","score":1,"author_fullname":"t2_a1x7xr5v","approved_by":null,"mod_note":null,"all_awardings":[],"body":"I know haha, just wanted to vent a little...","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n05qown","is_submitter":true,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I know haha, just wanted to vent a little...&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lm5a05","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lm5a05/what_if_we_remove_reasoning_models_think_process/n05qown/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751069131,"author_flair_text":null,"treatment_tags":[],"created_utc":1751069131,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n05qavr","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DeProgrammer99","can_mod_post":false,"send_replies":true,"parent_id":"t1_n05q4ll","score":2,"author_fullname":"t2_w4j8t","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yeah, I saw that. Weird. Wasn't me. 😅","edited":false,"author_flair_css_class":null,"name":"t1_n05qavr","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yeah, I saw that. Weird. Wasn&amp;#39;t me. 😅&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lm5a05","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lm5a05/what_if_we_remove_reasoning_models_think_process/n05qavr/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751068988,"author_flair_text":null,"collapsed":false,"created_utc":1751068988,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n05q4ll","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DistractedSentient","can_mod_post":false,"send_replies":true,"parent_id":"t1_n053jx6","score":0,"author_fullname":"t2_a1x7xr5v","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"This is what I replied to you: \\"This makes a lot of sense, thanks for the detailed comment!\\"\\n\\nAnd I got downvoted. Lol.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n05q4ll","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;This is what I replied to you: &amp;quot;This makes a lot of sense, thanks for the detailed comment!&amp;quot;&lt;/p&gt;\\n\\n&lt;p&gt;And I got downvoted. Lol.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lm5a05","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lm5a05/what_if_we_remove_reasoning_models_think_process/n05q4ll/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751068925,"author_flair_text":null,"treatment_tags":[],"created_utc":1751068925,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}},"user_reports":[],"saved":false,"id":"n053jx6","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DeProgrammer99","can_mod_post":false,"send_replies":true,"parent_id":"t1_n04z0e9","score":2,"author_fullname":"t2_w4j8t","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Think of an LLM like a reader with severe memory loss. Think of the KV cache like a picture. The LLM looks at the picture to decide what to draw next. They scribble a little in the corner aaand then immediately forget everything that ever happened. From their perspective, whether the whole picture was drawn by them or by someone else, they have no idea. They can't tell. It has no impact.\\n\\nThat's the laymen's version.\\n\\nWhen we run inference, we actually add tokens like this to start off:\\n\\n&lt;|turn|&gt;user  \\nHey, Qwen, do a little dance!  \\n&lt;|turn|&gt;assistant\\n\\nThen we tokenize that text and calculate the key and value for each token by running them through the LLM (possibly thousands of tokens at the same time).\\n\\nOnce the whole KV cache is ready, we go through all the LLM's calculations to infer the next token and add that to the KV cache as well.\\n\\nOnce the whole KV cache is ready, we go through all the LLM's calculations to infer the next token and add that to the KV cache as well.\\n\\nOnce the whole... yeah, see, it's a loop, and the only persistent part *and* the only part that changes is the KV cache. Either way, it starts with \\"the whole KV cache\\" and ends with the next token being added to the KV cache.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n053jx6","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Think of an LLM like a reader with severe memory loss. Think of the KV cache like a picture. The LLM looks at the picture to decide what to draw next. They scribble a little in the corner aaand then immediately forget everything that ever happened. From their perspective, whether the whole picture was drawn by them or by someone else, they have no idea. They can&amp;#39;t tell. It has no impact.&lt;/p&gt;\\n\\n&lt;p&gt;That&amp;#39;s the laymen&amp;#39;s version.&lt;/p&gt;\\n\\n&lt;p&gt;When we run inference, we actually add tokens like this to start off:&lt;/p&gt;\\n\\n&lt;p&gt;&amp;lt;|turn|&amp;gt;user&lt;br/&gt;\\nHey, Qwen, do a little dance!&lt;br/&gt;\\n&amp;lt;|turn|&amp;gt;assistant&lt;/p&gt;\\n\\n&lt;p&gt;Then we tokenize that text and calculate the key and value for each token by running them through the LLM (possibly thousands of tokens at the same time).&lt;/p&gt;\\n\\n&lt;p&gt;Once the whole KV cache is ready, we go through all the LLM&amp;#39;s calculations to infer the next token and add that to the KV cache as well.&lt;/p&gt;\\n\\n&lt;p&gt;Once the whole KV cache is ready, we go through all the LLM&amp;#39;s calculations to infer the next token and add that to the KV cache as well.&lt;/p&gt;\\n\\n&lt;p&gt;Once the whole... yeah, see, it&amp;#39;s a loop, and the only persistent part &lt;em&gt;and&lt;/em&gt; the only part that changes is the KV cache. Either way, it starts with &amp;quot;the whole KV cache&amp;quot; and ends with the next token being added to the KV cache.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lm5a05","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lm5a05/what_if_we_remove_reasoning_models_think_process/n053jx6/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751061186,"author_flair_text":null,"treatment_tags":[],"created_utc":1751061186,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n05ewx9","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"No-Consequence-1779","can_mod_post":false,"send_replies":true,"parent_id":"t1_n04z0e9","score":2,"author_fullname":"t2_1ping1tiaw","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"First you get the money, then you get the car, then you get the girls. ","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n05ewx9","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;First you get the money, then you get the car, then you get the girls. &lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lm5a05","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lm5a05/what_if_we_remove_reasoning_models_think_process/n05ewx9/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751065039,"author_flair_text":null,"treatment_tags":[],"created_utc":1751065039,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n04z0e9","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DistractedSentient","can_mod_post":false,"created_utc":1751059713,"send_replies":true,"parent_id":"t1_n04xjes","score":-1,"author_fullname":"t2_a1x7xr5v","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Oh man, I'm so sorry but I'm having trouble understanding what you're trying to say. It's like it all went over my head lol. Can you give me like an example of what you're talking about, or like... make it a little less technical? I know how to enable KV cache set to Q8 quant in Ollama but other than that not much technically, unfortunately. So I get that models do cache their responses, you're saying the models have their reasoning trace in their KV cache coupled with the output their about to generate so that's why we get better results compared to no\\\\_think or non-thinking dense models?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n04z0e9","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Oh man, I&amp;#39;m so sorry but I&amp;#39;m having trouble understanding what you&amp;#39;re trying to say. It&amp;#39;s like it all went over my head lol. Can you give me like an example of what you&amp;#39;re talking about, or like... make it a little less technical? I know how to enable KV cache set to Q8 quant in Ollama but other than that not much technically, unfortunately. So I get that models do cache their responses, you&amp;#39;re saying the models have their reasoning trace in their KV cache coupled with the output their about to generate so that&amp;#39;s why we get better results compared to no_think or non-thinking dense models?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lm5a05","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lm5a05/what_if_we_remove_reasoning_models_think_process/n04z0e9/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751059713,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":1,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-1}}],"before":null}},"user_reports":[],"saved":false,"id":"n04xjes","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DeProgrammer99","can_mod_post":false,"created_utc":1751059245,"send_replies":true,"parent_id":"t3_1lm5a05","score":4,"author_fullname":"t2_w4j8t","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"This is really easy to do with local models, and it's not worth testing, because the only state LLMs maintain (except for some approaches I've only seen in research papers) is the KV cache.\\n\\nYou have to apply a chat template for instruct models anyway. All you're doing normally is putting a couple tokens around the user's text and then placing a start token for the LLM's response. You preprocess all those tokens to generate the KV cache for them, and then the model uses them to infer the next token. You put that in the KV cache as well, and then you have the model infer the next token... At that point, it's no different than if you started off by including that first response token before the prompt processing step.","edited":1751059465,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n04xjes","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;This is really easy to do with local models, and it&amp;#39;s not worth testing, because the only state LLMs maintain (except for some approaches I&amp;#39;ve only seen in research papers) is the KV cache.&lt;/p&gt;\\n\\n&lt;p&gt;You have to apply a chat template for instruct models anyway. All you&amp;#39;re doing normally is putting a couple tokens around the user&amp;#39;s text and then placing a start token for the LLM&amp;#39;s response. You preprocess all those tokens to generate the KV cache for them, and then the model uses them to infer the next token. You put that in the KV cache as well, and then you have the model infer the next token... At that point, it&amp;#39;s no different than if you started off by including that first response token before the prompt processing step.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lm5a05/what_if_we_remove_reasoning_models_think_process/n04xjes/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751059245,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lm5a05","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ff54b802-c910-11ed-be9d-ea867d8afa86","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n06d46m","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Original_Finding2212","can_mod_post":false,"created_utc":1751077550,"send_replies":true,"parent_id":"t1_n05lcnr","score":1,"author_fullname":"t2_78ipric6","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I did a bit -\\n\\n&gt; I have added “short-hand” reasoning with “logic jumps” that reduced generation but improved results. (Amazon Nova Pro)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n06d46m","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"Llama 33B"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I did a bit -&lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;I have added “short-hand” reasoning with “logic jumps” that reduced generation but improved results. (Amazon Nova Pro)&lt;/p&gt;\\n&lt;/blockquote&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lm5a05","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lm5a05/what_if_we_remove_reasoning_models_think_process/n06d46m/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751077550,"author_flair_text":"Llama 33B","treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n05lcnr","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Secure_Reflection409","can_mod_post":false,"created_utc":1751067256,"send_replies":true,"parent_id":"t3_1lm5a05","score":2,"author_fullname":"t2_by77ogdhr","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Have you ever watched the show House? \\n\\n\\nSome people need to talk shit for a bit, swim in that shit, then gold appears. Like mining your thoughts.\\n\\n\\nI dunno if LLMs work the same way but intuitively it does kinda make sense.\\n\\n\\nNot sure if you can shortcut the process in the way you're thinking but good luck.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n05lcnr","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Have you ever watched the show House? &lt;/p&gt;\\n\\n&lt;p&gt;Some people need to talk shit for a bit, swim in that shit, then gold appears. Like mining your thoughts.&lt;/p&gt;\\n\\n&lt;p&gt;I dunno if LLMs work the same way but intuitively it does kinda make sense.&lt;/p&gt;\\n\\n&lt;p&gt;Not sure if you can shortcut the process in the way you&amp;#39;re thinking but good luck.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lm5a05/what_if_we_remove_reasoning_models_think_process/n05lcnr/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751067256,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lm5a05","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n04y5le","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DistractedSentient","can_mod_post":false,"created_utc":1751059441,"send_replies":true,"parent_id":"t1_n04x14v","score":0,"author_fullname":"t2_a1x7xr5v","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Right, so you mean the reasoning models are not just outputting the answers that they learned in their training data combined with their emergent abilities but because of their reasoning process context, they give a better answer? I've seen models deviate slightly, sometimes heavily, from their reasoning trace, that's why I was curious about it. Probably the minds behind creating and deploying these models already experimented with what I propose, but there aren't any articles that I can find on the internet that talks about specifically tricking the model into making it assume it finished its reasoning process and comparing the result to the original reasoning answer.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n04y5le","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Right, so you mean the reasoning models are not just outputting the answers that they learned in their training data combined with their emergent abilities but because of their reasoning process context, they give a better answer? I&amp;#39;ve seen models deviate slightly, sometimes heavily, from their reasoning trace, that&amp;#39;s why I was curious about it. Probably the minds behind creating and deploying these models already experimented with what I propose, but there aren&amp;#39;t any articles that I can find on the internet that talks about specifically tricking the model into making it assume it finished its reasoning process and comparing the result to the original reasoning answer.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lm5a05","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lm5a05/what_if_we_remove_reasoning_models_think_process/n04y5le/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751059441,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}},"user_reports":[],"saved":false,"id":"n04x14v","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"mnt_brain","can_mod_post":false,"created_utc":1751059084,"send_replies":true,"parent_id":"t3_1lm5a05","score":0,"author_fullname":"t2_1mtt9dytfn","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Context","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n04x14v","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Context&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lm5a05/what_if_we_remove_reasoning_models_think_process/n04x14v/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751059084,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lm5a05","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n050c96","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DistractedSentient","can_mod_post":false,"created_utc":1751060139,"send_replies":true,"parent_id":"t3_1lm5a05","score":0,"author_fullname":"t2_a1x7xr5v","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"EDIT: The mod approved my post, it was just automod that removed it!","edited":1751068853,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n050c96","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;EDIT: The mod approved my post, it was just automod that removed it!&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lm5a05/what_if_we_remove_reasoning_models_think_process/n050c96/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751060139,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lm5a05","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}}]`),r=()=>e.jsx(t,{data:a});export{r as default};
