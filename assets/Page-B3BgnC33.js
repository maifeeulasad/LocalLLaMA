import{j as e}from"./index-BQFNqx_J.js";import{R as l}from"./RedditPostRenderer-BkLorFru.js";import"./index-BGD6HmIH.js";const t=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"*I am a student who just cleared high school and will be joining college this year.I have interest in pursuing coding and AI/ml.*\\n\\n*Will a macbook air m4 base be enough for ml in my 4 year of college??*\\n\\n*Will also be getting a external SSD with that*","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Query","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lo9mcm","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.33,"author_flair_background_color":null,"subreddit_type":"public","ups":0,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_13u4ifaiq4","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":0,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1751295317,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;&lt;em&gt;I am a student who just cleared high school and will be joining college this year.I have interest in pursuing coding and AI/ml.&lt;/em&gt;&lt;/p&gt;\\n\\n&lt;p&gt;&lt;em&gt;Will a macbook air m4 base be enough for ml in my 4 year of college??&lt;/em&gt;&lt;/p&gt;\\n\\n&lt;p&gt;&lt;em&gt;Will also be getting a external SSD with that&lt;/em&gt;&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":true,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1lo9mcm","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"Sudden-Holiday-3582","discussion_type":null,"num_comments":13,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lo9mcm/query/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lo9mcm/query/","subreddit_subscribers":493240,"created_utc":1751295317,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0ld4q3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Sudden-Holiday-3582","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0lbbo1","score":0,"author_fullname":"t2_13u4ifaiq4","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"thank you","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0ld4q3","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;thank you&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lo9mcm","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lo9mcm/query/n0ld4q3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751297683,"author_flair_text":null,"treatment_tags":[],"created_utc":1751297683,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0lhfg9","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"mtmttuan","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0lfs0x","score":1,"author_fullname":"t2_6mjqz0at","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"&gt; how much do I need to run llm locally\\n\\nIt depends. You can run models &lt; 8b on 16 GB but they won't be very smart. If you really want to run more useful models then consider 32 or 64GB.\\n\\n&gt; I have heard keagle n colab are free for students is that true\\n\\nKaggle is always free with 30 hours of GPUs (2xT4 or P100) per week. Colab has smaller free limit and 20$ Colab Pro plan.","edited":false,"author_flair_css_class":null,"name":"t1_n0lhfg9","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;how much do I need to run llm locally&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;It depends. You can run models &amp;lt; 8b on 16 GB but they won&amp;#39;t be very smart. If you really want to run more useful models then consider 32 or 64GB.&lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;I have heard keagle n colab are free for students is that true&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;Kaggle is always free with 30 hours of GPUs (2xT4 or P100) per week. Colab has smaller free limit and 20$ Colab Pro plan.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lo9mcm","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lo9mcm/query/n0lhfg9/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751298923,"author_flair_text":null,"collapsed":false,"created_utc":1751298923,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0lfs0x","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Sudden-Holiday-3582","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0lbbo1","score":0,"author_fullname":"t2_13u4ifaiq4","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"how much do I need to run llm locally??\\n\\nI have heard keagle n colab are free for students is that true??","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0lfs0x","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;how much do I need to run llm locally??&lt;/p&gt;\\n\\n&lt;p&gt;I have heard keagle n colab are free for students is that true??&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lo9mcm","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lo9mcm/query/n0lfs0x/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751298454,"author_flair_text":null,"treatment_tags":[],"created_utc":1751298454,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}},"user_reports":[],"saved":false,"id":"n0lbbo1","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"mtmttuan","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0l9qdp","score":1,"author_fullname":"t2_6mjqz0at","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"If data science that you're thinking about is tabular data then no. You will probably not use any tabular data that need more than 16GB in college. If you also want to do NLP/CV then most NLP tasks run fine on 16GB of RAM unless it's LLM. CV might be trickier as images are storage and RAM consuming, you will hardly train models larger than ResNet50 on high-res images locally. Instead I recommend using Colab, Kaggle for training or asking your institution if you join their labs.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0lbbo1","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;If data science that you&amp;#39;re thinking about is tabular data then no. You will probably not use any tabular data that need more than 16GB in college. If you also want to do NLP/CV then most NLP tasks run fine on 16GB of RAM unless it&amp;#39;s LLM. CV might be trickier as images are storage and RAM consuming, you will hardly train models larger than ResNet50 on high-res images locally. Instead I recommend using Colab, Kaggle for training or asking your institution if you join their labs.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lo9mcm","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lo9mcm/query/n0lbbo1/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751297156,"author_flair_text":null,"treatment_tags":[],"created_utc":1751297156,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0l9qdp","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Sudden-Holiday-3582","can_mod_post":false,"created_utc":1751296694,"send_replies":true,"parent_id":"t1_n0l71if","score":0,"author_fullname":"t2_13u4ifaiq4","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I also want to explore data science.Will 16 gigs be a limiting factor in that??","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0l9qdp","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I also want to explore data science.Will 16 gigs be a limiting factor in that??&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lo9mcm","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lo9mcm/query/n0l9qdp/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751296694,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0l9vde","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"mtmttuan","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0l97jp","score":1,"author_fullname":"t2_6mjqz0at","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"LLM via cloud providers such as AWS, GCP, Azure. Less tweaking with the models (though you can still finetune it and host it but will be quite expensive), but you will probably be able to use more powerful models.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0l9vde","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;LLM via cloud providers such as AWS, GCP, Azure. Less tweaking with the models (though you can still finetune it and host it but will be quite expensive), but you will probably be able to use more powerful models.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lo9mcm","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lo9mcm/query/n0l9vde/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751296735,"author_flair_text":null,"treatment_tags":[],"created_utc":1751296735,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0lahm0","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Gregory-Wolf","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0l97jp","score":1,"author_fullname":"t2_gethr3mh","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"like [runpod.io](http://runpod.io)  \\nif you want local inference and Mac - then better consider Pro or Max models, they have more compute power. And as others mentioned - more RAM.\\n\\nSo If you plan to do local training/inference - then Pro/Max and maximum RAM you can get.  \\nIf you are ok with renting GPU in the cloud - then Air 16Gb will do nicely.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0lahm0","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;like &lt;a href=\\"http://runpod.io\\"&gt;runpod.io&lt;/a&gt;&lt;br/&gt;\\nif you want local inference and Mac - then better consider Pro or Max models, they have more compute power. And as others mentioned - more RAM.&lt;/p&gt;\\n\\n&lt;p&gt;So If you plan to do local training/inference - then Pro/Max and maximum RAM you can get.&lt;br/&gt;\\nIf you are ok with renting GPU in the cloud - then Air 16Gb will do nicely.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lo9mcm","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lo9mcm/query/n0lahm0/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751296913,"author_flair_text":null,"treatment_tags":[],"created_utc":1751296913,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0l97jp","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Sudden-Holiday-3582","can_mod_post":false,"created_utc":1751296541,"send_replies":true,"parent_id":"t1_n0l71if","score":-1,"author_fullname":"t2_13u4ifaiq4","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"what u mean by cloud hosted??","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0l97jp","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;what u mean by cloud hosted??&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lo9mcm","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lo9mcm/query/n0l97jp/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751296541,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0l71if","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"mtmttuan","can_mod_post":false,"created_utc":1751295908,"send_replies":true,"parent_id":"t3_1lo9mcm","score":4,"author_fullname":"t2_6mjqz0at","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"If you want to do ML/DL (non LLM stuff), then 16GB RAM will be enough. I survived college on 16GB RAM laptop with 4GB GPU. If you need anything more demanding, you can always use Kaggle or Colab.\\n\\nBut if you want LLM stuff, either consider cloud hosted or buy a version with more RAM.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0l71if","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;If you want to do ML/DL (non LLM stuff), then 16GB RAM will be enough. I survived college on 16GB RAM laptop with 4GB GPU. If you need anything more demanding, you can always use Kaggle or Colab.&lt;/p&gt;\\n\\n&lt;p&gt;But if you want LLM stuff, either consider cloud hosted or buy a version with more RAM.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lo9mcm/query/n0l71if/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751295908,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lo9mcm","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0l95ca","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Sudden-Holiday-3582","can_mod_post":false,"created_utc":1751296523,"send_replies":true,"parent_id":"t1_n0l69ji","score":1,"author_fullname":"t2_13u4ifaiq4","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"16gb","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0l95ca","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;16gb&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lo9mcm","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lo9mcm/query/n0l95ca/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751296523,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0l69ji","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ForsookComparison","can_mod_post":false,"created_utc":1751295678,"send_replies":true,"parent_id":"t3_1lo9mcm","score":1,"author_fullname":"t2_on5es7pe3","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Assuming by \\"base\\" you mean the 8(?) or 12GB version then no.\\n\\nDo whatever you can to stretch the amount of memory you have, including buying a used M3 based machine if it fits into your budget","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0l69ji","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Assuming by &amp;quot;base&amp;quot; you mean the 8(?) or 12GB version then no.&lt;/p&gt;\\n\\n&lt;p&gt;Do whatever you can to stretch the amount of memory you have, including buying a used M3 based machine if it fits into your budget&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lo9mcm/query/n0l69ji/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751295678,"author_flair_text":"llama.cpp","treatment_tags":[],"link_id":"t3_1lo9mcm","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0maplg","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"tomkod","can_mod_post":false,"created_utc":1751307135,"send_replies":true,"parent_id":"t3_1lo9mcm","score":2,"author_fullname":"t2_u6btg","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"This is both a very easy and very difficult question.\\n\\nEasy answer: Where I am (major R1 university) even the cheapest Chromebook is enough. Why? Because all our Engineering computer labs (for students) have remote access (to Linux or Windows), and the Engineering server farm has VMs with many configurations (different Linux versions, different Windows versions, different RAM, different cores) to fit different needs. Then, the university server farm has more VMs that are available for everyone (any student or employee) to fit other needs. Then, our super computing center has even more remote clusters (CPU only, GPU only, CPU/GPU, some very high RAM, some very high bandwidth, some very high core) to fit even more needs! These are more for research, but students are given access per request.\\n\\nYour case: Classroom problems are always sized to run a few minutes on limited CPU and limited RAM. If you take some supercomputing class, like parallel programming, or CFD/LES/DNS, or LLM, or astrophysics, or earth/climate, or particle transport, you'll be given cluster access, so your local computer doesn't matter (too much).\\n\\nIn that sense, the question about AI/ML is misplaced. In class, you will probably use KB (maybe MB) size datasets. But google around, and you can download a multi-TB data set. Do you think people who do AI/ML have multi-TB RAM laptops? They don't.\\n\\nIf you want to run everything locally, and you advance to “serious” research or industry problems (typically not what is done in classes), you will always hit a limit that goes beyond what you have. Do you have a computer 1TB RAM? Good for you, but next month your simulation will probably need 1.1TB RAM (or VRAM). And then what?\\n\\nIf you do advance to “serious” research or industry problems, you will most likely join a research group, which will have their own cluster, and probably access to university supercomputing center, and probably access to national supercomputing center. If you are in the USA, NSF and DOE run a number of them, they are easy to access if you have a serious reason. Others mentioned Kaggle and Colab, it is similar, except almost unlimited computing power available “for free” (don't worry, your research supervisor will pay for it).\\n\\nMy case: I run many supercomputer type simulations on my local computer. It is 32GB RAM, and it is plenty. Are my simulations (and LLMs) limited to 32GB? Absolutely not! Locally I will run only small tests (something that takes a few minutes to \\\\~1h, and max few GB RAM), and once it gets bigger, it will go to one of the clusters. This would be true if I had 8GB, or 64GB, or 512GB. I can always make the problem small enough to fit my computer, but ultimately I need 1000s of cores and TBs RAM, so no local computer will be sufficient.\\n\\nBack to LLM: There are under 1B models that run on less than 1GB (quantized), there are 100sB models that run on 100sGB, and everything in between. Note that “running” and “training” are two different beasts. There is Karpathy's NanoGPT (might have been renamed) that you can train yourself (and run) yourself on any mid-range computer from the last few years. For education purposes this is plenty. And once you get “serious” (see earlier paragraph about supercomputing centers), no local computer will be enough.\\n\\nRecommendation: Prioritize RAM. If you are desperate, you can always leave your computer running overnight to finish the simulation. But if you run out of RAM, no amount of time will help. Then, learn how to use remote servers (preferably without GUI). Once you do that, you can ignore your question and just get a Chromebook.\\n\\nGood luck!","edited":1751307961,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0maplg","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;This is both a very easy and very difficult question.&lt;/p&gt;\\n\\n&lt;p&gt;Easy answer: Where I am (major R1 university) even the cheapest Chromebook is enough. Why? Because all our Engineering computer labs (for students) have remote access (to Linux or Windows), and the Engineering server farm has VMs with many configurations (different Linux versions, different Windows versions, different RAM, different cores) to fit different needs. Then, the university server farm has more VMs that are available for everyone (any student or employee) to fit other needs. Then, our super computing center has even more remote clusters (CPU only, GPU only, CPU/GPU, some very high RAM, some very high bandwidth, some very high core) to fit even more needs! These are more for research, but students are given access per request.&lt;/p&gt;\\n\\n&lt;p&gt;Your case: Classroom problems are always sized to run a few minutes on limited CPU and limited RAM. If you take some supercomputing class, like parallel programming, or CFD/LES/DNS, or LLM, or astrophysics, or earth/climate, or particle transport, you&amp;#39;ll be given cluster access, so your local computer doesn&amp;#39;t matter (too much).&lt;/p&gt;\\n\\n&lt;p&gt;In that sense, the question about AI/ML is misplaced. In class, you will probably use KB (maybe MB) size datasets. But google around, and you can download a multi-TB data set. Do you think people who do AI/ML have multi-TB RAM laptops? They don&amp;#39;t.&lt;/p&gt;\\n\\n&lt;p&gt;If you want to run everything locally, and you advance to “serious” research or industry problems (typically not what is done in classes), you will always hit a limit that goes beyond what you have. Do you have a computer 1TB RAM? Good for you, but next month your simulation will probably need 1.1TB RAM (or VRAM). And then what?&lt;/p&gt;\\n\\n&lt;p&gt;If you do advance to “serious” research or industry problems, you will most likely join a research group, which will have their own cluster, and probably access to university supercomputing center, and probably access to national supercomputing center. If you are in the USA, NSF and DOE run a number of them, they are easy to access if you have a serious reason. Others mentioned Kaggle and Colab, it is similar, except almost unlimited computing power available “for free” (don&amp;#39;t worry, your research supervisor will pay for it).&lt;/p&gt;\\n\\n&lt;p&gt;My case: I run many supercomputer type simulations on my local computer. It is 32GB RAM, and it is plenty. Are my simulations (and LLMs) limited to 32GB? Absolutely not! Locally I will run only small tests (something that takes a few minutes to ~1h, and max few GB RAM), and once it gets bigger, it will go to one of the clusters. This would be true if I had 8GB, or 64GB, or 512GB. I can always make the problem small enough to fit my computer, but ultimately I need 1000s of cores and TBs RAM, so no local computer will be sufficient.&lt;/p&gt;\\n\\n&lt;p&gt;Back to LLM: There are under 1B models that run on less than 1GB (quantized), there are 100sB models that run on 100sGB, and everything in between. Note that “running” and “training” are two different beasts. There is Karpathy&amp;#39;s NanoGPT (might have been renamed) that you can train yourself (and run) yourself on any mid-range computer from the last few years. For education purposes this is plenty. And once you get “serious” (see earlier paragraph about supercomputing centers), no local computer will be enough.&lt;/p&gt;\\n\\n&lt;p&gt;Recommendation: Prioritize RAM. If you are desperate, you can always leave your computer running overnight to finish the simulation. But if you run out of RAM, no amount of time will help. Then, learn how to use remote servers (preferably without GUI). Once you do that, you can ignore your question and just get a Chromebook.&lt;/p&gt;\\n\\n&lt;p&gt;Good luck!&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lo9mcm/query/n0maplg/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751307135,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lo9mcm","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}}]`),n=()=>e.jsx(l,{data:t});export{n as default};
