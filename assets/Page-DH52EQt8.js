import{j as e}from"./index-DQXiEb7D.js";import{R as l}from"./RedditPostRenderer-BjndLgq8.js";import"./index-B-ILyjT1.js";const a=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"How is the support?  \\nWhat is the performance loss?\\n\\n  \\nI only really use LLM's with a RTX 3060 Ti, I was want to switch to AMD due to their open source drivers, I'll be using a mix of Linux &amp; Windows.","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"What's it currently like for people here running AMD GPUs with AI?","link_flair_richtext":[{"e":"text","t":"Discussion"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1ln1a6u","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.87,"author_flair_background_color":null,"subreddit_type":"public","ups":56,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_74by8ylq","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Discussion","can_mod_post":false,"score":56,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1751155886,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;How is the support?&lt;br/&gt;\\nWhat is the performance loss?&lt;/p&gt;\\n\\n&lt;p&gt;I only really use LLM&amp;#39;s with a RTX 3060 Ti, I was want to switch to AMD due to their open source drivers, I&amp;#39;ll be using a mix of Linux &amp;amp; Windows.&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"5f921ea4-c7bc-11ed-9c23-3a00622979b4","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#646d73","id":"1ln1a6u","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"83yWasTaken","discussion_type":null,"num_comments":73,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1ln1a6u/whats_it_currently_like_for_people_here_running/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1ln1a6u/whats_it_currently_like_for_people_here_running/","subreddit_subscribers":492929,"created_utc":1751155886,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0bzi3s","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"NNN_Throwaway2","can_mod_post":false,"created_utc":1751158024,"send_replies":true,"parent_id":"t3_1ln1a6u","score":43,"author_fullname":"t2_8rrihts9","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I use a 7900XTX on windows with lmstudio. No perf loss, works fine.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0bzi3s","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I use a 7900XTX on windows with lmstudio. No perf loss, works fine.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ln1a6u/whats_it_currently_like_for_people_here_running/n0bzi3s/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751158024,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1ln1a6u","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":43}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0cd6g0","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Comfortable_Relief62","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0ccqih","score":4,"author_fullname":"t2_io0snnov","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Definitely will do, I actually reinstalled rocm recently and found it much easier. I still compiled ollama myself, but probably don’t need to do that anymore either. Idk, when I first set it up, 6.14 was just released, so I’m sure software support was poor at the time.","edited":false,"author_flair_css_class":null,"name":"t1_n0cd6g0","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Definitely will do, I actually reinstalled rocm recently and found it much easier. I still compiled ollama myself, but probably don’t need to do that anymore either. Idk, when I first set it up, 6.14 was just released, so I’m sure software support was poor at the time.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1ln1a6u","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ln1a6u/whats_it_currently_like_for_people_here_running/n0cd6g0/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751163432,"author_flair_text":null,"collapsed":false,"created_utc":1751163432,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}}],"before":null}},"user_reports":[],"saved":false,"id":"n0ccqih","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"simracerman","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0cbvf2","score":4,"author_fullname":"t2_vbzgnic","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Try now. There's been some amazing updates to Vulkan backends, and the AMD drivers are a lot more mature.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0ccqih","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Try now. There&amp;#39;s been some amazing updates to Vulkan backends, and the AMD drivers are a lot more mature.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ln1a6u","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ln1a6u/whats_it_currently_like_for_people_here_running/n0ccqih/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751163251,"author_flair_text":null,"treatment_tags":[],"created_utc":1751163251,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}}],"before":null}},"user_reports":[],"saved":false,"id":"n0cbvf2","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Comfortable_Relief62","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0c7vip","score":3,"author_fullname":"t2_io0snnov","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Hmm, great question. I’m not opposed to using Vulkan and I’m a bit of a newb in the space. That being said, I tried using Vulkan in LM Studio but wasn’t able to get it to run on the 9070. I haven’t tried since months ago, so maybe I’ll revisit that.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0cbvf2","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Hmm, great question. I’m not opposed to using Vulkan and I’m a bit of a newb in the space. That being said, I tried using Vulkan in LM Studio but wasn’t able to get it to run on the 9070. I haven’t tried since months ago, so maybe I’ll revisit that.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ln1a6u","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ln1a6u/whats_it_currently_like_for_people_here_running/n0cbvf2/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751162901,"author_flair_text":null,"treatment_tags":[],"created_utc":1751162901,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0d1jqx","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"fallingdowndizzyvr","can_mod_post":false,"created_utc":1751174299,"send_replies":true,"parent_id":"t1_n0cy3uk","score":1,"author_fullname":"t2_o65i6kx","approved_by":null,"mod_note":null,"all_awardings":[],"body":"&gt; Interesting, I’ll check that. Hope that gets patched as I don’t want to move away from Vulkan+Windows.\\n\\nThe thing is, since shared memory is the same as dedicated memory on this machine. You can use up to 79GB of memory on Windows with Vulkan. 32GB dedicated + 48GB shared.\\n\\n&gt; After using the machine for a little while, do you think the $2k is justified for this mini PC?\\n\\nYes. I think so. I only paid $1800 though.","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n0d1jqx","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;Interesting, I’ll check that. Hope that gets patched as I don’t want to move away from Vulkan+Windows.&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;The thing is, since shared memory is the same as dedicated memory on this machine. You can use up to 79GB of memory on Windows with Vulkan. 32GB dedicated + 48GB shared.&lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;After using the machine for a little while, do you think the $2k is justified for this mini PC?&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;Yes. I think so. I only paid $1800 though.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ln1a6u/whats_it_currently_like_for_people_here_running/n0d1jqx/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751174299,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1ln1a6u","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":8,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0cy3uk","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"simracerman","can_mod_post":false,"created_utc":1751172635,"send_replies":true,"parent_id":"t1_n0cso1k","score":4,"author_fullname":"t2_vbzgnic","approved_by":null,"mod_note":null,"all_awardings":[],"body":"Interesting, I’ll check that. Hope that gets patched as I don’t want to move away from Vulkan+Windows.\\n\\nAfter using the machine for a little while, do you think the $2k is justified for this mini PC?","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n0cy3uk","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Interesting, I’ll check that. Hope that gets patched as I don’t want to move away from Vulkan+Windows.&lt;/p&gt;\\n\\n&lt;p&gt;After using the machine for a little while, do you think the $2k is justified for this mini PC?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1ln1a6u","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ln1a6u/whats_it_currently_like_for_people_here_running/n0cy3uk/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751172635,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":7,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}}],"before":null}},"user_reports":[],"saved":false,"id":"n0cso1k","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"fallingdowndizzyvr","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0cleca","score":5,"author_fullname":"t2_o65i6kx","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"&gt; Intrigued, why did Windows show 79GB available RAM to the GPU only? Beelink claims they can make up to 96GB available.\\n\\nRead the first impressions thread. I went over it there. It's a Vulkan problem in Windows where Vulkan won't allocate more than 32GB of dedicated VRAM. Linux doesn't have that problem. Right now I'm running ROCm under Windows and it sees 111GB. Which is also what Vulkan under Linux sees as well.","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n0cso1k","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;Intrigued, why did Windows show 79GB available RAM to the GPU only? Beelink claims they can make up to 96GB available.&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;Read the first impressions thread. I went over it there. It&amp;#39;s a Vulkan problem in Windows where Vulkan won&amp;#39;t allocate more than 32GB of dedicated VRAM. Linux doesn&amp;#39;t have that problem. Right now I&amp;#39;m running ROCm under Windows and it sees 111GB. Which is also what Vulkan under Linux sees as well.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1ln1a6u","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ln1a6u/whats_it_currently_like_for_people_here_running/n0cso1k/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751170064,"author_flair_text":null,"treatment_tags":[],"created_utc":1751170064,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":6,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}}],"before":null}},"user_reports":[],"saved":false,"id":"n0cleca","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"simracerman","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0ch9nq","score":5,"author_fullname":"t2_vbzgnic","approved_by":null,"mod_note":null,"all_awardings":[],"body":"Man, I’ve been eyeing the Max+ for a while. Thanks for the insights!\\n\\nIntrigued, why did Windows show 79GB available RAM to the GPU only? Beelink claims they can make up to 96GB available.","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n0cleca","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Man, I’ve been eyeing the Max+ for a while. Thanks for the insights!&lt;/p&gt;\\n\\n&lt;p&gt;Intrigued, why did Windows show 79GB available RAM to the GPU only? Beelink claims they can make up to 96GB available.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1ln1a6u","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ln1a6u/whats_it_currently_like_for_people_here_running/n0cleca/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751166870,"author_flair_text":null,"treatment_tags":[],"created_utc":1751166870,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0gecrx","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"henfiber","can_mod_post":false,"created_utc":1751224792,"send_replies":true,"parent_id":"t1_n0ge8ov","score":1,"author_fullname":"t2_lw9me25","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Sure, I can do it for you. Thanks.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0gecrx","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Sure, I can do it for you. Thanks.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ln1a6u/whats_it_currently_like_for_people_here_running/n0gecrx/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751224792,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1ln1a6u","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":9,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0ge8ov","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"fallingdowndizzyvr","can_mod_post":false,"created_utc":1751224755,"send_replies":true,"parent_id":"t1_n0g7m87","score":2,"author_fullname":"t2_o65i6kx","approved_by":null,"mod_note":null,"all_awardings":[],"body":"Gotcha. I can run it but can you post it there? I don't github. I'll just post another message here with the results.","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n0ge8ov","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Gotcha. I can run it but can you post it there? I don&amp;#39;t github. I&amp;#39;ll just post another message here with the results.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ln1a6u/whats_it_currently_like_for_people_here_running/n0ge8ov/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751224755,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1ln1a6u","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":8,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"more","data":{"count":0,"name":"t1__","id":"_","parent_id":"t1_n0jdyn8","depth":10,"children":[]}}],"before":null}},"user_reports":[],"saved":false,"id":"n0jdyn8","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"henfiber","can_mod_post":false,"created_utc":1751266831,"send_replies":true,"parent_id":"t1_n0j39kq","score":1,"author_fullname":"t2_lw9me25","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Thank you. So that's too close, no need to submit them separately.\\n\\nThese are good results, but there is some potential for improvement in PP when compared with results from Nvidia (e.g., 3060). 20 tokens/TFLOP vs 30 tokens/TFLOP.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0jdyn8","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thank you. So that&amp;#39;s too close, no need to submit them separately.&lt;/p&gt;\\n\\n&lt;p&gt;These are good results, but there is some potential for improvement in PP when compared with results from Nvidia (e.g., 3060). 20 tokens/TFLOP vs 30 tokens/TFLOP.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ln1a6u/whats_it_currently_like_for_people_here_running/n0jdyn8/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751266831,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1ln1a6u","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":9,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0j39kq","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"fallingdowndizzyvr","can_mod_post":false,"created_utc":1751260902,"send_replies":true,"parent_id":"t1_n0g7m87","score":2,"author_fullname":"t2_o65i6kx","approved_by":null,"mod_note":null,"all_awardings":[],"body":"Here you go. It's the same as the numbers already listed.\\n\\n    | model                          |       size |     params | backend    | ngl | fa |            test |                  t/s |\\n    | ------------------------------ | ---------: | ---------: | ---------- | --: | -: | --------------: | -------------------: |\\n    | llama 7B Q4_0                  |   3.56 GiB |     6.74 B | RPC,Vulkan | 100 |  0 |           pp512 |       1247.83 ± 3.78 |\\n    | llama 7B Q4_0                  |   3.56 GiB |     6.74 B | RPC,Vulkan | 100 |  0 |           tg128 |         47.73 ± 0.09 |\\n    | llama 7B Q4_0                  |   3.56 GiB |     6.74 B | RPC,Vulkan | 100 |  1 |           pp512 |       1338.68 ± 1.71 |\\n    | llama 7B Q4_0                  |   3.56 GiB |     6.74 B | RPC,Vulkan | 100 |  1 |           tg128 |         47.32 ± 0.02 |","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n0j39kq","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Here you go. It&amp;#39;s the same as the numbers already listed.&lt;/p&gt;\\n\\n&lt;pre&gt;&lt;code&gt;| model                          |       size |     params | backend    | ngl | fa |            test |                  t/s |\\n| ------------------------------ | ---------: | ---------: | ---------- | --: | -: | --------------: | -------------------: |\\n| llama 7B Q4_0                  |   3.56 GiB |     6.74 B | RPC,Vulkan | 100 |  0 |           pp512 |       1247.83 ± 3.78 |\\n| llama 7B Q4_0                  |   3.56 GiB |     6.74 B | RPC,Vulkan | 100 |  0 |           tg128 |         47.73 ± 0.09 |\\n| llama 7B Q4_0                  |   3.56 GiB |     6.74 B | RPC,Vulkan | 100 |  1 |           pp512 |       1338.68 ± 1.71 |\\n| llama 7B Q4_0                  |   3.56 GiB |     6.74 B | RPC,Vulkan | 100 |  1 |           tg128 |         47.32 ± 0.02 |\\n&lt;/code&gt;&lt;/pre&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ln1a6u/whats_it_currently_like_for_people_here_running/n0j39kq/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751260902,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1ln1a6u","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":8,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n0g7m87","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"henfiber","can_mod_post":false,"created_utc":1751222628,"send_replies":true,"parent_id":"t1_n0g7518","score":1,"author_fullname":"t2_lw9me25","approved_by":null,"mod_note":null,"all_awardings":[],"body":"Sorry mixed the links,  updated my message with the correct ones.","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n0g7m87","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Sorry mixed the links,  updated my message with the correct ones.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1ln1a6u","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ln1a6u/whats_it_currently_like_for_people_here_running/n0g7m87/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751222628,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":7,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0g7518","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"fallingdowndizzyvr","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0evxo2","score":1,"author_fullname":"t2_o65i6kx","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"&gt;  post your results here so we can have a relative performance estimate for the 395+ on GMK X2?\\n\\nPost the results where? That's a link to a gguf.","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n0g7518","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;post your results here so we can have a relative performance estimate for the 395+ on GMK X2?&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;Post the results where? That&amp;#39;s a link to a gguf.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1ln1a6u","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ln1a6u/whats_it_currently_like_for_people_here_running/n0g7518/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751222476,"author_flair_text":null,"treatment_tags":[],"created_utc":1751222476,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":6,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0evxo2","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"henfiber","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0ch9nq","score":2,"author_fullname":"t2_lw9me25","approved_by":null,"mod_note":null,"all_awardings":[],"body":"Could you also run a benchmark with [llama-2-7b.Q4\\\\_0](https://huggingface.co/TheBloke/Llama-2-7B-GGUF/resolve/main/llama-2-7b.Q4_0.gguf) and post your results [here](https://github.com/ggml-org/llama.cpp/discussions/10879) so we can have a relative performance estimate for the 395+ on GMK X2?\\n\\nThere is a 395+ result on Arch linux submitted there but they do not mention the platform (laptop, mini-pc etc.).\\n\\nEDIT: Fixed the links","edited":1751222579,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n0evxo2","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Could you also run a benchmark with &lt;a href=\\"https://huggingface.co/TheBloke/Llama-2-7B-GGUF/resolve/main/llama-2-7b.Q4_0.gguf\\"&gt;llama-2-7b.Q4_0&lt;/a&gt; and post your results &lt;a href=\\"https://github.com/ggml-org/llama.cpp/discussions/10879\\"&gt;here&lt;/a&gt; so we can have a relative performance estimate for the 395+ on GMK X2?&lt;/p&gt;\\n\\n&lt;p&gt;There is a 395+ result on Arch linux submitted there but they do not mention the platform (laptop, mini-pc etc.).&lt;/p&gt;\\n\\n&lt;p&gt;EDIT: Fixed the links&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1ln1a6u","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ln1a6u/whats_it_currently_like_for_people_here_running/n0evxo2/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751207625,"author_flair_text":null,"treatment_tags":[],"created_utc":1751207625,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n0ch9nq","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"fallingdowndizzyvr","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0ccksl","score":6,"author_fullname":"t2_o65i6kx","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"&gt; I think PP is a bit faster in ROCM/CUDA\\n\\nIt used to be. But no longer. It may be faster in some situation for some GPU, but even for PP Vulkan is generally faster.\\n\\nI've posted a lot of numbers that show Vulkan is faster. Here are some from 8 days ago.\\n\\nhttps://www.reddit.com/r/LocalLLaMA/comments/1lgdi7i/gmk_x2amd_max_395_w128gb_second_impressions_linux/","edited":false,"author_flair_css_class":null,"name":"t1_n0ch9nq","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;I think PP is a bit faster in ROCM/CUDA&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;It used to be. But no longer. It may be faster in some situation for some GPU, but even for PP Vulkan is generally faster.&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;ve posted a lot of numbers that show Vulkan is faster. Here are some from 8 days ago.&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://www.reddit.com/r/LocalLLaMA/comments/1lgdi7i/gmk_x2amd_max_395_w128gb_second_impressions_linux/\\"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1lgdi7i/gmk_x2amd_max_395_w128gb_second_impressions_linux/&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1ln1a6u","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ln1a6u/whats_it_currently_like_for_people_here_running/n0ch9nq/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751165109,"author_flair_text":null,"collapsed":false,"created_utc":1751165109,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}}],"before":null}},"user_reports":[],"saved":false,"id":"n0ccksl","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"simracerman","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0c94lv","score":7,"author_fullname":"t2_vbzgnic","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I think PP is a bit faster in ROCM/CUDA, but in real life it's not observed at least from my tests. Tokens/sec are incredibly fast on Vulkan. Enough for me to never bother with ROCM.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0ccksl","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I think PP is a bit faster in ROCM/CUDA, but in real life it&amp;#39;s not observed at least from my tests. Tokens/sec are incredibly fast on Vulkan. Enough for me to never bother with ROCM.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ln1a6u","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ln1a6u/whats_it_currently_like_for_people_here_running/n0ccksl/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751163186,"author_flair_text":null,"treatment_tags":[],"created_utc":1751163186,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":7}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0g8t6t","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"colin_colout","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0g6ks1","score":1,"author_fullname":"t2_14l4ya","approved_by":null,"mod_note":null,"all_awardings":[],"body":"Ahhh...and 6.4.1 is buggy and barely works in llama.cpp (ask me how I know lol). Llama.cpp's rocm ci/cd for docker is disable since they keep it frozen at 6.3 (which works for me but I gotta pretend I'm 11.0.2 instead of .3). \\n\\nROCm is a hot mess, but it gives me better performance on my hardware once I found the right incantations on my config. By the time support for a card is stable, it's already obsolete lol. \\n\\nThe strix halo support is concerning...","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n0g8t6t","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Ahhh...and 6.4.1 is buggy and barely works in llama.cpp (ask me how I know lol). Llama.cpp&amp;#39;s rocm ci/cd for docker is disable since they keep it frozen at 6.3 (which works for me but I gotta pretend I&amp;#39;m 11.0.2 instead of .3). &lt;/p&gt;\\n\\n&lt;p&gt;ROCm is a hot mess, but it gives me better performance on my hardware once I found the right incantations on my config. By the time support for a card is stable, it&amp;#39;s already obsolete lol. &lt;/p&gt;\\n\\n&lt;p&gt;The strix halo support is concerning...&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1ln1a6u","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ln1a6u/whats_it_currently_like_for_people_here_running/n0g8t6t/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751223011,"author_flair_text":null,"treatment_tags":[],"created_utc":1751223011,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0g6ks1","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"fallingdowndizzyvr","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0f0wv4","score":2,"author_fullname":"t2_o65i6kx","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"&gt; I'm on llama.cpp btw, so not sure about other software. Discrete GPUs might be a different story, but rocm is the only way right now for 780m at least (and I suspect many of the Strix Halo memory and performance issues could be related but I don't have one to deep five... Yet)\\n\\nThe problem with Strix Halo is ROCm support. Supposedly 6.4.1 had it. It's only partial support. I'm currently running 6.5, which is not an official release. And probably never will be. Since 7 is the next release.\\n\\nOn the otherhand, Vulkan fully supports Strix Halo.","edited":false,"author_flair_css_class":null,"name":"t1_n0g6ks1","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;I&amp;#39;m on llama.cpp btw, so not sure about other software. Discrete GPUs might be a different story, but rocm is the only way right now for 780m at least (and I suspect many of the Strix Halo memory and performance issues could be related but I don&amp;#39;t have one to deep five... Yet)&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;The problem with Strix Halo is ROCm support. Supposedly 6.4.1 had it. It&amp;#39;s only partial support. I&amp;#39;m currently running 6.5, which is not an official release. And probably never will be. Since 7 is the next release.&lt;/p&gt;\\n\\n&lt;p&gt;On the otherhand, Vulkan fully supports Strix Halo.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1ln1a6u","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ln1a6u/whats_it_currently_like_for_people_here_running/n0g6ks1/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751222297,"author_flair_text":null,"collapsed":false,"created_utc":1751222297,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0hlenj","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"colin_colout","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0h7r25","score":2,"author_fullname":"t2_14l4ya","approved_by":null,"mod_note":null,"all_awardings":[],"body":"Linux. Llama.cpp's default rocm docker container. Some other tinkering but that will get you most of the way   also these environment vars:\\n          - 'HSA_OVERRIDE_GFX_VERSION=11.0.2'\\n          - 'GGML_CUDA_ENABLE_UNIFIED_MEMORY=1'\\n          - 'HCC_AMDGPU_TARGETS=gfx1103'\\n          - 'HSA_ENABLE_SDMA=0'","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n0hlenj","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Linux. Llama.cpp&amp;#39;s default rocm docker container. Some other tinkering but that will get you most of the way   also these environment vars:\\n          - &amp;#39;HSA_OVERRIDE_GFX_VERSION=11.0.2&amp;#39;\\n          - &amp;#39;GGML_CUDA_ENABLE_UNIFIED_MEMORY=1&amp;#39;\\n          - &amp;#39;HCC_AMDGPU_TARGETS=gfx1103&amp;#39;\\n          - &amp;#39;HSA_ENABLE_SDMA=0&amp;#39;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1ln1a6u","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ln1a6u/whats_it_currently_like_for_people_here_running/n0hlenj/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751239060,"author_flair_text":null,"treatment_tags":[],"created_utc":1751239060,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n0h7r25","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"simracerman","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0f0wv4","score":1,"author_fullname":"t2_vbzgnic","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I have a 680m, but our iGPUs are close performance wise.\\n\\nCurious, how did you get ROCM to work with 780m? I know Linux is possible but Windows not yet.","edited":false,"author_flair_css_class":null,"name":"t1_n0h7r25","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I have a 680m, but our iGPUs are close performance wise.&lt;/p&gt;\\n\\n&lt;p&gt;Curious, how did you get ROCM to work with 780m? I know Linux is possible but Windows not yet.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1ln1a6u","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ln1a6u/whats_it_currently_like_for_people_here_running/n0h7r25/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751234269,"author_flair_text":null,"collapsed":false,"created_utc":1751234269,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0f0wv4","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"colin_colout","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0c94lv","score":3,"author_fullname":"t2_14l4ya","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I benchmark both on my 780m iGPU, and ROCm was the clear winner for me. \\n\\nFor small prompts vulkan prompt processing was ~40tk/a on qwen-30b-a3b and ROCm was 240. As you approach 6k context, they both decreased to about half. \\n\\nPart of the culprit is surely batch size limit of Vulkan. 768 batch size did wonders (matched my shader count), and ACTUAL UMA support (reading the model directly from RAM without having to copy to RAM and again to GTT) is only possible with rocm right now. \\n\\nI'm on llama.cpp btw, so not sure about other software. Discrete GPUs might be a different story, but rocm is the only way right now for 780m at least (and I suspect many of the Strix Halo memory and performance issues could be related but I don't have one to deep five... Yet)","edited":1751209879,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0f0wv4","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I benchmark both on my 780m iGPU, and ROCm was the clear winner for me. &lt;/p&gt;\\n\\n&lt;p&gt;For small prompts vulkan prompt processing was ~40tk/a on qwen-30b-a3b and ROCm was 240. As you approach 6k context, they both decreased to about half. &lt;/p&gt;\\n\\n&lt;p&gt;Part of the culprit is surely batch size limit of Vulkan. 768 batch size did wonders (matched my shader count), and ACTUAL UMA support (reading the model directly from RAM without having to copy to RAM and again to GTT) is only possible with rocm right now. &lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;m on llama.cpp btw, so not sure about other software. Discrete GPUs might be a different story, but rocm is the only way right now for 780m at least (and I suspect many of the Strix Halo memory and performance issues could be related but I don&amp;#39;t have one to deep five... Yet)&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ln1a6u","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ln1a6u/whats_it_currently_like_for_people_here_running/n0f0wv4/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751209222,"author_flair_text":null,"treatment_tags":[],"created_utc":1751209222,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n0c94lv","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"fallingdowndizzyvr","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0c7vip","score":5,"author_fullname":"t2_o65i6kx","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Vulkan is faster. Yet there are so many people that still push the false narrative that CUDA/ROCm are faster.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0c94lv","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Vulkan is faster. Yet there are so many people that still push the false narrative that CUDA/ROCm are faster.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ln1a6u","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ln1a6u/whats_it_currently_like_for_people_here_running/n0c94lv/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751161802,"author_flair_text":null,"treatment_tags":[],"created_utc":1751161802,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}}],"before":null}},"user_reports":[],"saved":false,"id":"n0c7vip","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"simracerman","can_mod_post":false,"created_utc":1751161310,"send_replies":true,"parent_id":"t1_n0c5msz","score":11,"author_fullname":"t2_vbzgnic","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Curious why not Vulkan? It's faster in my experience, and supported by llama.cpp, LM Studio, and Koboldcpp.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0c7vip","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Curious why not Vulkan? It&amp;#39;s faster in my experience, and supported by llama.cpp, LM Studio, and Koboldcpp.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ln1a6u","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ln1a6u/whats_it_currently_like_for_people_here_running/n0c7vip/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751161310,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":11}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0cn0eq","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Yes_but_I_think","can_mod_post":false,"created_utc":1751167562,"send_replies":true,"parent_id":"t1_n0c5msz","score":3,"author_fullname":"t2_rea1qh6m","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Supported architecture by popular inference engines - works fine.\\n\\nNew architecture - only python / cuda specific implementation published in GitHub / Huggingface - wait for support, if at all.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0cn0eq","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Supported architecture by popular inference engines - works fine.&lt;/p&gt;\\n\\n&lt;p&gt;New architecture - only python / cuda specific implementation published in GitHub / Huggingface - wait for support, if at all.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ln1a6u","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ln1a6u/whats_it_currently_like_for_people_here_running/n0cn0eq/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751167562,"author_flair_text":"llama.cpp","treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0cogka","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"83yWasTaken","can_mod_post":false,"created_utc":1751168203,"send_replies":true,"parent_id":"t1_n0c5msz","score":1,"author_fullname":"t2_74by8ylq","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Interesting, was looking into this too, what about stuff like text to video etc. or is it mostly only a set amount of libraries that rocm supports? Is there anything else out there other than rocm?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0cogka","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Interesting, was looking into this too, what about stuff like text to video etc. or is it mostly only a set amount of libraries that rocm supports? Is there anything else out there other than rocm?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ln1a6u","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ln1a6u/whats_it_currently_like_for_people_here_running/n0cogka/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751168203,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0c5msz","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Comfortable_Relief62","can_mod_post":false,"created_utc":1751160413,"send_replies":true,"parent_id":"t3_1ln1a6u","score":16,"author_fullname":"t2_io0snnov","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Getting the 9070 working with Rocm was kind of painful. I think it’s been improved over the last couple months tho. Basically, had to make sure I’m on 6.14 Linux kernel, installed rocm latest, and compiled ollama for that. Sounds easy but took some toil lol","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0c5msz","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Getting the 9070 working with Rocm was kind of painful. I think it’s been improved over the last couple months tho. Basically, had to make sure I’m on 6.14 Linux kernel, installed rocm latest, and compiled ollama for that. Sounds easy but took some toil lol&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ln1a6u/whats_it_currently_like_for_people_here_running/n0c5msz/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751160413,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1ln1a6u","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":16}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0fwypl","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"83yWasTaken","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0dnopa","score":2,"author_fullname":"t2_74by8ylq","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Thank you, super useful","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0fwypl","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thank you, super useful&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ln1a6u","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ln1a6u/whats_it_currently_like_for_people_here_running/n0fwypl/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751219292,"author_flair_text":null,"treatment_tags":[],"created_utc":1751219292,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n0dnopa","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"CatalyticDragon","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0co73g","score":8,"author_fullname":"t2_3h1nb","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"&gt;a lot of frameworks don't support it AMDs infustructure\\n\\nAMD is a founding member of the Torch foundation so that works well.  [Tensorflow ](https://rocm.docs.amd.com/projects/install-on-linux/en/latest/install/3rd-party/tensorflow-install.html)works as well (with Keras 2/3).  [Jax](https://rocm.blogs.amd.com/artificial-intelligence/jax-triton/README.html), [Triton](https://rocm.docs.amd.com/projects/radeon/en/latest/docs/install/native_linux/install-triton.html), [DirectML](https://gpuopen.com/news/amd-microsoft-directml-stable-diffusion/), all fine.  Vulkan Compute is working really well too.\\n\\nThe list of frameworks which do not support AMD is actually pretty short. All I can think of is *Apache MXNet* and *Microsoft Cognitive Toolkit*. Neither of which I've had the need to use.\\n\\n&gt;I heard rocm 6 or something like that has enabled some of these frameworks \\n\\nROCm 6.0 (Dec 15, 2023) was a big step up in many regards (performance, compatibility) and each new point release since brought additional improvements. [ROCm 7.0](https://www.amd.com/en/products/software/rocm/whats-new.html) which is releasing soon is also shaping up to be another big jump in performance and in hardware support (extending up to MI355 and down to Ryzen AI MAX).","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0dnopa","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;a lot of frameworks don&amp;#39;t support it AMDs infustructure&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;AMD is a founding member of the Torch foundation so that works well.  &lt;a href=\\"https://rocm.docs.amd.com/projects/install-on-linux/en/latest/install/3rd-party/tensorflow-install.html\\"&gt;Tensorflow &lt;/a&gt;works as well (with Keras 2/3).  &lt;a href=\\"https://rocm.blogs.amd.com/artificial-intelligence/jax-triton/README.html\\"&gt;Jax&lt;/a&gt;, &lt;a href=\\"https://rocm.docs.amd.com/projects/radeon/en/latest/docs/install/native_linux/install-triton.html\\"&gt;Triton&lt;/a&gt;, &lt;a href=\\"https://gpuopen.com/news/amd-microsoft-directml-stable-diffusion/\\"&gt;DirectML&lt;/a&gt;, all fine.  Vulkan Compute is working really well too.&lt;/p&gt;\\n\\n&lt;p&gt;The list of frameworks which do not support AMD is actually pretty short. All I can think of is &lt;em&gt;Apache MXNet&lt;/em&gt; and &lt;em&gt;Microsoft Cognitive Toolkit&lt;/em&gt;. Neither of which I&amp;#39;ve had the need to use.&lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;I heard rocm 6 or something like that has enabled some of these frameworks &lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;ROCm 6.0 (Dec 15, 2023) was a big step up in many regards (performance, compatibility) and each new point release since brought additional improvements. &lt;a href=\\"https://www.amd.com/en/products/software/rocm/whats-new.html\\"&gt;ROCm 7.0&lt;/a&gt; which is releasing soon is also shaping up to be another big jump in performance and in hardware support (extending up to MI355 and down to Ryzen AI MAX).&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ln1a6u","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ln1a6u/whats_it_currently_like_for_people_here_running/n0dnopa/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751187009,"author_flair_text":null,"treatment_tags":[],"created_utc":1751187009,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":8}}],"before":null}},"user_reports":[],"saved":false,"id":"n0co73g","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"83yWasTaken","can_mod_post":false,"created_utc":1751168084,"send_replies":true,"parent_id":"t1_n0bync2","score":5,"author_fullname":"t2_74by8ylq","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"This is what I was trying to get at, I'm quite experienced at running models, Nvidia uses the CUDA technology and a lot of frameworks don't support it AMDs infustructure, I heard rocm 6 or something like that has enabled some of these frameworks / libraries to work but idk / if there's anything else out there","edited":1751168341,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0co73g","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;This is what I was trying to get at, I&amp;#39;m quite experienced at running models, Nvidia uses the CUDA technology and a lot of frameworks don&amp;#39;t support it AMDs infustructure, I heard rocm 6 or something like that has enabled some of these frameworks / libraries to work but idk / if there&amp;#39;s anything else out there&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ln1a6u","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ln1a6u/whats_it_currently_like_for_people_here_running/n0co73g/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751168084,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}}],"before":null}},"user_reports":[],"saved":false,"id":"n0bync2","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"CatalyticDragon","can_mod_post":false,"created_utc":1751157695,"send_replies":true,"parent_id":"t3_1ln1a6u","score":28,"author_fullname":"t2_3h1nb","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"You have asked a very vague question here. What cards, what programs, what models, what operating system?\\n\\nIf you don't know the answer because you are just starting out and want to play around then the answer is support is good and performance is good.\\n\\nI can fully load 32 billon parameter models on my 7900XTX and get 25tokens/second and have no problem with image generation, voice detection, text 2 speech, etc etc. \\n\\nIf you are starting out then just grab LM Studio and Amuse AI and enjoy. \\n\\nIf you are getting more advanced you can run any model with AMD cards and on linux you can use Comfy UI, host local models, and build custom applications with python. \\n\\nIn the past some software has been finnicky because developers have only focused on NVIDIA's software stack but things have changed dramatically in the past year and are still improving all the time.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0bync2","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You have asked a very vague question here. What cards, what programs, what models, what operating system?&lt;/p&gt;\\n\\n&lt;p&gt;If you don&amp;#39;t know the answer because you are just starting out and want to play around then the answer is support is good and performance is good.&lt;/p&gt;\\n\\n&lt;p&gt;I can fully load 32 billon parameter models on my 7900XTX and get 25tokens/second and have no problem with image generation, voice detection, text 2 speech, etc etc. &lt;/p&gt;\\n\\n&lt;p&gt;If you are starting out then just grab LM Studio and Amuse AI and enjoy. &lt;/p&gt;\\n\\n&lt;p&gt;If you are getting more advanced you can run any model with AMD cards and on linux you can use Comfy UI, host local models, and build custom applications with python. &lt;/p&gt;\\n\\n&lt;p&gt;In the past some software has been finnicky because developers have only focused on NVIDIA&amp;#39;s software stack but things have changed dramatically in the past year and are still improving all the time.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ln1a6u/whats_it_currently_like_for_people_here_running/n0bync2/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751157695,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1ln1a6u","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":28}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0hv52z","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"wekede","can_mod_post":false,"created_utc":1751242605,"send_replies":true,"parent_id":"t1_n0hrdym","score":1,"author_fullname":"t2_kn9o5","approved_by":null,"mod_note":null,"all_awardings":[],"body":"Huh, interesting. Thanks","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n0hv52z","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Huh, interesting. Thanks&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ln1a6u/whats_it_currently_like_for_people_here_running/n0hv52z/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751242605,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1ln1a6u","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":8,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0hrdym","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"fallingdowndizzyvr","can_mod_post":false,"created_utc":1751241209,"send_replies":true,"parent_id":"t1_n0h5znk","score":1,"author_fullname":"t2_o65i6kx","approved_by":null,"mod_note":null,"all_awardings":[],"body":"I think this post will be of interest to you.\\n\\nhttps://www.reddit.com/r/LocalLLaMA/comments/1ll1xdj/2_gpus_cuda_vulkan_llamacpp_build_setup/n0dmkt0/","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n0hrdym","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I think this post will be of interest to you.&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://www.reddit.com/r/LocalLLaMA/comments/1ll1xdj/2_gpus_cuda_vulkan_llamacpp_build_setup/n0dmkt0/\\"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1ll1xdj/2_gpus_cuda_vulkan_llamacpp_build_setup/n0dmkt0/&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1ln1a6u","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ln1a6u/whats_it_currently_like_for_people_here_running/n0hrdym/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751241209,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":7,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0h5znk","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"wekede","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0g6t43","score":1,"author_fullname":"t2_kn9o5","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Oh, well I don't have my multigpu system setup currently, replacing a card atm. \\n\\nI'll take your word for it if it's more performant and test myself, I was just curious why.","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n0h5znk","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Oh, well I don&amp;#39;t have my multigpu system setup currently, replacing a card atm. &lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;ll take your word for it if it&amp;#39;s more performant and test myself, I was just curious why.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1ln1a6u","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ln1a6u/whats_it_currently_like_for_people_here_running/n0h5znk/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751233679,"author_flair_text":null,"treatment_tags":[],"created_utc":1751233679,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":6,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0g6t43","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"fallingdowndizzyvr","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0f5ulc","score":1,"author_fullname":"t2_o65i6kx","approved_by":null,"mod_note":null,"all_awardings":[],"body":"Give that a try and get back to me.","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n0g6t43","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Give that a try and get back to me.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1ln1a6u","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ln1a6u/whats_it_currently_like_for_people_here_running/n0g6t43/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751222371,"author_flair_text":null,"treatment_tags":[],"created_utc":1751222371,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0f5ulc","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"wekede","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0bzhyj","score":1,"author_fullname":"t2_kn9o5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Why an RPC server and not running both backends on a single llama.cpp instance?","edited":false,"author_flair_css_class":null,"name":"t1_n0f5ulc","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Why an RPC server and not running both backends on a single llama.cpp instance?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1ln1a6u","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ln1a6u/whats_it_currently_like_for_people_here_running/n0f5ulc/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751210798,"author_flair_text":null,"collapsed":false,"created_utc":1751210798,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0c02qe","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"No-Manufacturer-3315","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0bzxcl","score":1,"author_fullname":"t2_174b0j5k4u","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Cool I’ll give that a try thanks","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n0c02qe","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Cool I’ll give that a try thanks&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1ln1a6u","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ln1a6u/whats_it_currently_like_for_people_here_running/n0c02qe/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751158246,"author_flair_text":null,"treatment_tags":[],"created_utc":1751158246,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":6,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0bzxcl","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"fallingdowndizzyvr","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0bzmi6","score":8,"author_fullname":"t2_o65i6kx","approved_by":null,"mod_note":null,"all_awardings":[],"body":"No. RPC is Remote Procedure Call. It doesn't have to be a remote pc although it can be. Both GPUs can be in the same box.","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n0bzxcl","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;No. RPC is Remote Procedure Call. It doesn&amp;#39;t have to be a remote pc although it can be. Both GPUs can be in the same box.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1ln1a6u","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ln1a6u/whats_it_currently_like_for_people_here_running/n0bzxcl/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751158187,"author_flair_text":null,"treatment_tags":[],"created_utc":1751158187,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":8}}],"before":null}},"user_reports":[],"saved":false,"id":"n0bzmi6","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"No-Manufacturer-3315","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0bzhyj","score":-2,"author_fullname":"t2_174b0j5k4u","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"One pc, RPC is remote pc right","edited":false,"author_flair_css_class":null,"name":"t1_n0bzmi6","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;One pc, RPC is remote pc right&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1ln1a6u","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ln1a6u/whats_it_currently_like_for_people_here_running/n0bzmi6/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751158072,"author_flair_text":null,"collapsed":false,"created_utc":1751158072,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-2}}],"before":null}},"user_reports":[],"saved":false,"id":"n0bzhyj","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"fallingdowndizzyvr","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0bxydp","score":9,"author_fullname":"t2_o65i6kx","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"&gt; How? \\n\\nUse llama.cpp pure and unwrapped. Start a RPC server on the 7900xt using ROCm and then run llama-cli on the 4090 using CUDA pointing to the 7900xt's RPC server.\\n\\n&gt;  But one large model spilt across the cards needs a common backend from my efforts at least\\n\\nNo. It doesn't. I even run a Mac as part of the gaggle when running large models.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0bzhyj","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;How? &lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;Use llama.cpp pure and unwrapped. Start a RPC server on the 7900xt using ROCm and then run llama-cli on the 4090 using CUDA pointing to the 7900xt&amp;#39;s RPC server.&lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;But one large model spilt across the cards needs a common backend from my efforts at least&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;No. It doesn&amp;#39;t. I even run a Mac as part of the gaggle when running large models.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ln1a6u","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ln1a6u/whats_it_currently_like_for_people_here_running/n0bzhyj/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751158022,"author_flair_text":null,"treatment_tags":[],"created_utc":1751158022,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":9}}],"before":null}},"user_reports":[],"saved":false,"id":"n0bxydp","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"No-Manufacturer-3315","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0bxou3","score":1,"author_fullname":"t2_174b0j5k4u","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"How? Ollama and lm studio need one back end to spilt the model. If I ran separate models on each card yes that works and I’ve done that. But one large model spilt across the cards needs a common backend from my efforts at least","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0bxydp","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;How? Ollama and lm studio need one back end to spilt the model. If I ran separate models on each card yes that works and I’ve done that. But one large model spilt across the cards needs a common backend from my efforts at least&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ln1a6u","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ln1a6u/whats_it_currently_like_for_people_here_running/n0bxydp/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751157427,"author_flair_text":null,"treatment_tags":[],"created_utc":1751157427,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0bxou3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"fallingdowndizzyvr","can_mod_post":false,"created_utc":1751157325,"send_replies":true,"parent_id":"t1_n0bv35i","score":-3,"author_fullname":"t2_o65i6kx","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"&gt;  I can use both cards together if I use the vulkan back end but that comes with a big performance hit.\\n\\nThat has nothing to do with Vulkan per se. If you use CUDA on the 4090 and ROCm on the 7900xt you'll get the same performance hit when using them together.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0bxou3","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;I can use both cards together if I use the vulkan back end but that comes with a big performance hit.&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;That has nothing to do with Vulkan per se. If you use CUDA on the 4090 and ROCm on the 7900xt you&amp;#39;ll get the same performance hit when using them together.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ln1a6u","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ln1a6u/whats_it_currently_like_for_people_here_running/n0bxou3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751157325,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":1,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0fyo34","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"My_Unbiased_Opinion","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0dza2y","score":1,"author_fullname":"t2_esiyl0yb","approved_by":null,"mod_note":null,"all_awardings":[],"body":"Interesting. I think this is a win win in the long run for everyone involved here. \\n\\n\\nTIL","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n0fyo34","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Interesting. I think this is a win win in the long run for everyone involved here. &lt;/p&gt;\\n\\n&lt;p&gt;TIL&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1ln1a6u","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ln1a6u/whats_it_currently_like_for_people_here_running/n0fyo34/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751219814,"author_flair_text":null,"treatment_tags":[],"created_utc":1751219814,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0dza2y","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"LumpyWelds","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0ctnyk","score":2,"author_fullname":"t2_32hdazgq","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Well..  it's kinda, sort-of a new engine.\\n\\n**\\"We set out to** '***support'*** **a new engine** that makes multimodal models first-class citizens, and getting Ollama’s partners to contribute more directly to the community **via the GGML tensor library**.\\" -- Ollama\\n\\nThey are moving from llama.cpp to ggml.  ggml is llama.cpp's core inference libs written as a separate project by the same author who started llama.cpp, Georgi Gerganov.\\n\\nI think Georgi's pulling a Doom.  Idsoftware wrote doom, but their money maker was the 3D libs underneath.  Georgi wrote llama.cpp, but by separating the inference code underneath into libs that anyone can directly use (GGML),  Georgi can then offer paid support through his new company, ggml.ai.\\n\\nOllama is providing front end support by customizing the inputs according to the requirements of each model family and then passing it to GGML for inference.  This is actually a really good design.","edited":1751194228,"author_flair_css_class":null,"name":"t1_n0dza2y","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Well..  it&amp;#39;s kinda, sort-of a new engine.&lt;/p&gt;\\n\\n&lt;p&gt;&lt;strong&gt;&amp;quot;We set out to&lt;/strong&gt; &amp;#39;&lt;strong&gt;&lt;em&gt;support&amp;#39;&lt;/em&gt;&lt;/strong&gt; &lt;strong&gt;a new engine&lt;/strong&gt; that makes multimodal models first-class citizens, and getting Ollama’s partners to contribute more directly to the community &lt;strong&gt;via the GGML tensor library&lt;/strong&gt;.&amp;quot; -- Ollama&lt;/p&gt;\\n\\n&lt;p&gt;They are moving from llama.cpp to ggml.  ggml is llama.cpp&amp;#39;s core inference libs written as a separate project by the same author who started llama.cpp, Georgi Gerganov.&lt;/p&gt;\\n\\n&lt;p&gt;I think Georgi&amp;#39;s pulling a Doom.  Idsoftware wrote doom, but their money maker was the 3D libs underneath.  Georgi wrote llama.cpp, but by separating the inference code underneath into libs that anyone can directly use (GGML),  Georgi can then offer paid support through his new company, ggml.ai.&lt;/p&gt;\\n\\n&lt;p&gt;Ollama is providing front end support by customizing the inputs according to the requirements of each model family and then passing it to GGML for inference.  This is actually a really good design.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1ln1a6u","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ln1a6u/whats_it_currently_like_for_people_here_running/n0dza2y/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751194017,"author_flair_text":null,"collapsed":false,"created_utc":1751194017,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n0ctnyk","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"My_Unbiased_Opinion","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0cfwpw","score":1,"author_fullname":"t2_esiyl0yb","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Sorry, I should have said \\"moving away\\". They have a new engine (that is now the primary engine) and llama.cpp is the fallback. \\n\\n\\nhttps://ollama.com/blog/multimodal-models\\n\\n\\nOne big issue right now I'm dealing with is the new engine does not support KVcache quant. ","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0ctnyk","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Sorry, I should have said &amp;quot;moving away&amp;quot;. They have a new engine (that is now the primary engine) and llama.cpp is the fallback. &lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://ollama.com/blog/multimodal-models\\"&gt;https://ollama.com/blog/multimodal-models&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;One big issue right now I&amp;#39;m dealing with is the new engine does not support KVcache quant. &lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ln1a6u","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ln1a6u/whats_it_currently_like_for_people_here_running/n0ctnyk/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751170529,"author_flair_text":null,"treatment_tags":[],"created_utc":1751170529,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0cfwpw","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"LumpyWelds","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0c3jrm","score":3,"author_fullname":"t2_32hdazgq","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"When did this happen?  Their github still has the llama.cpp tree in their source\\n\\n[https://github.com/ollama/ollama/tree/main/llama/llama.cpp/src](https://github.com/ollama/ollama/tree/main/llama/llama.cpp/src)","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0cfwpw","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;When did this happen?  Their github still has the llama.cpp tree in their source&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://github.com/ollama/ollama/tree/main/llama/llama.cpp/src\\"&gt;https://github.com/ollama/ollama/tree/main/llama/llama.cpp/src&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ln1a6u","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ln1a6u/whats_it_currently_like_for_people_here_running/n0cfwpw/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751164558,"author_flair_text":null,"treatment_tags":[],"created_utc":1751164558,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n0c3jrm","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"My_Unbiased_Opinion","can_mod_post":false,"created_utc":1751159594,"send_replies":true,"parent_id":"t1_n0bv35i","score":-2,"author_fullname":"t2_esiyl0yb","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I'm not sure if this is the case anymore since Ollama has moved away from llama.cpp. \\n\\n\\nThis might not be a good thing (for now) since Ollama is going to have to reimplement a lot of stuff and I'm not sure if multivendor support is good or not. ","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0c3jrm","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;m not sure if this is the case anymore since Ollama has moved away from llama.cpp. &lt;/p&gt;\\n\\n&lt;p&gt;This might not be a good thing (for now) since Ollama is going to have to reimplement a lot of stuff and I&amp;#39;m not sure if multivendor support is good or not. &lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ln1a6u","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ln1a6u/whats_it_currently_like_for_people_here_running/n0c3jrm/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751159594,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-2}}],"before":null}},"user_reports":[],"saved":false,"id":"n0bv35i","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"No-Manufacturer-3315","can_mod_post":false,"created_utc":1751156345,"send_replies":true,"parent_id":"t3_1ln1a6u","score":12,"author_fullname":"t2_174b0j5k4u","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I have a 4090 and a 7900xt. \\nI can use both cards together if I use the vulkan back end but that comes with a big performance hit.\\n \\nI generally stick to just the 4090 unless I want to try bigger models with both cards.\\n\\nFor backend i had to recompile  llama.cpp inside ollama to get the rocm drivers to work with the 7900.  Not sure if it’s better now but what I ended up liking the best was LM studio where you can select you backend being the ROCM vs Vulkan vs cuda  vs openblas super easily. \\n\\nRecommend that path if you continue down the amd road","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0bv35i","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I have a 4090 and a 7900xt. \\nI can use both cards together if I use the vulkan back end but that comes with a big performance hit.&lt;/p&gt;\\n\\n&lt;p&gt;I generally stick to just the 4090 unless I want to try bigger models with both cards.&lt;/p&gt;\\n\\n&lt;p&gt;For backend i had to recompile  llama.cpp inside ollama to get the rocm drivers to work with the 7900.  Not sure if it’s better now but what I ended up liking the best was LM studio where you can select you backend being the ROCM vs Vulkan vs cuda  vs openblas super easily. &lt;/p&gt;\\n\\n&lt;p&gt;Recommend that path if you continue down the amd road&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ln1a6u/whats_it_currently_like_for_people_here_running/n0bv35i/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751156345,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1ln1a6u","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":12}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0d7s9r","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"05032-MendicantBias","can_mod_post":false,"created_utc":1751177612,"send_replies":true,"parent_id":"t3_1ln1a6u","score":3,"author_fullname":"t2_6id3lwou","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"For LLM you don't even need ROCm, Vulkan works fine. There are cases where ROCm is faster than Vulkan, but just as many where Vulkan is faster than ROCm.\\n\\n[For diffusion it's hardcore.](https://github.com/OrsoEric/HOWTO-7900XTX-Win-ROCM)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0d7s9r","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;For LLM you don&amp;#39;t even need ROCm, Vulkan works fine. There are cases where ROCm is faster than Vulkan, but just as many where Vulkan is faster than ROCm.&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://github.com/OrsoEric/HOWTO-7900XTX-Win-ROCM\\"&gt;For diffusion it&amp;#39;s hardcore.&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ln1a6u/whats_it_currently_like_for_people_here_running/n0d7s9r/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751177612,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1ln1a6u","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0cd193","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"mrpop2213","can_mod_post":false,"created_utc":1751163373,"send_replies":true,"parent_id":"t3_1ln1a6u","score":2,"author_fullname":"t2_2p6t2k","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"The GPU on the framework 16 has been more than capable of running 8 to 12b quants for local text gen (30tok/s through koboldcpp-rocm on linux). Image gen is kinda slow but I rarely if ever need it so I'm happy with it","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0cd193","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The GPU on the framework 16 has been more than capable of running 8 to 12b quants for local text gen (30tok/s through koboldcpp-rocm on linux). Image gen is kinda slow but I rarely if ever need it so I&amp;#39;m happy with it&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ln1a6u/whats_it_currently_like_for_people_here_running/n0cd193/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751163373,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1ln1a6u","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0cswv0","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"elephantgif","can_mod_post":false,"created_utc":1751170176,"send_replies":true,"parent_id":"t3_1ln1a6u","score":2,"author_fullname":"t2_ly0a55dz","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I'm running llama 8B on my XTX with Arch. It was a pain to set up, but now runs comperably to the 4090 I have on a laptop.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0cswv0","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;m running llama 8B on my XTX with Arch. It was a pain to set up, but now runs comperably to the 4090 I have on a laptop.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ln1a6u/whats_it_currently_like_for_people_here_running/n0cswv0/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751170176,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1ln1a6u","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0cujyb","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"INeedMoreShoes","can_mod_post":false,"created_utc":1751170953,"send_replies":true,"parent_id":"t3_1ln1a6u","score":2,"author_fullname":"t2_antux","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"7800xt and 5060ti. Couldn’t get good AI images with the 7800xt, but it destroys the 5060ri (16gb) in LLM TPS with any model I’ve thrown  at it. Using Ollama.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0cujyb","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;7800xt and 5060ti. Couldn’t get good AI images with the 7800xt, but it destroys the 5060ri (16gb) in LLM TPS with any model I’ve thrown  at it. Using Ollama.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ln1a6u/whats_it_currently_like_for_people_here_running/n0cujyb/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751170953,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1ln1a6u","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0f2xx5","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Starcast","can_mod_post":false,"created_utc":1751209870,"send_replies":true,"parent_id":"t1_n0d3doo","score":1,"author_fullname":"t2_3kfxu","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I don't think that's true? My AMD 6800 ran it fine on windows last I tried (I don't try anymore, just use vulkan because I only dabble at best)\\n\\nhttps://rocm.docs.amd.com/projects/install-on-windows/en/latest/reference/system-requirements.html","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0f2xx5","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I don&amp;#39;t think that&amp;#39;s true? My AMD 6800 ran it fine on windows last I tried (I don&amp;#39;t try anymore, just use vulkan because I only dabble at best)&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://rocm.docs.amd.com/projects/install-on-windows/en/latest/reference/system-requirements.html\\"&gt;https://rocm.docs.amd.com/projects/install-on-windows/en/latest/reference/system-requirements.html&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ln1a6u","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ln1a6u/whats_it_currently_like_for_people_here_running/n0f2xx5/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751209870,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0d3doo","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"techno156","can_mod_post":false,"created_utc":1751175233,"send_replies":true,"parent_id":"t3_1ln1a6u","score":2,"author_fullname":"t2_b3s88","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"As someone with an RX 570, support is terrible with older cards, since AMD tends to drop ROCM support really quickly. Anything older than a 7000 series (~3 years old) is not supported any more, for example. Whereas nVidia CUDA will happily run on a GeForce of similar vintage. Some of the more advanced features won't work, or demand higher performance, but it is still perfectly viable.\\n\\nFor me, the only way to get any GPU acceleration at all was to use Vulkan, but when it works, I have no complaints.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0d3doo","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;As someone with an RX 570, support is terrible with older cards, since AMD tends to drop ROCM support really quickly. Anything older than a 7000 series (~3 years old) is not supported any more, for example. Whereas nVidia CUDA will happily run on a GeForce of similar vintage. Some of the more advanced features won&amp;#39;t work, or demand higher performance, but it is still perfectly viable.&lt;/p&gt;\\n\\n&lt;p&gt;For me, the only way to get any GPU acceleration at all was to use Vulkan, but when it works, I have no complaints.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ln1a6u/whats_it_currently_like_for_people_here_running/n0d3doo/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751175233,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1ln1a6u","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0dfcou","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"custodiam99","can_mod_post":false,"created_utc":1751181930,"send_replies":true,"parent_id":"t3_1ln1a6u","score":2,"author_fullname":"t2_nqnhgqqf5","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"No problems with RX 7900XTX (Windows 11, LM Studio).","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0dfcou","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;No problems with RX 7900XTX (Windows 11, LM Studio).&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ln1a6u/whats_it_currently_like_for_people_here_running/n0dfcou/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751181930,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1ln1a6u","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0enijo","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Emergency-Engine-182","can_mod_post":false,"created_utc":1751204735,"send_replies":true,"parent_id":"t3_1ln1a6u","score":2,"author_fullname":"t2_1mt5ttlska","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I am using a Radeon RX 9070XT with, Ryzen 7 9800X3D with 64gb of RAM at 5200MHz CL40.  \\nRunning on Arch Linux 6.15.2  \\nROCM version 1.15  \\nMesa 1.25\\n\\nAs similar to Comfortable\\\\_Reflief62 I got it working with all the things specified above and they are all working very well. I would not classify myself as someone who is particularly tech savvy but dove straight in. I know not related to the specific post but I found actually using Arch Linux was a good resolve and found a few different services online that answered a lot of questions.\\n\\nTook me about a week of getting to understand Arch Linux, reinstalling it once and seeing what worked.\\n\\nI avoided Vulcan because I am also using Blender and found nothing really that would allow me to use my graphics card with Vulcan as it is so new. Whilst I may be wrong using these specific drivers I was able to get everything functional to use Ollama and OpenWebUI. I have also been able to get it working with Video and Image generations too.\\n\\nI have used both with Docker to make them run.\\n\\nMy limitation now is the hardware rather than it not working well at all.\\n\\nEven as a newbie I have found that it all works very well.\\n\\nEdit: I used Arch because of it is a rolling distro and needed the most up to date drivers. Whilst I think this to anyone who is experienced will see why I have done it some may not.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0enijo","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I am using a Radeon RX 9070XT with, Ryzen 7 9800X3D with 64gb of RAM at 5200MHz CL40.&lt;br/&gt;\\nRunning on Arch Linux 6.15.2&lt;br/&gt;\\nROCM version 1.15&lt;br/&gt;\\nMesa 1.25&lt;/p&gt;\\n\\n&lt;p&gt;As similar to Comfortable_Reflief62 I got it working with all the things specified above and they are all working very well. I would not classify myself as someone who is particularly tech savvy but dove straight in. I know not related to the specific post but I found actually using Arch Linux was a good resolve and found a few different services online that answered a lot of questions.&lt;/p&gt;\\n\\n&lt;p&gt;Took me about a week of getting to understand Arch Linux, reinstalling it once and seeing what worked.&lt;/p&gt;\\n\\n&lt;p&gt;I avoided Vulcan because I am also using Blender and found nothing really that would allow me to use my graphics card with Vulcan as it is so new. Whilst I may be wrong using these specific drivers I was able to get everything functional to use Ollama and OpenWebUI. I have also been able to get it working with Video and Image generations too.&lt;/p&gt;\\n\\n&lt;p&gt;I have used both with Docker to make them run.&lt;/p&gt;\\n\\n&lt;p&gt;My limitation now is the hardware rather than it not working well at all.&lt;/p&gt;\\n\\n&lt;p&gt;Even as a newbie I have found that it all works very well.&lt;/p&gt;\\n\\n&lt;p&gt;Edit: I used Arch because of it is a rolling distro and needed the most up to date drivers. Whilst I think this to anyone who is experienced will see why I have done it some may not.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ln1a6u/whats_it_currently_like_for_people_here_running/n0enijo/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751204735,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1ln1a6u","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0bxlf3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"fallingdowndizzyvr","can_mod_post":false,"created_utc":1751157290,"send_replies":true,"parent_id":"t3_1ln1a6u","score":3,"author_fullname":"t2_o65i6kx","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"For LLMs, it's no harder for AMD than it is Nvidia. Just use Vulkan. Which is what I do by default anyways even on my Nvidia GPUs. Vulkan is just as fast if not faster depending on the GPU.\\n\\nNow for video gen...... Nvidia is still faster. Mainly because of the lack of official support for Triton and thus sage. Officially it's Nvidia only but it an be possible to get it to work on AMD. I'm trying to get it to run on my Max+ as we speak.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0bxlf3","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;For LLMs, it&amp;#39;s no harder for AMD than it is Nvidia. Just use Vulkan. Which is what I do by default anyways even on my Nvidia GPUs. Vulkan is just as fast if not faster depending on the GPU.&lt;/p&gt;\\n\\n&lt;p&gt;Now for video gen...... Nvidia is still faster. Mainly because of the lack of official support for Triton and thus sage. Officially it&amp;#39;s Nvidia only but it an be possible to get it to work on AMD. I&amp;#39;m trying to get it to run on my Max+ as we speak.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ln1a6u/whats_it_currently_like_for_people_here_running/n0bxlf3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751157290,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1ln1a6u","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":1,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0dai6w","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Rich_Repeat_22","can_mod_post":false,"created_utc":1751179129,"send_replies":true,"parent_id":"t3_1ln1a6u","score":1,"author_fullname":"t2_viufiki6","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Working with 7900XT on both windows and Linux. \\n\\nEven on Windows still using Llama.cpp, LMStudio, ComfyUI - ZLUDA, and of course Amuse 3.x. Regarding the latter we hope to see LORAs soon.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0dai6w","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Working with 7900XT on both windows and Linux. &lt;/p&gt;\\n\\n&lt;p&gt;Even on Windows still using Llama.cpp, LMStudio, ComfyUI - ZLUDA, and of course Amuse 3.x. Regarding the latter we hope to see LORAs soon.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ln1a6u/whats_it_currently_like_for_people_here_running/n0dai6w/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751179129,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1ln1a6u","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0dga1e","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ttkciar","can_mod_post":false,"created_utc":1751182487,"send_replies":true,"parent_id":"t3_1ln1a6u","score":1,"author_fullname":"t2_cpegz","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"llama.cpp compiled to the Vulkan back-end makes it easy-peasy, and its performance caught up with ROCm a few weeks ago.\\n\\nThere is no performance loss.  I strongly recommend AMD if you're using llama.cpp for inference.\\n\\nIf you're looking to train/fine-tune, though, that's a whole other story.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0dga1e","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;llama.cpp compiled to the Vulkan back-end makes it easy-peasy, and its performance caught up with ROCm a few weeks ago.&lt;/p&gt;\\n\\n&lt;p&gt;There is no performance loss.  I strongly recommend AMD if you&amp;#39;re using llama.cpp for inference.&lt;/p&gt;\\n\\n&lt;p&gt;If you&amp;#39;re looking to train/fine-tune, though, that&amp;#39;s a whole other story.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ln1a6u/whats_it_currently_like_for_people_here_running/n0dga1e/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751182487,"author_flair_text":"llama.cpp","treatment_tags":[],"link_id":"t3_1ln1a6u","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0dk5nk","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"FastDecode1","can_mod_post":false,"created_utc":1751184830,"send_replies":false,"parent_id":"t3_1ln1a6u","score":1,"author_fullname":"t2_prnin4bw1","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Has worked well for me out of the box since I started using LLMs last year.\\n\\nRX 6600, Linux Mint, llama.cpp.\\n\\nPerformance loss compared to what? I only have an AMD GPU and have no desire to switch, unless there's another GPU manufacturer that offers open-source drivers that just work.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0dk5nk","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Has worked well for me out of the box since I started using LLMs last year.&lt;/p&gt;\\n\\n&lt;p&gt;RX 6600, Linux Mint, llama.cpp.&lt;/p&gt;\\n\\n&lt;p&gt;Performance loss compared to what? I only have an AMD GPU and have no desire to switch, unless there&amp;#39;s another GPU manufacturer that offers open-source drivers that just work.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ln1a6u/whats_it_currently_like_for_people_here_running/n0dk5nk/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751184830,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1ln1a6u","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0e1vyl","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"custodiam99","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0e1htv","score":2,"author_fullname":"t2_nqnhgqqf5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Oh OK my bad.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0e1vyl","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Oh OK my bad.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ln1a6u","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ln1a6u/whats_it_currently_like_for_people_here_running/n0e1vyl/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751195421,"author_flair_text":null,"treatment_tags":[],"created_utc":1751195421,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n0e1htv","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Willing_Landscape_61","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0dunlr","score":1,"author_fullname":"t2_8lvrytgw","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I'm not talking about quants, I'm talking about fine tuning!","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0e1htv","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;m not talking about quants, I&amp;#39;m talking about fine tuning!&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ln1a6u","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ln1a6u/whats_it_currently_like_for_people_here_running/n0e1htv/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751195212,"author_flair_text":null,"treatment_tags":[],"created_utc":1751195212,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0dunlr","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"custodiam99","can_mod_post":false,"created_utc":1751191289,"send_replies":true,"parent_id":"t1_n0drc9l","score":1,"author_fullname":"t2_nqnhgqqf5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Unsloth GGUFs run just fine.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0dunlr","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Unsloth GGUFs run just fine.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ln1a6u","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ln1a6u/whats_it_currently_like_for_people_here_running/n0dunlr/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751191289,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0drc9l","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Willing_Landscape_61","can_mod_post":false,"created_utc":1751189272,"send_replies":true,"parent_id":"t3_1ln1a6u","score":1,"author_fullname":"t2_8lvrytgw","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"What boggles my mind is that I never heard of unsloth on AMD GPU here !\\nhttps://rocm.docs.amd.com/projects/ai-developer-hub/en/latest/notebooks/fine_tune/unsloth_Llama3_1_8B_GRPO.html","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0drc9l","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;What boggles my mind is that I never heard of unsloth on AMD GPU here !\\n&lt;a href=\\"https://rocm.docs.amd.com/projects/ai-developer-hub/en/latest/notebooks/fine_tune/unsloth_Llama3_1_8B_GRPO.html\\"&gt;https://rocm.docs.amd.com/projects/ai-developer-hub/en/latest/notebooks/fine_tune/unsloth_Llama3_1_8B_GRPO.html&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ln1a6u/whats_it_currently_like_for_people_here_running/n0drc9l/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751189272,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1ln1a6u","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0e030w","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"FOE-tan","can_mod_post":false,"created_utc":1751194459,"send_replies":true,"parent_id":"t3_1ln1a6u","score":1,"author_fullname":"t2_tvyin","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I just built a new PC with a regular RX 9070 for a mix of AI stuff and gaming. I  can run 24B Mistral  Small/27B  Gemma models at IQ3-M with 16k context perfectly fine in koboldcpp at speds I find to be pretty good.\\n\\nHOWEVER, looking at benchmarks, the cheaper RTX 5060Ti does LLM stuff faster than any current-gen AMD GPU atm, so for an AI-only use case, Nvidia is probably still the way to go, unfortunately.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0e030w","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I just built a new PC with a regular RX 9070 for a mix of AI stuff and gaming. I  can run 24B Mistral  Small/27B  Gemma models at IQ3-M with 16k context perfectly fine in koboldcpp at speds I find to be pretty good.&lt;/p&gt;\\n\\n&lt;p&gt;HOWEVER, looking at benchmarks, the cheaper RTX 5060Ti does LLM stuff faster than any current-gen AMD GPU atm, so for an AI-only use case, Nvidia is probably still the way to go, unfortunately.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ln1a6u/whats_it_currently_like_for_people_here_running/n0e030w/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751194459,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1ln1a6u","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0hskjh","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"10F1","can_mod_post":false,"created_utc":1751241646,"send_replies":true,"parent_id":"t3_1ln1a6u","score":1,"author_fullname":"t2_7lp8z","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Working fine on 7900xtx using the vulkan backend, rocm is slower and uses more memory.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0hskjh","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Working fine on 7900xtx using the vulkan backend, rocm is slower and uses more memory.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ln1a6u/whats_it_currently_like_for_people_here_running/n0hskjh/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751241646,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1ln1a6u","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"7dba5c08-72f1-11ee-9b6f-ca195bc297d4","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0j9ncj","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"PraxisOG","can_mod_post":false,"created_utc":1751264367,"send_replies":true,"parent_id":"t3_1ln1a6u","score":1,"author_fullname":"t2_3f9vjjno","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I put together a rig to run 70b models at iq3xs using two rx6800 cards for 32gb vram. 30b class models run around 16 tok/s with pretty good context. A 3090 would have been much faster, but also more expensive for less vram. My cards don't get rocm support in Linux, but vulkan should work instead when windows 10 security ends. Support for image generation is pretty bad.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0j9ncj","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"Llama 70B"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I put together a rig to run 70b models at iq3xs using two rx6800 cards for 32gb vram. 30b class models run around 16 tok/s with pretty good context. A 3090 would have been much faster, but also more expensive for less vram. My cards don&amp;#39;t get rocm support in Linux, but vulkan should work instead when windows 10 security ends. Support for image generation is pretty bad.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ln1a6u/whats_it_currently_like_for_people_here_running/n0j9ncj/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751264367,"author_flair_text":"Llama 70B","treatment_tags":[],"link_id":"t3_1ln1a6u","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0jar8d","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"PurpleWinterDawn","can_mod_post":false,"created_utc":1751264992,"send_replies":true,"parent_id":"t3_1ln1a6u","score":1,"author_fullname":"t2_dncq6xad","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Got a 5700XT and a 7800XT happily humming along on kobold.cpp and aur/ollama-rocm-git under Manjaro, although the 5700XT system needed the package build file for ollama modified to put gfx1010 as its compilation target.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0jar8d","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Got a 5700XT and a 7800XT happily humming along on kobold.cpp and aur/ollama-rocm-git under Manjaro, although the 5700XT system needed the package build file for ollama modified to put gfx1010 as its compilation target.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ln1a6u/whats_it_currently_like_for_people_here_running/n0jar8d/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751264992,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1ln1a6u","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0jbgd8","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"custodiam99","can_mod_post":false,"created_utc":1751265388,"send_replies":true,"parent_id":"t1_n0butvc","score":1,"author_fullname":"t2_nqnhgqqf5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Not really, it is plug and play in Windows 11 with LM Studio.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0jbgd8","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Not really, it is plug and play in Windows 11 with LM Studio.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ln1a6u","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ln1a6u/whats_it_currently_like_for_people_here_running/n0jbgd8/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751265388,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0butvc","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Melting735","can_mod_post":false,"created_utc":1751156250,"send_replies":true,"parent_id":"t3_1ln1a6u","score":-3,"author_fullname":"t2_5go93eic","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"AMD’s come a long way but LLM stuff still leans NVIDIA. You’ll probably need to mess with configs more and it won’t be as plug and play","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0butvc","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;AMD’s come a long way but LLM stuff still leans NVIDIA. You’ll probably need to mess with configs more and it won’t be as plug and play&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ln1a6u/whats_it_currently_like_for_people_here_running/n0butvc/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751156250,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1ln1a6u","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":1,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0cpt8q","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Thrumpwart","can_mod_post":false,"created_utc":1751168806,"send_replies":true,"parent_id":"t3_1ln1a6u","score":-4,"author_fullname":"t2_iol3buybk","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Remember that scene from 300 where Leonidas makes love to his wife played by Lena Headey? Like that.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0cpt8q","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Remember that scene from 300 where Leonidas makes love to his wife played by Lena Headey? Like that.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ln1a6u/whats_it_currently_like_for_people_here_running/n0cpt8q/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751168806,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1ln1a6u","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0cpu5z","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":"LOW_SCORE","no_follow":true,"author":"Thrumpwart","can_mod_post":false,"created_utc":1751168817,"send_replies":true,"parent_id":"t3_1ln1a6u","score":-5,"author_fullname":"t2_iol3buybk","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":true,"body":"Remember that scene from 300 where Leonidas makes love to his wife played by Lena Headey? Like that.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0cpu5z","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Remember that scene from 300 where Leonidas makes love to his wife played by Lena Headey? Like that.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":"comment score below threshold","distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ln1a6u/whats_it_currently_like_for_people_here_running/n0cpu5z/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751168817,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1ln1a6u","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-5}}],"before":null}}]`),o=()=>e.jsx(l,{data:a});export{o as default};
