import{j as e}from"./index-DLSqWzaI.js";import{R as l}from"./RedditPostRenderer-CysRo2D_.js";import"./index-COXiL3Lo.js";const t=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"I'm currently using llama\\\\_cpp with python bindings, but have heard that vLLM can be much faster, especially when patching.\\n\\nBut I'm not sure how to migrate my workflow that uses a Qwen3 gguf over to vLLM","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Does vLLM not support Qwen3 ggufs? What sort of models/quants are people running in vLLM?","link_flair_richtext":[{"e":"text","t":"Discussion"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1m03sio","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.79,"author_flair_background_color":null,"subreddit_type":"public","ups":8,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_bndbg","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Discussion","can_mod_post":false,"score":8,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1752540242,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;m currently using llama_cpp with python bindings, but have heard that vLLM can be much faster, especially when patching.&lt;/p&gt;\\n\\n&lt;p&gt;But I&amp;#39;m not sure how to migrate my workflow that uses a Qwen3 gguf over to vLLM&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"5f921ea4-c7bc-11ed-9c23-3a00622979b4","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#646d73","id":"1m03sio","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"AuspiciousApple","discussion_type":null,"num_comments":4,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1m03sio/does_vllm_not_support_qwen3_ggufs_what_sort_of/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1m03sio/does_vllm_not_support_qwen3_ggufs_what_sort_of/","subreddit_subscribers":499294,"created_utc":1752540242,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n37x4zz","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"smahs9","can_mod_post":false,"created_utc":1752562543,"send_replies":true,"parent_id":"t1_n376zr5","score":1,"author_fullname":"t2_neyagc1uz","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Depends on the chip. FP8 support is not universal (block scaled quants are not supported on blackwell, which is what Qwen used in their published FP8). Between awq and gptq, without machete, awq gives more stable latency distribution at large batch sizes, but gptq has wider model coverage.\\n\\nThat said, watch out for the [split KV PR](https://github.com/ggml-org/llama.cpp/pull/14363) in llama.cpp, and subsequent server support.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n37x4zz","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Depends on the chip. FP8 support is not universal (block scaled quants are not supported on blackwell, which is what Qwen used in their published FP8). Between awq and gptq, without machete, awq gives more stable latency distribution at large batch sizes, but gptq has wider model coverage.&lt;/p&gt;\\n\\n&lt;p&gt;That said, watch out for the &lt;a href=\\"https://github.com/ggml-org/llama.cpp/pull/14363\\"&gt;split KV PR&lt;/a&gt; in llama.cpp, and subsequent server support.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m03sio","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m03sio/does_vllm_not_support_qwen3_ggufs_what_sort_of/n37x4zz/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752562543,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n376zr5","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Conscious_Cut_6144","can_mod_post":false,"created_utc":1752550041,"send_replies":true,"parent_id":"t3_1m03sio","score":7,"author_fullname":"t2_9hl4ymvj","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"gguf support is less than 1/2 baked in vllm.\\n\\nAWQ, GPTQ, FP8 are the types of models you should be looking to use with VLLM  \\n(the filenames will all be .safetensors)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n376zr5","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;gguf support is less than 1/2 baked in vllm.&lt;/p&gt;\\n\\n&lt;p&gt;AWQ, GPTQ, FP8 are the types of models you should be looking to use with VLLM&lt;br/&gt;\\n(the filenames will all be .safetensors)&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m03sio/does_vllm_not_support_qwen3_ggufs_what_sort_of/n376zr5/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752550041,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m03sio","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":7}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n38iwxj","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AuspiciousApple","can_mod_post":false,"created_utc":1752575022,"send_replies":true,"parent_id":"t1_n36myw3","score":1,"author_fullname":"t2_bndbg","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I've seen that much higher throughput stat before, but could you contextualise it for me?\\n\\nHow does the AWQ compare to ggufs in terms of accuracy and throughput? Is it 5x faster when both are using a Q4 quantised version? And is a Q4 AWQ similar to a Q4 gguf?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n38iwxj","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;ve seen that much higher throughput stat before, but could you contextualise it for me?&lt;/p&gt;\\n\\n&lt;p&gt;How does the AWQ compare to ggufs in terms of accuracy and throughput? Is it 5x faster when both are using a Q4 quantised version? And is a Q4 AWQ similar to a Q4 gguf?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m03sio","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m03sio/does_vllm_not_support_qwen3_ggufs_what_sort_of/n38iwxj/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752575022,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n36myw3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"--dany--","can_mod_post":false,"created_utc":1752542739,"send_replies":true,"parent_id":"t3_1m03sio","score":2,"author_fullname":"t2_bjeo1gwy","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"You may use the official awq with qwen 3. It’s about handling parallels, about 5x throughput on my side vs llama.cpp.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n36myw3","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You may use the official awq with qwen 3. It’s about handling parallels, about 5x throughput on my side vs llama.cpp.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m03sio/does_vllm_not_support_qwen3_ggufs_what_sort_of/n36myw3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752542739,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m03sio","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}}]`),r=()=>e.jsx(l,{data:t});export{r as default};
