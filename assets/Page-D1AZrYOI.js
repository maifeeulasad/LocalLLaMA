import{j as e}from"./index-DFOnUtq9.js";import{R as l}from"./RedditPostRenderer-B-dx19nm.js";import"./index-CUOQn61u.js";const t=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Quick question: which GPU should I buy to run local LLMs which won‚Äôt ruin my budget. ü•≤\\n\\nCurrently running with an NVIDIA 1070 with 8GB VRAM. \\n\\nQwen3:8b runs fine. But these size of models seems a bit dump compared to everything above that. (But everything above won‚Äôt run on it (or slow as hell) ü§£\\n\\nId love to use it for:\\nRAG / CAG\\nTools (MCP)\\nResearch (deep research and e.g with searxng)\\nCoding \\n\\n\\nI know. Intense requests.. but, yeah. Won‚Äôt like to put my personal files for vectoring into the cloud üòÖ)\\n\\nEven when you‚Äôve other recommendations, pls share. :)\\n\\nThanks in advance!","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Which GPU to upgrade from 1070?","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lnfdch","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.43,"author_flair_background_color":null,"subreddit_type":"public","ups":0,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_5lmlxzw0","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":0,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1751205604,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Quick question: which GPU should I buy to run local LLMs which won‚Äôt ruin my budget. ü•≤&lt;/p&gt;\\n\\n&lt;p&gt;Currently running with an NVIDIA 1070 with 8GB VRAM. &lt;/p&gt;\\n\\n&lt;p&gt;Qwen3:8b runs fine. But these size of models seems a bit dump compared to everything above that. (But everything above won‚Äôt run on it (or slow as hell) ü§£&lt;/p&gt;\\n\\n&lt;p&gt;Id love to use it for:\\nRAG / CAG\\nTools (MCP)\\nResearch (deep research and e.g with searxng)\\nCoding &lt;/p&gt;\\n\\n&lt;p&gt;I know. Intense requests.. but, yeah. Won‚Äôt like to put my personal files for vectoring into the cloud üòÖ)&lt;/p&gt;\\n\\n&lt;p&gt;Even when you‚Äôve other recommendations, pls share. :)&lt;/p&gt;\\n\\n&lt;p&gt;Thanks in advance!&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":true,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1lnfdch","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"TjFr00","discussion_type":null,"num_comments":9,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lnfdch/which_gpu_to_upgrade_from_1070/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lnfdch/which_gpu_to_upgrade_from_1070/","subreddit_subscribers":492929,"created_utc":1751205604,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0gc1nd","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"TjFr00","can_mod_post":false,"created_utc":1751224050,"send_replies":true,"parent_id":"t1_n0etgnd","score":1,"author_fullname":"t2_5lmlxzw0","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Didn‚Äôt knew that it‚Äôs an option to merge them oO. Really interesting. I‚Äôll take this approach üëç","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0gc1nd","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Didn‚Äôt knew that it‚Äôs an option to merge them oO. Really interesting. I‚Äôll take this approach üëç&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnfdch","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnfdch/which_gpu_to_upgrade_from_1070/n0gc1nd/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751224050,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0etgnd","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"AppearanceHeavy6724","can_mod_post":false,"created_utc":1751206810,"send_replies":true,"parent_id":"t3_1lnfdch","score":6,"author_fullname":"t2_uz37qfx5","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"3060 + your 1070 = 20GiB","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0etgnd","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;3060 + your 1070 = 20GiB&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnfdch/which_gpu_to_upgrade_from_1070/n0etgnd/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751206810,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnfdch","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0flwv7","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ForsookComparison","can_mod_post":false,"created_utc":1751215897,"send_replies":true,"parent_id":"t3_1lnfdch","score":2,"author_fullname":"t2_on5es7pe3","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"&gt; which won't ruin my budget \\n\\nThis doesn't give us anything to work off of, what's your actual budget max/target?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0flwv7","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;which won&amp;#39;t ruin my budget &lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;This doesn&amp;#39;t give us anything to work off of, what&amp;#39;s your actual budget max/target?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnfdch/which_gpu_to_upgrade_from_1070/n0flwv7/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751215897,"author_flair_text":"llama.cpp","treatment_tags":[],"link_id":"t3_1lnfdch","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0esbqo","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"j0holo","can_mod_post":false,"created_utc":1751206421,"send_replies":true,"parent_id":"t3_1lnfdch","score":1,"author_fullname":"t2_hodrk","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"8gb of vram is just not enough to run larger models. You are running in the issue that larger models spill over to system ram which is much slower in bandwidth compared to VRAM.\\n\\nDo you have a budget? You can look at second hand GPUs with 12 or 16gb of VRAM.  \\nPersonally I use an Intel Arc B580 but that does require some tinkering except when you run the Intel ML studio software on WIndows.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0esbqo","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;8gb of vram is just not enough to run larger models. You are running in the issue that larger models spill over to system ram which is much slower in bandwidth compared to VRAM.&lt;/p&gt;\\n\\n&lt;p&gt;Do you have a budget? You can look at second hand GPUs with 12 or 16gb of VRAM.&lt;br/&gt;\\nPersonally I use an Intel Arc B580 but that does require some tinkering except when you run the Intel ML studio software on WIndows.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnfdch/which_gpu_to_upgrade_from_1070/n0esbqo/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751206421,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnfdch","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0iyqkr","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ProfessionUpbeat4500","can_mod_post":false,"created_utc":1751258632,"send_replies":true,"parent_id":"t3_1lnfdch","score":1,"author_fullname":"t2_h79wu0k74","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"24gb 5080 and 5070 are launching soon...wait i guess","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0iyqkr","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;24gb 5080 and 5070 are launching soon...wait i guess&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnfdch/which_gpu_to_upgrade_from_1070/n0iyqkr/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751258632,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnfdch","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"7dba5c08-72f1-11ee-9b6f-ca195bc297d4","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0j4vs8","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"PraxisOG","can_mod_post":false,"created_utc":1751261759,"send_replies":true,"parent_id":"t3_1lnfdch","score":1,"author_fullname":"t2_3f9vjjno","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Depends what your budget is and how much you value gaming performance. The best budget option you could go with is a 3060, probably followed by a 4060ti 16gb, then a 3090. You should be able to use your 1070 with newer cards, but some features might not be supported.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0j4vs8","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"Llama 70B"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Depends what your budget is and how much you value gaming performance. The best budget option you could go with is a 3060, probably followed by a 4060ti 16gb, then a 3090. You should be able to use your 1070 with newer cards, but some features might not be supported.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnfdch/which_gpu_to_upgrade_from_1070/n0j4vs8/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751261759,"author_flair_text":"Llama 70B","treatment_tags":[],"link_id":"t3_1lnfdch","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0j6oxm","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"kironlau","can_mod_post":false,"created_utc":1751262738,"send_replies":true,"parent_id":"t3_1lnfdch","score":1,"author_fullname":"t2_tb0dz2ds","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"any gpu with 16gb vramÔºåis fine for LLM\\n\\nif other ai modelsÔºåflux or wanÔºåvoice clone is your interest, CUDA is almost the easiest choice.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0j6oxm","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;any gpu with 16gb vramÔºåis fine for LLM&lt;/p&gt;\\n\\n&lt;p&gt;if other ai modelsÔºåflux or wanÔºåvoice clone is your interest, CUDA is almost the easiest choice.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnfdch/which_gpu_to_upgrade_from_1070/n0j6oxm/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751262738,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnfdch","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0fv6mt","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"BryanBTC","can_mod_post":false,"created_utc":1751218749,"send_replies":true,"parent_id":"t3_1lnfdch","score":1,"author_fullname":"t2_19qcp487j4","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Dude, if you're building a PC, the 5090 with 32GB of VRAM is a sweet spot. It'll handle pretty much anything you throw at it. Seriously, it's a beast for the price. But hey, if your wallet is feeling a little light, the 5060Ti with 16GB is also a great option. You won't regret either choice, they're both solid!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0fv6mt","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Dude, if you&amp;#39;re building a PC, the 5090 with 32GB of VRAM is a sweet spot. It&amp;#39;ll handle pretty much anything you throw at it. Seriously, it&amp;#39;s a beast for the price. But hey, if your wallet is feeling a little light, the 5060Ti with 16GB is also a great option. You won&amp;#39;t regret either choice, they&amp;#39;re both solid!&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnfdch/which_gpu_to_upgrade_from_1070/n0fv6mt/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751218749,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnfdch","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),n=()=>e.jsx(l,{data:t});export{n as default};
