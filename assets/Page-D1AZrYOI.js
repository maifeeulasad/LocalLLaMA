import{j as e}from"./index-DFOnUtq9.js";import{R as l}from"./RedditPostRenderer-B-dx19nm.js";import"./index-CUOQn61u.js";const t=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Quick question: which GPU should I buy to run local LLMs which wonâ€™t ruin my budget. ðŸ¥²\\n\\nCurrently running with an NVIDIA 1070 with 8GB VRAM. \\n\\nQwen3:8b runs fine. But these size of models seems a bit dump compared to everything above that. (But everything above wonâ€™t run on it (or slow as hell) ðŸ¤£\\n\\nId love to use it for:\\nRAG / CAG\\nTools (MCP)\\nResearch (deep research and e.g with searxng)\\nCoding \\n\\n\\nI know. Intense requests.. but, yeah. Wonâ€™t like to put my personal files for vectoring into the cloud ðŸ˜…)\\n\\nEven when youâ€™ve other recommendations, pls share. :)\\n\\nThanks in advance!","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Which GPU to upgrade from 1070?","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lnfdch","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.43,"author_flair_background_color":null,"subreddit_type":"public","ups":0,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_5lmlxzw0","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":0,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1751205604,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Quick question: which GPU should I buy to run local LLMs which wonâ€™t ruin my budget. ðŸ¥²&lt;/p&gt;\\n\\n&lt;p&gt;Currently running with an NVIDIA 1070 with 8GB VRAM. &lt;/p&gt;\\n\\n&lt;p&gt;Qwen3:8b runs fine. But these size of models seems a bit dump compared to everything above that. (But everything above wonâ€™t run on it (or slow as hell) ðŸ¤£&lt;/p&gt;\\n\\n&lt;p&gt;Id love to use it for:\\nRAG / CAG\\nTools (MCP)\\nResearch (deep research and e.g with searxng)\\nCoding &lt;/p&gt;\\n\\n&lt;p&gt;I know. Intense requests.. but, yeah. Wonâ€™t like to put my personal files for vectoring into the cloud ðŸ˜…)&lt;/p&gt;\\n\\n&lt;p&gt;Even when youâ€™ve other recommendations, pls share. :)&lt;/p&gt;\\n\\n&lt;p&gt;Thanks in advance!&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":true,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1lnfdch","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"TjFr00","discussion_type":null,"num_comments":9,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lnfdch/which_gpu_to_upgrade_from_1070/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lnfdch/which_gpu_to_upgrade_from_1070/","subreddit_subscribers":492929,"created_utc":1751205604,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0gc1nd","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"TjFr00","can_mod_post":false,"created_utc":1751224050,"send_replies":true,"parent_id":"t1_n0etgnd","score":1,"author_fullname":"t2_5lmlxzw0","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Didnâ€™t knew that itâ€™s an option to merge them oO. Really interesting. Iâ€™ll take this approach ðŸ‘","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0gc1nd","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Didnâ€™t knew that itâ€™s an option to merge them oO. Really interesting. Iâ€™ll take this approach ðŸ‘&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lnfdch","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnfdch/which_gpu_to_upgrade_from_1070/n0gc1nd/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751224050,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0etgnd","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"AppearanceHeavy6724","can_mod_post":false,"created_utc":1751206810,"send_replies":true,"parent_id":"t3_1lnfdch","score":6,"author_fullname":"t2_uz37qfx5","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"3060 + your 1070 = 20GiB","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0etgnd","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;3060 + your 1070 = 20GiB&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnfdch/which_gpu_to_upgrade_from_1070/n0etgnd/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751206810,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnfdch","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0flwv7","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ForsookComparison","can_mod_post":false,"created_utc":1751215897,"send_replies":true,"parent_id":"t3_1lnfdch","score":2,"author_fullname":"t2_on5es7pe3","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"&gt; which won't ruin my budget \\n\\nThis doesn't give us anything to work off of, what's your actual budget max/target?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0flwv7","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;which won&amp;#39;t ruin my budget &lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;This doesn&amp;#39;t give us anything to work off of, what&amp;#39;s your actual budget max/target?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnfdch/which_gpu_to_upgrade_from_1070/n0flwv7/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751215897,"author_flair_text":"llama.cpp","treatment_tags":[],"link_id":"t3_1lnfdch","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0esbqo","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"j0holo","can_mod_post":false,"created_utc":1751206421,"send_replies":true,"parent_id":"t3_1lnfdch","score":1,"author_fullname":"t2_hodrk","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"8gb of vram is just not enough to run larger models. You are running in the issue that larger models spill over to system ram which is much slower in bandwidth compared to VRAM.\\n\\nDo you have a budget? You can look at second hand GPUs with 12 or 16gb of VRAM.  \\nPersonally I use an Intel Arc B580 but that does require some tinkering except when you run the Intel ML studio software on WIndows.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0esbqo","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;8gb of vram is just not enough to run larger models. You are running in the issue that larger models spill over to system ram which is much slower in bandwidth compared to VRAM.&lt;/p&gt;\\n\\n&lt;p&gt;Do you have a budget? You can look at second hand GPUs with 12 or 16gb of VRAM.&lt;br/&gt;\\nPersonally I use an Intel Arc B580 but that does require some tinkering except when you run the Intel ML studio software on WIndows.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnfdch/which_gpu_to_upgrade_from_1070/n0esbqo/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751206421,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnfdch","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0iyqkr","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ProfessionUpbeat4500","can_mod_post":false,"created_utc":1751258632,"send_replies":true,"parent_id":"t3_1lnfdch","score":1,"author_fullname":"t2_h79wu0k74","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"24gb 5080 and 5070 are launching soon...wait i guess","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0iyqkr","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;24gb 5080 and 5070 are launching soon...wait i guess&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnfdch/which_gpu_to_upgrade_from_1070/n0iyqkr/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751258632,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnfdch","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"7dba5c08-72f1-11ee-9b6f-ca195bc297d4","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0j4vs8","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"PraxisOG","can_mod_post":false,"created_utc":1751261759,"send_replies":true,"parent_id":"t3_1lnfdch","score":1,"author_fullname":"t2_3f9vjjno","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Depends what your budget is and how much you value gaming performance. The best budget option you could go with is a 3060, probably followed by a 4060ti 16gb, then a 3090. You should be able to use your 1070 with newer cards, but some features might not be supported.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0j4vs8","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"Llama 70B"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Depends what your budget is and how much you value gaming performance. The best budget option you could go with is a 3060, probably followed by a 4060ti 16gb, then a 3090. You should be able to use your 1070 with newer cards, but some features might not be supported.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnfdch/which_gpu_to_upgrade_from_1070/n0j4vs8/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751261759,"author_flair_text":"Llama 70B","treatment_tags":[],"link_id":"t3_1lnfdch","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0j6oxm","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"kironlau","can_mod_post":false,"created_utc":1751262738,"send_replies":true,"parent_id":"t3_1lnfdch","score":1,"author_fullname":"t2_tb0dz2ds","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"any gpu with 16gb vramï¼Œis fine for LLM\\n\\nif other ai modelsï¼Œflux or wanï¼Œvoice clone is your interest, CUDA is almost the easiest choice.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0j6oxm","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;any gpu with 16gb vramï¼Œis fine for LLM&lt;/p&gt;\\n\\n&lt;p&gt;if other ai modelsï¼Œflux or wanï¼Œvoice clone is your interest, CUDA is almost the easiest choice.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnfdch/which_gpu_to_upgrade_from_1070/n0j6oxm/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751262738,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnfdch","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0fv6mt","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"BryanBTC","can_mod_post":false,"created_utc":1751218749,"send_replies":true,"parent_id":"t3_1lnfdch","score":1,"author_fullname":"t2_19qcp487j4","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Dude, if you're building a PC, the 5090 with 32GB of VRAM is a sweet spot. It'll handle pretty much anything you throw at it. Seriously, it's a beast for the price. But hey, if your wallet is feeling a little light, the 5060Ti with 16GB is also a great option. You won't regret either choice, they're both solid!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0fv6mt","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Dude, if you&amp;#39;re building a PC, the 5090 with 32GB of VRAM is a sweet spot. It&amp;#39;ll handle pretty much anything you throw at it. Seriously, it&amp;#39;s a beast for the price. But hey, if your wallet is feeling a little light, the 5060Ti with 16GB is also a great option. You won&amp;#39;t regret either choice, they&amp;#39;re both solid!&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lnfdch/which_gpu_to_upgrade_from_1070/n0fv6mt/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751218749,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lnfdch","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),n=()=>e.jsx(l,{data:t});export{n as default};
