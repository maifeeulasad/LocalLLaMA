import{j as e}from"./index-xfnGEtuL.js";import{R as t}from"./RedditPostRenderer-DAZRIZZK.js";import"./index-BaNn5-RR.js";const a=JSON.parse('[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Update from 5 july 2025:  \\nI\'ve resolved this issue with ollama for AMD and replacing ROCm libraries.  \\n  \\nHello!  \\nI\'m wandering if it possible to use iGPU for inference in Windows despite the dGPU is online and connected to the Display.  \\nThe whole idea that I can use idling iGPU for the AI tasks (small 7b models).  \\nThe MUX switch itself is not limiting the iGPU for the general tasks (not related to the video rendering, right?).  \\nI\'ve a modern laptop with a ryzen 7840hs and MUX switch for the dGPU - RTX4060.  \\nI know, that I can do opposite - run a display on the iGPU and use dGPU for the AI inference.\\n\\nHow to:\\n\\n* Download [https://github.com/likelovewant/ollama-for-amd](https://github.com/likelovewant/ollama-for-amd)\\n* Download modified rocm libs for 780m (gfx1103): [https://github.com/likelovewant/ROCmLibs-for-gfx1103-AMD780M-APU](https://github.com/likelovewant/ROCmLibs-for-gfx1103-AMD780M-APU)\\n* Replace rocm libs in the ollama (follow instructions on the ollama-for-amd project)\\n* Enjoy!\\n\\n`total duration: 1m1.7299746s`  \\n`load duration: 28.6558ms`  \\n`prompt eval count: 15 token(s)`  \\n`prompt eval duration: 169.7987ms`  \\n`prompt eval rate: 88.34 tokens/s`  \\n`eval count: 583 token(s)`  \\n`eval duration: 1m1.5301253s`  \\n`eval rate: 9.48 tokens/s`\\n\\nhttps://preview.redd.it/2ijxnzigy3bf1.png?width=1125&amp;format=png&amp;auto=webp&amp;s=e5322a6e67ef4ef7f6c5f6cfacf54b0eb76e30c0","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Utilize iGPU (AMD Radeon 780m) even if the dGPU is running via MUX switch","link_flair_richtext":[{"e":"text","t":"Discussion"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":70,"top_awarded_type":null,"hide_score":false,"media_metadata":{"2ijxnzigy3bf1":{"status":"valid","e":"Image","m":"image/png","p":[{"y":136,"x":108,"u":"https://preview.redd.it/2ijxnzigy3bf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=db8fcdefae698047e7c43fcc9cf0e3d460378013"},{"y":272,"x":216,"u":"https://preview.redd.it/2ijxnzigy3bf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=000b44c9ef03747383d0662cd10b7bb90f292a29"},{"y":404,"x":320,"u":"https://preview.redd.it/2ijxnzigy3bf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=bfde51021f455a19bcca98a6e3da497cbcd881ec"},{"y":808,"x":640,"u":"https://preview.redd.it/2ijxnzigy3bf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=26b0bd685ae4fd3a78f1bec4824e9b91e7d58869"},{"y":1212,"x":960,"u":"https://preview.redd.it/2ijxnzigy3bf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=ca075af8a1307dbbaab832d19afb2872e7d24a78"},{"y":1364,"x":1080,"u":"https://preview.redd.it/2ijxnzigy3bf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=4d10ae0766d23448af456f26ebcf29c540230bb1"}],"s":{"y":1421,"x":1125,"u":"https://preview.redd.it/2ijxnzigy3bf1.png?width=1125&amp;format=png&amp;auto=webp&amp;s=e5322a6e67ef4ef7f6c5f6cfacf54b0eb76e30c0"},"id":"2ijxnzigy3bf1"}},"name":"t3_1lsaczg","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.67,"author_flair_background_color":null,"ups":2,"total_awards_received":0,"media_embed":{},"thumbnail_width":140,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_82tuxz2q","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Discussion","can_mod_post":false,"score":2,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"https://external-preview.redd.it/3AWK6ga3cDp-UjrXq3T9-DGYbB6vgHQDwg1qQQM3DFc.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=25025ca1608212effce373a7407b8ef848304be7","edited":1751744505,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"post_hint":"self","content_categories":null,"is_self":true,"subreddit_type":"public","created":1751723484,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Update from 5 july 2025:&lt;br/&gt;\\nI&amp;#39;ve resolved this issue with ollama for AMD and replacing ROCm libraries.  &lt;/p&gt;\\n\\n&lt;p&gt;Hello!&lt;br/&gt;\\nI&amp;#39;m wandering if it possible to use iGPU for inference in Windows despite the dGPU is online and connected to the Display.&lt;br/&gt;\\nThe whole idea that I can use idling iGPU for the AI tasks (small 7b models).&lt;br/&gt;\\nThe MUX switch itself is not limiting the iGPU for the general tasks (not related to the video rendering, right?).&lt;br/&gt;\\nI&amp;#39;ve a modern laptop with a ryzen 7840hs and MUX switch for the dGPU - RTX4060.&lt;br/&gt;\\nI know, that I can do opposite - run a display on the iGPU and use dGPU for the AI inference.&lt;/p&gt;\\n\\n&lt;p&gt;How to:&lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;Download &lt;a href=\\"https://github.com/likelovewant/ollama-for-amd\\"&gt;https://github.com/likelovewant/ollama-for-amd&lt;/a&gt;&lt;/li&gt;\\n&lt;li&gt;Download modified rocm libs for 780m (gfx1103): &lt;a href=\\"https://github.com/likelovewant/ROCmLibs-for-gfx1103-AMD780M-APU\\"&gt;https://github.com/likelovewant/ROCmLibs-for-gfx1103-AMD780M-APU&lt;/a&gt;&lt;/li&gt;\\n&lt;li&gt;Replace rocm libs in the ollama (follow instructions on the ollama-for-amd project)&lt;/li&gt;\\n&lt;li&gt;Enjoy!&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;p&gt;&lt;code&gt;total duration: 1m1.7299746s&lt;/code&gt;&lt;br/&gt;\\n&lt;code&gt;load duration: 28.6558ms&lt;/code&gt;&lt;br/&gt;\\n&lt;code&gt;prompt eval count: 15 token(s)&lt;/code&gt;&lt;br/&gt;\\n&lt;code&gt;prompt eval duration: 169.7987ms&lt;/code&gt;&lt;br/&gt;\\n&lt;code&gt;prompt eval rate: 88.34 tokens/s&lt;/code&gt;&lt;br/&gt;\\n&lt;code&gt;eval count: 583 token(s)&lt;/code&gt;&lt;br/&gt;\\n&lt;code&gt;eval duration: 1m1.5301253s&lt;/code&gt;&lt;br/&gt;\\n&lt;code&gt;eval rate: 9.48 tokens/s&lt;/code&gt;&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://preview.redd.it/2ijxnzigy3bf1.png?width=1125&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e5322a6e67ef4ef7f6c5f6cfacf54b0eb76e30c0\\"&gt;https://preview.redd.it/2ijxnzigy3bf1.png?width=1125&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=e5322a6e67ef4ef7f6c5f6cfacf54b0eb76e30c0&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":true,"is_crosspostable":false,"pinned":false,"over_18":false,"preview":{"images":[{"source":{"url":"https://external-preview.redd.it/3AWK6ga3cDp-UjrXq3T9-DGYbB6vgHQDwg1qQQM3DFc.png?auto=webp&amp;s=c7ceaa88750addb21e920c5e11469eb8aed66305","width":1200,"height":600},"resolutions":[{"url":"https://external-preview.redd.it/3AWK6ga3cDp-UjrXq3T9-DGYbB6vgHQDwg1qQQM3DFc.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=d5960d476ea4194f61592e054c2304a93e90b6cf","width":108,"height":54},{"url":"https://external-preview.redd.it/3AWK6ga3cDp-UjrXq3T9-DGYbB6vgHQDwg1qQQM3DFc.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=1fc656388c7bbc7b08b8e4e6ead8cf2913340dce","width":216,"height":108},{"url":"https://external-preview.redd.it/3AWK6ga3cDp-UjrXq3T9-DGYbB6vgHQDwg1qQQM3DFc.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=ef8e46ee8bae87a4a35328d16c351d686a3df088","width":320,"height":160},{"url":"https://external-preview.redd.it/3AWK6ga3cDp-UjrXq3T9-DGYbB6vgHQDwg1qQQM3DFc.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=62d44de2eb2d27863a6c83b4cc777b0791accde7","width":640,"height":320},{"url":"https://external-preview.redd.it/3AWK6ga3cDp-UjrXq3T9-DGYbB6vgHQDwg1qQQM3DFc.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=0df28c9f3cd3c033dadb5940a59cf4592aafb6ee","width":960,"height":480},{"url":"https://external-preview.redd.it/3AWK6ga3cDp-UjrXq3T9-DGYbB6vgHQDwg1qQQM3DFc.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=18dbbdd4cd0ca14cd965e3eedb8e130806093f7f","width":1080,"height":540}],"variants":{},"id":"3AWK6ga3cDp-UjrXq3T9-DGYbB6vgHQDwg1qQQM3DFc"}],"enabled":false},"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"5f921ea4-c7bc-11ed-9c23-3a00622979b4","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"mod_note":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"num_reports":null,"removal_reason":null,"link_flair_background_color":"#646d73","id":"1lsaczg","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"panther_ra","discussion_type":null,"num_comments":7,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lsaczg/utilize_igpu_amd_radeon_780m_even_if_the_dgpu_is/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lsaczg/utilize_igpu_amd_radeon_780m_even_if_the_dgpu_is/","subreddit_subscribers":494986,"created_utc":1751723484,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1hy9ej","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"shing3232","can_mod_post":false,"created_utc":1751735458,"send_replies":true,"parent_id":"t1_n1gxp1q","score":1,"author_fullname":"t2_ze4mg","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"just use llama.cpp","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1hy9ej","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;just use llama.cpp&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lsaczg","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lsaczg/utilize_igpu_amd_radeon_780m_even_if_the_dgpu_is/n1hy9ej/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751735458,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1gxp1q","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"panther_ra","can_mod_post":false,"created_utc":1751723610,"send_replies":true,"parent_id":"t3_1lsaczg","score":2,"author_fullname":"t2_82tuxz2q","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"P.S: If I\'m running LM-Studio via Vulkan backend - i still cannot see a Radeon 780m as a selectable GPU, only Nvidia.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1gxp1q","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;P.S: If I&amp;#39;m running LM-Studio via Vulkan backend - i still cannot see a Radeon 780m as a selectable GPU, only Nvidia.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lsaczg/utilize_igpu_amd_radeon_780m_even_if_the_dgpu_is/n1gxp1q/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751723610,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lsaczg","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1hf4jb","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"panther_ra","can_mod_post":false,"created_utc":1751729387,"send_replies":true,"parent_id":"t1_n1h1gn2","score":1,"author_fullname":"t2_82tuxz2q","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Can you please share the link or more info about this patching?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1hf4jb","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Can you please share the link or more info about this patching?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lsaczg","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lsaczg/utilize_igpu_amd_radeon_780m_even_if_the_dgpu_is/n1hf4jb/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751729387,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1h1gn2","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"curios-al","can_mod_post":false,"created_utc":1751724968,"send_replies":true,"parent_id":"t3_1lsaczg","score":2,"author_fullname":"t2_te425vy7","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"You can patch llama.cpp so it will stop ignoring APU/iGPU in the presence of dGPU and then explicitly specify what GPU (iGPU or dGPU) to use. At least you can do that in Linux. But the opposite way (iGPU for display and dGPU for inference) is much faster.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1h1gn2","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You can patch llama.cpp so it will stop ignoring APU/iGPU in the presence of dGPU and then explicitly specify what GPU (iGPU or dGPU) to use. At least you can do that in Linux. But the opposite way (iGPU for display and dGPU for inference) is much faster.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lsaczg/utilize_igpu_amd_radeon_780m_even_if_the_dgpu_is/n1h1gn2/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751724968,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lsaczg","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1iep01","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"InternalWeather1719","can_mod_post":false,"created_utc":1751740585,"send_replies":true,"parent_id":"t3_1lsaczg","score":2,"author_fullname":"t2_80r1tpe3","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I have encountered the same problem as you. I wanted to use the iGPU for inference on a PC with a dGPU, but the iGPU wasn\'t recognized. I reported the issue to LM Studio, and although they responded, we didn\'t found a solution in the end.\\n\\n \\nHere\'s what I usually do now: Disable the dGPU in Device Manager, then completely quit and restart LM Studio (make sure it\'s fully closed). This allows LM Studio to use the iGPU. After that, you can re-enable the dGPU in Device Manager.\\n\\n \\nHope this helps!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1iep01","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I have encountered the same problem as you. I wanted to use the iGPU for inference on a PC with a dGPU, but the iGPU wasn&amp;#39;t recognized. I reported the issue to LM Studio, and although they responded, we didn&amp;#39;t found a solution in the end.&lt;/p&gt;\\n\\n&lt;p&gt; \\nHere&amp;#39;s what I usually do now: Disable the dGPU in Device Manager, then completely quit and restart LM Studio (make sure it&amp;#39;s fully closed). This allows LM Studio to use the iGPU. After that, you can re-enable the dGPU in Device Manager.&lt;/p&gt;\\n\\n&lt;p&gt; \\nHope this helps!&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lsaczg/utilize_igpu_amd_radeon_780m_even_if_the_dgpu_is/n1iep01/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751740585,"author_flair_text":"llama.cpp","treatment_tags":[],"link_id":"t3_1lsaczg","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1h1a2t","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"RobotRobotWhatDoUSee","can_mod_post":false,"created_utc":1751724905,"send_replies":true,"parent_id":"t3_1lsaczg","score":1,"author_fullname":"t2_m78cdz1nv","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"What OS are you on? How much sysyem RAM does the iGPU have access to?\\n\\nI\'m not familiar with running both an AMD and NVIDIA gpu at the same time, and I suspect that may be part of it. \\n\\nDoes your LM-Studio recognize the 780M when the dgpu is \\"off\\"?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1h1a2t","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;What OS are you on? How much sysyem RAM does the iGPU have access to?&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;m not familiar with running both an AMD and NVIDIA gpu at the same time, and I suspect that may be part of it. &lt;/p&gt;\\n\\n&lt;p&gt;Does your LM-Studio recognize the 780M when the dgpu is &amp;quot;off&amp;quot;?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lsaczg/utilize_igpu_amd_radeon_780m_even_if_the_dgpu_is/n1h1a2t/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751724905,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lsaczg","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1ha1uc","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"matteogeniaccio","can_mod_post":false,"created_utc":1751727784,"send_replies":true,"parent_id":"t3_1lsaczg","score":1,"author_fullname":"t2_hoxc8","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Yes. you can. Usually you have to enable it on the BIOS, otherwise it only shows one GPU.\\n\\nThe option is called IGD multi-monitor or similar.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1ha1uc","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yes. you can. Usually you have to enable it on the BIOS, otherwise it only shows one GPU.&lt;/p&gt;\\n\\n&lt;p&gt;The option is called IGD multi-monitor or similar.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lsaczg/utilize_igpu_amd_radeon_780m_even_if_the_dgpu_is/n1ha1uc/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751727784,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lsaczg","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]'),r=()=>e.jsx(t,{data:a});export{r as default};
