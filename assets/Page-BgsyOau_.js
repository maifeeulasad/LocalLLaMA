import{j as e}from"./index-CqAPCjw5.js";import{R as l}from"./RedditPostRenderer-4oBDAtGr.js";import"./index-D3Sdy_Op.js";const t=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"My system:   \\nMSI B650 Edge WiFi  \\nRyzen 9900X  \\nG.Skill 96GB (6200MHz)  \\nAMD Asus TUF 7900XTX\\n\\nCurrently, I mainly use Qwen3 32B 4q models with a context size of 40K+ tokens for programming purposes. (Yes, I'm aware that alternatives like DevStral and others are not bad either, but this specific model suits me best). I primarily run them via LM Studio or directly through Llama.cpp.\\n\\nI lack performance on large contexts and would prefer to be able to run more extensive models (though this is certainly not the main priority right now).\\n\\nOptions I'm considering:\\n\\n1. Sell my 7900XTX for about $600 and order an RTX 5090.\\n2. Sell my motherboard for 100$, order an MSI X670 Ace ( 400$, it often appears on sales at that price) and wait for the AMD AI PRO 9070.\\n\\nI've ruled out older, cheaper MI Instinct MI50 cards due to ROCm support termination.\\n\\nI’ve been thinking about this for a long time but still can’t decide, even after reading countless articles and reviews :)","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"What upgrade option is better with $2000 available for my configuration?","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1m305vc","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.6,"author_flair_background_color":null,"subreddit_type":"public","ups":2,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_b51tl28l","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":2,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1752839242,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;My system:&lt;br/&gt;\\nMSI B650 Edge WiFi&lt;br/&gt;\\nRyzen 9900X&lt;br/&gt;\\nG.Skill 96GB (6200MHz)&lt;br/&gt;\\nAMD Asus TUF 7900XTX&lt;/p&gt;\\n\\n&lt;p&gt;Currently, I mainly use Qwen3 32B 4q models with a context size of 40K+ tokens for programming purposes. (Yes, I&amp;#39;m aware that alternatives like DevStral and others are not bad either, but this specific model suits me best). I primarily run them via LM Studio or directly through Llama.cpp.&lt;/p&gt;\\n\\n&lt;p&gt;I lack performance on large contexts and would prefer to be able to run more extensive models (though this is certainly not the main priority right now).&lt;/p&gt;\\n\\n&lt;p&gt;Options I&amp;#39;m considering:&lt;/p&gt;\\n\\n&lt;ol&gt;\\n&lt;li&gt;Sell my 7900XTX for about $600 and order an RTX 5090.&lt;/li&gt;\\n&lt;li&gt;Sell my motherboard for 100$, order an MSI X670 Ace ( 400$, it often appears on sales at that price) and wait for the AMD AI PRO 9070.&lt;/li&gt;\\n&lt;/ol&gt;\\n\\n&lt;p&gt;I&amp;#39;ve ruled out older, cheaper MI Instinct MI50 cards due to ROCm support termination.&lt;/p&gt;\\n\\n&lt;p&gt;I’ve been thinking about this for a long time but still can’t decide, even after reading countless articles and reviews :)&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1m305vc","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"Easy_Kitchen7819","discussion_type":null,"num_comments":7,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1m305vc/what_upgrade_option_is_better_with_2000_available/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1m305vc/what_upgrade_option_is_better_with_2000_available/","subreddit_subscribers":501232,"created_utc":1752839242,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3svlks","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"AdamDhahabi","can_mod_post":false,"created_utc":1752840126,"send_replies":true,"parent_id":"t3_1m305vc","score":5,"author_fullname":"t2_x5lnbc2","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"RTX 5090 would be very comfortable with 1.79 TB/s memory bandwidth. And 32GB VRAM allows for more context size, way above 40K in your case Qwen3 32B Q4.  \\nWhen Qwen3 coder finally drops, you're all set.  \\nI recently found out about a 48GB RTX 8000 but it only has 672 GB/s memory bandwidth. Maybe doable speed-wise with Qwen3 32b speculative decoding, a lot slower compared to RTX 5090, maybe 20\\\\~30 t/s, and you could go crazy with context size. Not sure if that is a good trade-off, speed for extra context.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3svlks","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;RTX 5090 would be very comfortable with 1.79 TB/s memory bandwidth. And 32GB VRAM allows for more context size, way above 40K in your case Qwen3 32B Q4.&lt;br/&gt;\\nWhen Qwen3 coder finally drops, you&amp;#39;re all set.&lt;br/&gt;\\nI recently found out about a 48GB RTX 8000 but it only has 672 GB/s memory bandwidth. Maybe doable speed-wise with Qwen3 32b speculative decoding, a lot slower compared to RTX 5090, maybe 20~30 t/s, and you could go crazy with context size. Not sure if that is a good trade-off, speed for extra context.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m305vc/what_upgrade_option_is_better_with_2000_available/n3svlks/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752840126,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m305vc","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3tcopn","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Secure_Reflection409","can_mod_post":false,"created_utc":1752845970,"send_replies":true,"parent_id":"t3_1m305vc","score":1,"author_fullname":"t2_by77ogdhr","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I'm in a very similar situation.\\n\\n\\nI'm thinking 5090, paired with my 4080, will run Q4KL natively at 32k on the 5090 without watering down the KV cache (I think...) and I could potentially run the Q8, too, with 48GB total.\\n\\n\\nThe only other thing I've been tempted by are those 48GB 4090s on eBay from a far away land with zero warranty that run at 195dB.\\n\\n\\nI can maybe pretend I don't care about the warranty but I just don't think I could deal with the noise on those cards, that's assuming a 48GB card even turns up and I'm not chasing eBay for a 2.5k refund for the next 6 months.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3tcopn","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;m in a very similar situation.&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;m thinking 5090, paired with my 4080, will run Q4KL natively at 32k on the 5090 without watering down the KV cache (I think...) and I could potentially run the Q8, too, with 48GB total.&lt;/p&gt;\\n\\n&lt;p&gt;The only other thing I&amp;#39;ve been tempted by are those 48GB 4090s on eBay from a far away land with zero warranty that run at 195dB.&lt;/p&gt;\\n\\n&lt;p&gt;I can maybe pretend I don&amp;#39;t care about the warranty but I just don&amp;#39;t think I could deal with the noise on those cards, that&amp;#39;s assuming a 48GB card even turns up and I&amp;#39;m not chasing eBay for a 2.5k refund for the next 6 months.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m305vc/what_upgrade_option_is_better_with_2000_available/n3tcopn/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752845970,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m305vc","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3uesbs","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"segmond","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3ub8x1","score":1,"author_fullname":"t2_ah13x","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I'm speaking based only on my experience, I know about 3090s.  if 7900xtx has the same performance, then buy more.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3uesbs","is_submitter":false,"collapsed":false,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;m speaking based only on my experience, I know about 3090s.  if 7900xtx has the same performance, then buy more.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m305vc","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m305vc/what_upgrade_option_is_better_with_2000_available/n3uesbs/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752856908,"author_flair_text":"llama.cpp","treatment_tags":[],"created_utc":1752856908,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3ub8x1","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Easy_Kitchen7819","can_mod_post":false,"created_utc":1752855882,"send_replies":true,"parent_id":"t1_n3u02rn","score":1,"author_fullname":"t2_b51tl28l","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"But 7900xtx have a similar performance","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3ub8x1","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;But 7900xtx have a similar performance&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m305vc","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m305vc/what_upgrade_option_is_better_with_2000_available/n3ub8x1/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752855882,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3u02rn","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"segmond","can_mod_post":false,"created_utc":1752852726,"send_replies":true,"parent_id":"t3_1m305vc","score":1,"author_fullname":"t2_ah13x","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"me personally will try and add 3 3090s or 2 3090s","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3u02rn","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;me personally will try and add 3 3090s or 2 3090s&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m305vc/what_upgrade_option_is_better_with_2000_available/n3u02rn/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752852726,"author_flair_text":"llama.cpp","treatment_tags":[],"link_id":"t3_1m305vc","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3td791","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Secure_Reflection409","can_mod_post":false,"created_utc":1752846130,"send_replies":true,"parent_id":"t1_n3sy9q7","score":3,"author_fullname":"t2_by77ogdhr","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"If I dropped 8k on a pro, I'd need that fucker to be earning me strong money on the daily.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3td791","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;If I dropped 8k on a pro, I&amp;#39;d need that fucker to be earning me strong money on the daily.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m305vc","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m305vc/what_upgrade_option_is_better_with_2000_available/n3td791/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752846130,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n3sy9q7","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"GPTrack_ai","can_mod_post":false,"created_utc":1752841138,"send_replies":true,"parent_id":"t3_1m305vc","score":1,"author_fullname":"t2_1tpuoj72sa","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"If you cannot afford at least a RTX Pro 6000, save your money until you can.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3sy9q7","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;If you cannot afford at least a RTX Pro 6000, save your money until you can.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m305vc/what_upgrade_option_is_better_with_2000_available/n3sy9q7/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752841138,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m305vc","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),o=()=>e.jsx(l,{data:t});export{o as default};
