import{j as e}from"./index-DACS7Nh6.js";import{R as t}from"./RedditPostRenderer-Dqa1NZuX.js";import"./index-DiMIVQx4.js";const a=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"I'm new to ML and fine tuning.\\n\\nRecently I've tried fine tuning gemma 3 on google collab on an 85k dataset (Dolly, Alpaca + custom) and it took 3 hours with Keras on a single A100 gpu. But then I couldn't convert it to pytorch because the conversion script by Keras doesn't support the gemma 3 yet and so I abandoned this project because of that.\\n\\nI then tried fine tuning with transformers and even though I've tried it on an H100 (100+ GB VRAM), it was showing like 30+ hours. I then tried with unsloth to afford a cheaper GPU and it was showing 200+ hours on an L40.\\n\\nI learned that Keras has the advantage of mixed precision, which was why it was so much faster. But I expected transformers to have something similar. Or at least something that would narrow the gap of 10x difference.\\n\\nI'm wondering is Keras really so much better in performance or am I doing it wrong with transformers? And is there a way to convert a gemma 3 model from Keras to transformers or I really must do it with transformers. The goal is to load it to HF and query with vLLM.\\n\\nThank you in advance\\n\\nSorry, this post ","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Keras vs Transformers fine tuning","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1m44tnz","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.84,"author_flair_background_color":null,"subreddit_type":"public","ups":4,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_iimuspe7h","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":4,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1752953435,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;m new to ML and fine tuning.&lt;/p&gt;\\n\\n&lt;p&gt;Recently I&amp;#39;ve tried fine tuning gemma 3 on google collab on an 85k dataset (Dolly, Alpaca + custom) and it took 3 hours with Keras on a single A100 gpu. But then I couldn&amp;#39;t convert it to pytorch because the conversion script by Keras doesn&amp;#39;t support the gemma 3 yet and so I abandoned this project because of that.&lt;/p&gt;\\n\\n&lt;p&gt;I then tried fine tuning with transformers and even though I&amp;#39;ve tried it on an H100 (100+ GB VRAM), it was showing like 30+ hours. I then tried with unsloth to afford a cheaper GPU and it was showing 200+ hours on an L40.&lt;/p&gt;\\n\\n&lt;p&gt;I learned that Keras has the advantage of mixed precision, which was why it was so much faster. But I expected transformers to have something similar. Or at least something that would narrow the gap of 10x difference.&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;m wondering is Keras really so much better in performance or am I doing it wrong with transformers? And is there a way to convert a gemma 3 model from Keras to transformers or I really must do it with transformers. The goal is to load it to HF and query with vLLM.&lt;/p&gt;\\n\\n&lt;p&gt;Thank you in advance&lt;/p&gt;\\n\\n&lt;p&gt;Sorry, this post &lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1m44tnz","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"Ok-Refrigerator6609","discussion_type":null,"num_comments":4,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1m44tnz/keras_vs_transformers_fine_tuning/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1m44tnz/keras_vs_transformers_fine_tuning/","subreddit_subscribers":501526,"created_utc":1752953435,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n41pe6z","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"TacticalRock","can_mod_post":false,"created_utc":1752954049,"send_replies":true,"parent_id":"t3_1m44tnz","score":3,"author_fullname":"t2_1f2ibv45","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I'd check out Unloth docs for finetuning. That's what I did and it was pretty quick and easy to follow. I specifically used their orpo and kto notebooks and adapted them to fit my use case.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n41pe6z","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;d check out Unloth docs for finetuning. That&amp;#39;s what I did and it was pretty quick and easy to follow. I specifically used their orpo and kto notebooks and adapted them to fit my use case.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m44tnz/keras_vs_transformers_fine_tuning/n41pe6z/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752954049,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m44tnz","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n41tpo7","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"rnosov","can_mod_post":false,"created_utc":1752955446,"send_replies":true,"parent_id":"t3_1m44tnz","score":2,"author_fullname":"t2_18x6fa","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Keras mixed precision is similar to using bfloat16 in transformers so it won't be faster because of that. Are sure that you're training Gemmas of the same size? Like you're training 4b with Keras and 27b with transformers. Also, if you're using LLM to generate training script make sure it's not faking it (ask how I know). H100's have less than 100GB so you've used something else.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n41tpo7","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Keras mixed precision is similar to using bfloat16 in transformers so it won&amp;#39;t be faster because of that. Are sure that you&amp;#39;re training Gemmas of the same size? Like you&amp;#39;re training 4b with Keras and 27b with transformers. Also, if you&amp;#39;re using LLM to generate training script make sure it&amp;#39;s not faking it (ask how I know). H100&amp;#39;s have less than 100GB so you&amp;#39;ve used something else.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m44tnz/keras_vs_transformers_fine_tuning/n41tpo7/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752955446,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m44tnz","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n42g89r","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"un_passant","can_mod_post":false,"created_utc":1752962841,"send_replies":true,"parent_id":"t3_1m44tnz","score":1,"author_fullname":"t2_7rqtc","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Most unlikely. You should share the actual code/notebooks you used because you probably didn't do what you thought you did.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n42g89r","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Most unlikely. You should share the actual code/notebooks you used because you probably didn&amp;#39;t do what you thought you did.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m44tnz/keras_vs_transformers_fine_tuning/n42g89r/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752962841,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m44tnz","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n42ro4l","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Unlucky-Message8866","can_mod_post":false,"created_utc":1752966860,"send_replies":true,"parent_id":"t3_1m44tnz","score":1,"author_fullname":"t2_de5gm7w40","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"transformers is built on top of pytorch and pytorch supports mixed precision in various float formats depending on the gpu arch. there's also optimized kernels (xformers/flash attention) and you can also use third party libraries to go further (bitsandbytes). there's also 8bit optimizers and some other memory optimization techniques (like gradient checkpointing).","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n42ro4l","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;transformers is built on top of pytorch and pytorch supports mixed precision in various float formats depending on the gpu arch. there&amp;#39;s also optimized kernels (xformers/flash attention) and you can also use third party libraries to go further (bitsandbytes). there&amp;#39;s also 8bit optimizers and some other memory optimization techniques (like gradient checkpointing).&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m44tnz/keras_vs_transformers_fine_tuning/n42ro4l/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752966860,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m44tnz","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),n=()=>e.jsx(t,{data:a});export{n as default};
