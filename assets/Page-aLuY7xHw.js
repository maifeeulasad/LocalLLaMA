import{j as e}from"./index-CNyNkRpk.js";import{R as t}from"./RedditPostRenderer-Dza0u9i2.js";import"./index-BUchu_-K.js";const l=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"One of the biggest challenges I keep running into with RAG systems is grounding — the LLM ends up getting too much irrelevant or noisy context from retrieval. This not only affects quality but also drives up token usage and latency.\\n\\nCurious how others are solving this. Are you using rerankers or something else after initial retrieval?\\n\\nWhat solutions are working for people?","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"How to provide most accurate context to LLMs?","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lvqc2u","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.78,"author_flair_background_color":null,"subreddit_type":"public","ups":5,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_14nv7firsf","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":5,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1752085680,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;One of the biggest challenges I keep running into with RAG systems is grounding — the LLM ends up getting too much irrelevant or noisy context from retrieval. This not only affects quality but also drives up token usage and latency.&lt;/p&gt;\\n\\n&lt;p&gt;Curious how others are solving this. Are you using rerankers or something else after initial retrieval?&lt;/p&gt;\\n\\n&lt;p&gt;What solutions are working for people?&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1lvqc2u","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"Silent_Hat_691","discussion_type":null,"num_comments":8,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lvqc2u/how_to_provide_most_accurate_context_to_llms/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lvqc2u/how_to_provide_most_accurate_context_to_llms/","subreddit_subscribers":497023,"created_utc":1752085680,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n29tfrz","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"rockybaby2025","can_mod_post":false,"created_utc":1752104777,"send_replies":true,"parent_id":"t1_n280dn4","score":1,"author_fullname":"t2_1t3515o2d2","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Curious, chunking is automated and can't draw the line.\\n\\nHow do you ensure the text is split clearly? Or do you mean manually splitting large text files from a main source eg a book is manually split down into chapters and then chunk within chapters?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n29tfrz","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Curious, chunking is automated and can&amp;#39;t draw the line.&lt;/p&gt;\\n\\n&lt;p&gt;How do you ensure the text is split clearly? Or do you mean manually splitting large text files from a main source eg a book is manually split down into chapters and then chunk within chapters?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lvqc2u","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lvqc2u/how_to_provide_most_accurate_context_to_llms/n29tfrz/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752104777,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n280dn4","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"fizzy1242","can_mod_post":false,"created_utc":1752085756,"send_replies":true,"parent_id":"t3_1lvqc2u","score":4,"author_fullname":"t2_16zcsx","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"make sure you split the text clearly and use smaller chunks for retrieval so it doesn't fetch too much","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n280dn4","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;make sure you split the text clearly and use smaller chunks for retrieval so it doesn&amp;#39;t fetch too much&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lvqc2u/how_to_provide_most_accurate_context_to_llms/n280dn4/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752085756,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lvqc2u","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n283i67","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Silent_Hat_691","can_mod_post":false,"created_utc":1752086625,"send_replies":true,"parent_id":"t1_n282ghn","score":2,"author_fullname":"t2_14nv7firsf","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yes I am using vector search. Summarization will definitely loose context. Have you tried any rerankers?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n283i67","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yes I am using vector search. Summarization will definitely loose context. Have you tried any rerankers?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lvqc2u","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lvqc2u/how_to_provide_most_accurate_context_to_llms/n283i67/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752086625,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n282ghn","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"buppermint","can_mod_post":false,"created_utc":1752086336,"send_replies":true,"parent_id":"t3_1lvqc2u","score":3,"author_fullname":"t2_1stek4aion","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Are you using embedding similarity as the first retrieval step? A pretty easy improvement is that instead of using the full texts to generate an embedding, first use an LLM to summarize it, then embed the summary.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n282ghn","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Are you using embedding similarity as the first retrieval step? A pretty easy improvement is that instead of using the full texts to generate an embedding, first use an LLM to summarize it, then embed the summary.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lvqc2u/how_to_provide_most_accurate_context_to_llms/n282ghn/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752086336,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lvqc2u","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n28dine","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"complead","can_mod_post":false,"created_utc":1752089402,"send_replies":true,"parent_id":"t3_1lvqc2u","score":2,"author_fullname":"t2_uzn88fhh","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"For tackling grounding issues in RAG systems, efficient indexing can really help control noisy context. [This article](https://pub.towardsai.net/unlocking-the-power-of-efficient-vector-search-in-rag-applications-c2e3a0c551d5) breaks down different vector search methods like IVF and HNSW, which optimize for speed or recall. Pairing the right index with your workload priorities (e.g., using IVF-PQ if RAM is tight) can refine retrieval accuracy before any reranking steps. This might help mitigate the noise and improve context quality.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n28dine","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;For tackling grounding issues in RAG systems, efficient indexing can really help control noisy context. &lt;a href=\\"https://pub.towardsai.net/unlocking-the-power-of-efficient-vector-search-in-rag-applications-c2e3a0c551d5\\"&gt;This article&lt;/a&gt; breaks down different vector search methods like IVF and HNSW, which optimize for speed or recall. Pairing the right index with your workload priorities (e.g., using IVF-PQ if RAM is tight) can refine retrieval accuracy before any reranking steps. This might help mitigate the noise and improve context quality.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lvqc2u/how_to_provide_most_accurate_context_to_llms/n28dine/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752089402,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lvqc2u","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n28k39b","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"GhostArchitect01","can_mod_post":false,"created_utc":1752091218,"send_replies":true,"parent_id":"t3_1lvqc2u","score":1,"author_fullname":"t2_1miosjge4f","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"https://github.com/GhostArchitect01/token-decoder-maps","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n28k39b","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;&lt;a href=\\"https://github.com/GhostArchitect01/token-decoder-maps\\"&gt;https://github.com/GhostArchitect01/token-decoder-maps&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lvqc2u/how_to_provide_most_accurate_context_to_llms/n28k39b/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752091218,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lvqc2u","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n283bl4","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Agreeable-Prompt-666","can_mod_post":false,"created_utc":1752086575,"send_replies":true,"parent_id":"t3_1lvqc2u","score":2,"author_fullname":"t2_1l3z4stvkq","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"there's literally a million ways to do this, depends what you want? a chat bot will require more ordered recencey context, while a librarian style bot will require more relevant context. then each of those methods can be a mix of each other.\\n\\nfor relevancy there's alot of methods to score messages to rank them, some very simple, some complicated. then the trick is, to tune all that stuff with assigning weights, and max context lengths to the various knobs to see what gets returned for any given user entry.\\n\\nif hardware is available you could do multiple calls, a light weight pre-processor LLM, subject/topic definition, various ways to do similarity checks to find the historic subjects, then go to town.... its all a bit too much\\n\\ngo with prebuilt python memory frameworks, theres alot of free ones. unless you want to learn, but that will likely drive you crazy.","edited":1752086849,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n283bl4","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;there&amp;#39;s literally a million ways to do this, depends what you want? a chat bot will require more ordered recencey context, while a librarian style bot will require more relevant context. then each of those methods can be a mix of each other.&lt;/p&gt;\\n\\n&lt;p&gt;for relevancy there&amp;#39;s alot of methods to score messages to rank them, some very simple, some complicated. then the trick is, to tune all that stuff with assigning weights, and max context lengths to the various knobs to see what gets returned for any given user entry.&lt;/p&gt;\\n\\n&lt;p&gt;if hardware is available you could do multiple calls, a light weight pre-processor LLM, subject/topic definition, various ways to do similarity checks to find the historic subjects, then go to town.... its all a bit too much&lt;/p&gt;\\n\\n&lt;p&gt;go with prebuilt python memory frameworks, theres alot of free ones. unless you want to learn, but that will likely drive you crazy.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lvqc2u/how_to_provide_most_accurate_context_to_llms/n283bl4/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752086575,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lvqc2u","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}}]`),s=()=>e.jsx(t,{data:l});export{s as default};
