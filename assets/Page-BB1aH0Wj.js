import{j as e}from"./index-DLSqWzaI.js";import{R as l}from"./RedditPostRenderer-CysRo2D_.js";import"./index-COXiL3Lo.js";const a=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"In the overview of the NVIDIA RTX PRO 6000 Blackwell GPU Max-Q Workstation Edition, it says, “Seamlessly scale from one to four GPUs, multiplying your compute power and enabling you to pioneer new frontiers in AI, data science, and graphics.”  \\nDoes this mean that if I want to load a 70B parameter LLM using Fully Sharded Data Parallel (FSDP), the maximum number of GPUs I can utilize is four?  \\nAnd with each GPU having 96GB of memory, does that mean the maximum available VRAM for a single model would be 96 \\\\* 4 = 384GB?","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Limitation of NVIDIA RTX PRO 6000 Blackwell Max-Q Workstation Edition","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lvaq6n","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.72,"author_flair_background_color":null,"subreddit_type":"public","ups":3,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_c56fvsrk","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":3,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1752038844,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;In the overview of the NVIDIA RTX PRO 6000 Blackwell GPU Max-Q Workstation Edition, it says, “Seamlessly scale from one to four GPUs, multiplying your compute power and enabling you to pioneer new frontiers in AI, data science, and graphics.”&lt;br/&gt;\\nDoes this mean that if I want to load a 70B parameter LLM using Fully Sharded Data Parallel (FSDP), the maximum number of GPUs I can utilize is four?&lt;br/&gt;\\nAnd with each GPU having 96GB of memory, does that mean the maximum available VRAM for a single model would be 96 * 4 = 384GB?&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1lvaq6n","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"Normal-Bookkeeper-86","discussion_type":null,"num_comments":4,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lvaq6n/limitation_of_nvidia_rtx_pro_6000_blackwell_maxq/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lvaq6n/limitation_of_nvidia_rtx_pro_6000_blackwell_maxq/","subreddit_subscribers":497025,"created_utc":1752038844,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2a1n2v","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Normal-Bookkeeper-86","can_mod_post":false,"send_replies":true,"parent_id":"t1_n29iqed","score":1,"author_fullname":"t2_c56fvsrk","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I really appreciate your confirmation.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2a1n2v","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I really appreciate your confirmation.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lvaq6n","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lvaq6n/limitation_of_nvidia_rtx_pro_6000_blackwell_maxq/n2a1n2v/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752107584,"author_flair_text":null,"treatment_tags":[],"created_utc":1752107584,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n29iqed","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"eloquentemu","can_mod_post":false,"send_replies":true,"parent_id":"t1_n25g1j2","score":1,"author_fullname":"t2_lpdsy","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yeah, that should be fine.  I think it's just that Nvidia expects you'd use the server edition for that instead of the Max-Q.  Keep in mind that you can always limit the power to 300W on the server edition.  The Max-Q is really for ATX (or other low airflow cases) to be able to install several cards.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n29iqed","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yeah, that should be fine.  I think it&amp;#39;s just that Nvidia expects you&amp;#39;d use the server edition for that instead of the Max-Q.  Keep in mind that you can always limit the power to 300W on the server edition.  The Max-Q is really for ATX (or other low airflow cases) to be able to install several cards.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lvaq6n","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lvaq6n/limitation_of_nvidia_rtx_pro_6000_blackwell_maxq/n29iqed/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752101304,"author_flair_text":null,"treatment_tags":[],"created_utc":1752101304,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n25g1j2","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Normal-Bookkeeper-86","can_mod_post":false,"created_utc":1752057361,"send_replies":true,"parent_id":"t1_n24tbkm","score":0,"author_fullname":"t2_c56fvsrk","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Thank you for reply. As your mention, I would  install the 8 Max-Q GPU  in a server so that I could use 8 GPU to load big LLM all at once probably.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n25g1j2","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thank you for reply. As your mention, I would  install the 8 Max-Q GPU  in a server so that I could use 8 GPU to load big LLM all at once probably.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lvaq6n","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lvaq6n/limitation_of_nvidia_rtx_pro_6000_blackwell_maxq/n25g1j2/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752057361,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}},"user_reports":[],"saved":false,"id":"n24tbkm","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"eloquentemu","can_mod_post":false,"created_utc":1752044855,"send_replies":true,"parent_id":"t3_1lvaq6n","score":3,"author_fullname":"t2_lpdsy","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"That really depends on software not the card.\\n\\n\\nMy guess is that they market it as \\"4\\" because the Max-Q is targeting workstations which usually will only fit 4 dual slot cards.  A dedicated GPU server will usually hold 8 (using the passively cooled version) while the blow-through card isn't meant to be densely packed, probably only ideally having one or two in a workstation.\\n\\n\\nFour is also 1200W which gets pretty close to the ~1500W that a 120V 15A circuit (US standard) can supply, especially when you consider the CPU and other power draw.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n24tbkm","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;That really depends on software not the card.&lt;/p&gt;\\n\\n&lt;p&gt;My guess is that they market it as &amp;quot;4&amp;quot; because the Max-Q is targeting workstations which usually will only fit 4 dual slot cards.  A dedicated GPU server will usually hold 8 (using the passively cooled version) while the blow-through card isn&amp;#39;t meant to be densely packed, probably only ideally having one or two in a workstation.&lt;/p&gt;\\n\\n&lt;p&gt;Four is also 1200W which gets pretty close to the ~1500W that a 120V 15A circuit (US standard) can supply, especially when you consider the CPU and other power draw.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lvaq6n/limitation_of_nvidia_rtx_pro_6000_blackwell_maxq/n24tbkm/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752044855,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lvaq6n","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}}]`),n=()=>e.jsx(l,{data:a});export{n as default};
