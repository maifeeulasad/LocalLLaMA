import{j as e}from"./index-CNyNkRpk.js";import{R as t}from"./RedditPostRenderer-Dza0u9i2.js";import"./index-BUchu_-K.js";const a=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Dear LocalLLaMA,\\n\\nI have been a member since the very beginning (ish) and have learned so much from many of you. I’m hoping to get some more specific advice on the best vision-language models for extracting cursive handwritten text from a large set of documents (about 400 000 images). I have access to A40, A100, and V100 GPUs and can run multiple GPUs of the same type in parallel if necessary.\\n\\nHere’s my current workflow:\\n\\n1. I feed a high-quality scan into a YOLO model to extract the region containing the name information.\\n2. I construct a prompt asking the model to provide the full name (surname + given name). I already have each individual’s surname (from previous manual work) and include it in the prompt.\\n3. I feed the cropped image and prompt simultaneously to two models:\\n   * [Qwen/Qwen2.5-VL-7B-Instruct](https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct)\\n   * [OpenGVLab/InternVL3-8B](https://huggingface.co/OpenGVLab/InternVL3-8B)\\n4. Some light post-processing later, I only accept results where both models agree 100%, to avoid errors and hallucinations (since I assume the their errors are uncorrelated).\\n\\nWith this setup, only about 25–35 % of outputs agree. Qwen2.5-VL outperforms InternVL3 by about 18–20 %, but as you see my performance is limited by the weaker model. I’ve also experimented with Moondream (which performed poorly for my task, but still ok considering it's size) and THUDM/GLM-4.1V-9B-Thinking (which didn’t impress during my short experiment and is slow). I even fine-tuned Qwen2.5-VL-7B locally on 2 000 gold-standard sample using Unsloth, but saw no significant improvement over the base model. The smart play would be to finetune InternVL3 since its the weak link but I'm not good at finetuning and it seems more complex because it needs to be fed split images that are stitched together by the model (and more importantly unsure if Unsloth ready)\\n\\nI have a ton of question but these should cover most:\\n\\n* Are there alternative VL models I should consider instead of InternVL3?\\n* Are there quantized versions of these (or other) models that offer similar accuracy with lower VRAM requirements, so I can process larger batches cheapely at least and can rerun the images until I get agreement?\\n* Any other suggestions or insights on improving my workflow would be immensely appreciated.\\n\\nThank you again guys!","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Need advice on how to improve Handwritten Text Recognition of names using Vision models (for academic research purposes)","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lwpi5p","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":1,"author_flair_background_color":null,"subreddit_type":"public","ups":2,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_10vzjm","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":2,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"post_hint":"self","content_categories":null,"is_self":true,"mod_note":null,"created":1752184678,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Dear LocalLLaMA,&lt;/p&gt;\\n\\n&lt;p&gt;I have been a member since the very beginning (ish) and have learned so much from many of you. I’m hoping to get some more specific advice on the best vision-language models for extracting cursive handwritten text from a large set of documents (about 400 000 images). I have access to A40, A100, and V100 GPUs and can run multiple GPUs of the same type in parallel if necessary.&lt;/p&gt;\\n\\n&lt;p&gt;Here’s my current workflow:&lt;/p&gt;\\n\\n&lt;ol&gt;\\n&lt;li&gt;I feed a high-quality scan into a YOLO model to extract the region containing the name information.&lt;/li&gt;\\n&lt;li&gt;I construct a prompt asking the model to provide the full name (surname + given name). I already have each individual’s surname (from previous manual work) and include it in the prompt.&lt;/li&gt;\\n&lt;li&gt;I feed the cropped image and prompt simultaneously to two models:\\n\\n&lt;ul&gt;\\n&lt;li&gt;&lt;a href=\\"https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct\\"&gt;Qwen/Qwen2.5-VL-7B-Instruct&lt;/a&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;a href=\\"https://huggingface.co/OpenGVLab/InternVL3-8B\\"&gt;OpenGVLab/InternVL3-8B&lt;/a&gt;&lt;/li&gt;\\n&lt;/ul&gt;&lt;/li&gt;\\n&lt;li&gt;Some light post-processing later, I only accept results where both models agree 100%, to avoid errors and hallucinations (since I assume the their errors are uncorrelated).&lt;/li&gt;\\n&lt;/ol&gt;\\n\\n&lt;p&gt;With this setup, only about 25–35 % of outputs agree. Qwen2.5-VL outperforms InternVL3 by about 18–20 %, but as you see my performance is limited by the weaker model. I’ve also experimented with Moondream (which performed poorly for my task, but still ok considering it&amp;#39;s size) and THUDM/GLM-4.1V-9B-Thinking (which didn’t impress during my short experiment and is slow). I even fine-tuned Qwen2.5-VL-7B locally on 2 000 gold-standard sample using Unsloth, but saw no significant improvement over the base model. The smart play would be to finetune InternVL3 since its the weak link but I&amp;#39;m not good at finetuning and it seems more complex because it needs to be fed split images that are stitched together by the model (and more importantly unsure if Unsloth ready)&lt;/p&gt;\\n\\n&lt;p&gt;I have a ton of question but these should cover most:&lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;Are there alternative VL models I should consider instead of InternVL3?&lt;/li&gt;\\n&lt;li&gt;Are there quantized versions of these (or other) models that offer similar accuracy with lower VRAM requirements, so I can process larger batches cheapely at least and can rerun the images until I get agreement?&lt;/li&gt;\\n&lt;li&gt;Any other suggestions or insights on improving my workflow would be immensely appreciated.&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;p&gt;Thank you again guys!&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"preview":{"images":[{"source":{"url":"https://external-preview.redd.it/FjlIBWa3fCz4O3VIWU3O9fF8Jb7iY6HD2x0Bcm3wfGI.png?auto=webp&amp;s=17bf72c4d47d131612ab2f5b554d85da02a85539","width":1200,"height":648},"resolutions":[{"url":"https://external-preview.redd.it/FjlIBWa3fCz4O3VIWU3O9fF8Jb7iY6HD2x0Bcm3wfGI.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=0d0bf812fba94f9f50669a2e76037d0e7886bde2","width":108,"height":58},{"url":"https://external-preview.redd.it/FjlIBWa3fCz4O3VIWU3O9fF8Jb7iY6HD2x0Bcm3wfGI.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=99a214b39375ee0ec6cdcffc1958d0a4b34e4690","width":216,"height":116},{"url":"https://external-preview.redd.it/FjlIBWa3fCz4O3VIWU3O9fF8Jb7iY6HD2x0Bcm3wfGI.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=8e945b1c768d68948eaa7f830a3b219c2df4c13c","width":320,"height":172},{"url":"https://external-preview.redd.it/FjlIBWa3fCz4O3VIWU3O9fF8Jb7iY6HD2x0Bcm3wfGI.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=b846b869885ffbeadf6199126a3c0fab1ed22be2","width":640,"height":345},{"url":"https://external-preview.redd.it/FjlIBWa3fCz4O3VIWU3O9fF8Jb7iY6HD2x0Bcm3wfGI.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=c94fbe6213102c60c53f38bc3207e1f1bde9733a","width":960,"height":518},{"url":"https://external-preview.redd.it/FjlIBWa3fCz4O3VIWU3O9fF8Jb7iY6HD2x0Bcm3wfGI.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e2dee8bb7abc532aeb2cfeec783420f84dba72c6","width":1080,"height":583}],"variants":{},"id":"FjlIBWa3fCz4O3VIWU3O9fF8Jb7iY6HD2x0Bcm3wfGI"}],"enabled":false},"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1lwpi5p","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"joosefm9","discussion_type":null,"num_comments":3,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lwpi5p/need_advice_on_how_to_improve_handwritten_text/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lwpi5p/need_advice_on_how_to_improve_handwritten_text/","subreddit_subscribers":497504,"created_utc":1752184678,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2gc5aj","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"zipperlein","can_mod_post":false,"created_utc":1752189103,"send_replies":true,"parent_id":"t3_1lwpi5p","score":2,"author_fullname":"t2_x3duw","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I don't have much expierence with vision-models.I don't know what's the purpose of your work. If your chasing accuracy or speed. But if GPUs are not a problem u could try a escalating appraoch maybe. Qwen2.5 and InternVL3 both go up to 72B size. Feed the input into the smallest models first and go up in size if they do not agree? Otherwise if you are not limited to VL-models and just need simething fast for the task, there are dedicated OCR models as well. Sth like this for example. \\n\\n  \\n[https://huggingface.co/microsoft/trocr-base-handwritten](https://huggingface.co/microsoft/trocr-base-handwritten)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2gc5aj","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I don&amp;#39;t have much expierence with vision-models.I don&amp;#39;t know what&amp;#39;s the purpose of your work. If your chasing accuracy or speed. But if GPUs are not a problem u could try a escalating appraoch maybe. Qwen2.5 and InternVL3 both go up to 72B size. Feed the input into the smallest models first and go up in size if they do not agree? Otherwise if you are not limited to VL-models and just need simething fast for the task, there are dedicated OCR models as well. Sth like this for example. &lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://huggingface.co/microsoft/trocr-base-handwritten\\"&gt;https://huggingface.co/microsoft/trocr-base-handwritten&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwpi5p/need_advice_on_how_to_improve_handwritten_text/n2gc5aj/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752189103,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lwpi5p","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2hcyjr","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"exacly","can_mod_post":false,"created_utc":1752201920,"send_replies":true,"parent_id":"t3_1lwpi5p","score":2,"author_fullname":"t2_tdjc5sg","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Did you evaluate Mistral-Small 3.2 and Gemma3 for your use case? Both models have vision capabilities with  Unsloth quants available.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2hcyjr","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Did you evaluate Mistral-Small 3.2 and Gemma3 for your use case? Both models have vision capabilities with  Unsloth quants available.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwpi5p/need_advice_on_how_to_improve_handwritten_text/n2hcyjr/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752201920,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lwpi5p","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2ihw7c","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"erazortt","can_mod_post":false,"created_utc":1752221307,"send_replies":true,"parent_id":"t3_1lwpi5p","score":2,"author_fullname":"t2_6z7m9i7r","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I second Gemma3. The 27B appears to being really good at visual and at reading handwriting. The QAT version (or other Q4, Q5 or even Q6 quants) will run great on just a single 24GB VRAM GPU.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2ihw7c","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I second Gemma3. The 27B appears to being really good at visual and at reading handwriting. The QAT version (or other Q4, Q5 or even Q6 quants) will run great on just a single 24GB VRAM GPU.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwpi5p/need_advice_on_how_to_improve_handwritten_text/n2ihw7c/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752221307,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lwpi5p","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}}]`),o=()=>e.jsx(t,{data:a});export{o as default};
