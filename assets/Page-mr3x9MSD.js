import{j as e}from"./index-xfnGEtuL.js";import{R as t}from"./RedditPostRenderer-DAZRIZZK.js";import"./index-BaNn5-RR.js";const a=JSON.parse('[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Disclaimer: This post is a translation of a long piece written by an anonymous user claiming to be a former Huawei Noah’s Ark Lab employee involved in the development of the Pangu LLM. The authenticity of the claims cannot be independently verified, so please read with caution.\\n\\nThe original GitHub repo: https://github.com/HW-whistleblower/True-Story-of-Pangu\\n\\n---\\n\\nA user claiming to be part of the Pangu LLM team at Huawei’s Noah’s Ark Lab has posted a detailed, emotionally charged account describing what they call the “bitterness and darkness” behind Pangu’s development. The story spans from technical failures and internal conflicts to allegations of misconduct at both technical and leadership levels. Here are the main takeaways:\\n\\nTeam Structure and Work Conditions: The Pangu development was allegedly conducted under the \\"Siye\\" (四野) organizational umbrella, with multiple vertical teams (e.g., 4th and 16th). Developers were relocated to Suzhou for long periods, working weekends under extreme pressure. Despite some corporate perks (like afternoon tea), burnout and separation from families were widespread.\\n\\nFrom Research to Delivery: Originally envisioned as a research-oriented lab, the Noah team became a delivery team with constant internal reviews and product integration deadlines (with XiaoYi \\"Huawei Voice Assistant\\", Huawei Cloud, etc.). Many members left due to overwork and disillusionment.\\n\\nTraining Struggles and Tokenizer Failures: Early models like the 71B and 135B dense versions allegedly suffered from extremely inefficient tokenizers—each symbol or character took one token. Attempts to switch tokenizers (borrowing from the smaller model lab) resulted in major performance regressions and bugs.\\n\\nAllegations of “Shelling” from Other Models: The author accuses an internal “small model lab,” led by a director named Wang Yunhe, of repeatedly \\"wrapping\\" external models like Qwen 1.5 and DeepSeek-v3, tweaking them slightly, and passing them off as self-developed Huawei models. Specific examples include:\\n\\nThe so-called “Pangu 135B v2,” alleged to be a lightly modified Qwen-110B.\\n\\nA 72B MoE model claimed to be derived from Qwen 2.5 14B.\\n\\nThe supposed 718B MoE training effort, which allegedly just fine-tuned a frozen DeepSeek-v3 checkpoint without changing file paths or model code names.\\n\\n\\nInternal Resistance and Third-Gen Efforts: Some engineers, including the author, pushed for a clean-slate, from-scratch training approach using Huawei’s Ascend chips. The 38B v3 and later 135B v3 were described as genuine efforts with stable training curves, no loss spikes, and results comparable to domestic competitors in 2024. These efforts were seen as a point of pride and a rare case of “real” innovation.\\n\\nInternal Politics and Misattribution: Despite the work of the main team, the small model lab allegedly took credit, absorbed code and data, and bypassed internal governance mechanisms (e.g., bloodline tracking, version control). The author says this created a two-tier system where political connections outweighed technical contributions.\\n\\nFinal Disillusionment: The whistleblower decided to resign and requested their name be removed from published technical reports, calling them a permanent stain. The person admits their complicity in earlier decisions and expresses regret for not standing up sooner.\\n\\n---\\n\\nIt seems the author was in great emotional distress while writing this. Regardless of the story’s authenticity, I wish him/her peace and clarity in the future.","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"After Huawei Pangu LLM faced plagiarism allegations, an anonymous insider shares their side of the story","link_flair_richtext":[{"e":"text","t":"Discussion"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":true,"name":"t3_1lsyd4g","quarantine":false,"link_flair_text_color":"light","upvote_ratio":1,"author_flair_background_color":null,"subreddit_type":"public","ups":3,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_sqi8xxun","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Discussion","can_mod_post":false,"score":3,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1751798966,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Disclaimer: This post is a translation of a long piece written by an anonymous user claiming to be a former Huawei Noah’s Ark Lab employee involved in the development of the Pangu LLM. The authenticity of the claims cannot be independently verified, so please read with caution.&lt;/p&gt;\\n\\n&lt;p&gt;The original GitHub repo: &lt;a href=\\"https://github.com/HW-whistleblower/True-Story-of-Pangu\\"&gt;https://github.com/HW-whistleblower/True-Story-of-Pangu&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;hr/&gt;\\n\\n&lt;p&gt;A user claiming to be part of the Pangu LLM team at Huawei’s Noah’s Ark Lab has posted a detailed, emotionally charged account describing what they call the “bitterness and darkness” behind Pangu’s development. The story spans from technical failures and internal conflicts to allegations of misconduct at both technical and leadership levels. Here are the main takeaways:&lt;/p&gt;\\n\\n&lt;p&gt;Team Structure and Work Conditions: The Pangu development was allegedly conducted under the &amp;quot;Siye&amp;quot; (四野) organizational umbrella, with multiple vertical teams (e.g., 4th and 16th). Developers were relocated to Suzhou for long periods, working weekends under extreme pressure. Despite some corporate perks (like afternoon tea), burnout and separation from families were widespread.&lt;/p&gt;\\n\\n&lt;p&gt;From Research to Delivery: Originally envisioned as a research-oriented lab, the Noah team became a delivery team with constant internal reviews and product integration deadlines (with XiaoYi &amp;quot;Huawei Voice Assistant&amp;quot;, Huawei Cloud, etc.). Many members left due to overwork and disillusionment.&lt;/p&gt;\\n\\n&lt;p&gt;Training Struggles and Tokenizer Failures: Early models like the 71B and 135B dense versions allegedly suffered from extremely inefficient tokenizers—each symbol or character took one token. Attempts to switch tokenizers (borrowing from the smaller model lab) resulted in major performance regressions and bugs.&lt;/p&gt;\\n\\n&lt;p&gt;Allegations of “Shelling” from Other Models: The author accuses an internal “small model lab,” led by a director named Wang Yunhe, of repeatedly &amp;quot;wrapping&amp;quot; external models like Qwen 1.5 and DeepSeek-v3, tweaking them slightly, and passing them off as self-developed Huawei models. Specific examples include:&lt;/p&gt;\\n\\n&lt;p&gt;The so-called “Pangu 135B v2,” alleged to be a lightly modified Qwen-110B.&lt;/p&gt;\\n\\n&lt;p&gt;A 72B MoE model claimed to be derived from Qwen 2.5 14B.&lt;/p&gt;\\n\\n&lt;p&gt;The supposed 718B MoE training effort, which allegedly just fine-tuned a frozen DeepSeek-v3 checkpoint without changing file paths or model code names.&lt;/p&gt;\\n\\n&lt;p&gt;Internal Resistance and Third-Gen Efforts: Some engineers, including the author, pushed for a clean-slate, from-scratch training approach using Huawei’s Ascend chips. The 38B v3 and later 135B v3 were described as genuine efforts with stable training curves, no loss spikes, and results comparable to domestic competitors in 2024. These efforts were seen as a point of pride and a rare case of “real” innovation.&lt;/p&gt;\\n\\n&lt;p&gt;Internal Politics and Misattribution: Despite the work of the main team, the small model lab allegedly took credit, absorbed code and data, and bypassed internal governance mechanisms (e.g., bloodline tracking, version control). The author says this created a two-tier system where political connections outweighed technical contributions.&lt;/p&gt;\\n\\n&lt;p&gt;Final Disillusionment: The whistleblower decided to resign and requested their name be removed from published technical reports, calling them a permanent stain. The person admits their complicity in earlier decisions and expresses regret for not standing up sooner.&lt;/p&gt;\\n\\n&lt;hr/&gt;\\n\\n&lt;p&gt;It seems the author was in great emotional distress while writing this. Regardless of the story’s authenticity, I wish him/her peace and clarity in the future.&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"5f921ea4-c7bc-11ed-9c23-3a00622979b4","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#646d73","id":"1lsyd4g","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"nekofneko","discussion_type":null,"num_comments":1,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lsyd4g/after_huawei_pangu_llm_faced_plagiarism/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lsyd4g/after_huawei_pangu_llm_faced_plagiarism/","subreddit_subscribers":494986,"created_utc":1751798966,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1m605h","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"sunshinecheung","can_mod_post":false,"created_utc":1751799247,"send_replies":true,"parent_id":"t3_1lsyd4g","score":2,"author_fullname":"t2_u398xzta","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Pangu-Distill-Qwen/Deepseek lol","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1m605h","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Pangu-Distill-Qwen/Deepseek lol&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lsyd4g/after_huawei_pangu_llm_faced_plagiarism/n1m605h/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751799247,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lsyd4g","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}}]'),s=()=>e.jsx(t,{data:a});export{s as default};
