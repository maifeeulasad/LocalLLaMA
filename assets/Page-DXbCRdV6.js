import{j as e}from"./index-F0NXdzZX.js";import{R as t}from"./RedditPostRenderer-CoEZB1dt.js";import"./index-DrtravXm.js";const n=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"I've been trying to find the best option of LLM to run for RP for my rig. I've gone through a few and decided to make a little benchmark of what I found to be good LLMs for roleplaying.\\n\\nSystem Info:  \\nNVIDIA system information report created on: 07/02/2025 00:29:00\\n\\nNVIDIA App version: 11.0.4.\\n\\nOperating system: Microsoft Windows 11 Home, Version 10.0\\n\\nDirectX runtime version: DirectX 12\\n\\nDriver: Game Ready Driver - 576.88 - Tue Jul 1, 2025\\n\\nCPU: 13th Gen Intel(R) Core(TM) i9-13980HX\\n\\nRAM: 64.0 GB\\n\\nStorage: SSD - 3.6 TB\\n\\nGraphics card\\n\\nGPU processor: NVIDIA GeForce RTX 4070 Laptop GPU\\n\\nDirect3D feature level: 12\\\\_1\\n\\nCUDA cores: 4608\\n\\nGraphics clock: 2175 MHz\\n\\nMax-Q technologies: Gen-5\\n\\nDynamic Boost: Yes\\n\\nWhisperMode: No\\n\\nAdvanced Optimus: Yes\\n\\nMaximum graphics power: 140 W\\n\\nMemory data rate: 16.00 Gbps\\n\\nMemory interface: 128-bit\\n\\nMemory bandwidth: 256.032 GB/s\\n\\nTotal available graphics memory: 40765 MB\\n\\nDedicated video memory: 8188 MB GDDR6\\n\\nSystem video memory: 0 MB\\n\\nShared system memory: 32577 MB\\n\\n\\\\*\\\\*RTX 4070 Laptop LLM Performance Summary (8GB VRAM, i9-13980HX, 56GB RAM, 8 Threads)\\\\*\\\\*\\n\\n  \\nViolet-Eclipse-2x12B: - Model Size: 24B (MoE) - Quantization: Q4\\\\_K\\\\_S - Total Layers: 41 (25/41 GPU Offloaded - 61%) - Context Size: 16,000 Tokens - GPU VRAM Used: \\\\~7.6 GB - Processing Speed: 478.25 T/s - Generation Speed: 4.53 T/s - Notes: Fastest generation speed for conversational use. -   \\n  \\nSnowpiercer-15B: - Model Size: 15B - Quantization: Q4\\\\_K\\\\_S - Total Layers: 51 (35/51 GPU Offloaded - 68.6%) - Context Size: 24,000 Tokens - GPU VRAM Used: \\\\~7.2 GB - Processing Speed: 584.86 T/s - Generation Speed: 3.35 T/s - Notes: Good balance of context and speed, higher GPU layer offload % for its size. -   \\n  \\nSnowpiercer-15B (Original Run): - Model Size: 15B - Quantization: Q4\\\\_K\\\\_S - Total Layers: 51 (32/51 GPU Offloaded - 62.7%) - Context Size: 32,000 Tokens - GPU VRAM Used: \\\\~7.1 GB - Processing Speed: 489.47 T/s - Generation Speed: 2.99 T/s - Notes: Original run with higher context, slightly lower speed. -   \\n  \\nMistral-Nemo-12B: - Model Size: 12B - Quantization: Q4\\\\_K\\\\_S - Total Layers: 40 (28/40 GPU Offloaded - 70%) - Context Size: 65,536 Tokens (Exceptional!) - GPU VRAM Used: \\\\~7.2 GB - Processing Speed: 413.61 T/s - Generation Speed: 2.01 T/s - Notes: Exceptional context depth on 8GB VRAM; VRAM efficient model file. Slower generation.","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Laptop Benchmark for 4070 8GB VRAM, 64GB RAM","link_flair_richtext":[{"e":"text","t":"Discussion"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":true,"name":"t3_1lpn5k5","quarantine":false,"link_flair_text_color":"light","upvote_ratio":1,"author_flair_background_color":null,"subreddit_type":"public","ups":1,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_rakjd58ob","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Discussion","can_mod_post":false,"score":1,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":1751432415,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1751431927,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;ve been trying to find the best option of LLM to run for RP for my rig. I&amp;#39;ve gone through a few and decided to make a little benchmark of what I found to be good LLMs for roleplaying.&lt;/p&gt;\\n\\n&lt;p&gt;System Info:&lt;br/&gt;\\nNVIDIA system information report created on: 07/02/2025 00:29:00&lt;/p&gt;\\n\\n&lt;p&gt;NVIDIA App version: 11.0.4.&lt;/p&gt;\\n\\n&lt;p&gt;Operating system: Microsoft Windows 11 Home, Version 10.0&lt;/p&gt;\\n\\n&lt;p&gt;DirectX runtime version: DirectX 12&lt;/p&gt;\\n\\n&lt;p&gt;Driver: Game Ready Driver - 576.88 - Tue Jul 1, 2025&lt;/p&gt;\\n\\n&lt;p&gt;CPU: 13th Gen Intel(R) Core(TM) i9-13980HX&lt;/p&gt;\\n\\n&lt;p&gt;RAM: 64.0 GB&lt;/p&gt;\\n\\n&lt;p&gt;Storage: SSD - 3.6 TB&lt;/p&gt;\\n\\n&lt;p&gt;Graphics card&lt;/p&gt;\\n\\n&lt;p&gt;GPU processor: NVIDIA GeForce RTX 4070 Laptop GPU&lt;/p&gt;\\n\\n&lt;p&gt;Direct3D feature level: 12_1&lt;/p&gt;\\n\\n&lt;p&gt;CUDA cores: 4608&lt;/p&gt;\\n\\n&lt;p&gt;Graphics clock: 2175 MHz&lt;/p&gt;\\n\\n&lt;p&gt;Max-Q technologies: Gen-5&lt;/p&gt;\\n\\n&lt;p&gt;Dynamic Boost: Yes&lt;/p&gt;\\n\\n&lt;p&gt;WhisperMode: No&lt;/p&gt;\\n\\n&lt;p&gt;Advanced Optimus: Yes&lt;/p&gt;\\n\\n&lt;p&gt;Maximum graphics power: 140 W&lt;/p&gt;\\n\\n&lt;p&gt;Memory data rate: 16.00 Gbps&lt;/p&gt;\\n\\n&lt;p&gt;Memory interface: 128-bit&lt;/p&gt;\\n\\n&lt;p&gt;Memory bandwidth: 256.032 GB/s&lt;/p&gt;\\n\\n&lt;p&gt;Total available graphics memory: 40765 MB&lt;/p&gt;\\n\\n&lt;p&gt;Dedicated video memory: 8188 MB GDDR6&lt;/p&gt;\\n\\n&lt;p&gt;System video memory: 0 MB&lt;/p&gt;\\n\\n&lt;p&gt;Shared system memory: 32577 MB&lt;/p&gt;\\n\\n&lt;p&gt;**RTX 4070 Laptop LLM Performance Summary (8GB VRAM, i9-13980HX, 56GB RAM, 8 Threads)**&lt;/p&gt;\\n\\n&lt;p&gt;Violet-Eclipse-2x12B: - Model Size: 24B (MoE) - Quantization: Q4_K_S - Total Layers: 41 (25/41 GPU Offloaded - 61%) - Context Size: 16,000 Tokens - GPU VRAM Used: ~7.6 GB - Processing Speed: 478.25 T/s - Generation Speed: 4.53 T/s - Notes: Fastest generation speed for conversational use. -   &lt;/p&gt;\\n\\n&lt;p&gt;Snowpiercer-15B: - Model Size: 15B - Quantization: Q4_K_S - Total Layers: 51 (35/51 GPU Offloaded - 68.6%) - Context Size: 24,000 Tokens - GPU VRAM Used: ~7.2 GB - Processing Speed: 584.86 T/s - Generation Speed: 3.35 T/s - Notes: Good balance of context and speed, higher GPU layer offload % for its size. -   &lt;/p&gt;\\n\\n&lt;p&gt;Snowpiercer-15B (Original Run): - Model Size: 15B - Quantization: Q4_K_S - Total Layers: 51 (32/51 GPU Offloaded - 62.7%) - Context Size: 32,000 Tokens - GPU VRAM Used: ~7.1 GB - Processing Speed: 489.47 T/s - Generation Speed: 2.99 T/s - Notes: Original run with higher context, slightly lower speed. -   &lt;/p&gt;\\n\\n&lt;p&gt;Mistral-Nemo-12B: - Model Size: 12B - Quantization: Q4_K_S - Total Layers: 40 (28/40 GPU Offloaded - 70%) - Context Size: 65,536 Tokens (Exceptional!) - GPU VRAM Used: ~7.2 GB - Processing Speed: 413.61 T/s - Generation Speed: 2.01 T/s - Notes: Exceptional context depth on 8GB VRAM; VRAM efficient model file. Slower generation.&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":true,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"5f921ea4-c7bc-11ed-9c23-3a00622979b4","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#646d73","id":"1lpn5k5","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"Hyena_Cackle","discussion_type":null,"num_comments":0,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lpn5k5/laptop_benchmark_for_4070_8gb_vram_64gb_ram/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lpn5k5/laptop_benchmark_for_4070_8gb_vram_64gb_ram/","subreddit_subscribers":493457,"created_utc":1751431927,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[],"before":null}}]`),r=()=>e.jsx(t,{data:n});export{r as default};
