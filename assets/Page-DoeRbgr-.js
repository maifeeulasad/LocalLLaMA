import{j as e}from"./index-CeRg6Q3f.js";import{R as t}from"./RedditPostRenderer-D7n1g-D8.js";import"./index-DPToWe3n.js";const a=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Hey Yall, I currently have access to a lot of compute through my schools research computer (tons of A100s) and I can use upto a reasonable amount of compute. I have never run any llm locally because I am on a Macbook Air. So I want to know good ways to use this compute without going overkill I can easily procure and use atleast more than 1 a100 but its done in like time allotted batches. I want to fine tune an llm model to test around (use to train it on successful cover letters and college essays) in order to use it. What are some good models I can use. I have upto 100 gb of storage and lots of compute. Thanks in advance. I would like to maybe also use some coding llms.","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Looking for \\"local\\" models to run on Super Computer","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1luacs4","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.5,"author_flair_background_color":null,"subreddit_type":"public","ups":0,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_93yl561w","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":0,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":1751934876,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1751933396,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Hey Yall, I currently have access to a lot of compute through my schools research computer (tons of A100s) and I can use upto a reasonable amount of compute. I have never run any llm locally because I am on a Macbook Air. So I want to know good ways to use this compute without going overkill I can easily procure and use atleast more than 1 a100 but its done in like time allotted batches. I want to fine tune an llm model to test around (use to train it on successful cover letters and college essays) in order to use it. What are some good models I can use. I have upto 100 gb of storage and lots of compute. Thanks in advance. I would like to maybe also use some coding llms.&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":true,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1luacs4","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"golden34567","discussion_type":null,"num_comments":12,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1luacs4/looking_for_local_models_to_run_on_super_computer/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1luacs4/looking_for_local_models_to_run_on_super_computer/","subreddit_subscribers":496034,"created_utc":1751933396,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1wvlkm","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Subject-Reach7646","can_mod_post":false,"created_utc":1751939654,"send_replies":true,"parent_id":"t1_n1wo2uy","score":2,"author_fullname":"t2_1socz1cgsc","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I’ve been using DSPy for a bit over a year now, and absolutely love it for a lot of projects. They have made quite a few major changes over that time period, but some of their recent  modules are awesome. I use dspy.Refine all the time.\\n\\nI get that it’s a bit of a mentality shift, but I find it super easy to use. It’s mostly just learning how it works that seems to be the barrier for people. If you can write a pydantic model, you can create a dspy signature. You’re 80% of the way there. Async support is getting a lot better too.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1wvlkm","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I’ve been using DSPy for a bit over a year now, and absolutely love it for a lot of projects. They have made quite a few major changes over that time period, but some of their recent  modules are awesome. I use dspy.Refine all the time.&lt;/p&gt;\\n\\n&lt;p&gt;I get that it’s a bit of a mentality shift, but I find it super easy to use. It’s mostly just learning how it works that seems to be the barrier for people. If you can write a pydantic model, you can create a dspy signature. You’re 80% of the way there. Async support is getting a lot better too.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1luacs4","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1luacs4/looking_for_local_models_to_run_on_super_computer/n1wvlkm/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751939654,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1xkuif","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"golden34567","can_mod_post":false,"created_utc":1751949446,"send_replies":true,"parent_id":"t1_n1wo2uy","score":2,"author_fullname":"t2_93yl561w","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Thank you so much!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1xkuif","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thank you so much!&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1luacs4","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1luacs4/looking_for_local_models_to_run_on_super_computer/n1xkuif/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751949446,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n1wo2uy","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Double_Cause4609","can_mod_post":false,"created_utc":1751937066,"send_replies":true,"parent_id":"t3_1luacs4","score":4,"author_fullname":"t2_1kubzxt2ww","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Truly, compute is wasted on those with it.\\n\\nRegardless, a few things that come to mind:\\n\\nYou may want to try DSPy; I've been playing with it, and you can actually use it to produce a system that is greater than the sum of its parts; a larger teacher LLM (probably Llama 3.3 70B...?), could teach a smaller local model (on your macbook) to get closer to your target performance. This is a relatively lightweight and mostly stateless option in that it's \\"just\\" prompt engineering. Sort of.\\n\\nYou would be surprised at the performance gains.\\n\\nIn particular, my first intuition would be to train a text classifier using DSPy to classify components of what makes up good essays for example (you could have a variety of numerical categories for this). Then, you could rate a ton of them offline on your laptop while you're waiting for your next compute allocation, and then do a \\"real\\" training run with weight optimization on a larger model.\\n\\nWith a group of essays now scored effectively, you could do Reinforcement Learning offline, or you could just do pairwise comparisons and use them in DPO (no clue how stable the training would be, though; preference optimization usually requires the content be on-policy). Alternatively, you could use your trained text classifier from DSPy and pair it with BLEUBERI (you would be using the best essays for this), to produce a robust reward function and do something like GRPO.\\n\\nMy intuition is that just doing raw SFT (standard next token prediction) on essays probably won't do what you think it will, and will almost certainly be disappointing.\\n\\nIf you do get a decent amount of time, another option is to do a QAT fine tune of an existing LLM so that it runs effectively on your local device (this might be paired with logit distillation from the original or a larger model).\\n\\nIf you want to test the DSPy pipeline ahead of time it's pretty easy to get started with openrouter.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1wo2uy","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Truly, compute is wasted on those with it.&lt;/p&gt;\\n\\n&lt;p&gt;Regardless, a few things that come to mind:&lt;/p&gt;\\n\\n&lt;p&gt;You may want to try DSPy; I&amp;#39;ve been playing with it, and you can actually use it to produce a system that is greater than the sum of its parts; a larger teacher LLM (probably Llama 3.3 70B...?), could teach a smaller local model (on your macbook) to get closer to your target performance. This is a relatively lightweight and mostly stateless option in that it&amp;#39;s &amp;quot;just&amp;quot; prompt engineering. Sort of.&lt;/p&gt;\\n\\n&lt;p&gt;You would be surprised at the performance gains.&lt;/p&gt;\\n\\n&lt;p&gt;In particular, my first intuition would be to train a text classifier using DSPy to classify components of what makes up good essays for example (you could have a variety of numerical categories for this). Then, you could rate a ton of them offline on your laptop while you&amp;#39;re waiting for your next compute allocation, and then do a &amp;quot;real&amp;quot; training run with weight optimization on a larger model.&lt;/p&gt;\\n\\n&lt;p&gt;With a group of essays now scored effectively, you could do Reinforcement Learning offline, or you could just do pairwise comparisons and use them in DPO (no clue how stable the training would be, though; preference optimization usually requires the content be on-policy). Alternatively, you could use your trained text classifier from DSPy and pair it with BLEUBERI (you would be using the best essays for this), to produce a robust reward function and do something like GRPO.&lt;/p&gt;\\n\\n&lt;p&gt;My intuition is that just doing raw SFT (standard next token prediction) on essays probably won&amp;#39;t do what you think it will, and will almost certainly be disappointing.&lt;/p&gt;\\n\\n&lt;p&gt;If you do get a decent amount of time, another option is to do a QAT fine tune of an existing LLM so that it runs effectively on your local device (this might be paired with logit distillation from the original or a larger model).&lt;/p&gt;\\n\\n&lt;p&gt;If you want to test the DSPy pipeline ahead of time it&amp;#39;s pretty easy to get started with openrouter.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1luacs4/looking_for_local_models_to_run_on_super_computer/n1wo2uy/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751937066,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1luacs4","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ef488598-491f-11ef-a847-9a3dd315819c","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1x69yc","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"CanineAssBandit","can_mod_post":false,"created_utc":1751943445,"send_replies":true,"parent_id":"t1_n1wrwb1","score":1,"author_fullname":"t2_4mn6lxw7","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"This here is the real answer.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1x69yc","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"Llama 405B"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;This here is the real answer.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1luacs4","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1luacs4/looking_for_local_models_to_run_on_super_computer/n1x69yc/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751943445,"author_flair_text":"Llama 405B","treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1xky6s","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"golden34567","can_mod_post":false,"created_utc":1751949493,"send_replies":true,"parent_id":"t1_n1wrwb1","score":1,"author_fullname":"t2_93yl561w","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yea DeepSeek 671B could definitely be a fun project to run","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1xky6s","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yea DeepSeek 671B could definitely be a fun project to run&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1luacs4","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1luacs4/looking_for_local_models_to_run_on_super_computer/n1xky6s/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751949493,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1wrwb1","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"xoexohexox","can_mod_post":false,"created_utc":1751938381,"send_replies":true,"parent_id":"t3_1luacs4","score":2,"author_fullname":"t2_323db","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Your compute is pretty overkill for inference, have you considered DMPO training, distillation, creating LoRAs, stuff like that? You could be generating tens of thousands of training examples in a few hours and then creating your own custom fine tunes and merges. If you wanted to I guess you could run DeepSeek 671B, that could be fun. Goliath 120B at full precision. Nemotron 340B. Llama 3.1 405B. Qwen 3 235B.\\n\\nBut I guess the more interesting question is what are you gonna do with it? What I would do is start cranking out interesting and systematically balanced synthetic datasets.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1wrwb1","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Your compute is pretty overkill for inference, have you considered DMPO training, distillation, creating LoRAs, stuff like that? You could be generating tens of thousands of training examples in a few hours and then creating your own custom fine tunes and merges. If you wanted to I guess you could run DeepSeek 671B, that could be fun. Goliath 120B at full precision. Nemotron 340B. Llama 3.1 405B. Qwen 3 235B.&lt;/p&gt;\\n\\n&lt;p&gt;But I guess the more interesting question is what are you gonna do with it? What I would do is start cranking out interesting and systematically balanced synthetic datasets.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1luacs4/looking_for_local_models_to_run_on_super_computer/n1wrwb1/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751938381,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1luacs4","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1wodn7","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"JohnnyLovesData","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1whmrv","score":1,"author_fullname":"t2_rsw4yc1","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"\\"Oh, you're a computer alright. Just not a super one.\\"\\n\\nhttps://preview.redd.it/pvrj051owjbf1.png?width=1080&amp;format=png&amp;auto=webp&amp;s=94d2292e241b692c52b95ff67d93e2c31d9d15c7","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n1wodn7","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;&amp;quot;Oh, you&amp;#39;re a computer alright. Just not a super one.&amp;quot;&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://preview.redd.it/pvrj051owjbf1.png?width=1080&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=94d2292e241b692c52b95ff67d93e2c31d9d15c7\\"&gt;https://preview.redd.it/pvrj051owjbf1.png?width=1080&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=94d2292e241b692c52b95ff67d93e2c31d9d15c7&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1luacs4","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1luacs4/looking_for_local_models_to_run_on_super_computer/n1wodn7/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751937168,"media_metadata":{"pvrj051owjbf1":{"status":"valid","e":"Image","m":"image/png","p":[{"y":108,"x":108,"u":"https://preview.redd.it/pvrj051owjbf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=bf8330cccb4cee848f740cfb5e91b2e1011091a4"},{"y":216,"x":216,"u":"https://preview.redd.it/pvrj051owjbf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=c0d47f29dd4cbee1316ca981a46188ca5b93d999"},{"y":320,"x":320,"u":"https://preview.redd.it/pvrj051owjbf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=9c8e80058ecb11be7fef6525ed932c9016c95a51"},{"y":640,"x":640,"u":"https://preview.redd.it/pvrj051owjbf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=402c80f9b3c8988ef749cf7c00281f5cbf24edcd"},{"y":960,"x":960,"u":"https://preview.redd.it/pvrj051owjbf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=f078848cb216bb317d40d8f541bba8127d1bf350"},{"y":1080,"x":1080,"u":"https://preview.redd.it/pvrj051owjbf1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e59824f1da04198a32b3bd74ed8044c6d44db6a6"}],"s":{"y":1080,"x":1080,"u":"https://preview.redd.it/pvrj051owjbf1.png?width=1080&amp;format=png&amp;auto=webp&amp;s=94d2292e241b692c52b95ff67d93e2c31d9d15c7"},"id":"pvrj051owjbf1"}},"author_flair_text":null,"treatment_tags":[],"created_utc":1751937168,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1whmrv","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"golden34567","can_mod_post":false,"created_utc":1751934858,"send_replies":true,"parent_id":"t1_n1wf3ss","score":1,"author_fullname":"t2_93yl561w","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Haha, typo I have a macbook air : )","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1whmrv","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Haha, typo I have a macbook air : )&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1luacs4","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1luacs4/looking_for_local_models_to_run_on_super_computer/n1whmrv/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751934858,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1wf3ss","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"l33t-Mt","can_mod_post":false,"created_utc":1751933987,"send_replies":true,"parent_id":"t3_1luacs4","score":3,"author_fullname":"t2_7pbaf","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"How to become Macbook Air?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1wf3ss","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;How to become Macbook Air?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1luacs4/looking_for_local_models_to_run_on_super_computer/n1wf3ss/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751933987,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1luacs4","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1womo5","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ArsNeph","can_mod_post":false,"created_utc":1751937254,"send_replies":true,"parent_id":"t3_1luacs4","score":1,"author_fullname":"t2_vt0xkv60d","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Well, if you have access to A100s and understand how to use them, then there's a lot of things you could do, but first you have to understand how to use it. The 100GB storage is a big limitation though. I'd recommend starting by running Llama 3.3 70B using Transformers/ExLlamaV2/Llama.cpp. To fine tune it in FP16, you would need more storage space. I recommend starting with an 8B-14B. I suggest trying Unsloth for single GPU or Axolotl for multi. \\n\\nAnother good idea is using the models and batch inferencing with VLLM to create massive, highly specialized synthetic datasets. Think about creative and niche ways you can use this compute. The author of Kokoro trained their model using lots of synthetic data and extra compute as well","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1womo5","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Well, if you have access to A100s and understand how to use them, then there&amp;#39;s a lot of things you could do, but first you have to understand how to use it. The 100GB storage is a big limitation though. I&amp;#39;d recommend starting by running Llama 3.3 70B using Transformers/ExLlamaV2/Llama.cpp. To fine tune it in FP16, you would need more storage space. I recommend starting with an 8B-14B. I suggest trying Unsloth for single GPU or Axolotl for multi. &lt;/p&gt;\\n\\n&lt;p&gt;Another good idea is using the models and batch inferencing with VLLM to create massive, highly specialized synthetic datasets. Think about creative and niche ways you can use this compute. The author of Kokoro trained their model using lots of synthetic data and extra compute as well&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1luacs4/looking_for_local_models_to_run_on_super_computer/n1womo5/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751937254,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1luacs4","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1wjiui","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"golden34567","can_mod_post":false,"created_utc":1751935506,"send_replies":true,"parent_id":"t1_n1wie9b","score":1,"author_fullname":"t2_93yl561w","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Since its an academic use I would be breaking terms and risking myself by mining crypto, I am currently being trained to be able to use it. If I were to make a tool or project like this I can justify it to others.   \\nGoal is to just use this compute somehow because I am currently using a very little part of it for assignments. I just wanted to know what would be like a good use of these. As for the data I have always been collecting essays that I think that are good examples of good writing (upto 150 essays (500 words each)).","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1wjiui","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Since its an academic use I would be breaking terms and risking myself by mining crypto, I am currently being trained to be able to use it. If I were to make a tool or project like this I can justify it to others.&lt;br/&gt;\\nGoal is to just use this compute somehow because I am currently using a very little part of it for assignments. I just wanted to know what would be like a good use of these. As for the data I have always been collecting essays that I think that are good examples of good writing (upto 150 essays (500 words each)).&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1luacs4","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1luacs4/looking_for_local_models_to_run_on_super_computer/n1wjiui/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751935506,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1wie9b","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Virtualization_Freak","can_mod_post":false,"created_utc":1751935122,"send_replies":true,"parent_id":"t3_1luacs4","score":0,"author_fullname":"t2_7f82u","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I think it'd be more beneficial to mine some XMR or other crypto and just buy the few credits you need to run any LLM on a rented out GPU.\\n\\nTo be very blunt, do you have the skills to make use of what has been given to you?\\n\\nWhat is your goal here? Some vibe coding or chatting with a specific model? \\n\\nDo you have scripts and data already in place to feed this much compute in such a short time?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1wie9b","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I think it&amp;#39;d be more beneficial to mine some XMR or other crypto and just buy the few credits you need to run any LLM on a rented out GPU.&lt;/p&gt;\\n\\n&lt;p&gt;To be very blunt, do you have the skills to make use of what has been given to you?&lt;/p&gt;\\n\\n&lt;p&gt;What is your goal here? Some vibe coding or chatting with a specific model? &lt;/p&gt;\\n\\n&lt;p&gt;Do you have scripts and data already in place to feed this much compute in such a short time?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1luacs4/looking_for_local_models_to_run_on_super_computer/n1wie9b/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751935122,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1luacs4","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":1,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}}]`),n=()=>e.jsx(t,{data:a});export{n as default};
