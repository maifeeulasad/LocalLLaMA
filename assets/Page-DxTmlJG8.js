import{j as e}from"./index-Cd3v0jxz.js";import{R as l}from"./RedditPostRenderer-cI96YLhy.js";import"./index-DGBoKyQm.js";const a=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Hi everyone,\\n\\nAccording to ArtificialAnalysis data (from their hardware benchmarks, like at [https://artificialanalysis.ai/benchmarks/hardware?focus-model=deepseek-r1](https://artificialanalysis.ai/benchmarks/hardware?focus-model=deepseek-r1)), the performance difference between NVIDIA's 8x H200 and 8x B200 systems seems minimal, especially in concurrent load scaling for models like DeepSeek R1 or Llama 3.3 70B. For instance, token processing speeds don't show a huge gap despite B200's superior specs on paper.\\n\\nIs this due to specific benchmark conditions, like focusing on multi-GPU scaling or model dependencies, or could it be something else like optimization levels? Has anyone seen similar results in other tests, or is this just an artifact of their methodology? I'd love to hear your thoughts or any insights from real-world usage!\\n\\nThanks!","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Why is B200 performing similarly to H200? (ArtificialAnalysis)","link_flair_richtext":[{"e":"text","t":"Discussion"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1m7ypyb","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.8,"author_flair_background_color":null,"subreddit_type":"public","ups":16,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_93zqvlmj","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Discussion","can_mod_post":false,"score":16,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"post_hint":"self","content_categories":null,"is_self":true,"mod_note":null,"created":1753345144,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Hi everyone,&lt;/p&gt;\\n\\n&lt;p&gt;According to ArtificialAnalysis data (from their hardware benchmarks, like at &lt;a href=\\"https://artificialanalysis.ai/benchmarks/hardware?focus-model=deepseek-r1\\"&gt;https://artificialanalysis.ai/benchmarks/hardware?focus-model=deepseek-r1&lt;/a&gt;), the performance difference between NVIDIA&amp;#39;s 8x H200 and 8x B200 systems seems minimal, especially in concurrent load scaling for models like DeepSeek R1 or Llama 3.3 70B. For instance, token processing speeds don&amp;#39;t show a huge gap despite B200&amp;#39;s superior specs on paper.&lt;/p&gt;\\n\\n&lt;p&gt;Is this due to specific benchmark conditions, like focusing on multi-GPU scaling or model dependencies, or could it be something else like optimization levels? Has anyone seen similar results in other tests, or is this just an artifact of their methodology? I&amp;#39;d love to hear your thoughts or any insights from real-world usage!&lt;/p&gt;\\n\\n&lt;p&gt;Thanks!&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"preview":{"images":[{"source":{"url":"https://external-preview.redd.it/RVufpUx3tddh_BCVq7ZzBCU7nLRDZ_d0EprIuvN6J-E.png?auto=webp&amp;s=efc17c9f241b4403d22cbacfe5d71900ee1cf85a","width":1260,"height":700},"resolutions":[{"url":"https://external-preview.redd.it/RVufpUx3tddh_BCVq7ZzBCU7nLRDZ_d0EprIuvN6J-E.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=700f91dbca11e5a7030b915550ae877ef725a0d4","width":108,"height":60},{"url":"https://external-preview.redd.it/RVufpUx3tddh_BCVq7ZzBCU7nLRDZ_d0EprIuvN6J-E.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=b97954336b79c1390848d0e44fa056a85de68672","width":216,"height":120},{"url":"https://external-preview.redd.it/RVufpUx3tddh_BCVq7ZzBCU7nLRDZ_d0EprIuvN6J-E.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=65f53b80ab9674ee645013e3e8eeac4f953d657e","width":320,"height":177},{"url":"https://external-preview.redd.it/RVufpUx3tddh_BCVq7ZzBCU7nLRDZ_d0EprIuvN6J-E.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=47f397e4a22ed5ec7e82aad070eb446319603abc","width":640,"height":355},{"url":"https://external-preview.redd.it/RVufpUx3tddh_BCVq7ZzBCU7nLRDZ_d0EprIuvN6J-E.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=0f4359d47b78f5c1aa35de8804dbe36a749fc11a","width":960,"height":533},{"url":"https://external-preview.redd.it/RVufpUx3tddh_BCVq7ZzBCU7nLRDZ_d0EprIuvN6J-E.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=62eb4b7216f41af6600fc4df79cfa67425c19442","width":1080,"height":600}],"variants":{},"id":"RVufpUx3tddh_BCVq7ZzBCU7nLRDZ_d0EprIuvN6J-E"}],"enabled":false},"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"5f921ea4-c7bc-11ed-9c23-3a00622979b4","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#646d73","id":"1m7ypyb","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"Cyp9715","discussion_type":null,"num_comments":13,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1m7ypyb/why_is_b200_performing_similarly_to_h200/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1m7ypyb/why_is_b200_performing_similarly_to_h200/","subreddit_subscribers":504023,"created_utc":1753345144,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4v6dzi","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"djm07231","can_mod_post":false,"created_utc":1753346123,"send_replies":true,"parent_id":"t3_1m7ypyb","score":10,"author_fullname":"t2_zuxto","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I think that Blackwell place more emphasis on “rack-scale” architectures.\\n\\nIt does seem to emphasize how the compute scales in a datacenter rather than just a single node.\\n\\nSo this would probably work best for training of large models where multi-node communications and cohesiveness matters more and Trillion-parameter scale models where multi-node inference is a must.\\n\\nFor models smaller than this where multi-node scaling isn’t as important it performance might not scale as much.\\n\\nAnother obvious point is that Blackwell is still pretty new so software inference stack hasn’t been optimized as much compared to Hopper.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4v6dzi","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I think that Blackwell place more emphasis on “rack-scale” architectures.&lt;/p&gt;\\n\\n&lt;p&gt;It does seem to emphasize how the compute scales in a datacenter rather than just a single node.&lt;/p&gt;\\n\\n&lt;p&gt;So this would probably work best for training of large models where multi-node communications and cohesiveness matters more and Trillion-parameter scale models where multi-node inference is a must.&lt;/p&gt;\\n\\n&lt;p&gt;For models smaller than this where multi-node scaling isn’t as important it performance might not scale as much.&lt;/p&gt;\\n\\n&lt;p&gt;Another obvious point is that Blackwell is still pretty new so software inference stack hasn’t been optimized as much compared to Hopper.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7ypyb/why_is_b200_performing_similarly_to_h200/n4v6dzi/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753346123,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m7ypyb","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":10}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4vklhm","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"shing3232","can_mod_post":false,"created_utc":1753353845,"send_replies":true,"parent_id":"t3_1m7ypyb","score":7,"author_fullname":"t2_ze4mg","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"There is not big different in fp8/bf16 performance between b200 and h200","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4vklhm","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;There is not big different in fp8/bf16 performance between b200 and h200&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7ypyb/why_is_b200_performing_similarly_to_h200/n4vklhm/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753353845,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m7ypyb","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":7}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4v71p0","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Dr4kin","can_mod_post":false,"created_utc":1753346501,"send_replies":true,"parent_id":"t3_1m7ypyb","score":3,"author_fullname":"t2_x5gqb","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"A lot of performance also comes from node shrinkage which Blackwell didn't have.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4v71p0","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;A lot of performance also comes from node shrinkage which Blackwell didn&amp;#39;t have.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7ypyb/why_is_b200_performing_similarly_to_h200/n4v71p0/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753346501,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m7ypyb","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"total_awards_received":0,"approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"ups":3,"removal_reason":null,"link_id":"t3_1m7ypyb","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"total_awards_received":0,"approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"ups":1,"removal_reason":null,"link_id":"t3_1m7ypyb","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4ycvwe","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Tyme4Trouble","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4yad5w","score":1,"author_fullname":"t2_973amyap","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I don’t believe that’s true. The die to die interconnect is fast enough and low enough latency that they are cache coherent. In other words there is no appreciable performance hit for one die accessing memory attached to another. They behave logically like one GPU. NUMA doesn’t work that way. \\n\\nI’ll note that B300 doesn’t actually have this die to die interconnect because it caused thermal issues. Or at least that’s what Ian Buck told me.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4ycvwe","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I don’t believe that’s true. The die to die interconnect is fast enough and low enough latency that they are cache coherent. In other words there is no appreciable performance hit for one die accessing memory attached to another. They behave logically like one GPU. NUMA doesn’t work that way. &lt;/p&gt;\\n\\n&lt;p&gt;I’ll note that B300 doesn’t actually have this die to die interconnect because it caused thermal issues. Or at least that’s what Ian Buck told me.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m7ypyb","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7ypyb/why_is_b200_performing_similarly_to_h200/n4ycvwe/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753384497,"author_flair_text":null,"treatment_tags":[],"created_utc":1753384497,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4yad5w","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":"DELETED","no_follow":true,"author":"[deleted]","can_mod_post":false,"created_utc":1753383786,"send_replies":true,"parent_id":"t1_n4xql2n","score":1,"approved_by":null,"report_reasons":null,"all_awardings":[],"subreddit_id":"t5_81eyvm","body":"[deleted]","edited":false,"author_flair_css_class":null,"downs":0,"is_submitter":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;[deleted]&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"associated_award":null,"stickied":false,"subreddit_type":"public","can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":"dark","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7ypyb/why_is_b200_performing_similarly_to_h200/n4yad5w/","num_reports":null,"locked":false,"name":"t1_n4yad5w","created":1753383786,"subreddit":"LocalLLaMA","author_flair_text":null,"treatment_tags":[],"collapsed":true,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":"","collapsed_because_crowd_control":null,"mod_reports":[],"mod_note":null,"distinguished":null}}],"before":null}},"user_reports":[],"saved":false,"id":"n4xql2n","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Tyme4Trouble","can_mod_post":false,"created_utc":1753378262,"send_replies":true,"parent_id":"t1_n4w9dau","score":2,"author_fullname":"t2_973amyap","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"No they don’t. H200 SXM has a memory bandwidth of 4.8TB/ B200 has a memory bandwidth of 8TB/s. \\n\\nAt system level that’s 38.4TB/s vs 64TB/s","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4xql2n","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;No they don’t. H200 SXM has a memory bandwidth of 4.8TB/ B200 has a memory bandwidth of 8TB/s. &lt;/p&gt;\\n\\n&lt;p&gt;At system level that’s 38.4TB/s vs 64TB/s&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m7ypyb","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7ypyb/why_is_b200_performing_similarly_to_h200/n4xql2n/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753378262,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n4w9dau","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":"DELETED","no_follow":true,"author":"[deleted]","can_mod_post":false,"send_replies":true,"parent_id":"t3_1m7ypyb","score":3,"approved_by":null,"report_reasons":null,"all_awardings":[],"subreddit_id":"t5_81eyvm","body":"[deleted]","edited":false,"downs":0,"author_flair_css_class":null,"collapsed":true,"is_submitter":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;[deleted]&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"associated_award":null,"stickied":false,"subreddit_type":"public","can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":"dark","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7ypyb/why_is_b200_performing_similarly_to_h200/n4w9dau/","num_reports":null,"locked":false,"name":"t1_n4w9dau","created":1753363293,"subreddit":"LocalLLaMA","author_flair_text":null,"treatment_tags":[],"created_utc":1753363293,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"","collapsed_because_crowd_control":null,"mod_reports":[],"mod_note":null,"distinguished":null}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"1c60b73a-72f2-11ee-bdcc-8e2a7b94d6a0","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4vwjpg","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Temporary-Size7310","can_mod_post":false,"created_utc":1753358894,"send_replies":true,"parent_id":"t3_1m7ypyb","score":2,"author_fullname":"t2_v2yh9yxw","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"B200 or any Blackwell are brillant with NVFP4 keeping quite the same quality compared to FP8, the document doesn't show what they use to compare ? \\nVLLM, TensorRT-LLM, which Quant type and size ?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4vwjpg","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"textgen web UI"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;B200 or any Blackwell are brillant with NVFP4 keeping quite the same quality compared to FP8, the document doesn&amp;#39;t show what they use to compare ? \\nVLLM, TensorRT-LLM, which Quant type and size ?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7ypyb/why_is_b200_performing_similarly_to_h200/n4vwjpg/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753358894,"author_flair_text":"textgen web UI","treatment_tags":[],"link_id":"t3_1m7ypyb","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4xpq1g","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"GPTrack_ai","can_mod_post":false,"created_utc":1753378033,"send_replies":true,"parent_id":"t3_1m7ypyb","score":2,"author_fullname":"t2_1tpuoj72sa","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"the big difference is FP4. that matters bove everything else! for all other stuff (FP8) the performace in my tests was approx. 2x over H200.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4xpq1g","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;the big difference is FP4. that matters bove everything else! for all other stuff (FP8) the performace in my tests was approx. 2x over H200.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7ypyb/why_is_b200_performing_similarly_to_h200/n4xpq1g/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753378033,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m7ypyb","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4xlsih","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"KeinNiemand","can_mod_post":false,"created_utc":1753376993,"send_replies":true,"parent_id":"t3_1m7ypyb","score":1,"author_fullname":"t2_nuxa9","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I guess Blackwell just not a big improvment in general, like for the gaming cards the gen over gen improvemts for most 50 series cards is like 10% or less.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4xlsih","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I guess Blackwell just not a big improvment in general, like for the gaming cards the gen over gen improvemts for most 50 series cards is like 10% or less.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7ypyb/why_is_b200_performing_similarly_to_h200/n4xlsih/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753376993,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m7ypyb","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4xnw00","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"The_GSingh","can_mod_post":false,"created_utc":1753377549,"send_replies":true,"parent_id":"t3_1m7ypyb","score":1,"author_fullname":"t2_fy4qc98m","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Blackwell is more designed for enterprise use like training or working as a server to process multiple queries.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4xnw00","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Blackwell is more designed for enterprise use like training or working as a server to process multiple queries.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7ypyb/why_is_b200_performing_similarly_to_h200/n4xnw00/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753377549,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m7ypyb","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n501l4k","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Ok_Warning2146","can_mod_post":false,"created_utc":1753403052,"send_replies":true,"parent_id":"t3_1m7ypyb","score":1,"author_fullname":"t2_s6sfw4yy","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Well, Blackwell has the same transistor density as Hopper since they are made on the same process node. 5090 is just a bigger 4090 with GDDR7 VRAM. However, in the server environment, you can't make it much bigger due to heat.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n501l4k","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Well, Blackwell has the same transistor density as Hopper since they are made on the same process node. 5090 is just a bigger 4090 with GDDR7 VRAM. However, in the server environment, you can&amp;#39;t make it much bigger due to heat.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7ypyb/why_is_b200_performing_similarly_to_h200/n501l4k/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753403052,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m7ypyb","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4xygoi","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"shing3232","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4xqt8v","score":1,"author_fullname":"t2_ze4mg","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"There might have something to do with utrilization of Tensor Core.\\n\\nInference is not likely to fully load the tensorcore\\n\\nTraining are much heavy on Compute where inference is IO heavy","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n4xygoi","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;There might have something to do with utrilization of Tensor Core.&lt;/p&gt;\\n\\n&lt;p&gt;Inference is not likely to fully load the tensorcore&lt;/p&gt;\\n\\n&lt;p&gt;Training are much heavy on Compute where inference is IO heavy&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m7ypyb","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7ypyb/why_is_b200_performing_similarly_to_h200/n4xygoi/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753380395,"author_flair_text":null,"treatment_tags":[],"created_utc":1753380395,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4xqt8v","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Tyme4Trouble","can_mod_post":false,"created_utc":1753378322,"send_replies":true,"parent_id":"t1_n4vksfe","score":2,"author_fullname":"t2_973amyap","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Normalizing for precision B200 is a little over 2x H200","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4xqt8v","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Normalizing for precision B200 is a little over 2x H200&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m7ypyb","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7ypyb/why_is_b200_performing_similarly_to_h200/n4xqt8v/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753378322,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n4vksfe","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"shing3232","can_mod_post":false,"created_utc":1753353936,"send_replies":true,"parent_id":"t3_1m7ypyb","score":0,"author_fullname":"t2_ze4mg","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"There is not big different in fp8/bf16 performance between b200 and h200","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4vksfe","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;There is not big different in fp8/bf16 performance between b200 and h200&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7ypyb/why_is_b200_performing_similarly_to_h200/n4vksfe/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753353936,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m7ypyb","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}}]`),o=()=>e.jsx(l,{data:a});export{o as default};
