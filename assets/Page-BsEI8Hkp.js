import{j as e}from"./index-Bu7qcPAU.js";import{R as l}from"./RedditPostRenderer-CbHA7O5q.js";import"./index-BKgbfxhf.js";const a=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Hey all,\\nWe’re a hospital building an on-prem system for health and medical data analytics using LLMs. Our setup includes an RTX 6000 Pro and a 5090, and we’re working with a $10~$19k budget.\\n\\nI have already tried Gemma3 on 5090 but can’t unleash the 96gb vram capabilities.\\n\\nWe’re looking to:\\n\\t•\\tRun a large open-source LLM locally (currently putting eyes in llama4)\\n\\t•\\tDo fine-tuning (LoRA or full) on structured clinical data and unstructured medical notes\\n\\t•\\tUse the model for summarization, Q&amp;A, and EHR-related tasks\\n\\nWe’d love recommendations on:\\n\\t1.\\tThe best large open-source LLM to use in this context\\n\\t2.\\tHow much CPU matters for performance (inference + fine-tuning) alongside these GPUs\\n\\nWould really appreciate any suggestions based on real-world setups—especially if you’ve done similar work in the health/biomed space.\\n\\nThanks in advance!\\n","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Best large open-source LLM for health/medical data analytics (RTX 6000 Pro, $10k budget)","link_flair_richtext":[{"e":"text","t":"Discussion"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lwrd38","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.74,"author_flair_background_color":null,"subreddit_type":"public","ups":14,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_e4ojre534","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Discussion","can_mod_post":false,"score":14,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":1752191297,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1752189389,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Hey all,\\nWe’re a hospital building an on-prem system for health and medical data analytics using LLMs. Our setup includes an RTX 6000 Pro and a 5090, and we’re working with a $10~$19k budget.&lt;/p&gt;\\n\\n&lt;p&gt;I have already tried Gemma3 on 5090 but can’t unleash the 96gb vram capabilities.&lt;/p&gt;\\n\\n&lt;p&gt;We’re looking to:\\n    • Run a large open-source LLM locally (currently putting eyes in llama4)\\n    • Do fine-tuning (LoRA or full) on structured clinical data and unstructured medical notes\\n    • Use the model for summarization, Q&amp;amp;A, and EHR-related tasks&lt;/p&gt;\\n\\n&lt;p&gt;We’d love recommendations on:\\n    1.  The best large open-source LLM to use in this context\\n    2.  How much CPU matters for performance (inference + fine-tuning) alongside these GPUs&lt;/p&gt;\\n\\n&lt;p&gt;Would really appreciate any suggestions based on real-world setups—especially if you’ve done similar work in the health/biomed space.&lt;/p&gt;\\n\\n&lt;p&gt;Thanks in advance!&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"5f921ea4-c7bc-11ed-9c23-3a00622979b4","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#646d73","id":"1lwrd38","is_robot_indexable":true,"num_duplicates":1,"report_reasons":null,"author":"LeastExperience1579","discussion_type":null,"num_comments":28,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lwrd38/best_large_opensource_llm_for_healthmedical_data/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lwrd38/best_large_opensource_llm_for_healthmedical_data/","subreddit_subscribers":497504,"created_utc":1752189389,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2gm64w","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Voxandr","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2gizm0","score":6,"author_fullname":"t2_86dk0gye","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I had tried Qwen3 for transcription -&gt; medical reports. Its horrible . Gemma / Medgemma have best world and medical knowledge.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2gm64w","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I had tried Qwen3 for transcription -&amp;gt; medical reports. Its horrible . Gemma / Medgemma have best world and medical knowledge.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lwrd38","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwrd38/best_large_opensource_llm_for_healthmedical_data/n2gm64w/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752192470,"author_flair_text":null,"treatment_tags":[],"created_utc":1752192470,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2i4oo3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AppearanceHeavy6724","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2hv6gm","score":1,"author_fullname":"t2_uz37qfx5","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"llama 4 has too small experts for its size. the pushed \\"small expert\\" idea too far.","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n2i4oo3","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;llama 4 has too small experts for its size. the pushed &amp;quot;small expert&amp;quot; idea too far.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lwrd38","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwrd38/best_large_opensource_llm_for_healthmedical_data/n2i4oo3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752214075,"author_flair_text":null,"treatment_tags":[],"created_utc":1752214075,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":6,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2hv6gm","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DepthHour1669","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2gmb0x","score":1,"author_fullname":"t2_t6glzswk","approved_by":null,"mod_note":null,"all_awardings":[],"body":"Llama 4 architecture is fine, it’s literally the same as deepseek v3. It just has bad data it was trained on. \\n\\nTaking Medgemma’s weights and distilling Llama 4 Scout with it might not be a bad idea. \\n\\nOr take Medgemma and distill it into Hunyuan 80b A13b. That would fit pretty well in 96gb.","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n2hv6gm","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Llama 4 architecture is fine, it’s literally the same as deepseek v3. It just has bad data it was trained on. &lt;/p&gt;\\n\\n&lt;p&gt;Taking Medgemma’s weights and distilling Llama 4 Scout with it might not be a bad idea. &lt;/p&gt;\\n\\n&lt;p&gt;Or take Medgemma and distill it into Hunyuan 80b A13b. That would fit pretty well in 96gb.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lwrd38","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwrd38/best_large_opensource_llm_for_healthmedical_data/n2hv6gm/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752209433,"author_flair_text":null,"treatment_tags":[],"created_utc":1752209433,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2gmb0x","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Voxandr","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2gjnaj","score":3,"author_fullname":"t2_86dk0gye","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"not a fan of Llama4 , it is sub-par in all benchmarks and use cases of many of us. (you will get a lot of downvotes hre too since all of us hate it lol) .","edited":false,"author_flair_css_class":null,"name":"t1_n2gmb0x","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;not a fan of Llama4 , it is sub-par in all benchmarks and use cases of many of us. (you will get a lot of downvotes hre too since all of us hate it lol) .&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lwrd38","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwrd38/best_large_opensource_llm_for_healthmedical_data/n2gmb0x/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752192518,"author_flair_text":null,"collapsed":false,"created_utc":1752192518,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n2gjnaj","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"LeastExperience1579","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2gizm0","score":1,"author_fullname":"t2_e4ojre534","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I haven’t tried medgemma with different quantization other than q4.\\nDo you think it would beat llama4 scout ?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2gjnaj","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I haven’t tried medgemma with different quantization other than q4.\\nDo you think it would beat llama4 scout ?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lwrd38","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwrd38/best_large_opensource_llm_for_healthmedical_data/n2gjnaj/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752191600,"author_flair_text":null,"treatment_tags":[],"created_utc":1752191600,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2gizm0","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Tenzu9","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2gh874","score":10,"author_fullname":"t2_10wgss","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Medgemma 27B F8 or F16\\n\\nQwen3 235B A22B Q4? No idea if it has any medical data pertained into it.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2gizm0","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Medgemma 27B F8 or F16&lt;/p&gt;\\n\\n&lt;p&gt;Qwen3 235B A22B Q4? No idea if it has any medical data pertained into it.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lwrd38","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwrd38/best_large_opensource_llm_for_healthmedical_data/n2gizm0/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752191378,"author_flair_text":null,"treatment_tags":[],"created_utc":1752191378,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":10}}],"before":null}},"user_reports":[],"saved":false,"id":"n2gh874","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"LeastExperience1579","can_mod_post":false,"created_utc":1752190787,"send_replies":true,"parent_id":"t1_n2gevv0","score":1,"author_fullname":"t2_e4ojre534","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Thanks, we might have ~3 users using our system at the same time.\\n\\nI have already tried 5090 with gemma3 but I wonder if we should get the rtx6000 pro. Do you have any suggestion of what model we can run on that GPU?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2gh874","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thanks, we might have ~3 users using our system at the same time.&lt;/p&gt;\\n\\n&lt;p&gt;I have already tried 5090 with gemma3 but I wonder if we should get the rtx6000 pro. Do you have any suggestion of what model we can run on that GPU?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lwrd38","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwrd38/best_large_opensource_llm_for_healthmedical_data/n2gh874/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752190787,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2gio4q","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"LeastExperience1579","can_mod_post":false,"created_utc":1752191270,"send_replies":true,"parent_id":"t1_n2gevv0","score":0,"author_fullname":"t2_e4ojre534","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Haven’t tried llama4 and it’s vision capabilities might help","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2gio4q","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Haven’t tried llama4 and it’s vision capabilities might help&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lwrd38","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwrd38/best_large_opensource_llm_for_healthmedical_data/n2gio4q/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752191270,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}},"user_reports":[],"saved":false,"id":"n2gevv0","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Voxandr","can_mod_post":false,"created_utc":1752190008,"send_replies":true,"parent_id":"t3_1lwrd38","score":8,"author_fullname":"t2_86dk0gye","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"\\\\- Use Gemma-3 models , they are best in medical knowledge  \\n\\\\- How many users running at the same time? use vLLM for inference server , it can handle multiple users very well.  \\n\\\\- If you have good budget just use models that can run on full GPU.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2gevv0","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;- Use Gemma-3 models , they are best in medical knowledge&lt;br/&gt;\\n- How many users running at the same time? use vLLM for inference server , it can handle multiple users very well.&lt;br/&gt;\\n- If you have good budget just use models that can run on full GPU.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwrd38/best_large_opensource_llm_for_healthmedical_data/n2gevv0/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752190008,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lwrd38","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":8}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2iay4g","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"LeastExperience1579","can_mod_post":false,"created_utc":1752217398,"send_replies":true,"parent_id":"t1_n2hg8az","score":1,"author_fullname":"t2_e4ojre534","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"We are still in a state of trying","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2iay4g","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;We are still in a state of trying&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lwrd38","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwrd38/best_large_opensource_llm_for_healthmedical_data/n2iay4g/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752217398,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2hg8az","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Red_Redditor_Reddit","can_mod_post":false,"created_utc":1752203169,"send_replies":true,"parent_id":"t3_1lwrd38","score":5,"author_fullname":"t2_8eelmfjg","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I don't know about the medical world, but the biggest issue I've seen is people simply misusing the system.  People are treating these LLM's like it's god himself speaking and having zero common sense from just being lazy.  They'll get an answer from these LLM's and will not do any kind of sanity or validity checking.  Don't assume that people will use it the way you're intending.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2hg8az","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I don&amp;#39;t know about the medical world, but the biggest issue I&amp;#39;ve seen is people simply misusing the system.  People are treating these LLM&amp;#39;s like it&amp;#39;s god himself speaking and having zero common sense from just being lazy.  They&amp;#39;ll get an answer from these LLM&amp;#39;s and will not do any kind of sanity or validity checking.  Don&amp;#39;t assume that people will use it the way you&amp;#39;re intending.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwrd38/best_large_opensource_llm_for_healthmedical_data/n2hg8az/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752203169,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lwrd38","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2iah9x","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"LeastExperience1579","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2i9ct1","score":1,"author_fullname":"t2_e4ojre534","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I find qwen3 has a significantly better math performance","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2iah9x","is_submitter":true,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I find qwen3 has a significantly better math performance&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lwrd38","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwrd38/best_large_opensource_llm_for_healthmedical_data/n2iah9x/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752217143,"author_flair_text":null,"treatment_tags":[],"created_utc":1752217143,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2i9ct1","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"generaluser123","can_mod_post":false,"created_utc":1752216542,"send_replies":true,"parent_id":"t1_n2huhxh","score":1,"author_fullname":"t2_127304","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Can you please explain how will that help?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2i9ct1","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Can you please explain how will that help?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lwrd38","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwrd38/best_large_opensource_llm_for_healthmedical_data/n2i9ct1/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752216542,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2huhxh","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"DepthHour1669","can_mod_post":false,"created_utc":1752209122,"send_replies":true,"parent_id":"t3_1lwrd38","score":6,"author_fullname":"t2_t6glzswk","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"1. Medgemma\\n\\n2. You want medgemma but with a stronger model. Maybe use unsloth to distill medgemma into qwen?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2huhxh","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;ol&gt;\\n&lt;li&gt;&lt;p&gt;Medgemma&lt;/p&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;p&gt;You want medgemma but with a stronger model. Maybe use unsloth to distill medgemma into qwen?&lt;/p&gt;&lt;/li&gt;\\n&lt;/ol&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwrd38/best_large_opensource_llm_for_healthmedical_data/n2huhxh/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752209122,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lwrd38","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2jh73w","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Informal_Librarian","can_mod_post":false,"created_utc":1752238109,"send_replies":true,"parent_id":"t3_1lwrd38","score":3,"author_fullname":"t2_obq9bdp","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Seems worth checking out “II-Medical-8B-1706”built by Stability AI founders new company.\\n\\nhttps://huggingface.co/Intelligent-Internet/II-Medical-8B-1706\\n\\nhttps://huggingface.co/Intelligent-Internet/II-Medical-8B-1706-GGUF\\n\\nClaims to outperform MedGemma.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2jh73w","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Seems worth checking out “II-Medical-8B-1706”built by Stability AI founders new company.&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://huggingface.co/Intelligent-Internet/II-Medical-8B-1706\\"&gt;https://huggingface.co/Intelligent-Internet/II-Medical-8B-1706&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://huggingface.co/Intelligent-Internet/II-Medical-8B-1706-GGUF\\"&gt;https://huggingface.co/Intelligent-Internet/II-Medical-8B-1706-GGUF&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;Claims to outperform MedGemma.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwrd38/best_large_opensource_llm_for_healthmedical_data/n2jh73w/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752238109,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lwrd38","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2j0h6h","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Ylsid","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2iats9","score":2,"author_fullname":"t2_6lmlc","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"If Open WebUI lets you ensure the people who are using your service get easily verifiable information, sure, it works. People will say they like a thing because it's easy to use, not necessarily because it's useful. I imagine you'd not want your users getting decieved, or skipping medical information, getting hallucinated on, etc","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2j0h6h","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;If Open WebUI lets you ensure the people who are using your service get easily verifiable information, sure, it works. People will say they like a thing because it&amp;#39;s easy to use, not necessarily because it&amp;#39;s useful. I imagine you&amp;#39;d not want your users getting decieved, or skipping medical information, getting hallucinated on, etc&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lwrd38","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwrd38/best_large_opensource_llm_for_healthmedical_data/n2j0h6h/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752231440,"author_flair_text":null,"treatment_tags":[],"created_utc":1752231440,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n2iats9","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"LeastExperience1579","can_mod_post":false,"created_utc":1752217331,"send_replies":true,"parent_id":"t1_n2hedc8","score":1,"author_fullname":"t2_e4ojre534","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Thank you, we are currently using Open WebUI and people like it.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2iats9","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thank you, we are currently using Open WebUI and people like it.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lwrd38","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwrd38/best_large_opensource_llm_for_healthmedical_data/n2iats9/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752217331,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2hedc8","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Ylsid","can_mod_post":false,"created_utc":1752202452,"send_replies":true,"parent_id":"t3_1lwrd38","score":2,"author_fullname":"t2_6lmlc","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"If it's RAG, I hear GLM is good. You'll need GPUs for this. You'll also need a really solid software stack to organise around limited context. And last, take care it doesn't create extra work by forcing people into long rabbit holes to verify RAG content. But that's mostly UX","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2hedc8","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;If it&amp;#39;s RAG, I hear GLM is good. You&amp;#39;ll need GPUs for this. You&amp;#39;ll also need a really solid software stack to organise around limited context. And last, take care it doesn&amp;#39;t create extra work by forcing people into long rabbit holes to verify RAG content. But that&amp;#39;s mostly UX&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwrd38/best_large_opensource_llm_for_healthmedical_data/n2hedc8/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752202452,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lwrd38","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2i6csc","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"medcanned","can_mod_post":false,"created_utc":1752214946,"send_replies":true,"parent_id":"t3_1lwrd38","score":2,"author_fullname":"t2_f8n4h4xe","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"This recent preprint may interest you then :\\nhttps://www.researchsquare.com/article/rs-7029913/v1","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2i6csc","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;This recent preprint may interest you then :\\n&lt;a href=\\"https://www.researchsquare.com/article/rs-7029913/v1\\"&gt;https://www.researchsquare.com/article/rs-7029913/v1&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwrd38/best_large_opensource_llm_for_healthmedical_data/n2i6csc/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752214946,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lwrd38","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2ifovd","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Beamsters","can_mod_post":false,"created_utc":1752220065,"send_replies":true,"parent_id":"t3_1lwrd38","score":2,"author_fullname":"t2_xy2fzv1","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Even with some easy questions like grouping some words into word categories for language learning, Gemma 3 still made quite a few mistakes or questionable answers. Dealing with this kind of critical information needs a systematic verification system build on top.\\n\\nThese could be mitigated by\\n\\n- Asking another instance of the same model to check and verify answer from the first one.\\n- Asking another model to give a correctness score and flag the result.\\n- Use another instance to produce a second opinion.\\nEtc.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2ifovd","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Even with some easy questions like grouping some words into word categories for language learning, Gemma 3 still made quite a few mistakes or questionable answers. Dealing with this kind of critical information needs a systematic verification system build on top.&lt;/p&gt;\\n\\n&lt;p&gt;These could be mitigated by&lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;Asking another instance of the same model to check and verify answer from the first one.&lt;/li&gt;\\n&lt;li&gt;Asking another model to give a correctness score and flag the result.&lt;/li&gt;\\n&lt;li&gt;Use another instance to produce a second opinion.\\nEtc.&lt;/li&gt;\\n&lt;/ul&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwrd38/best_large_opensource_llm_for_healthmedical_data/n2ifovd/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752220065,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lwrd38","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2igyyh","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MelodicRecognition7","can_mod_post":false,"created_utc":1752220782,"send_replies":true,"parent_id":"t3_1lwrd38","score":2,"author_fullname":"t2_1eex9ug5","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"&gt; I have already tried Gemma3 on 5090 but can’t unleash the 96gb vram capabilities.\\n\\njust add vision and context :D\\n\\ngemma3 27B in Q8 quant + mmproj in Q8 + 131072 context uses exactly 96 GB VRAM. Although you might not really need all that context and might want to lower it down to 16k-32k expecially if you have multiple simultaneous users.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2igyyh","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;I have already tried Gemma3 on 5090 but can’t unleash the 96gb vram capabilities.&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;just add vision and context :D&lt;/p&gt;\\n\\n&lt;p&gt;gemma3 27B in Q8 quant + mmproj in Q8 + 131072 context uses exactly 96 GB VRAM. Although you might not really need all that context and might want to lower it down to 16k-32k expecially if you have multiple simultaneous users.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwrd38/best_large_opensource_llm_for_healthmedical_data/n2igyyh/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752220782,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lwrd38","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2ib3xf","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Failiiix","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2i4smx","score":2,"author_fullname":"t2_xbip5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Yes. Especially in the health sector you have to build a really reliable system. With multiple checks and balances. Simply put, RAG is not enough! It's not magic. \\n\\nUse the biggest LLM and build a robust RAG systems. Really fine tune the LLM parameters like temp, top p, top k, and use that extra vram for models that check the results. I have seen health care llm systems with up to 7 agents working sequentially to ensure the output is good. The more checks you build the more reliable it gets. Especially build deterministic checks after LLM Generation. I can imagine a system that checks all numbers in the generated output with the database, before continuing. Each step has to have the power to restart the process if something is off. \\n\\nGo for reliability first, speed second! \\n\\nProblem is, that even if you do that it can fail and someone has to check it..\\n\\nBe really careful and go big on testing. ( I would go for 10k+ runs for validation!)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2ib3xf","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yes. Especially in the health sector you have to build a really reliable system. With multiple checks and balances. Simply put, RAG is not enough! It&amp;#39;s not magic. &lt;/p&gt;\\n\\n&lt;p&gt;Use the biggest LLM and build a robust RAG systems. Really fine tune the LLM parameters like temp, top p, top k, and use that extra vram for models that check the results. I have seen health care llm systems with up to 7 agents working sequentially to ensure the output is good. The more checks you build the more reliable it gets. Especially build deterministic checks after LLM Generation. I can imagine a system that checks all numbers in the generated output with the database, before continuing. Each step has to have the power to restart the process if something is off. &lt;/p&gt;\\n\\n&lt;p&gt;Go for reliability first, speed second! &lt;/p&gt;\\n\\n&lt;p&gt;Problem is, that even if you do that it can fail and someone has to check it..&lt;/p&gt;\\n\\n&lt;p&gt;Be really careful and go big on testing. ( I would go for 10k+ runs for validation!)&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lwrd38","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwrd38/best_large_opensource_llm_for_healthmedical_data/n2ib3xf/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752217485,"author_flair_text":null,"treatment_tags":[],"created_utc":1752217485,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2j8w0s","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"InvertedVantage","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2iadav","score":2,"author_fullname":"t2_4me51","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"There is no way to resolve hallucinations. An LLM is a random text generator so you will always have some of that text be hallucinated.","edited":false,"author_flair_css_class":null,"name":"t1_n2j8w0s","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;There is no way to resolve hallucinations. An LLM is a random text generator so you will always have some of that text be hallucinated.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lwrd38","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwrd38/best_large_opensource_llm_for_healthmedical_data/n2j8w0s/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752235057,"author_flair_text":null,"collapsed":false,"created_utc":1752235057,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n2iadav","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"LeastExperience1579","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2i4smx","score":1,"author_fullname":"t2_e4ojre534","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I am not a expert in this, could you share some resources I can find to resolve this, thank you","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2iadav","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I am not a expert in this, could you share some resources I can find to resolve this, thank you&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lwrd38","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwrd38/best_large_opensource_llm_for_healthmedical_data/n2iadav/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752217085,"author_flair_text":null,"treatment_tags":[],"created_utc":1752217085,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2i4smx","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"AppearanceHeavy6724","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2gwd2l","score":4,"author_fullname":"t2_uz37qfx5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Even with RAG models (esp. Gemma 3) may (and often do) hallucinate.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2i4smx","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Even with RAG models (esp. Gemma 3) may (and often do) hallucinate.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lwrd38","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwrd38/best_large_opensource_llm_for_healthmedical_data/n2i4smx/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752214133,"author_flair_text":null,"treatment_tags":[],"created_utc":1752214133,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}}],"before":null}},"user_reports":[],"saved":false,"id":"n2gwd2l","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"LeastExperience1579","can_mod_post":false,"created_utc":1752196045,"send_replies":true,"parent_id":"t1_n2gp7fc","score":1,"author_fullname":"t2_e4ojre534","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"We are trying to use it with RAG mostly","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2gwd2l","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;We are trying to use it with RAG mostly&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lwrd38","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwrd38/best_large_opensource_llm_for_healthmedical_data/n2gwd2l/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752196045,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2gp7fc","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"InvertedVantage","can_mod_post":false,"created_utc":1752193541,"send_replies":true,"parent_id":"t3_1lwrd38","score":2,"author_fullname":"t2_4me51","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"How are you going to deal with hallucinations?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2gp7fc","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;How are you going to deal with hallucinations?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwrd38/best_large_opensource_llm_for_healthmedical_data/n2gp7fc/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752193541,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lwrd38","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"total_awards_received":0,"approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"ups":1,"removal_reason":null,"link_id":"t3_1lwrd38","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2ix8ej","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Tuxedotux83","can_mod_post":false,"created_utc":1752229881,"send_replies":true,"parent_id":"t1_n2i5aup","score":1,"author_fullname":"t2_tjvcxt3jh","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"What 4B model you are using that has such amazing reasoning skills? From my experience even 7B-14B models with reasoning are still not capable enough for real workloads with accuracy","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2ix8ej","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;What 4B model you are using that has such amazing reasoning skills? From my experience even 7B-14B models with reasoning are still not capable enough for real workloads with accuracy&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lwrd38","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwrd38/best_large_opensource_llm_for_healthmedical_data/n2ix8ej/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752229881,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2i5aup","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":"DELETED","no_follow":true,"author":"[deleted]","can_mod_post":false,"send_replies":true,"parent_id":"t3_1lwrd38","score":1,"approved_by":null,"report_reasons":null,"all_awardings":[],"subreddit_id":"t5_81eyvm","body":"[deleted]","edited":false,"downs":0,"author_flair_css_class":null,"collapsed":true,"is_submitter":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;[deleted]&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"associated_award":null,"stickied":false,"subreddit_type":"public","can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":"dark","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwrd38/best_large_opensource_llm_for_healthmedical_data/n2i5aup/","num_reports":null,"locked":false,"name":"t1_n2i5aup","created":1752214396,"subreddit":"LocalLLaMA","author_flair_text":null,"treatment_tags":[],"created_utc":1752214396,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"","collapsed_because_crowd_control":null,"mod_reports":[],"mod_note":null,"distinguished":null}}],"before":null}}]`),o=()=>e.jsx(l,{data:a});export{o as default};
