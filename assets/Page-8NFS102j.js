import{j as e}from"./index-Bu7qcPAU.js";import{R as l}from"./RedditPostRenderer-CbHA7O5q.js";import"./index-BKgbfxhf.js";const t=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"I am building a home server under $6000. I want to run concurrent queries on local q4 or q8 quantised LLM. Should I buy w7900 or anything else??","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Will w7900 work?","link_flair_richtext":[{"e":"text","t":"Resources"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1ltmrvo","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.43,"author_flair_background_color":null,"subreddit_type":"public","ups":0,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_1n0ydmtkw5","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Resources","can_mod_post":false,"score":0,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1751868785,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;I am building a home server under $6000. I want to run concurrent queries on local q4 or q8 quantised LLM. Should I buy w7900 or anything else??&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":true,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#ccac2b","id":"1ltmrvo","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"DatakeeperFun7770","discussion_type":null,"num_comments":6,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1ltmrvo/will_w7900_work/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1ltmrvo/will_w7900_work/","subreddit_subscribers":496034,"created_utc":1751868785,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1rlmdz","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"rgroadie2707","can_mod_post":false,"created_utc":1751870237,"send_replies":true,"parent_id":"t1_n1rkkpm","score":2,"author_fullname":"t2_7bb63l85","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"What would be your recommendation if one has around 3500usd to spend for GPUs. What would be the best recommendation for this budget","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1rlmdz","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;What would be your recommendation if one has around 3500usd to spend for GPUs. What would be the best recommendation for this budget&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ltmrvo","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltmrvo/will_w7900_work/n1rlmdz/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751870237,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n1rkkpm","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Tyme4Trouble","can_mod_post":false,"created_utc":1751869647,"send_replies":true,"parent_id":"t3_1ltmrvo","score":2,"author_fullname":"t2_973amyap","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Yes it’ll work.\\nI have one I’ve used for LLM inference and fine tuning in axolotl. It’s fine. Couple of things to be aware of. \\n\\nIts floating point and integer performance is very poor compared to a RTX 6000 ADA. 120 T(FL)OPS FP16/Int8 and no sparsity support. The 6000 ADA is about 3-6x faster.\\n\\nFor inference at low batch this isn’t that big a deal. Both cards have 48GB and 960GB/s of memory bandwidth.\\n\\nSo, if you are running something like 1-4 concurrent requests and the prompt isn’t huge, you won’t notice a difference. \\n\\nMight look at the W9600 it’s only 32GB and 600GB/s but it’s much faster than the W7900.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1rkkpm","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yes it’ll work.\\nI have one I’ve used for LLM inference and fine tuning in axolotl. It’s fine. Couple of things to be aware of. &lt;/p&gt;\\n\\n&lt;p&gt;Its floating point and integer performance is very poor compared to a RTX 6000 ADA. 120 T(FL)OPS FP16/Int8 and no sparsity support. The 6000 ADA is about 3-6x faster.&lt;/p&gt;\\n\\n&lt;p&gt;For inference at low batch this isn’t that big a deal. Both cards have 48GB and 960GB/s of memory bandwidth.&lt;/p&gt;\\n\\n&lt;p&gt;So, if you are running something like 1-4 concurrent requests and the prompt isn’t huge, you won’t notice a difference. &lt;/p&gt;\\n\\n&lt;p&gt;Might look at the W9600 it’s only 32GB and 600GB/s but it’s much faster than the W7900.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltmrvo/will_w7900_work/n1rkkpm/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751869647,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1ltmrvo","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1ruof3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"randomfoo2","can_mod_post":false,"created_utc":1751875515,"send_replies":true,"parent_id":"t3_1ltmrvo","score":1,"author_fullname":"t2_eztox","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I have a W7900 (and multiple 3090s, 4090s, etc). I believe that W7900s will probably be about $3000 used. Personally, I would go for 2 x 3090 instead if your focus is inference. You can set the power limit to 300W each and you should be able to get 4 x used 3090 (96 GB of total VRAM) for about the same price.\\n\\n  \\nRun vLLM or SGLang w/ 2 x TP=2 and use a session aware router for max perf.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1ruof3","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I have a W7900 (and multiple 3090s, 4090s, etc). I believe that W7900s will probably be about $3000 used. Personally, I would go for 2 x 3090 instead if your focus is inference. You can set the power limit to 300W each and you should be able to get 4 x used 3090 (96 GB of total VRAM) for about the same price.&lt;/p&gt;\\n\\n&lt;p&gt;Run vLLM or SGLang w/ 2 x TP=2 and use a session aware router for max perf.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltmrvo/will_w7900_work/n1ruof3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751875515,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1ltmrvo","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"7dba5c08-72f1-11ee-9b6f-ca195bc297d4","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1s2vfx","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"PraxisOG","can_mod_post":false,"created_utc":1751880484,"send_replies":true,"parent_id":"t3_1ltmrvo","score":1,"author_fullname":"t2_3f9vjjno","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Depends what you're trying to run. Two 3090s would run models up to 70b at really good speeds, of which you have very good selection.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1s2vfx","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"Llama 70B"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Depends what you&amp;#39;re trying to run. Two 3090s would run models up to 70b at really good speeds, of which you have very good selection.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltmrvo/will_w7900_work/n1s2vfx/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751880484,"author_flair_text":"Llama 70B","treatment_tags":[],"link_id":"t3_1ltmrvo","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1y6l7m","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DatakeeperFun7770","can_mod_post":false,"created_utc":1751960897,"send_replies":true,"parent_id":"t1_n1y69sc","score":1,"author_fullname":"t2_1n0ydmtkw5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Why are you guys so against it? It has the most vram. What else should I look for?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1y6l7m","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Why are you guys so against it? It has the most vram. What else should I look for?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ltmrvo","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltmrvo/will_w7900_work/n1y6l7m/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751960897,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1y69sc","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Rich_Repeat_22","can_mod_post":false,"created_utc":1751960712,"send_replies":true,"parent_id":"t3_1ltmrvo","score":1,"author_fullname":"t2_viufiki6","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"If you find the W7900 for $1000 yes, otherwise, HELLLL NOOOOOOOO.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1y69sc","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;If you find the W7900 for $1000 yes, otherwise, HELLLL NOOOOOOOO.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltmrvo/will_w7900_work/n1y69sc/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751960712,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1ltmrvo","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),n=()=>e.jsx(l,{data:t});export{n as default};
