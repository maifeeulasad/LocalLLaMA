import{j as e}from"./index-BOnf-UhU.js";import{R as l}from"./RedditPostRenderer-Ce39qICS.js";import"./index-CDase6VD.js";const t=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Hi,\\n\\nThere are two JS libraries, Transformers.js and WebLLM, for embedding language models in a web application. They seems to target different applications, with a significant(?) overlap.\\n\\nWhat is your experience with any of these, in terms of efficency, coverage, and precision, for a non-interactive (i.e. not chat with user) application? Does any of them offer better support for more cutting-edge models?\\n\\nConsider text-summarisation as an example application. Which one is better in providing that?","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Transformers.js vs WebLLM","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lw6jz5","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.94,"author_flair_background_color":null,"subreddit_type":"public","ups":16,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_127kho","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":16,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":1752136093,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1752132310,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\\n\\n&lt;p&gt;There are two JS libraries, Transformers.js and WebLLM, for embedding language models in a web application. They seems to target different applications, with a significant(?) overlap.&lt;/p&gt;\\n\\n&lt;p&gt;What is your experience with any of these, in terms of efficency, coverage, and precision, for a non-interactive (i.e. not chat with user) application? Does any of them offer better support for more cutting-edge models?&lt;/p&gt;\\n\\n&lt;p&gt;Consider text-summarisation as an example application. Which one is better in providing that?&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1lw6jz5","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"ihatebeinganonymous","discussion_type":null,"num_comments":2,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lw6jz5/transformersjs_vs_webllm/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lw6jz5/transformersjs_vs_webllm/","subreddit_subscribers":497354,"created_utc":1752132310,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2hv3py","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ihatebeinganonymous","can_mod_post":false,"created_utc":1752209399,"send_replies":true,"parent_id":"t1_n2fzj4p","score":2,"author_fullname":"t2_127kho","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Many thanks. I didn't know about those other libraries.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2hv3py","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Many thanks. I didn&amp;#39;t know about those other libraries.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lw6jz5","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lw6jz5/transformersjs_vs_webllm/n2hv3py/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752209399,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n2fzj4p","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Felladrin","can_mod_post":false,"created_utc":1752185042,"send_replies":true,"parent_id":"t3_1lw6jz5","score":2,"author_fullname":"t2_wp9mv","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"There are also Wllama, Ratchet and Candle libraries, which allows running LLMs on the web browser, but Transformers.js and MLC are indeed the most famous.\\n\\nTransformers.js is more on the bleeding edge due to being supported by Hugging Face. Also it supports much more tasks than MLC. MLC, as far as I know, only supports text-generation, text-embedding and image generation (old stable-diffusion release).\\n\\nBoth run on the inference on WebGPU. But Transformers.js can also run on CPU, if needed.\\n\\nMLC splits the files in several shards (similar to how gguf-split splits GGUF files), while transformers.js uses large ONNX single-files, which causes mobile devices fail to load those large files. So if you’re targeting LLMs running on mobile device browsers with WebGPU support, MLC fits better. But if you’re targeting Desktop browsers, then using Transformers.js will allow you to use more recent models earlier.\\n\\nNow, if you are planning to run LLMs on CPU in the browser, then I recommend using Wllama over Transformers.js, because the multi-threading from ONNX Web Runtime, doesn’t work well (at least it didn’t, last time I checked it). Wllama uses GGUF files, so you can basically run any mode that llama.cpp supports (as far as the sum of gguf shards size is below 4 GB, due to WASM limitations).","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2fzj4p","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;There are also Wllama, Ratchet and Candle libraries, which allows running LLMs on the web browser, but Transformers.js and MLC are indeed the most famous.&lt;/p&gt;\\n\\n&lt;p&gt;Transformers.js is more on the bleeding edge due to being supported by Hugging Face. Also it supports much more tasks than MLC. MLC, as far as I know, only supports text-generation, text-embedding and image generation (old stable-diffusion release).&lt;/p&gt;\\n\\n&lt;p&gt;Both run on the inference on WebGPU. But Transformers.js can also run on CPU, if needed.&lt;/p&gt;\\n\\n&lt;p&gt;MLC splits the files in several shards (similar to how gguf-split splits GGUF files), while transformers.js uses large ONNX single-files, which causes mobile devices fail to load those large files. So if you’re targeting LLMs running on mobile device browsers with WebGPU support, MLC fits better. But if you’re targeting Desktop browsers, then using Transformers.js will allow you to use more recent models earlier.&lt;/p&gt;\\n\\n&lt;p&gt;Now, if you are planning to run LLMs on CPU in the browser, then I recommend using Wllama over Transformers.js, because the multi-threading from ONNX Web Runtime, doesn’t work well (at least it didn’t, last time I checked it). Wllama uses GGUF files, so you can basically run any mode that llama.cpp supports (as far as the sum of gguf shards size is below 4 GB, due to WASM limitations).&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lw6jz5/transformersjs_vs_webllm/n2fzj4p/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752185042,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lw6jz5","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}}]`),n=()=>e.jsx(l,{data:t});export{n as default};
