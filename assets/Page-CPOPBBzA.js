import{j as e}from"./index-M4edQi1P.js";import{R as a}from"./RedditPostRenderer-CESBGIIy.js";import"./index-DFpL1mt4.js";const l=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Hi guys,   \\nis this the correct python payload format for ollama?\\n\\n    {\\n    \\"role\\": \\"user\\",\\n      \\"content\\": \\"what is in this image?\\",\\n      \\"images\\": [\\"iVBORw0KQuS...\\"] #base64\\n    }\\n\\nI am asking because for both openrouter and ollama running the same gemma12b passed the same input and image encodings, openrouter returned sense and ollama seemed to have no clue about the image it's describing. Ollama documentation says this is right, but myself tested for a while and I couldn't get the same result from oenrouter and ollama. My goal is to making a python image to llm to text parser. \\n\\nThanks for helping!","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Ollama API image payload format for python","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lst2uk","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.2,"author_flair_background_color":null,"subreddit_type":"public","ups":0,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_77yd9w74","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":0,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1751777897,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Hi guys,&lt;br/&gt;\\nis this the correct python payload format for ollama?&lt;/p&gt;\\n\\n&lt;pre&gt;&lt;code&gt;{\\n&amp;quot;role&amp;quot;: &amp;quot;user&amp;quot;,\\n  &amp;quot;content&amp;quot;: &amp;quot;what is in this image?&amp;quot;,\\n  &amp;quot;images&amp;quot;: [&amp;quot;iVBORw0KQuS...&amp;quot;] #base64\\n}\\n&lt;/code&gt;&lt;/pre&gt;\\n\\n&lt;p&gt;I am asking because for both openrouter and ollama running the same gemma12b passed the same input and image encodings, openrouter returned sense and ollama seemed to have no clue about the image it&amp;#39;s describing. Ollama documentation says this is right, but myself tested for a while and I couldn&amp;#39;t get the same result from oenrouter and ollama. My goal is to making a python image to llm to text parser. &lt;/p&gt;\\n\\n&lt;p&gt;Thanks for helping!&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":true,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1lst2uk","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"Ok-Internal9317","discussion_type":null,"num_comments":2,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lst2uk/ollama_api_image_payload_format_for_python/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lst2uk/ollama_api_image_payload_format_for_python/","subreddit_subscribers":495652,"created_utc":1751777897,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1pdld2","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Ok-Internal9317","can_mod_post":false,"created_utc":1751838059,"send_replies":true,"parent_id":"t1_n1lnblr","score":1,"author_fullname":"t2_77yd9w74","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Thanks, yes indeed it was very frustrating and what a surprise gemma12b in ollama is text only; 512x512 is too small no? (I haven't experimented with such small resolution and I'll try it out)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1pdld2","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thanks, yes indeed it was very frustrating and what a surprise gemma12b in ollama is text only; 512x512 is too small no? (I haven&amp;#39;t experimented with such small resolution and I&amp;#39;ll try it out)&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lst2uk","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lst2uk/ollama_api_image_payload_format_for_python/n1pdld2/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751838059,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1lnblr","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"godndiogoat","can_mod_post":false,"created_utc":1751788074,"send_replies":true,"parent_id":"t3_1lst2uk","score":1,"author_fullname":"t2_1o8b7or53v","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Gemma 12b in Ollama is text-only, so no matter how you pack the base64 the model just throws the bytes into the prompt and guesses. The same name on OpenRouter is silently mapped to a llava-augmented fork, which is why it looks smarter. Keep the payload you already have, but spin up a vision model that Ollama actually supports, e.g. llava:13b, cogvlm:17b, bakllava:8b, or even phi3-vision if you side-load it. In python just add model='llava:13b' to the /api/chat call and keep images=[b64] as you’re doing. Strip newlines from the string and make sure it’s jpeg or png under 2-3 MB; larger images choke. I route the base64 through Pillow to resize to 512 on the long edge before dumping. For post-processing captions LangChain’s OutputParser saves a lot of typing, while FastAPI lets you expose it as microservice; APIWrapper.ai handles the retry logic when you batch multiple shots. Switch to a vision-ready model and the same payload will start giving sane answers.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1lnblr","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Gemma 12b in Ollama is text-only, so no matter how you pack the base64 the model just throws the bytes into the prompt and guesses. The same name on OpenRouter is silently mapped to a llava-augmented fork, which is why it looks smarter. Keep the payload you already have, but spin up a vision model that Ollama actually supports, e.g. llava:13b, cogvlm:17b, bakllava:8b, or even phi3-vision if you side-load it. In python just add model=&amp;#39;llava:13b&amp;#39; to the /api/chat call and keep images=[b64] as you’re doing. Strip newlines from the string and make sure it’s jpeg or png under 2-3 MB; larger images choke. I route the base64 through Pillow to resize to 512 on the long edge before dumping. For post-processing captions LangChain’s OutputParser saves a lot of typing, while FastAPI lets you expose it as microservice; APIWrapper.ai handles the retry logic when you batch multiple shots. Switch to a vision-ready model and the same payload will start giving sane answers.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lst2uk/ollama_api_image_payload_format_for_python/n1lnblr/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751788074,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lst2uk","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),n=()=>e.jsx(a,{data:l});export{n as default};
