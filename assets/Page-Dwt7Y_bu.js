import{j as e}from"./index-C_z07ZVC.js";import{R as t}from"./RedditPostRenderer-DPnSR41P.js";import"./index-DKzOAewW.js";const l=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Sorry for the newbie question, I wonder if I have multiple user's messages for context, question, tool output etc.. vs I concatenate them as one user message to send to chat/completions endpoint, would there be any difference. I do not have a good enough test set to check, please share if you know this has been studied before.   \\nMy best bet is to look at docs or source codes of API tools like vllm to see how it's handled. I tried searching but most results are on how to use the endpoints not how it works internally.  \\nSupposedly these messages together with system prompt and previous messages would be concatenated into one string somewhere, and new tokens would be generated based on that. Please share if you know this is done. Thanks.","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"how are chat completion messages handled in backend logic of API services like with vllm","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lopls4","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.67,"author_flair_background_color":null,"subreddit_type":"public","ups":1,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_9s4xf","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":1,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1751334642,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Sorry for the newbie question, I wonder if I have multiple user&amp;#39;s messages for context, question, tool output etc.. vs I concatenate them as one user message to send to chat/completions endpoint, would there be any difference. I do not have a good enough test set to check, please share if you know this has been studied before.&lt;br/&gt;\\nMy best bet is to look at docs or source codes of API tools like vllm to see how it&amp;#39;s handled. I tried searching but most results are on how to use the endpoints not how it works internally.&lt;br/&gt;\\nSupposedly these messages together with system prompt and previous messages would be concatenated into one string somewhere, and new tokens would be generated based on that. Please share if you know this is done. Thanks.&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":true,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1lopls4","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"woodenleaf","discussion_type":null,"num_comments":6,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lopls4/how_are_chat_completion_messages_handled_in/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lopls4/how_are_chat_completion_messages_handled_in/","subreddit_subscribers":493458,"created_utc":1751334642,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0pg39h","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ShengrenR","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0p1j53","score":1,"author_fullname":"t2_ji4n4","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"The main thing is maintaining the specific template pattern - the model may be 'used' to seeing user/assistant/user/assistant, but so long as you stuff two 'user' inputs into a single segment of the templating (&lt;|im\\\\_start|&gt; or &lt;s&gt; or whatever delineates 'turns') you'd likely be golden; so for chatml, for example:\\n\\n    &lt;|im_start|&gt;system\\n    You are a helpful assistant.&lt;|im_end|&gt;\\n    &lt;|im_start|&gt;user\\n    Hello! \\\\n And hello again!&lt;|im_end|&gt;\\n    &lt;|im_start|&gt;assistant\\n\\nWill likely work just fine, but &lt;|im\\\\_start|&gt;user \\\\\\\\n prompt1&lt;|im\\\\_end|&gt;&lt;|im\\\\_start|&gt;user \\\\\\\\n prompt2&lt;|im\\\\_end|&gt; etc will likely 'confuse' a lot of models.  If you're just sending a 'messages' object with dicts, the backend is applying the chat template via jinja rules - and will usually just get mad at you if you try to send 2 user messages back to back - in that case, you'd likely just want to keep the user/assist/user pattern but stuff multiple user inputs into a single 'user' message.\\n\\nThis all depends on what you're actually trying to DO here though - why is the user sending multiple inputs without expecting an assistant reply in between and why isn't it just a single, longer, input. If it's responding to some external request for input on something, you can just engineer that part in your code - stuff in more text there to make it make sense.\\n\\nThe entire input context is just a play and you're the stage director - you can do anything you like to it - make it make sense to the assistant coming next and you'll likely have better outputs.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0pg39h","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The main thing is maintaining the specific template pattern - the model may be &amp;#39;used&amp;#39; to seeing user/assistant/user/assistant, but so long as you stuff two &amp;#39;user&amp;#39; inputs into a single segment of the templating (&amp;lt;|im_start|&amp;gt; or &amp;lt;s&amp;gt; or whatever delineates &amp;#39;turns&amp;#39;) you&amp;#39;d likely be golden; so for chatml, for example:&lt;/p&gt;\\n\\n&lt;pre&gt;&lt;code&gt;&amp;lt;|im_start|&amp;gt;system\\nYou are a helpful assistant.&amp;lt;|im_end|&amp;gt;\\n&amp;lt;|im_start|&amp;gt;user\\nHello! \\\\n And hello again!&amp;lt;|im_end|&amp;gt;\\n&amp;lt;|im_start|&amp;gt;assistant\\n&lt;/code&gt;&lt;/pre&gt;\\n\\n&lt;p&gt;Will likely work just fine, but &amp;lt;|im_start|&amp;gt;user \\\\n prompt1&amp;lt;|im_end|&amp;gt;&amp;lt;|im_start|&amp;gt;user \\\\n prompt2&amp;lt;|im_end|&amp;gt; etc will likely &amp;#39;confuse&amp;#39; a lot of models.  If you&amp;#39;re just sending a &amp;#39;messages&amp;#39; object with dicts, the backend is applying the chat template via jinja rules - and will usually just get mad at you if you try to send 2 user messages back to back - in that case, you&amp;#39;d likely just want to keep the user/assist/user pattern but stuff multiple user inputs into a single &amp;#39;user&amp;#39; message.&lt;/p&gt;\\n\\n&lt;p&gt;This all depends on what you&amp;#39;re actually trying to DO here though - why is the user sending multiple inputs without expecting an assistant reply in between and why isn&amp;#39;t it just a single, longer, input. If it&amp;#39;s responding to some external request for input on something, you can just engineer that part in your code - stuff in more text there to make it make sense.&lt;/p&gt;\\n\\n&lt;p&gt;The entire input context is just a play and you&amp;#39;re the stage director - you can do anything you like to it - make it make sense to the assistant coming next and you&amp;#39;ll likely have better outputs.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lopls4","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lopls4/how_are_chat_completion_messages_handled_in/n0pg39h/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751345008,"author_flair_text":null,"treatment_tags":[],"created_utc":1751345008,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0p1j53","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"woodenleaf","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0ozvbv","score":1,"author_fullname":"t2_9s4xf","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"multiple user messages for one assistant answer might not be what the model sees during training, I guess i'll just have to figure out whether there's any difference in model's response or not by trying it out myself.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0p1j53","is_submitter":true,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;multiple user messages for one assistant answer might not be what the model sees during training, I guess i&amp;#39;ll just have to figure out whether there&amp;#39;s any difference in model&amp;#39;s response or not by trying it out myself.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lopls4","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lopls4/how_are_chat_completion_messages_handled_in/n0p1j53/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751338910,"author_flair_text":null,"treatment_tags":[],"created_utc":1751338910,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0ozvbv","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"woodenleaf","can_mod_post":false,"created_utc":1751338280,"send_replies":true,"parent_id":"t1_n0oro7a","score":2,"author_fullname":"t2_9s4xf","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"thank you, I found it, though not from your link but the mentioned textgenerationpipeline one  \\n  \\nhttps://huggingface.co/docs/transformers/main/en/main\\\\_classes/pipelines#transformers.TextGenerationPipeline.  \\n  \\nParameters\\n\\n* **text\\\\_inputs** (\`str\`, \`list[str]\`, list\\\\[dict\\\\[str, str\\\\]\\\\], or \`list[list[dict[str, str]]]\`) — One or several prompts (or one list of prompts) to complete. If strings or a list of string are passed, this pipeline will continue each prompt. Alternatively, a “chat”, in the form of a list of dicts with “role” and “content” keys, can be passed, or a list of such chats. When chats are passed, the **model’s chat template** will be used to format them before passing them to the model\\n\\nSo basically different LLM models (the instruct ones) are trained with chat conversations in their own prompt template. For example the prompt template for llama3 can be found under the model documentation.  \\nsome discussion here  \\n[https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct/discussions/14](https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct/discussions/14)  \\nthen official doc here  \\n[https://github.com/meta-llama/llama3?tab=readme-ov-file#instruction-tuned-models](https://github.com/meta-llama/llama3?tab=readme-ov-file#instruction-tuned-models)  \\nsource code here  \\n[https://github.com/meta-llama/llama3/blob/main/llama/tokenizer.py#L202](https://github.com/meta-llama/llama3/blob/main/llama/tokenizer.py#L202)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0ozvbv","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;thank you, I found it, though not from your link but the mentioned textgenerationpipeline one  &lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://huggingface.co/docs/transformers/main/en/main%5C_classes/pipelines#transformers.TextGenerationPipeline\\"&gt;https://huggingface.co/docs/transformers/main/en/main\\\\_classes/pipelines#transformers.TextGenerationPipeline&lt;/a&gt;.  &lt;/p&gt;\\n\\n&lt;p&gt;Parameters&lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;&lt;strong&gt;text_inputs&lt;/strong&gt; (&lt;code&gt;str&lt;/code&gt;, &lt;code&gt;list[str]&lt;/code&gt;, list[dict[str, str]], or &lt;code&gt;list[list[dict[str, str]]]&lt;/code&gt;) — One or several prompts (or one list of prompts) to complete. If strings or a list of string are passed, this pipeline will continue each prompt. Alternatively, a “chat”, in the form of a list of dicts with “role” and “content” keys, can be passed, or a list of such chats. When chats are passed, the &lt;strong&gt;model’s chat template&lt;/strong&gt; will be used to format them before passing them to the model&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;p&gt;So basically different LLM models (the instruct ones) are trained with chat conversations in their own prompt template. For example the prompt template for llama3 can be found under the model documentation.&lt;br/&gt;\\nsome discussion here&lt;br/&gt;\\n&lt;a href=\\"https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct/discussions/14\\"&gt;https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct/discussions/14&lt;/a&gt;&lt;br/&gt;\\nthen official doc here&lt;br/&gt;\\n&lt;a href=\\"https://github.com/meta-llama/llama3?tab=readme-ov-file#instruction-tuned-models\\"&gt;https://github.com/meta-llama/llama3?tab=readme-ov-file#instruction-tuned-models&lt;/a&gt;&lt;br/&gt;\\nsource code here&lt;br/&gt;\\n&lt;a href=\\"https://github.com/meta-llama/llama3/blob/main/llama/tokenizer.py#L202\\"&gt;https://github.com/meta-llama/llama3/blob/main/llama/tokenizer.py#L202&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lopls4","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lopls4/how_are_chat_completion_messages_handled_in/n0ozvbv/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751338280,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n0oro7a","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DinoAmino","can_mod_post":false,"created_utc":1751335275,"send_replies":true,"parent_id":"t3_1lopls4","score":2,"author_fullname":"t2_j1v7f","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"This HF page covers the basics of managing chat and chat history. Hope it helps \\n\\nhttps://huggingface.co/docs/transformers/main/conversations","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0oro7a","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;This HF page covers the basics of managing chat and chat history. Hope it helps &lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://huggingface.co/docs/transformers/main/conversations\\"&gt;https://huggingface.co/docs/transformers/main/conversations&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lopls4/how_are_chat_completion_messages_handled_in/n0oro7a/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751335275,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lopls4","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0pccft","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"complead","can_mod_post":false,"created_utc":1751343303,"send_replies":true,"parent_id":"t3_1lopls4","score":1,"author_fullname":"t2_uzn88fhh","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"It's worth considering how different APIs concatenate messages. While reviewing the source code is often insightful. Understanding how context switches or transitions between user messages are managed can be crucial in optimizing for accuracy and coherence.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0pccft","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It&amp;#39;s worth considering how different APIs concatenate messages. While reviewing the source code is often insightful. Understanding how context switches or transitions between user messages are managed can be crucial in optimizing for accuracy and coherence.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lopls4/how_are_chat_completion_messages_handled_in/n0pccft/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751343303,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lopls4","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),o=()=>e.jsx(t,{data:l});export{o as default};
