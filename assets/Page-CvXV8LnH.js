import{j as e}from"./index-Bu7qcPAU.js";import{R as t}from"./RedditPostRenderer-CbHA7O5q.js";import"./index-BKgbfxhf.js";const l=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"We’re looking to build a local compute cluster to run DeepSeek-V3 670B (or similar top-tier open-weight LLMs) for inference only, supporting ~100 simultaneous chatbot users with large context windows (ideally up to 128K tokens).\\n\\nOur preferred direction is an Apple Silicon cluster — likely Mac minis or studios with M-series chips — but we’re open to alternative architectures (e.g. GPU servers) if they offer significantly better performance or scalability.\\n\\nLooking for advice on:\\n\\n* Is it feasible to run 670B locally in that budget?\\n\\n* What’s the largest model realistically deployable with decent latency at 100-user scale?\\n\\n* Can Apple Silicon handle this effectively — and if so, which exact machines should we buy within $40K–$80K?\\n\\n* How would a setup like this handle long-context windows (e.g. 128K) in practice?\\n\\n* Are there alternative model/infra combos we should be considering?\\n\\nWould love to hear from anyone who’s attempted something like this or has strong opinions on maximizing local LLM performance per dollar. Specifics about things to investigate, recommendations on what to run it on, or where to look for a quote are greatly appreciated!\\n\\nEdit: I’ve reached the conclusion from you guys and my own research that full context window with the user county I specified isn’t feasible. Thoughts on how to appropriately adjust context window/quantization without major loss to bring things in line with budget are welcome. ","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Best Hardware Setup to Run DeepSeek-V3 670B Locally on $40K–$80K?","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1m2rw38","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.84,"author_flair_background_color":null,"subreddit_type":"public","ups":30,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_ud8e7o0","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":30,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":1752812370,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1752809658,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;We’re looking to build a local compute cluster to run DeepSeek-V3 670B (or similar top-tier open-weight LLMs) for inference only, supporting ~100 simultaneous chatbot users with large context windows (ideally up to 128K tokens).&lt;/p&gt;\\n\\n&lt;p&gt;Our preferred direction is an Apple Silicon cluster — likely Mac minis or studios with M-series chips — but we’re open to alternative architectures (e.g. GPU servers) if they offer significantly better performance or scalability.&lt;/p&gt;\\n\\n&lt;p&gt;Looking for advice on:&lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;&lt;p&gt;Is it feasible to run 670B locally in that budget?&lt;/p&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;p&gt;What’s the largest model realistically deployable with decent latency at 100-user scale?&lt;/p&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;p&gt;Can Apple Silicon handle this effectively — and if so, which exact machines should we buy within $40K–$80K?&lt;/p&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;p&gt;How would a setup like this handle long-context windows (e.g. 128K) in practice?&lt;/p&gt;&lt;/li&gt;\\n&lt;li&gt;&lt;p&gt;Are there alternative model/infra combos we should be considering?&lt;/p&gt;&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;p&gt;Would love to hear from anyone who’s attempted something like this or has strong opinions on maximizing local LLM performance per dollar. Specifics about things to investigate, recommendations on what to run it on, or where to look for a quote are greatly appreciated!&lt;/p&gt;\\n\\n&lt;p&gt;Edit: I’ve reached the conclusion from you guys and my own research that full context window with the user county I specified isn’t feasible. Thoughts on how to appropriately adjust context window/quantization without major loss to bring things in line with budget are welcome. &lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1m2rw38","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"PrevelantInsanity","discussion_type":null,"num_comments":60,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1m2rw38/best_hardware_setup_to_run_deepseekv3_670b/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1m2rw38/best_hardware_setup_to_run_deepseekv3_670b/","subreddit_subscribers":501232,"created_utc":1752809658,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3ufnq6","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"No_Afternoon_4260","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3ua9ee","score":2,"author_fullname":"t2_cj9kap4bx","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"This cpu has 2 CCD you'll never saturate the theoretical ram bandwidth you are aiming.\\nAnecdotally a 9175F had poor results even tho it has 16 CCD and higher clock.\\nYou need cores clocks and CCD for the the amd plateform, the CCD things seems to be more important for Turin.  \\nYou need to understand server cpu have numa memory domains that are shared between cores and memory controllers. All to say to really use a lot of ram slots you need enough memory controllers that are attached to cores. Cores communicate between them through a fabric and that induce a lot of challenges.  \\nit seems the sweet spot for our community is to get something with at least 8 CCD to hope having 80% (genoa) and 90% potential max ram bandwidth from theoretical.\\nThen take into account that our inference engine aren't really optimised for the challenges induces by what we've talked.  \\nGive it some potential with imho at least a fast 32 cores, that's where I draw the sweet spot for that plateform. But imo threadripper pro is a good alternative if a 9375F is too expensive","edited":false,"author_flair_css_class":null,"name":"t1_n3ufnq6","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;This cpu has 2 CCD you&amp;#39;ll never saturate the theoretical ram bandwidth you are aiming.\\nAnecdotally a 9175F had poor results even tho it has 16 CCD and higher clock.\\nYou need cores clocks and CCD for the the amd plateform, the CCD things seems to be more important for Turin.&lt;br/&gt;\\nYou need to understand server cpu have numa memory domains that are shared between cores and memory controllers. All to say to really use a lot of ram slots you need enough memory controllers that are attached to cores. Cores communicate between them through a fabric and that induce a lot of challenges.&lt;br/&gt;\\nit seems the sweet spot for our community is to get something with at least 8 CCD to hope having 80% (genoa) and 90% potential max ram bandwidth from theoretical.\\nThen take into account that our inference engine aren&amp;#39;t really optimised for the challenges induces by what we&amp;#39;ve talked.&lt;br/&gt;\\nGive it some potential with imho at least a fast 32 cores, that&amp;#39;s where I draw the sweet spot for that plateform. But imo threadripper pro is a good alternative if a 9375F is too expensive&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1m2rw38","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":"light","treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2rw38/best_hardware_setup_to_run_deepseekv3_670b/n3ufnq6/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752857154,"author_flair_text":"llama.cpp","collapsed":false,"created_utc":1752857154,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n3ua9ee","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Far-Item-1202","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3sw3eo","score":3,"author_fullname":"t2_efnrx2eqz","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Motherboard $730 (look for revision 2.00 or newer): https://www.newegg.com/supermicro-h13ssl-nt-amd-epyc-9004-series-processors/p/N82E16813183820\\n\\nCPU $660:\\nhttps://www.newegg.com/amd-epyc-9115-socket-sp5/p/N82E16819113865\\n\\nSP5 CPU cooler ~$100","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3ua9ee","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Motherboard $730 (look for revision 2.00 or newer): &lt;a href=\\"https://www.newegg.com/supermicro-h13ssl-nt-amd-epyc-9004-series-processors/p/N82E16813183820\\"&gt;https://www.newegg.com/supermicro-h13ssl-nt-amd-epyc-9004-series-processors/p/N82E16813183820&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;CPU $660:\\n&lt;a href=\\"https://www.newegg.com/amd-epyc-9115-socket-sp5/p/N82E16819113865\\"&gt;https://www.newegg.com/amd-epyc-9115-socket-sp5/p/N82E16819113865&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;SP5 CPU cooler ~$100&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2rw38","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2rw38/best_hardware_setup_to_run_deepseekv3_670b/n3ua9ee/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752855600,"author_flair_text":null,"treatment_tags":[],"created_utc":1752855600,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n3sw3eo","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"No_Afternoon_4260","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3s6uhc","score":3,"author_fullname":"t2_cj9kap4bx","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"&gt;You can buy on Amazon 768GB (12X64GB) of DDR5-6400 for $4,585.\\n\\n&gt;Buy a case, an AMD EPYC 9005 cpu, and a 12 ram slot server motherboard which supports that much ram, and for about $6500 total...\\n\\nSo you find a mobo and a cpu for 2k usd? You got to explain it to me 🫣","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3sw3eo","is_submitter":false,"collapsed":false,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;You can buy on Amazon 768GB (12X64GB) of DDR5-6400 for $4,585.&lt;/p&gt;\\n\\n&lt;p&gt;Buy a case, an AMD EPYC 9005 cpu, and a 12 ram slot server motherboard which supports that much ram, and for about $6500 total...&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;So you find a mobo and a cpu for 2k usd? You got to explain it to me 🫣&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2rw38","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2rw38/best_hardware_setup_to_run_deepseekv3_670b/n3sw3eo/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752840319,"author_flair_text":"llama.cpp","treatment_tags":[],"created_utc":1752840319,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3v5i0i","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"DepthHour1669","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3uc1dp","score":5,"author_fullname":"t2_t6glzswk","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Depends on which 9005 series CPU. Obviously the cheapest one will be slower than the most expensive one.\\n\\nI think this is a moot point though. I think the 3090 is 285TFLOPs, the cheapest 9005 is 10TFLOPs. Just buy a $600 3090 and throw it in the machine and you can process 128k tokens in 28 seconds. 32 seconds if you factor in 3090 bus lane bandwidth.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3v5i0i","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Depends on which 9005 series CPU. Obviously the cheapest one will be slower than the most expensive one.&lt;/p&gt;\\n\\n&lt;p&gt;I think this is a moot point though. I think the 3090 is 285TFLOPs, the cheapest 9005 is 10TFLOPs. Just buy a $600 3090 and throw it in the machine and you can process 128k tokens in 28 seconds. 32 seconds if you factor in 3090 bus lane bandwidth.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2rw38","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2rw38/best_hardware_setup_to_run_deepseekv3_670b/n3v5i0i/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752864472,"author_flair_text":null,"treatment_tags":[],"created_utc":1752864472,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}}],"before":null}},"user_reports":[],"saved":false,"id":"n3uc1dp","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MKU64","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3s6uhc","score":2,"author_fullname":"t2_wn7it","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Do you know how many TFLOPS would the EPYC 9005 give? One thing is memory bandwidth of course but time to first token is also important if you want a server to begin responding as fast as possible","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3uc1dp","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Do you know how many TFLOPS would the EPYC 9005 give? One thing is memory bandwidth of course but time to first token is also important if you want a server to begin responding as fast as possible&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2rw38","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2rw38/best_hardware_setup_to_run_deepseekv3_670b/n3uc1dp/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752856110,"author_flair_text":null,"treatment_tags":[],"created_utc":1752856110,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n3s6uhc","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"DepthHour1669","can_mod_post":false,"created_utc":1752827802,"send_replies":true,"parent_id":"t1_n3r989a","score":21,"author_fullname":"t2_t6glzswk","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"On the flip side, Apple Silicon isn't the best value at $5-7k either. Just the $10k tier. \\n\\nHowever, at the $5k-7k tier, there's a better option: 12-channel DDR5-6400 is 614GB/sec. The $10k Mac Studio 512gb has 819GB/sec memory bandwidth. \\n\\nhttps://www.amazon.com/NEMIX-RAM-12X64GB-PC5-51200-Registered/dp/B0F7J2WZ8J\\n\\nYou can buy on Amazon 768GB (12X64GB) of DDR5-6400 for $4,585.\\n\\nBuy a case, an AMD EPYC 9005 cpu, and a 12 ram slot server motherboard which supports that much ram, and for about $6500 total... which gives you 50% more RAM than the Mac Studio 512gb but at 75% of the memory bandwidth. \\n\\nWith 768GB ram, you can *run Deepseek R1* ***without quantizing***.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3s6uhc","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;On the flip side, Apple Silicon isn&amp;#39;t the best value at $5-7k either. Just the $10k tier. &lt;/p&gt;\\n\\n&lt;p&gt;However, at the $5k-7k tier, there&amp;#39;s a better option: 12-channel DDR5-6400 is 614GB/sec. The $10k Mac Studio 512gb has 819GB/sec memory bandwidth. &lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://www.amazon.com/NEMIX-RAM-12X64GB-PC5-51200-Registered/dp/B0F7J2WZ8J\\"&gt;https://www.amazon.com/NEMIX-RAM-12X64GB-PC5-51200-Registered/dp/B0F7J2WZ8J&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;You can buy on Amazon 768GB (12X64GB) of DDR5-6400 for $4,585.&lt;/p&gt;\\n\\n&lt;p&gt;Buy a case, an AMD EPYC 9005 cpu, and a 12 ram slot server motherboard which supports that much ram, and for about $6500 total... which gives you 50% more RAM than the Mac Studio 512gb but at 75% of the memory bandwidth. &lt;/p&gt;\\n\\n&lt;p&gt;With 768GB ram, you can &lt;em&gt;run Deepseek R1&lt;/em&gt; &lt;strong&gt;&lt;em&gt;without quantizing&lt;/em&gt;&lt;/strong&gt;.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2rw38","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2rw38/best_hardware_setup_to_run_deepseekv3_670b/n3s6uhc/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752827802,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":21}}],"before":null}},"user_reports":[],"saved":false,"id":"n3r989a","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"GradatimRecovery","can_mod_post":false,"created_utc":1752810693,"send_replies":true,"parent_id":"t3_1m2rw38","score":34,"author_fullname":"t2_z67jm","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"AS only makes sense if your budget is $10k. You can afford 8x RTX Pro 6000 blackwells you get a lot more performance/$ (maybe an order of magnitude) with that than you would a cluster of AS.","edited":1752812323,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3r989a","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;AS only makes sense if your budget is $10k. You can afford 8x RTX Pro 6000 blackwells you get a lot more performance/$ (maybe an order of magnitude) with that than you would a cluster of AS.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2rw38/best_hardware_setup_to_run_deepseekv3_670b/n3r989a/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752810693,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m2rw38","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":34}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"c00d2408-d9ce-11ed-89c3-96d9f437a7a1","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3rp0xs","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Conscious_Cut_6144","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3rev9e","score":8,"author_fullname":"t2_9hl4ymvj","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I just responded to the original post, but I would say 8 pro 6000’s would be ideal. 6 may be doable.\\n\\nSource: I have 8 of them on back order.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3rp0xs","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I just responded to the original post, but I would say 8 pro 6000’s would be ideal. 6 may be doable.&lt;/p&gt;\\n\\n&lt;p&gt;Source: I have 8 of them on back order.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2rw38","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2rw38/best_hardware_setup_to_run_deepseekv3_670b/n3rp0xs/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752818046,"author_flair_text":null,"treatment_tags":[],"created_utc":1752818046,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":8}}],"before":null}},"user_reports":[],"saved":false,"id":"n3rev9e","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"PrevelantInsanity","can_mod_post":false,"created_utc":1752813153,"send_replies":true,"parent_id":"t1_n3redee","score":2,"author_fullname":"t2_ud8e7o0","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"The RTX 6000 pro Blackwell route seems interesting to me. I don’t mind dropping to 4-bit quant. I don’t think that will harm output in a way that matters to me. \\n\\nContext does concern me a bit, as in my research it seems to get really big really fast. We’d only be at like 384gb of VRAM from four blackwells, which seems significant at 4-bit quant? Not sure though.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3rev9e","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The RTX 6000 pro Blackwell route seems interesting to me. I don’t mind dropping to 4-bit quant. I don’t think that will harm output in a way that matters to me. &lt;/p&gt;\\n\\n&lt;p&gt;Context does concern me a bit, as in my research it seems to get really big really fast. We’d only be at like 384gb of VRAM from four blackwells, which seems significant at 4-bit quant? Not sure though.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2rw38","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2rw38/best_hardware_setup_to_run_deepseekv3_670b/n3rev9e/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752813153,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n3redee","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"spookperson","can_mod_post":false,"created_utc":1752812929,"send_replies":true,"parent_id":"t3_1m2rw38","score":15,"author_fullname":"t2_is8jf","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"There are two main issues with building inference servers on Apple in my experience. One is the prompt processing speed - it will be much much lower than you want for large context (even if you're using Apple-optimized MLX). The other is concurrency/throughput software - so far I think even if you compile vllm on Apple you just get CPU-only support.\\n\\n\\nSo in my mind if you spend $10k on a Mac Studio to run very large models like Deepseek, at the moment it is only so-so at production workloads for a single person at a time (so-so because of slow prompt processing, single person because the Apple-compatible inference server software isn't great at throughput in continuous batching). So you could think of that level of budget supporting 4-8 people using the cluster at once but still dealing with very slow prompt processing.\\n\\n\\nOn the other hand, with that $40k-$80k budget you can get Intel/AMD server hardware that supports a bunch of pcie lanes and get a bunch of RTX 6000 Pro Blackwells. 4 of the 96gb cards would be enough to load 4-bit Deepseek and have room for context. You'll need more cards to support higher bit quants and more simultaneous users (and associated size of aggregate kv cache). Just be aware of your power/cooling requirements.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3redee","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"Vicuna"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;There are two main issues with building inference servers on Apple in my experience. One is the prompt processing speed - it will be much much lower than you want for large context (even if you&amp;#39;re using Apple-optimized MLX). The other is concurrency/throughput software - so far I think even if you compile vllm on Apple you just get CPU-only support.&lt;/p&gt;\\n\\n&lt;p&gt;So in my mind if you spend $10k on a Mac Studio to run very large models like Deepseek, at the moment it is only so-so at production workloads for a single person at a time (so-so because of slow prompt processing, single person because the Apple-compatible inference server software isn&amp;#39;t great at throughput in continuous batching). So you could think of that level of budget supporting 4-8 people using the cluster at once but still dealing with very slow prompt processing.&lt;/p&gt;\\n\\n&lt;p&gt;On the other hand, with that $40k-$80k budget you can get Intel/AMD server hardware that supports a bunch of pcie lanes and get a bunch of RTX 6000 Pro Blackwells. 4 of the 96gb cards would be enough to load 4-bit Deepseek and have room for context. You&amp;#39;ll need more cards to support higher bit quants and more simultaneous users (and associated size of aggregate kv cache). Just be aware of your power/cooling requirements.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2rw38/best_hardware_setup_to_run_deepseekv3_670b/n3redee/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752812929,"author_flair_text":"Vicuna","treatment_tags":[],"link_id":"t3_1m2rw38","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#9f9286","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":15}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3rlcva","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Mabuse00","can_mod_post":false,"created_utc":1752816203,"send_replies":true,"parent_id":"t1_n3rkaze","score":3,"author_fullname":"t2_1cf2k0je","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Just an idea you reminded me of - I've been using Deepseek R1 0528 on the Nvidia NIM API. (Which if you don't they have a ton of AI models free up to 40 requests per minute.)The way they pull it off is they have a requests queue to limit how many generations it can do at the same time. I think Nvidia's queue is 30, and I rarely see even a couple of seconds in line, and that's with them serving it free. I don't know what context they serve and I assume it's capped fairly low but their in-website chat uses 4K max token return and the only thing longer than a CVS reciept is a Deepseek think block.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3rlcva","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Just an idea you reminded me of - I&amp;#39;ve been using Deepseek R1 0528 on the Nvidia NIM API. (Which if you don&amp;#39;t they have a ton of AI models free up to 40 requests per minute.)The way they pull it off is they have a requests queue to limit how many generations it can do at the same time. I think Nvidia&amp;#39;s queue is 30, and I rarely see even a couple of seconds in line, and that&amp;#39;s with them serving it free. I don&amp;#39;t know what context they serve and I assume it&amp;#39;s capped fairly low but their in-website chat uses 4K max token return and the only thing longer than a CVS reciept is a Deepseek think block.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2rw38","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2rw38/best_hardware_setup_to_run_deepseekv3_670b/n3rlcva/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752816203,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3swief","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"No_Afternoon_4260","can_mod_post":false,"created_utc":1752840480,"send_replies":true,"parent_id":"t1_n3rkaze","score":1,"author_fullname":"t2_cj9kap4bx","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Ho the expert thing, because it's an moe batch also \\"increase the number of active expert\\" thus the batch effect is lessen. Interesting thanks","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3swief","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Ho the expert thing, because it&amp;#39;s an moe batch also &amp;quot;increase the number of active expert&amp;quot; thus the batch effect is lessen. Interesting thanks&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2rw38","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2rw38/best_hardware_setup_to_run_deepseekv3_670b/n3swief/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752840480,"author_flair_text":"llama.cpp","treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3rkaze","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"eloquentemu","can_mod_post":false,"created_utc":1752815690,"send_replies":true,"parent_id":"t3_1m2rw38","score":10,"author_fullname":"t2_lpdsy","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"To be up front, I haven't really done much with this, especially when it comes to managing multiple long contexts, so maybe there's something I'm overlooking.\\n\\n&gt; Is it feasible to run 670B locally in that budget?\\n\\nWithout knowing the quantization level and expected performance it's hard to say.  For low enough expectations, yes.  Let's say you want to run the FP8 model at 10t/s per user so 1000t/s (though you probably want more like 2000t/s peak to get each user ~10t/s on the mid-size context lengths).  That might not be possible.\\n\\nNote that while 1000t/s might look crazy you can batch inference, meaning process multiple tokens at once, for each user.  Because inference is mostly memory bound, if you have extra compute you can access the weights once and use them for multiple calculations.  Running Qwen3-30B as an example:\\n\\n|    PP |     TG |    B | S_PP t/s | S_TG t/s |\\n|-------|--------|------|----------|----------|\\n|   512 |    128 |    1 |  4162.09 |   170.35 |\\n|   512 |    128 |    4 |  4310.28 |   278.29 |\\n|   512 |    128 |   16 |  4045.05 |   672.99 |\\n|   512 |    128 |   64 |  3199.48 |  1335.82 |\\n\\nYou can see my 4090 'only' gets ~170t/s when processing one context, but gets 1335t/s processing 64 contexts simultaneously.  That's only 20t/s per user and dramatically slower than the 170t/s because this is an MoE like Deepseek.  For a single context only 3B parameters are used, but across 64 context nearly all 30B get used.  For reference Qwen3-32B also gets about 10t/s @ batch=64 but only 40t/s @ batch=1.\\n\\n&gt; Can Apple Silicon handle this effectively — and if so, which exact machines should we buy within $40K–$80K?\\n\\nI think the only real option would be the Mac Studio 512GB.  It runs the Q4 model at ~22t/s, however that is peak for one context (i.e. not batched).  A bit of Googling didn't come up with performance tests for batched execution of 671B on the Mac Studio, but they seem pretty compute bound and coupled with the MoE scaling problems as mentioned before I suspect they will probably cap around 60t/s for maybe batch=4.  So if you buy 8 for $80k you'll come up pretty short.  Unless you're okay running @ Q4 and ~5/ts.\\n\\nIf someone has batched benchmarks though I'd love to see.\\n\\n&gt; How would a setup like this handle long-context windows (e.g. 128K) in practice?\\n\\nDue to Deepseek's MLA 128k context is actually pretty cheap, relative to its size.  128k needs 16.6GB so times 100 is a lot of VRAM.  But, 128k context is also a lot.  It is a full novel, if not more.  You should consider how important that is and/or how simultaneous your users are. \\n\\n&gt; What’s the largest model realistically deployable with decent latency at 100-user scale?\\n\\n&gt; Are there alternative model/infra combos we should be considering?\\n\\nIt's hard to say without understanding your application.  However the 70B range of dense models might be worth a look, or Qwen3.  However, definitely watch the context size for those - I think Llama3.3-70B needs 41GB for 128k!\\n\\nQwen3-32B model might be a decent option.  If you quantize the context to q8 and limit length to 32k you only need 4.3GB which would let run the you serve ~74 users and the model at q8 at very roughly 20t/s from 4x RTX Pro 6000 Blackwell for a total cost of ~$50k.  Maybe that's ok?\\n\\nI guess just to guess about Deepseek... If you get 8x Pro6000 and run the model at q4, that leaves 484GB for context so 30 users at 128k.  Speed?  Hard to even speculate... Max in theory (based on bandwidth vs size of weights supposing all would be active) would be 35t/s, though, so &gt;10t/s seems reasonable at moderate context size.  Of course, 8x Pro6000 is just a touch under $80k already so you won't likely be able to make a decent system without going over budget.\\n\\nP.S. This got long enough, but you could also look into speculative decoding.  It's good for a moderate speed boost but I wouldn't count on it being more than a nice-to-have.  Like it might go from 10-&gt;14 but not 10-&gt;20 t/s.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3rkaze","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;To be up front, I haven&amp;#39;t really done much with this, especially when it comes to managing multiple long contexts, so maybe there&amp;#39;s something I&amp;#39;m overlooking.&lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;Is it feasible to run 670B locally in that budget?&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;Without knowing the quantization level and expected performance it&amp;#39;s hard to say.  For low enough expectations, yes.  Let&amp;#39;s say you want to run the FP8 model at 10t/s per user so 1000t/s (though you probably want more like 2000t/s peak to get each user ~10t/s on the mid-size context lengths).  That might not be possible.&lt;/p&gt;\\n\\n&lt;p&gt;Note that while 1000t/s might look crazy you can batch inference, meaning process multiple tokens at once, for each user.  Because inference is mostly memory bound, if you have extra compute you can access the weights once and use them for multiple calculations.  Running Qwen3-30B as an example:&lt;/p&gt;\\n\\n&lt;table&gt;&lt;thead&gt;\\n&lt;tr&gt;\\n&lt;th&gt;PP&lt;/th&gt;\\n&lt;th&gt;TG&lt;/th&gt;\\n&lt;th&gt;B&lt;/th&gt;\\n&lt;th&gt;S_PP t/s&lt;/th&gt;\\n&lt;th&gt;S_TG t/s&lt;/th&gt;\\n&lt;/tr&gt;\\n&lt;/thead&gt;&lt;tbody&gt;\\n&lt;tr&gt;\\n&lt;td&gt;512&lt;/td&gt;\\n&lt;td&gt;128&lt;/td&gt;\\n&lt;td&gt;1&lt;/td&gt;\\n&lt;td&gt;4162.09&lt;/td&gt;\\n&lt;td&gt;170.35&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;tr&gt;\\n&lt;td&gt;512&lt;/td&gt;\\n&lt;td&gt;128&lt;/td&gt;\\n&lt;td&gt;4&lt;/td&gt;\\n&lt;td&gt;4310.28&lt;/td&gt;\\n&lt;td&gt;278.29&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;tr&gt;\\n&lt;td&gt;512&lt;/td&gt;\\n&lt;td&gt;128&lt;/td&gt;\\n&lt;td&gt;16&lt;/td&gt;\\n&lt;td&gt;4045.05&lt;/td&gt;\\n&lt;td&gt;672.99&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;tr&gt;\\n&lt;td&gt;512&lt;/td&gt;\\n&lt;td&gt;128&lt;/td&gt;\\n&lt;td&gt;64&lt;/td&gt;\\n&lt;td&gt;3199.48&lt;/td&gt;\\n&lt;td&gt;1335.82&lt;/td&gt;\\n&lt;/tr&gt;\\n&lt;/tbody&gt;&lt;/table&gt;\\n\\n&lt;p&gt;You can see my 4090 &amp;#39;only&amp;#39; gets ~170t/s when processing one context, but gets 1335t/s processing 64 contexts simultaneously.  That&amp;#39;s only 20t/s per user and dramatically slower than the 170t/s because this is an MoE like Deepseek.  For a single context only 3B parameters are used, but across 64 context nearly all 30B get used.  For reference Qwen3-32B also gets about 10t/s @ batch=64 but only 40t/s @ batch=1.&lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;Can Apple Silicon handle this effectively — and if so, which exact machines should we buy within $40K–$80K?&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;I think the only real option would be the Mac Studio 512GB.  It runs the Q4 model at ~22t/s, however that is peak for one context (i.e. not batched).  A bit of Googling didn&amp;#39;t come up with performance tests for batched execution of 671B on the Mac Studio, but they seem pretty compute bound and coupled with the MoE scaling problems as mentioned before I suspect they will probably cap around 60t/s for maybe batch=4.  So if you buy 8 for $80k you&amp;#39;ll come up pretty short.  Unless you&amp;#39;re okay running @ Q4 and ~5/ts.&lt;/p&gt;\\n\\n&lt;p&gt;If someone has batched benchmarks though I&amp;#39;d love to see.&lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;How would a setup like this handle long-context windows (e.g. 128K) in practice?&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;Due to Deepseek&amp;#39;s MLA 128k context is actually pretty cheap, relative to its size.  128k needs 16.6GB so times 100 is a lot of VRAM.  But, 128k context is also a lot.  It is a full novel, if not more.  You should consider how important that is and/or how simultaneous your users are. &lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;What’s the largest model realistically deployable with decent latency at 100-user scale?&lt;/p&gt;\\n\\n&lt;p&gt;Are there alternative model/infra combos we should be considering?&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;It&amp;#39;s hard to say without understanding your application.  However the 70B range of dense models might be worth a look, or Qwen3.  However, definitely watch the context size for those - I think Llama3.3-70B needs 41GB for 128k!&lt;/p&gt;\\n\\n&lt;p&gt;Qwen3-32B model might be a decent option.  If you quantize the context to q8 and limit length to 32k you only need 4.3GB which would let run the you serve ~74 users and the model at q8 at very roughly 20t/s from 4x RTX Pro 6000 Blackwell for a total cost of ~$50k.  Maybe that&amp;#39;s ok?&lt;/p&gt;\\n\\n&lt;p&gt;I guess just to guess about Deepseek... If you get 8x Pro6000 and run the model at q4, that leaves 484GB for context so 30 users at 128k.  Speed?  Hard to even speculate... Max in theory (based on bandwidth vs size of weights supposing all would be active) would be 35t/s, though, so &amp;gt;10t/s seems reasonable at moderate context size.  Of course, 8x Pro6000 is just a touch under $80k already so you won&amp;#39;t likely be able to make a decent system without going over budget.&lt;/p&gt;\\n\\n&lt;p&gt;P.S. This got long enough, but you could also look into speculative decoding.  It&amp;#39;s good for a moderate speed boost but I wouldn&amp;#39;t count on it being more than a nice-to-have.  Like it might go from 10-&amp;gt;14 but not 10-&amp;gt;20 t/s.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2rw38/best_hardware_setup_to_run_deepseekv3_670b/n3rkaze/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752815690,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m2rw38","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":10}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3rc2jg","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Alpine_Privacy","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3rbhm7","score":2,"author_fullname":"t2_1t1panb3d8","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Your best bet would be to rent a cluster, deploy ure LLM ( expose say using openwebui or librechat ) do a small pilot and then finalise ure compute. Runpod is a great place to run this experiment. We use this approach works well for us.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3rc2jg","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Your best bet would be to rent a cluster, deploy ure LLM ( expose say using openwebui or librechat ) do a small pilot and then finalise ure compute. Runpod is a great place to run this experiment. We use this approach works well for us.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2rw38","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2rw38/best_hardware_setup_to_run_deepseekv3_670b/n3rc2jg/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752811908,"author_flair_text":null,"treatment_tags":[],"created_utc":1752811908,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n3rbhm7","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Alpine_Privacy","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3r8mv8","score":3,"author_fullname":"t2_1t1panb3d8","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Hey, I totally get you. I saw that same video and was mislead too! Its super hard for organisations to deploy LLMs securely and privately, been there done that 😅 best of luck, on ure build!","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3rbhm7","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Hey, I totally get you. I saw that same video and was mislead too! Its super hard for organisations to deploy LLMs securely and privately, been there done that 😅 best of luck, on ure build!&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2rw38","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2rw38/best_hardware_setup_to_run_deepseekv3_670b/n3rbhm7/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752811657,"author_flair_text":null,"treatment_tags":[],"created_utc":1752811657,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3rflzg","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Mabuse00","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3re4zd","score":2,"author_fullname":"t2_1cf2k0je","approved_by":null,"mod_note":null,"all_awardings":[],"body":"No worries. Getting creative with LLM's and the hardware I load it on is like... about all I ever want to do with my free time. So far one of my best wins has been running Qwen 3 235B on my 4090-based PC.\\n\\nImportant thing to know is these Apple M chips have amazing neural cores but you need to use CoreML which is its own learning curve, though there are some tools to let you convert Tensorflow or Pytorch to CoreML.\\n\\nhttps://github.com/apple/coremltools","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n3rflzg","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;No worries. Getting creative with LLM&amp;#39;s and the hardware I load it on is like... about all I ever want to do with my free time. So far one of my best wins has been running Qwen 3 235B on my 4090-based PC.&lt;/p&gt;\\n\\n&lt;p&gt;Important thing to know is these Apple M chips have amazing neural cores but you need to use CoreML which is its own learning curve, though there are some tools to let you convert Tensorflow or Pytorch to CoreML.&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://github.com/apple/coremltools\\"&gt;https://github.com/apple/coremltools&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1m2rw38","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2rw38/best_hardware_setup_to_run_deepseekv3_670b/n3rflzg/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752813491,"author_flair_text":null,"treatment_tags":[],"created_utc":1752813491,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3s19jd","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"LA_rent_Aficionado","can_mod_post":false,"created_utc":1752824639,"send_replies":true,"parent_id":"t1_n3rw4bp","score":1,"author_fullname":"t2_t8zbiflk","approved_by":null,"mod_note":null,"all_awardings":[],"body":"I’ve seen them in the 8k range, for 8 units he could maybe get a bulk discount and maybe an educational discount.  It’s a far better option if they ever want to pivot to other workflows as well be it image gen or training. But yes, even if you get it for $70k that’s still absurd lol","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n3s19jd","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I’ve seen them in the 8k range, for 8 units he could maybe get a bulk discount and maybe an educational discount.  It’s a far better option if they ever want to pivot to other workflows as well be it image gen or training. But yes, even if you get it for $70k that’s still absurd lol&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1m2rw38","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2rw38/best_hardware_setup_to_run_deepseekv3_670b/n3s19jd/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752824639,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":7,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3rw4bp","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Mabuse00","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3rtfpa","score":2,"author_fullname":"t2_1cf2k0je","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"But the money is no small matter. To run Deepseek, you need 8x RTX 6000 *Pro 96gb at $10k each.","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n3rw4bp","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;But the money is no small matter. To run Deepseek, you need 8x RTX 6000 *Pro 96gb at $10k each.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1m2rw38","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2rw38/best_hardware_setup_to_run_deepseekv3_670b/n3rw4bp/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752821777,"author_flair_text":null,"treatment_tags":[],"created_utc":1752821777,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":6,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n3rtfpa","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"LA_rent_Aficionado","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3re4zd","score":2,"author_fullname":"t2_t8zbiflk","approved_by":null,"mod_note":null,"all_awardings":[],"body":"A cluster of Mac Minis will be so much slower than say buying 8 RTX 6000, not to mention clusters add a whole other layer of complication.  It’s a master of money comparably, sure you’ll have more VRAM but it would wont compare to a dedicated GPU setup. Even with partial cpu offload.","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n3rtfpa","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;A cluster of Mac Minis will be so much slower than say buying 8 RTX 6000, not to mention clusters add a whole other layer of complication.  It’s a master of money comparably, sure you’ll have more VRAM but it would wont compare to a dedicated GPU setup. Even with partial cpu offload.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1m2rw38","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2rw38/best_hardware_setup_to_run_deepseekv3_670b/n3rtfpa/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752820353,"author_flair_text":null,"treatment_tags":[],"created_utc":1752820353,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3rwt98","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Mabuse00","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3re4zd","score":1,"author_fullname":"t2_1cf2k0je","approved_by":null,"mod_note":null,"all_awardings":[],"body":"By the way, I don't want to forget to mention, there are apparently already manufacturer's samples of the M4 Ultra being sent out here and there for review and they're looking like a decent speed boost over the M3 Ultra.","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n3rwt98","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;By the way, I don&amp;#39;t want to forget to mention, there are apparently already manufacturer&amp;#39;s samples of the M4 Ultra being sent out here and there for review and they&amp;#39;re looking like a decent speed boost over the M3 Ultra.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1m2rw38","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2rw38/best_hardware_setup_to_run_deepseekv3_670b/n3rwt98/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752822151,"author_flair_text":null,"treatment_tags":[],"created_utc":1752822151,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3re4zd","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"PrevelantInsanity","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3rdch1","score":1,"author_fullname":"t2_ud8e7o0","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"We were looking at a cluster of Mac minis/studios if that was the route we took, not just one. I admit a lack of insight here, but I am trying to consider what I can find info on. For context, I’m an undergraduate researcher trying to figure this out who has hit a bit of a wall.","edited":false,"author_flair_css_class":null,"name":"t1_n3re4zd","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;We were looking at a cluster of Mac minis/studios if that was the route we took, not just one. I admit a lack of insight here, but I am trying to consider what I can find info on. For context, I’m an undergraduate researcher trying to figure this out who has hit a bit of a wall.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1m2rw38","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2rw38/best_hardware_setup_to_run_deepseekv3_670b/n3re4zd/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752812821,"author_flair_text":null,"collapsed":false,"created_utc":1752812821,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3rdch1","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Mabuse00","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3rbq1u","score":3,"author_fullname":"t2_1cf2k0je","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Think he also needs to keep in mind that Deepseek R1 0528 in full precision / HF transformers is roughly 750gb. Even the most aggressive quants aren't likely to fit on 128gb of ram/vram.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3rdch1","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Think he also needs to keep in mind that Deepseek R1 0528 in full precision / HF transformers is roughly 750gb. Even the most aggressive quants aren&amp;#39;t likely to fit on 128gb of ram/vram.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2rw38","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2rw38/best_hardware_setup_to_run_deepseekv3_670b/n3rdch1/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752812466,"author_flair_text":null,"treatment_tags":[],"created_utc":1752812466,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n3rbq1u","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"photodesignch","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3r8mv8","score":2,"author_fullname":"t2_dnfi3","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"More or less..  keep in mind Mac is shared memory. If it’s 128gb you need to reserve at least 8gb for the OS.\\n\\nOn the other hand, pc is direct mapping. You need 128gb main memory and it would load the LLM first from cpu, then allocate another 128gb vram on GPU so it can mirror over.\\n\\nMac is obviously simpler, but dedicated gpu on pc should perform better.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3rbq1u","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;More or less..  keep in mind Mac is shared memory. If it’s 128gb you need to reserve at least 8gb for the OS.&lt;/p&gt;\\n\\n&lt;p&gt;On the other hand, pc is direct mapping. You need 128gb main memory and it would load the LLM first from cpu, then allocate another 128gb vram on GPU so it can mirror over.&lt;/p&gt;\\n\\n&lt;p&gt;Mac is obviously simpler, but dedicated gpu on pc should perform better.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2rw38","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2rw38/best_hardware_setup_to_run_deepseekv3_670b/n3rbq1u/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752811759,"author_flair_text":null,"treatment_tags":[],"created_utc":1752811759,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3rg6lb","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"rorowhat","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3r8mv8","score":2,"author_fullname":"t2_yq51a","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"As a general rule avoid Apple","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3rg6lb","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;As a general rule avoid Apple&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2rw38","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2rw38/best_hardware_setup_to_run_deepseekv3_670b/n3rg6lb/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752813752,"author_flair_text":null,"treatment_tags":[],"created_utc":1752813752,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":1,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n3r8mv8","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"PrevelantInsanity","can_mod_post":false,"created_utc":1752810447,"send_replies":true,"parent_id":"t1_n3r86iu","score":2,"author_fullname":"t2_ud8e7o0","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Perhaps I’ve misunderstood what I’ve been looking at, but I’ve seen people running these large models on clusters of Apple silicon devices given their MoE nature requiring less raw compute and more VRAM (unified memory!) for just storing the massive amounts of parameters in any fashion that won’t slow things to a halt or near it. \\n\\nIf I’m mistaken I admit that. Will look more.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3r8mv8","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Perhaps I’ve misunderstood what I’ve been looking at, but I’ve seen people running these large models on clusters of Apple silicon devices given their MoE nature requiring less raw compute and more VRAM (unified memory!) for just storing the massive amounts of parameters in any fashion that won’t slow things to a halt or near it. &lt;/p&gt;\\n\\n&lt;p&gt;If I’m mistaken I admit that. Will look more.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2rw38","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2rw38/best_hardware_setup_to_run_deepseekv3_670b/n3r8mv8/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752810447,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n3r86iu","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Alpine_Privacy","can_mod_post":false,"created_utc":1752810259,"send_replies":true,"parent_id":"t3_1m2rw38","score":13,"author_fullname":"t2_1t1panb3d8","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Mac mini noooo, watched a youtube video?, i think u will need 6xA100s to even run at Q4 quant, try to get them used. 10k x 6 =  60k in GPUs rest in cpu ram and all. You should look up KIMI K2 500Gb ram + even one A100 will do for it. Tokens per second would be abysmal though.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3r86iu","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Mac mini noooo, watched a youtube video?, i think u will need 6xA100s to even run at Q4 quant, try to get them used. 10k x 6 =  60k in GPUs rest in cpu ram and all. You should look up KIMI K2 500Gb ram + even one A100 will do for it. Tokens per second would be abysmal though.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2rw38/best_hardware_setup_to_run_deepseekv3_670b/n3r86iu/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752810259,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m2rw38","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":13}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3rbbki","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Aaaaaaaaaeeeee","can_mod_post":false,"created_utc":1752811585,"send_replies":true,"parent_id":"t3_1m2rw38","score":5,"author_fullname":"t2_el5pibmej","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"This report was pretty interesting using llama.cpp 4x H100, sharing it to keep your expectations low, Maybe you need to get a bargain idk. \\n\\n\\n- https://www.researchgate.net/profile/Leon-Sulfierry/publication/391867293_Efficient_Deployment_of_a_685B-Parameter_Open-Source_LLM_on_the_Brazilian_Santos_Dumont_Supercomputer/links/682b4e9fbe1b507dce8c03d5/Efficient-Deployment-of-a-685B-Parameter-Open-Source-LLM-on-the-Brazilian-Santos-Dumont-Supercomputer.pdf\\n\\n\\n\\nMaybe you can also get 8xA100 and then run a throughput oriented engine at 4bits.\\n\\n\\nIf you get 10 users on 1 512gb, and it's fast enough for them, then great, but it's less likely to be used for other research projects. ","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3rbbki","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;This report was pretty interesting using llama.cpp 4x H100, sharing it to keep your expectations low, Maybe you need to get a bargain idk. &lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;&lt;a href=\\"https://www.researchgate.net/profile/Leon-Sulfierry/publication/391867293_Efficient_Deployment_of_a_685B-Parameter_Open-Source_LLM_on_the_Brazilian_Santos_Dumont_Supercomputer/links/682b4e9fbe1b507dce8c03d5/Efficient-Deployment-of-a-685B-Parameter-Open-Source-LLM-on-the-Brazilian-Santos-Dumont-Supercomputer.pdf\\"&gt;https://www.researchgate.net/profile/Leon-Sulfierry/publication/391867293_Efficient_Deployment_of_a_685B-Parameter_Open-Source_LLM_on_the_Brazilian_Santos_Dumont_Supercomputer/links/682b4e9fbe1b507dce8c03d5/Efficient-Deployment-of-a-685B-Parameter-Open-Source-LLM-on-the-Brazilian-Santos-Dumont-Supercomputer.pdf&lt;/a&gt;&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;p&gt;Maybe you can also get 8xA100 and then run a throughput oriented engine at 4bits.&lt;/p&gt;\\n\\n&lt;p&gt;If you get 10 users on 1 512gb, and it&amp;#39;s fast enough for them, then great, but it&amp;#39;s less likely to be used for other research projects. &lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2rw38/best_hardware_setup_to_run_deepseekv3_670b/n3rbbki/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752811585,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m2rw38","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"d2642412-d9ce-11ed-ae30-32b11309f5bd","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"d2642412-d9ce-11ed-ae30-32b11309f5bd","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3udl2u","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ortegaalfredo","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3rbxr7","score":2,"author_fullname":"t2_g177e","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"To serve deepseek to 100 concurrent users (that means about 10000 users) we are talking DGX-level hardware, from 200k and up.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3udl2u","is_submitter":false,"collapsed":false,"author_flair_richtext":[{"e":"text","t":"Alpaca"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;To serve deepseek to 100 concurrent users (that means about 10000 users) we are talking DGX-level hardware, from 200k and up.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2rw38","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2rw38/best_hardware_setup_to_run_deepseekv3_670b/n3udl2u/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752856562,"author_flair_text":"Alpaca","treatment_tags":[],"created_utc":1752856562,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":"#bd9e9e","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n3rbxr7","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"PrevelantInsanity","can_mod_post":false,"created_utc":1752811851,"send_replies":true,"parent_id":"t1_n3rbr44","score":1,"author_fullname":"t2_ud8e7o0","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Reached that conclusion. Was working with specifications provided to me. Adapting to that, I’m welcoming thoughts on how to manage that # of ccu with decent quality of output/context window size through quantization or changes to context window","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3rbxr7","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Reached that conclusion. Was working with specifications provided to me. Adapting to that, I’m welcoming thoughts on how to manage that # of ccu with decent quality of output/context window size through quantization or changes to context window&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2rw38","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2rw38/best_hardware_setup_to_run_deepseekv3_670b/n3rbxr7/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752811851,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3rbr44","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"ortegaalfredo","can_mod_post":false,"created_utc":1752811771,"send_replies":true,"parent_id":"t3_1m2rw38","score":12,"author_fullname":"t2_g177e","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"100 users or 100 concurrents users? that's a difference. 100 users usually means 2 or 3 concurrent users, at most. That's something llama.cpp can do.\\n\\nFor 100 concurrent users you need a computer the size of a car.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3rbr44","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"Alpaca"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;100 users or 100 concurrents users? that&amp;#39;s a difference. 100 users usually means 2 or 3 concurrent users, at most. That&amp;#39;s something llama.cpp can do.&lt;/p&gt;\\n\\n&lt;p&gt;For 100 concurrent users you need a computer the size of a car.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2rw38/best_hardware_setup_to_run_deepseekv3_670b/n3rbr44/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752811771,"author_flair_text":"Alpaca","treatment_tags":[],"link_id":"t3_1m2rw38","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bd9e9e","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":12}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3ropur","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Conscious_Cut_6144","can_mod_post":false,"created_utc":1752817889,"send_replies":true,"parent_id":"t3_1m2rw38","score":5,"author_fullname":"t2_9hl4ymvj","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"At 80k a pro 6000 system is doable if you are willing to deal with “some assembly required”\\n\\n8 would be ideal. tp8.  \\n6 would run the fp4 version with tp2 pp3","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3ropur","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;At 80k a pro 6000 system is doable if you are willing to deal with “some assembly required”&lt;/p&gt;\\n\\n&lt;p&gt;8 would be ideal. tp8.&lt;br/&gt;\\n6 would run the fp4 version with tp2 pp3&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2rw38/best_hardware_setup_to_run_deepseekv3_670b/n3ropur/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752817889,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m2rw38","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3r8s3a","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"PrevelantInsanity","can_mod_post":false,"created_utc":1752810507,"send_replies":true,"parent_id":"t1_n3r8idk","score":2,"author_fullname":"t2_ud8e7o0","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Good idea. I’ll go check those forums out. Thanks!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3r8s3a","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Good idea. I’ll go check those forums out. Thanks!&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2rw38","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2rw38/best_hardware_setup_to_run_deepseekv3_670b/n3r8s3a/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752810507,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3rpa6b","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Conscious_Cut_6144","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3rgkl4","score":1,"author_fullname":"t2_9hl4ymvj","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Apple Silicon can’t really handle 100 users like an Nvidia system can. Great memory size and bandwidth, but lacking in compute.","edited":false,"author_flair_css_class":null,"name":"t1_n3rpa6b","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Apple Silicon can’t really handle 100 users like an Nvidia system can. Great memory size and bandwidth, but lacking in compute.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1m2rw38","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2rw38/best_hardware_setup_to_run_deepseekv3_670b/n3rpa6b/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752818176,"author_flair_text":null,"collapsed":false,"created_utc":1752818176,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3rgkl4","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Mabuse00","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3rf6mb","score":1,"author_fullname":"t2_1cf2k0je","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Hardware-wise, totally agree with you. That card is a beast. Software compatibility is just still catching up with Blackwell. I tried out a B200 last week and installed the current release of Pytorch and it was just like \\"nope.\\"","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3rgkl4","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Hardware-wise, totally agree with you. That card is a beast. Software compatibility is just still catching up with Blackwell. I tried out a B200 last week and installed the current release of Pytorch and it was just like &amp;quot;nope.&amp;quot;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2rw38","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2rw38/best_hardware_setup_to_run_deepseekv3_670b/n3rgkl4/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752813927,"author_flair_text":null,"treatment_tags":[],"created_utc":1752813927,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3rf6mb","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Nothing3561","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3re9qd","score":8,"author_fullname":"t2_pcyfii2k","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yeah but why limit yourself to consumer grade hardware?  The RTX Pro 6000 96gb card has a little less memory, but 1.79 TB/s.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3rf6mb","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yeah but why limit yourself to consumer grade hardware?  The RTX Pro 6000 96gb card has a little less memory, but 1.79 TB/s.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2rw38","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2rw38/best_hardware_setup_to_run_deepseekv3_670b/n3rf6mb/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752813296,"author_flair_text":null,"treatment_tags":[],"created_utc":1752813296,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":8}}],"before":null}},"user_reports":[],"saved":false,"id":"n3re9qd","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Mabuse00","can_mod_post":false,"created_utc":1752812882,"send_replies":true,"parent_id":"t1_n3r8idk","score":-4,"author_fullname":"t2_1cf2k0je","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I would absolutely recommend an Apple M3 Ultra over any other consumer grade hardware. That thing has 32 cores, 80 graphics cores, 32 neutral cores and 128gb of unified 800Gb/s ram. Even GPT had this to say:\\n\\nhttps://preview.redd.it/sml6ziam8kdf1.jpeg?width=1080&amp;format=pjpg&amp;auto=webp&amp;s=f4e608cc4bd2b64f9ef0ae84632e63b985af0d2e","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3re9qd","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I would absolutely recommend an Apple M3 Ultra over any other consumer grade hardware. That thing has 32 cores, 80 graphics cores, 32 neutral cores and 128gb of unified 800Gb/s ram. Even GPT had this to say:&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://preview.redd.it/sml6ziam8kdf1.jpeg?width=1080&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=f4e608cc4bd2b64f9ef0ae84632e63b985af0d2e\\"&gt;https://preview.redd.it/sml6ziam8kdf1.jpeg?width=1080&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=f4e608cc4bd2b64f9ef0ae84632e63b985af0d2e&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2rw38","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2rw38/best_hardware_setup_to_run_deepseekv3_670b/n3re9qd/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752812882,"media_metadata":{"sml6ziam8kdf1":{"status":"valid","e":"Image","m":"image/jpeg","p":[{"y":83,"x":108,"u":"https://preview.redd.it/sml6ziam8kdf1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=befa040bcef0b696228393cf2da144ed83ef969c"},{"y":166,"x":216,"u":"https://preview.redd.it/sml6ziam8kdf1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=a293485176c55391c5de758e94b8a12b465a7532"},{"y":246,"x":320,"u":"https://preview.redd.it/sml6ziam8kdf1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=258f0360924e8f881c7126d10c21010d91e3a9e0"},{"y":492,"x":640,"u":"https://preview.redd.it/sml6ziam8kdf1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=fc5ec9a295901e1730ce65952775d675395bc191"},{"y":738,"x":960,"u":"https://preview.redd.it/sml6ziam8kdf1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=09919eba16ba1ec249ec54c03d87918d1a74477c"},{"y":831,"x":1080,"u":"https://preview.redd.it/sml6ziam8kdf1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=ef4738369cff74ce1bed66bc1e3fdea0963e5b2f"}],"s":{"y":831,"x":1080,"u":"https://preview.redd.it/sml6ziam8kdf1.jpeg?width=1080&amp;format=pjpg&amp;auto=webp&amp;s=f4e608cc4bd2b64f9ef0ae84632e63b985af0d2e"},"id":"sml6ziam8kdf1"}},"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-4}}],"before":null}},"user_reports":[],"saved":false,"id":"n3r8idk","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"SteveRD1","can_mod_post":false,"created_utc":1752810395,"send_replies":true,"parent_id":"t3_1m2rw38","score":8,"author_fullname":"t2_ey837","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"There is some good discussion on running high quants of deepseek over on the Level1Tech forums (theres even people building quants there).\\n\\nYou could ask over there, seriously doubt anyone would recommend Apple!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3r8idk","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;There is some good discussion on running high quants of deepseek over on the Level1Tech forums (theres even people building quants there).&lt;/p&gt;\\n\\n&lt;p&gt;You could ask over there, seriously doubt anyone would recommend Apple!&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2rw38/best_hardware_setup_to_run_deepseekv3_670b/n3r8idk/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752810395,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m2rw38","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":8}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3ri1fs","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"GPTrack_ai","can_mod_post":false,"created_utc":1752814604,"send_replies":true,"parent_id":"t1_n3reebo","score":1,"author_fullname":"t2_1tpuoj72sa","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Available as server or desktop, BTW.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3ri1fs","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Available as server or desktop, BTW.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2rw38","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2rw38/best_hardware_setup_to_run_deepseekv3_670b/n3ri1fs/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752814604,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3reebo","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"GPTshop_ai","can_mod_post":false,"created_utc":1752812941,"send_replies":true,"parent_id":"t3_1m2rw38","score":4,"author_fullname":"t2_rkmud0isr","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"1.) GH200 624GB for 39k   \\n2.) DGX Station (GB300) 784GB for approx. 80k","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3reebo","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;1.) GH200 624GB for 39k&lt;br/&gt;\\n2.) DGX Station (GB300) 784GB for approx. 80k&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2rw38/best_hardware_setup_to_run_deepseekv3_670b/n3reebo/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752812941,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m2rw38","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3rb7sl","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Calm_List3479","can_mod_post":false,"created_utc":1752811540,"send_replies":true,"parent_id":"t3_1m2rw38","score":4,"author_fullname":"t2_nrekyf7l","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"You could look into Nvidia Digits/Spark. Maybe three of them could run this? The throughput wouldn't be great.\\n\\nA $300k 8xH200 running this FP8 w/ 128k tokens will only support 4 or so concurrent users.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3rb7sl","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You could look into Nvidia Digits/Spark. Maybe three of them could run this? The throughput wouldn&amp;#39;t be great.&lt;/p&gt;\\n\\n&lt;p&gt;A $300k 8xH200 running this FP8 w/ 128k tokens will only support 4 or so concurrent users.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2rw38/best_hardware_setup_to_run_deepseekv3_670b/n3rb7sl/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752811540,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m2rw38","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3rmj76","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"HiddenoO","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3rhc3t","score":1,"author_fullname":"t2_8127x","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"This. People still underestimate how different individual users' behavior can be from one another. Asking short questions with straight answers from knowledge is 1/100th to 1/10000th the compute per interaction compared to e.g. filling the context and generating code with reasoning.\\n\\nUnless it's exclusively the former which would also allow for smaller and potentially non-reasoning models, I don't see the 100 concurrent users working out at that budget at all.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3rmj76","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;This. People still underestimate how different individual users&amp;#39; behavior can be from one another. Asking short questions with straight answers from knowledge is 1/100th to 1/10000th the compute per interaction compared to e.g. filling the context and generating code with reasoning.&lt;/p&gt;\\n\\n&lt;p&gt;Unless it&amp;#39;s exclusively the former which would also allow for smaller and potentially non-reasoning models, I don&amp;#39;t see the 100 concurrent users working out at that budget at all.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2rw38","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2rw38/best_hardware_setup_to_run_deepseekv3_670b/n3rmj76/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752816791,"author_flair_text":null,"treatment_tags":[],"created_utc":1752816791,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3rhc3t","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"MosaicCantab","can_mod_post":false,"created_utc":1752814277,"send_replies":true,"parent_id":"t1_n3rd58d","score":5,"author_fullname":"t2_1ldnbkzx5a","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Without knowing what the users will be doing, it’s kind of hard to give guidance. But I frankly don’t believe you’ll be able to fit a model the size of DeepSeek on 100 concurrent users on $80k. \\n\\nEven with quantization, you’ll need far more computer if the users are doing any reasoning.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3rhc3t","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Without knowing what the users will be doing, it’s kind of hard to give guidance. But I frankly don’t believe you’ll be able to fit a model the size of DeepSeek on 100 concurrent users on $80k. &lt;/p&gt;\\n\\n&lt;p&gt;Even with quantization, you’ll need far more computer if the users are doing any reasoning.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2rw38","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2rw38/best_hardware_setup_to_run_deepseekv3_670b/n3rhc3t/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752814277,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}}],"before":null}},"user_reports":[],"saved":false,"id":"n3rd58d","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"PrevelantInsanity","can_mod_post":false,"created_utc":1752812378,"send_replies":true,"parent_id":"t3_1m2rw38","score":2,"author_fullname":"t2_ud8e7o0","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"The specification given to me was, more or less, “40-80k to spend, largest model we can run with peak 100 concurrent users.” I have found while researching this myself at the same time as posting around that that number of CCU increases the spec requirements hugely. I’m not sure how to best handle that— quantization of the same model, different model, shrunken context windows, or what.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3rd58d","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The specification given to me was, more or less, “40-80k to spend, largest model we can run with peak 100 concurrent users.” I have found while researching this myself at the same time as posting around that that number of CCU increases the spec requirements hugely. I’m not sure how to best handle that— quantization of the same model, different model, shrunken context windows, or what.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2rw38/best_hardware_setup_to_run_deepseekv3_670b/n3rd58d/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752812378,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m2rw38","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3rlc9e","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"claythearc","can_mod_post":false,"created_utc":1752816194,"send_replies":true,"parent_id":"t3_1m2rw38","score":1,"author_fullname":"t2_65rk0","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Apple silicon kinda works but the tok/s is very low, and you actually need substantially more of them than you’d think due to overhead from it sharing with system. You also hard lock yourself out of some models that require things like flash attention 2 (this one specifically may have support now, I haven’t checked - but it’s one example of a couple big ones). \\n\\nThese things are MoE so it’s better than it is in other models but that’s off set to a large degree from the thinking tokens it outputs. \\n\\nThe best way you can host it is still A100s which is probably like 60k in GPUs, realistically closer to 120-140k total for the system because you want a usable quant like q8, and need to hold the KV cache in memory. \\n\\nRealistically these models are just non existent for local hosting - the cost benefit just isn’t there for anything beyond like the Qwen 200/behemoth scale imo.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3rlc9e","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Apple silicon kinda works but the tok/s is very low, and you actually need substantially more of them than you’d think due to overhead from it sharing with system. You also hard lock yourself out of some models that require things like flash attention 2 (this one specifically may have support now, I haven’t checked - but it’s one example of a couple big ones). &lt;/p&gt;\\n\\n&lt;p&gt;These things are MoE so it’s better than it is in other models but that’s off set to a large degree from the thinking tokens it outputs. &lt;/p&gt;\\n\\n&lt;p&gt;The best way you can host it is still A100s which is probably like 60k in GPUs, realistically closer to 120-140k total for the system because you want a usable quant like q8, and need to hold the KV cache in memory. &lt;/p&gt;\\n\\n&lt;p&gt;Realistically these models are just non existent for local hosting - the cost benefit just isn’t there for anything beyond like the Qwen 200/behemoth scale imo.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2rw38/best_hardware_setup_to_run_deepseekv3_670b/n3rlc9e/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752816194,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m2rw38","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3rw59b","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AdventurousSwim1312","can_mod_post":false,"created_utc":1752821791,"send_replies":true,"parent_id":"t3_1m2rw38","score":1,"author_fullname":"t2_nqj4dsg7","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"With 100 users, you will need more compute, the low compute, large vram and medium bandwitch hold true for single user / low activation count model, but will quickly break otherwise.\\n\\nGiven your budget, I'd suggest looking either for a rack of 6 rtx pro 6000 black well (596 gb vram will allow you to host up to 1000 B parameters models) or a server with two or three AMD MI 350x (around 576 gb) that will be even faster (slightly inferior compute, but largely faster vram) but software might be a bit more messy to get to work.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3rw59b","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;With 100 users, you will need more compute, the low compute, large vram and medium bandwitch hold true for single user / low activation count model, but will quickly break otherwise.&lt;/p&gt;\\n\\n&lt;p&gt;Given your budget, I&amp;#39;d suggest looking either for a rack of 6 rtx pro 6000 black well (596 gb vram will allow you to host up to 1000 B parameters models) or a server with two or three AMD MI 350x (around 576 gb) that will be even faster (slightly inferior compute, but largely faster vram) but software might be a bit more messy to get to work.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2rw38/best_hardware_setup_to_run_deepseekv3_670b/n3rw59b/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752821791,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m2rw38","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3s1fx3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Ok_Warning2146","can_mod_post":false,"created_utc":1752824738,"send_replies":true,"parent_id":"t3_1m2rw38","score":1,"author_fullname":"t2_s6sfw4yy","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"8xH20 box. Each H20 is sold around 10k in china","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3s1fx3","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;8xH20 box. Each H20 is sold around 10k in china&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2rw38/best_hardware_setup_to_run_deepseekv3_670b/n3s1fx3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752824738,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m2rw38","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3s7yci","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Willing_Landscape_61","can_mod_post":false,"created_utc":1752828431,"send_replies":true,"parent_id":"t3_1m2rw38","score":1,"author_fullname":"t2_8lvrytgw","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"\\" 128K tokens\\"\\n\\"Apple Silicon handle this effectively?\\"\\nNo.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3s7yci","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;&amp;quot; 128K tokens&amp;quot;\\n&amp;quot;Apple Silicon handle this effectively?&amp;quot;\\nNo.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2rw38/best_hardware_setup_to_run_deepseekv3_670b/n3s7yci/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752828431,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m2rw38","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3t3v1g","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AlgorithmicMuse","can_mod_post":false,"created_utc":1752843123,"send_replies":true,"parent_id":"t3_1m2rw38","score":1,"author_fullname":"t2_1gzvdilba8","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"For the amount of dollars your are talking about , you  should be talking to Nvidia for a dedicated system not messing around  with garage shop solutions.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3t3v1g","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;For the amount of dollars your are talking about , you  should be talking to Nvidia for a dedicated system not messing around  with garage shop solutions.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2rw38/best_hardware_setup_to_run_deepseekv3_670b/n3t3v1g/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752843123,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m2rw38","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"1c60b73a-72f2-11ee-bdcc-8e2a7b94d6a0","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3tpear","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Equivalent-Bet-8771","can_mod_post":false,"created_utc":1752849728,"send_replies":true,"parent_id":"t3_1m2rw38","score":1,"author_fullname":"t2_l16sej0pt","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"The new Nvidia cards can do NVFP4 which should help reduce model size without much quantization loss.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3tpear","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"textgen web UI"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The new Nvidia cards can do NVFP4 which should help reduce model size without much quantization loss.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2rw38/best_hardware_setup_to_run_deepseekv3_670b/n3tpear/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752849728,"author_flair_text":"textgen web UI","treatment_tags":[],"link_id":"t3_1m2rw38","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3vg1j3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MachineZer0","can_mod_post":false,"created_utc":1752867603,"send_replies":true,"parent_id":"t3_1m2rw38","score":1,"author_fullname":"t2_55kl2a90","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"The worst hardware setup is a Gen 9 server with 600gb RAM and six Volta based GPUs for $2k. I was getting about 1tok/s on q4.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3vg1j3","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The worst hardware setup is a Gen 9 server with 600gb RAM and six Volta based GPUs for $2k. I was getting about 1tok/s on q4.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2rw38/best_hardware_setup_to_run_deepseekv3_670b/n3vg1j3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752867603,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m2rw38","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3vjjv8","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"k_means_clusterfuck","can_mod_post":false,"created_utc":1752868639,"send_replies":true,"parent_id":"t3_1m2rw38","score":1,"author_fullname":"t2_4bby1cv5","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Get them blackwells","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3vjjv8","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Get them blackwells&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2rw38/best_hardware_setup_to_run_deepseekv3_670b/n3vjjv8/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752868639,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m2rw38","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"c00d2408-d9ce-11ed-89c3-96d9f437a7a1","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3rf6qv","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"PrevelantInsanity","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3repfh","score":1,"author_fullname":"t2_ud8e7o0","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Bingo. ExxactCorp is a good tip. Thanks.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3rf6qv","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Bingo. ExxactCorp is a good tip. Thanks.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2rw38","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2rw38/best_hardware_setup_to_run_deepseekv3_670b/n3rf6qv/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752813298,"author_flair_text":null,"treatment_tags":[],"created_utc":1752813298,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3repfh","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"spookperson","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3r8tl1","score":1,"author_fullname":"t2_is8jf","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Time constraints on funding makes me wonder if you have education/nonprofit grants. If so, you may want to look at vendors with education/nonprofit discounts. I've heard people talking about getting workstations/GPUs from ExxactCorp with a discount on the components or build.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3repfh","is_submitter":false,"collapsed":false,"author_flair_richtext":[{"e":"text","t":"Vicuna"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Time constraints on funding makes me wonder if you have education/nonprofit grants. If so, you may want to look at vendors with education/nonprofit discounts. I&amp;#39;ve heard people talking about getting workstations/GPUs from ExxactCorp with a discount on the components or build.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2rw38","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2rw38/best_hardware_setup_to_run_deepseekv3_670b/n3repfh/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752813079,"author_flair_text":"Vicuna","treatment_tags":[],"created_utc":1752813079,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":"#9f9286","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3r8tl1","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"PrevelantInsanity","can_mod_post":false,"created_utc":1752810525,"send_replies":true,"parent_id":"t1_n3r8kxj","score":2,"author_fullname":"t2_ud8e7o0","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Time constraint on the funding. Good to know that’s on the horizon though. Thanks!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3r8tl1","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Time constraint on the funding. Good to know that’s on the horizon though. Thanks!&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2rw38","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2rw38/best_hardware_setup_to_run_deepseekv3_670b/n3r8tl1/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752810525,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n3r8kxj","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Fgfg1rox","can_mod_post":false,"created_utc":1752810425,"send_replies":true,"parent_id":"t3_1m2rw38","score":1,"author_fullname":"t2_ei5tnmg","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Why not wait for the new intel pro gpu’s and their project matrix? That complete system should only cost 10-15k and can run the full LLM, but if you can’t wait then I think you are on the right track.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3r8kxj","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Why not wait for the new intel pro gpu’s and their project matrix? That complete system should only cost 10-15k and can run the full LLM, but if you can’t wait then I think you are on the right track.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2rw38/best_hardware_setup_to_run_deepseekv3_670b/n3r8kxj/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752810425,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m2rw38","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3rukkg","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Mabuse00","can_mod_post":false,"created_utc":1752820954,"send_replies":true,"parent_id":"t3_1m2rw38","score":1,"author_fullname":"t2_1cf2k0je","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Yeah that's the trade-off you really have to take into consideration. Speed vs size. We're talking about clusters here so the numbers are at FP32 an M3 Ultra Mac Studio gets 28.4 Tflops and 128gb of unified ram for $4000. An RTX Pro 6000 96gb gets 118.11 Tflops and 96gb of VRAM for $10K. So if you took the RTX money and bought Mac Studios with it, you'd be getting 320gb of ram and 85.2 Tflops of FP32 compute. Sure it's a bit less but the extra ram is a big deal when you're getting into the realm of 750gb models like Deepseek R1 0528. \\n\\nTo get enough ram space to hold a model that size you can buy 6 M3 Ultra Studios for $24K or 8 RTX Pro 6000's for $8K. And once you're at that point the Macs will still add up to 170 Tflops at FP32, double that in FP16. For a hundred users who won't all be sending a completion request at the same moment anyway, that's more than plenty.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3rukkg","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yeah that&amp;#39;s the trade-off you really have to take into consideration. Speed vs size. We&amp;#39;re talking about clusters here so the numbers are at FP32 an M3 Ultra Mac Studio gets 28.4 Tflops and 128gb of unified ram for $4000. An RTX Pro 6000 96gb gets 118.11 Tflops and 96gb of VRAM for $10K. So if you took the RTX money and bought Mac Studios with it, you&amp;#39;d be getting 320gb of ram and 85.2 Tflops of FP32 compute. Sure it&amp;#39;s a bit less but the extra ram is a big deal when you&amp;#39;re getting into the realm of 750gb models like Deepseek R1 0528. &lt;/p&gt;\\n\\n&lt;p&gt;To get enough ram space to hold a model that size you can buy 6 M3 Ultra Studios for $24K or 8 RTX Pro 6000&amp;#39;s for $8K. And once you&amp;#39;re at that point the Macs will still add up to 170 Tflops at FP32, double that in FP16. For a hundred users who won&amp;#39;t all be sending a completion request at the same moment anyway, that&amp;#39;s more than plenty.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2rw38/best_hardware_setup_to_run_deepseekv3_670b/n3rukkg/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752820954,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m2rw38","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3sdfq0","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MelodicRecognition7","can_mod_post":false,"created_utc":1752831589,"send_replies":true,"parent_id":"t1_n3rhe0x","score":2,"author_fullname":"t2_1eex9ug5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"    Jolla is hoping to find success with the Mind2, a single-board computer designed to run LLMs in a box for improved privacy.\\n\\nyou will not get any usable performance with a single-board computer. What people here are looking for starts with 1kW power draw.\\n\\n    Processor: RK3588 CPU with integrated NPU (6 TOPS)\\n\\nthis is a typical Kickstarter e-waste to rip off \\"not an experts\\".","edited":1752831865,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3sdfq0","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;pre&gt;&lt;code&gt;Jolla is hoping to find success with the Mind2, a single-board computer designed to run LLMs in a box for improved privacy.\\n&lt;/code&gt;&lt;/pre&gt;\\n\\n&lt;p&gt;you will not get any usable performance with a single-board computer. What people here are looking for starts with 1kW power draw.&lt;/p&gt;\\n\\n&lt;pre&gt;&lt;code&gt;Processor: RK3588 CPU with integrated NPU (6 TOPS)\\n&lt;/code&gt;&lt;/pre&gt;\\n\\n&lt;p&gt;this is a typical Kickstarter e-waste to rip off &amp;quot;not an experts&amp;quot;.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2rw38","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2rw38/best_hardware_setup_to_run_deepseekv3_670b/n3sdfq0/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752831589,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n3rhe0x","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"snowowlshopscotch","can_mod_post":false,"created_utc":1752814301,"send_replies":true,"parent_id":"t3_1m2rw38","score":0,"author_fullname":"t2_xk3y8ocaz","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I am not an expert on this at all! Just genuinely interested and confused about why  Jolla Mind2 never gets mentioned here?\\n\\nAs I understand it, it is exactly what people here are looking for, or am I missing something?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3rhe0x","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I am not an expert on this at all! Just genuinely interested and confused about why  Jolla Mind2 never gets mentioned here?&lt;/p&gt;\\n\\n&lt;p&gt;As I understand it, it is exactly what people here are looking for, or am I missing something?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2rw38/best_hardware_setup_to_run_deepseekv3_670b/n3rhe0x/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752814301,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m2rw38","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}}]`),n=()=>e.jsx(t,{data:l});export{n as default};
