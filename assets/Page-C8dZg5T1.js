import{j as t}from"./index-CWmJdUH_.js";import{R as e}from"./RedditPostRenderer-D2iunoQ9.js";import"./index-BCg9RP6g.js";const a=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Hey, \\n\\n  \\nI'm trying to fine tune a model in order to give as an input a list of industrial tasks, and to have as an output the dependencies between those tasks.\\n\\n  \\nI heard instruction was also important for the llm to be more accurate but i'm not sure if the prompt i wrote is great for my project. What do you think ? \\n\\n**system\\\\_instruction = \\"\\"\\"**\\n\\n**You are an industrial planner.**\\n\\n**Your task is to parse a list of tasks and generate all the logical dependencies as a JSON object, as follows:**\\n\\n**{**\\n\\n**\\"dependencies\\": \\\\[\\\\[\\"Task A\\", \\"Task B\\"\\\\], \\\\[\\"Task A\\", \\"Task C\\"\\\\], ...\\\\]**\\n\\n**}**\\n\\n**Rules:**\\n\\n**- A task can trigger multiple other tasks in parallel.**\\n\\n**- In this case, each relationship must appear as a separate pair in the \\"dependencies\\" list.**\\n\\n**- Return only the JSON, without any explanation, comments, or additional text.**\\n\\n**\\"\\"\\"**","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Need advice on prompt instruction format","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1m0h6k5","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.25,"author_flair_background_color":null,"subreddit_type":"public","ups":0,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_1f72n02g6d","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":0,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1752584569,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Hey, &lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;m trying to fine tune a model in order to give as an input a list of industrial tasks, and to have as an output the dependencies between those tasks.&lt;/p&gt;\\n\\n&lt;p&gt;I heard instruction was also important for the llm to be more accurate but i&amp;#39;m not sure if the prompt i wrote is great for my project. What do you think ? &lt;/p&gt;\\n\\n&lt;p&gt;&lt;strong&gt;system_instruction = &amp;quot;&amp;quot;&amp;quot;&lt;/strong&gt;&lt;/p&gt;\\n\\n&lt;p&gt;&lt;strong&gt;You are an industrial planner.&lt;/strong&gt;&lt;/p&gt;\\n\\n&lt;p&gt;&lt;strong&gt;Your task is to parse a list of tasks and generate all the logical dependencies as a JSON object, as follows:&lt;/strong&gt;&lt;/p&gt;\\n\\n&lt;p&gt;&lt;strong&gt;{&lt;/strong&gt;&lt;/p&gt;\\n\\n&lt;p&gt;&lt;strong&gt;&amp;quot;dependencies&amp;quot;: [[&amp;quot;Task A&amp;quot;, &amp;quot;Task B&amp;quot;], [&amp;quot;Task A&amp;quot;, &amp;quot;Task C&amp;quot;], ...]&lt;/strong&gt;&lt;/p&gt;\\n\\n&lt;p&gt;&lt;strong&gt;}&lt;/strong&gt;&lt;/p&gt;\\n\\n&lt;p&gt;&lt;strong&gt;Rules:&lt;/strong&gt;&lt;/p&gt;\\n\\n&lt;p&gt;&lt;strong&gt;- A task can trigger multiple other tasks in parallel.&lt;/strong&gt;&lt;/p&gt;\\n\\n&lt;p&gt;&lt;strong&gt;- In this case, each relationship must appear as a separate pair in the &amp;quot;dependencies&amp;quot; list.&lt;/strong&gt;&lt;/p&gt;\\n\\n&lt;p&gt;&lt;strong&gt;- Return only the JSON, without any explanation, comments, or additional text.&lt;/strong&gt;&lt;/p&gt;\\n\\n&lt;p&gt;&lt;strong&gt;&amp;quot;&amp;quot;&amp;quot;&lt;/strong&gt;&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":true,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1m0h6k5","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"Head_Mushroom_3748","discussion_type":null,"num_comments":1,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1m0h6k5/need_advice_on_prompt_instruction_format/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1m0h6k5/need_advice_on_prompt_instruction_format/","subreddit_subscribers":499773,"created_utc":1752584569,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3c21r9","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"misterflyer","can_mod_post":false,"created_utc":1752614107,"send_replies":true,"parent_id":"t3_1m0h6k5","score":1,"author_fullname":"t2_maq0iwk","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Prob depends on the model tbh.\\n\\nBut I'd just put all of this info into ChatGPT *(or whatever mainstream LLM provider you use)*, and it will give you some great ideas.  When I asked GPT for info on finetuning a while back for my specific use case, GPT was a gold mine of information.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3c21r9","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Prob depends on the model tbh.&lt;/p&gt;\\n\\n&lt;p&gt;But I&amp;#39;d just put all of this info into ChatGPT &lt;em&gt;(or whatever mainstream LLM provider you use)&lt;/em&gt;, and it will give you some great ideas.  When I asked GPT for info on finetuning a while back for my specific use case, GPT was a gold mine of information.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m0h6k5/need_advice_on_prompt_instruction_format/n3c21r9/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752614107,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m0h6k5","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),s=()=>t.jsx(e,{data:a});export{s as default};
