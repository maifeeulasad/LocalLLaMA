import{j as e}from"./index-Bqs-ekb2.js";import{R as t}from"./RedditPostRenderer-DUVdf0-i.js";import"./index-D52ORTDm.js";const a=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"When it comes to Mamba I've heard that it can run in constant time and train in O(n) compared to transformers which run in O(n) and train in O(n\\\\^2). I've also heard that Mamba is better with memory and power usage. I'm a bit confused by Jamba since it's a mixture of the two with alternating Mamba and Transformer blocks. ","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Are the new architectures Mamba and Jamba better or worse than current existing Transformer architectures.","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lluwee","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.88,"author_flair_background_color":null,"subreddit_type":"public","ups":13,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_1sdcp9ffu7","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":13,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1751033490,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;When it comes to Mamba I&amp;#39;ve heard that it can run in constant time and train in O(n) compared to transformers which run in O(n) and train in O(n^2). I&amp;#39;ve also heard that Mamba is better with memory and power usage. I&amp;#39;m a bit confused by Jamba since it&amp;#39;s a mixture of the two with alternating Mamba and Transformer blocks. &lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1lluwee","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"Direct-Lifeguard-607","discussion_type":null,"num_comments":5,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lluwee/are_the_new_architectures_mamba_and_jamba_better/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lluwee/are_the_new_architectures_mamba_and_jamba_better/","subreddit_subscribers":492232,"created_utc":1751033490,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n02jwkz","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"LagOps91","can_mod_post":false,"created_utc":1751034441,"send_replies":true,"parent_id":"t3_1lluwee","score":8,"author_fullname":"t2_3wi6j7vwh","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"the latest hybrid model relase that i'm aware of is the following: [https://huggingface.co/tiiuae/Falcon-H1-34B-Instruct](https://huggingface.co/tiiuae/Falcon-H1-34B-Instruct)\\n\\nif their HF page is to be belived, then that model is indeed punching above it's weight class. I couldn't try it yet due to not having llamacpp support, but that is being worked on.\\n\\nif you look more closely it's true that the asymptotic scaling is as you have described, but only past 16k context do those hybrid models actually outperform transformer models in terms of compute reuqirements.\\n\\npersonally i think hybrid models are showing promise due to the possiblity of combining the strengths of attention with those of hidden state.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n02jwkz","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;the latest hybrid model relase that i&amp;#39;m aware of is the following: &lt;a href=\\"https://huggingface.co/tiiuae/Falcon-H1-34B-Instruct\\"&gt;https://huggingface.co/tiiuae/Falcon-H1-34B-Instruct&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;if their HF page is to be belived, then that model is indeed punching above it&amp;#39;s weight class. I couldn&amp;#39;t try it yet due to not having llamacpp support, but that is being worked on.&lt;/p&gt;\\n\\n&lt;p&gt;if you look more closely it&amp;#39;s true that the asymptotic scaling is as you have described, but only past 16k context do those hybrid models actually outperform transformer models in terms of compute reuqirements.&lt;/p&gt;\\n\\n&lt;p&gt;personally i think hybrid models are showing promise due to the possiblity of combining the strengths of attention with those of hidden state.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lluwee/are_the_new_architectures_mamba_and_jamba_better/n02jwkz/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751034441,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lluwee","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":8}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n04aun5","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"crantob","can_mod_post":false,"created_utc":1751052335,"send_replies":true,"parent_id":"t1_n0333xl","score":1,"author_fullname":"t2_gyg8tngx","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I was unaware that Transformers had caught up so much in terms of speed.   \\n\\nIn mid-2025, this seems like a good topic for a SOTA summary article, and I would appreciate any links to such.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n04aun5","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I was unaware that Transformers had caught up so much in terms of speed.   &lt;/p&gt;\\n\\n&lt;p&gt;In mid-2025, this seems like a good topic for a SOTA summary article, and I would appreciate any links to such.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lluwee","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lluwee/are_the_new_architectures_mamba_and_jamba_better/n04aun5/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751052335,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n06bkct","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Direct-Lifeguard-607","can_mod_post":false,"created_utc":1751076948,"send_replies":true,"parent_id":"t1_n0333xl","score":1,"author_fullname":"t2_1sdcp9ffu7","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I guess a follow up would be some potential I saw when reading about Jamba is that it apparently could reduce power usage and memory usage and yet maintain similar performance to transformer only equivalents. Do you know anything about that and if thats the case why more focus isnt going towards mamba transformer hybrids?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n06bkct","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I guess a follow up would be some potential I saw when reading about Jamba is that it apparently could reduce power usage and memory usage and yet maintain similar performance to transformer only equivalents. Do you know anything about that and if thats the case why more focus isnt going towards mamba transformer hybrids?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lluwee","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lluwee/are_the_new_architectures_mamba_and_jamba_better/n06bkct/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751076948,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0333xl","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Double_Cause4609","can_mod_post":false,"created_utc":1751039917,"send_replies":true,"parent_id":"t3_1lluwee","score":3,"author_fullname":"t2_1kubzxt2ww","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Do you mean the models themselves or the technology behind them?\\n\\nThe actual tech (the architecture) is probably about equivalent to Transformers, give or tech. Keep in mind that we've optimized self attention so much that it doesn't actually require the old estimates for vanilla Attention as described in Attention is All You Need, and it's a lot closer to linear now (ie: with Flash Attention, MLA, etc).\\n\\nThey've both kind of converged on about the same performance.\\n\\nThere are technically differences for things other than language (like, there's certain kinds of math only RNNs can do, SSMs are best at other kinds, Transformers better still at others, etc etc), but for the most part, in language modelling, the most important thing is just the scale of the network.\\n\\nAnd the O(n) is tricky because it depends on if we're talking about training time or training memory usage which are different.\\n\\nAnyway: The models themselves, tend to be roughly what you'd expect for their training data and compute in a normal Transformer LLM.\\n\\nI'm sorry it's a boring answer, but in short: Transformers were made easier to run, SSMs/RNNs got better in performance and now they both perform roughly equivalently. For crazy long context SSMs/RNNs (and probably convolutional language models, too) have static memory use, but it's hard to tell how long their effective context window you can actually use is.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0333xl","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Do you mean the models themselves or the technology behind them?&lt;/p&gt;\\n\\n&lt;p&gt;The actual tech (the architecture) is probably about equivalent to Transformers, give or tech. Keep in mind that we&amp;#39;ve optimized self attention so much that it doesn&amp;#39;t actually require the old estimates for vanilla Attention as described in Attention is All You Need, and it&amp;#39;s a lot closer to linear now (ie: with Flash Attention, MLA, etc).&lt;/p&gt;\\n\\n&lt;p&gt;They&amp;#39;ve both kind of converged on about the same performance.&lt;/p&gt;\\n\\n&lt;p&gt;There are technically differences for things other than language (like, there&amp;#39;s certain kinds of math only RNNs can do, SSMs are best at other kinds, Transformers better still at others, etc etc), but for the most part, in language modelling, the most important thing is just the scale of the network.&lt;/p&gt;\\n\\n&lt;p&gt;And the O(n) is tricky because it depends on if we&amp;#39;re talking about training time or training memory usage which are different.&lt;/p&gt;\\n\\n&lt;p&gt;Anyway: The models themselves, tend to be roughly what you&amp;#39;d expect for their training data and compute in a normal Transformer LLM.&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;m sorry it&amp;#39;s a boring answer, but in short: Transformers were made easier to run, SSMs/RNNs got better in performance and now they both perform roughly equivalently. For crazy long context SSMs/RNNs (and probably convolutional language models, too) have static memory use, but it&amp;#39;s hard to tell how long their effective context window you can actually use is.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lluwee/are_the_new_architectures_mamba_and_jamba_better/n0333xl/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751039917,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lluwee","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n036a4e","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"The_Hardcard","can_mod_post":false,"created_utc":1751040813,"send_replies":true,"parent_id":"t3_1lluwee","score":1,"author_fullname":"t2_aom89enmo","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Seems to be that this thread would also warrant discussion on TITANS and Transformers squared, both put forward many months ago as solutions to at least some limitations of transformers.\\n\\n  \\nThere are way too many models for me to keep track, but I’m pretty sure no big impact models have come out with either algorithm.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n036a4e","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Seems to be that this thread would also warrant discussion on TITANS and Transformers squared, both put forward many months ago as solutions to at least some limitations of transformers.&lt;/p&gt;\\n\\n&lt;p&gt;There are way too many models for me to keep track, but I’m pretty sure no big impact models have come out with either algorithm.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lluwee/are_the_new_architectures_mamba_and_jamba_better/n036a4e/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751040813,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lluwee","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),n=()=>e.jsx(t,{data:a});export{n as default};
