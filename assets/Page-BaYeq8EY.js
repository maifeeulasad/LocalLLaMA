import{j as e}from"./index-CWmJdUH_.js";import{R as t}from"./RedditPostRenderer-D2iunoQ9.js";import"./index-BCg9RP6g.js";const a=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"I got sglang running a few months ago with Qwen3 30B-A3B and its performance impressed me so much that there is no desire from me at this point to run 70B+ models because I can reach over 600tok/s with a single 3090 with it (8 inferences running in parallel, 150 or so for a single inference, 140tok/s with power limited to 250W)\\n\\nMy question I'd like to answer now is how much of a leap can I expect to see with 5090? I will be gaming and doing image/video generation with the 5090 as well if I get one, and I have no plans to sell my pair of 3090s (though it would be at profit so i could potentially do that to save money)\\n\\nHowever lately there's not a lot of time for games and besides all titles I play still do run fine on Ampere even though I have a 4K 240hz monitor so I was really trying to get a 5090 this year but I guess I just have a sour taste in my mouth about it all. Image generation is fine with 24GB but video in particular could benefit from more grunt. Still, it's not been a tier 1 hobby of mine so it's really kind of a side benefit. There are also other things i like to do aspirationally (tinker with algorithms in CUDA and so on) that it would be cool to have but two 3090s is already so incredibly far beyond what I need for that. \\n\\n5090 are poised to become possible to obtain soon it seems, so I want some more complete data. \\n\\nI'd like to see if someone with a 5090 running linux can test my docker image and tell me what inference performance you're able to get, to help me make this purchasing decision. \\n\\nHere is the dockerfile: [https://gist.github.com/unphased/59c0774882ec6d478274ec10c84a2336](https://gist.github.com/unphased/59c0774882ec6d478274ec10c84a2336)\\n\\n* I can provide a built docker image (it is 18GB though) if you have trouble building or running that dockerfile. the instructions are in a comment inside, and should work even if you are not familiar with docker or k8s. If we need to fall back to running the image though I'd like to troubleshoot with you a bit so that I can potentially improve my dockerfile.\\n* if you want to actually human-readably view the output, I use a (dependencyless) python script that extracts out the streamed output tokens from that curl response, I provide it here: [https://gist.github.com/unphased/b31a7dd3e58397a44cc356e4bfed160b](https://gist.github.com/unphased/b31a7dd3e58397a44cc356e4bfed160b) What you would do is take the example curl command and add \` | python3 stream_parser.py\`\\n\\nMy 600+ tok/s performance number is had on my 3090 by modifying the input curl request to put 8 separate messages into the curl request. Let me know if you're having trouble figuring out the syntax for that... My hope is a 5090 should have the arithmetic intensity that it probably wants 12 or even more to batch in parallel to get the highest possible throughput. I would be hoping for a 3 or 4x speedup compared to 3090 but I somehow doubt that will be the case for single inference but it may be the case with multiple inference (which on an efficient runtime like sglang seems to be able to extract compute performance while saturating mem bandwidth). From a theoretical point of view, 1.79TB/s over 936GB/s should yield a speedup of 96% for single inference. That's actually quite a bit better than I expected...\\n\\nNow if we can hit 3x or 4x total throughput going from 3090 to 5090 that will be a go for me and I'll gladly purchase one. If not... I dunno if I can justify the cost. If it only provides a 2x speed gain over a 3090, that means in terms of LLM heavy lifting it is only consolidating my two 3090s into one GPU, and gives only a mild efficiency win (two 3090s at 250W vs one 5090 at probably 400W, not much less, only saving 100W) and no performance win which would not be all that compelling. If 4x though, that would represent some serious consolidation factor. My gut is telling me to expect something like 3.3x speedup. Which I hope is enough to push me over the edge because I sure do want the shiny. I just gotta talk myself into it.\\n\\nIf you look at the docker logs (which in the way i tell you to launch it will be visible in the terminal) it will show the latest tok/s metric. \\n\\nThank you.","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"5090 batched inference performance?","link_flair_richtext":[{"e":"text","t":"Discussion"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1m6hqi8","quarantine":false,"link_flair_text_color":"light","upvote_ratio":1,"author_flair_background_color":null,"subreddit_type":"public","ups":2,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_iifi6ul2l","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Discussion","can_mod_post":false,"score":2,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":1753200765,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"post_hint":"self","content_categories":null,"is_self":true,"mod_note":null,"created":1753199447,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;I got sglang running a few months ago with Qwen3 30B-A3B and its performance impressed me so much that there is no desire from me at this point to run 70B+ models because I can reach over 600tok/s with a single 3090 with it (8 inferences running in parallel, 150 or so for a single inference, 140tok/s with power limited to 250W)&lt;/p&gt;\\n\\n&lt;p&gt;My question I&amp;#39;d like to answer now is how much of a leap can I expect to see with 5090? I will be gaming and doing image/video generation with the 5090 as well if I get one, and I have no plans to sell my pair of 3090s (though it would be at profit so i could potentially do that to save money)&lt;/p&gt;\\n\\n&lt;p&gt;However lately there&amp;#39;s not a lot of time for games and besides all titles I play still do run fine on Ampere even though I have a 4K 240hz monitor so I was really trying to get a 5090 this year but I guess I just have a sour taste in my mouth about it all. Image generation is fine with 24GB but video in particular could benefit from more grunt. Still, it&amp;#39;s not been a tier 1 hobby of mine so it&amp;#39;s really kind of a side benefit. There are also other things i like to do aspirationally (tinker with algorithms in CUDA and so on) that it would be cool to have but two 3090s is already so incredibly far beyond what I need for that. &lt;/p&gt;\\n\\n&lt;p&gt;5090 are poised to become possible to obtain soon it seems, so I want some more complete data. &lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;d like to see if someone with a 5090 running linux can test my docker image and tell me what inference performance you&amp;#39;re able to get, to help me make this purchasing decision. &lt;/p&gt;\\n\\n&lt;p&gt;Here is the dockerfile: &lt;a href=\\"https://gist.github.com/unphased/59c0774882ec6d478274ec10c84a2336\\"&gt;https://gist.github.com/unphased/59c0774882ec6d478274ec10c84a2336&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;I can provide a built docker image (it is 18GB though) if you have trouble building or running that dockerfile. the instructions are in a comment inside, and should work even if you are not familiar with docker or k8s. If we need to fall back to running the image though I&amp;#39;d like to troubleshoot with you a bit so that I can potentially improve my dockerfile.&lt;/li&gt;\\n&lt;li&gt;if you want to actually human-readably view the output, I use a (dependencyless) python script that extracts out the streamed output tokens from that curl response, I provide it here: &lt;a href=\\"https://gist.github.com/unphased/b31a7dd3e58397a44cc356e4bfed160b\\"&gt;https://gist.github.com/unphased/b31a7dd3e58397a44cc356e4bfed160b&lt;/a&gt; What you would do is take the example curl command and add &lt;code&gt;| python3 stream_parser.py&lt;/code&gt;&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;p&gt;My 600+ tok/s performance number is had on my 3090 by modifying the input curl request to put 8 separate messages into the curl request. Let me know if you&amp;#39;re having trouble figuring out the syntax for that... My hope is a 5090 should have the arithmetic intensity that it probably wants 12 or even more to batch in parallel to get the highest possible throughput. I would be hoping for a 3 or 4x speedup compared to 3090 but I somehow doubt that will be the case for single inference but it may be the case with multiple inference (which on an efficient runtime like sglang seems to be able to extract compute performance while saturating mem bandwidth). From a theoretical point of view, 1.79TB/s over 936GB/s should yield a speedup of 96% for single inference. That&amp;#39;s actually quite a bit better than I expected...&lt;/p&gt;\\n\\n&lt;p&gt;Now if we can hit 3x or 4x total throughput going from 3090 to 5090 that will be a go for me and I&amp;#39;ll gladly purchase one. If not... I dunno if I can justify the cost. If it only provides a 2x speed gain over a 3090, that means in terms of LLM heavy lifting it is only consolidating my two 3090s into one GPU, and gives only a mild efficiency win (two 3090s at 250W vs one 5090 at probably 400W, not much less, only saving 100W) and no performance win which would not be all that compelling. If 4x though, that would represent some serious consolidation factor. My gut is telling me to expect something like 3.3x speedup. Which I hope is enough to push me over the edge because I sure do want the shiny. I just gotta talk myself into it.&lt;/p&gt;\\n\\n&lt;p&gt;If you look at the docker logs (which in the way i tell you to launch it will be visible in the terminal) it will show the latest tok/s metric. &lt;/p&gt;\\n\\n&lt;p&gt;Thank you.&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"preview":{"images":[{"source":{"url":"https://external-preview.redd.it/OAXSl8SY6T3JK9MGQyKxkoYbqZ71HQRYXLeB8CV0NXg.png?auto=webp&amp;s=c7cbcc7517e2406e2326e7a1eb6bdb9022c27fda","width":1280,"height":640},"resolutions":[{"url":"https://external-preview.redd.it/OAXSl8SY6T3JK9MGQyKxkoYbqZ71HQRYXLeB8CV0NXg.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=796041decb8c1250cbc2f301331b72f7385b477d","width":108,"height":54},{"url":"https://external-preview.redd.it/OAXSl8SY6T3JK9MGQyKxkoYbqZ71HQRYXLeB8CV0NXg.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=2e3562243f324d16bc6d9dd09adb1da4e0b100b5","width":216,"height":108},{"url":"https://external-preview.redd.it/OAXSl8SY6T3JK9MGQyKxkoYbqZ71HQRYXLeB8CV0NXg.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=564e5f4bb6808064a14eb3965a6911671c3c9807","width":320,"height":160},{"url":"https://external-preview.redd.it/OAXSl8SY6T3JK9MGQyKxkoYbqZ71HQRYXLeB8CV0NXg.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=0f53460a90493497883ab4cacbbb58e2acb464c4","width":640,"height":320},{"url":"https://external-preview.redd.it/OAXSl8SY6T3JK9MGQyKxkoYbqZ71HQRYXLeB8CV0NXg.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=7a4f79362039959fa37eab208ae001245ccfe6e3","width":960,"height":480},{"url":"https://external-preview.redd.it/OAXSl8SY6T3JK9MGQyKxkoYbqZ71HQRYXLeB8CV0NXg.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=912f966e123e94e32e7975fe8aebac89450a6b98","width":1080,"height":540}],"variants":{},"id":"OAXSl8SY6T3JK9MGQyKxkoYbqZ71HQRYXLeB8CV0NXg"}],"enabled":false},"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"5f921ea4-c7bc-11ed-9c23-3a00622979b4","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#646d73","id":"1m6hqi8","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"michaelsoft__binbows","discussion_type":null,"num_comments":4,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1m6hqi8/5090_batched_inference_performance/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1m6hqi8/5090_batched_inference_performance/","subreddit_subscribers":503254,"created_utc":1753199447,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4jpd2u","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"michaelsoft__binbows","can_mod_post":false,"created_utc":1753200227,"send_replies":true,"parent_id":"t3_1m6hqi8","score":2,"author_fullname":"t2_iifi6ul2l","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"oof, i just ran another batch-8 test and it brought the workstation down. that's.... bad, i haven't seen that happen yet. \\n\\nthe software is impressive though. It reached a peak of 690tok/s near the beginning (this is with the FTW3 3090 with 420W TDP). unfortunately I don't have the logs. Maybe this power splitter is crap though, i'm gonna go grab my UPS unit and put it behind that again and do more testing.","edited":1753239077,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4jpd2u","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;oof, i just ran another batch-8 test and it brought the workstation down. that&amp;#39;s.... bad, i haven&amp;#39;t seen that happen yet. &lt;/p&gt;\\n\\n&lt;p&gt;the software is impressive though. It reached a peak of 690tok/s near the beginning (this is with the FTW3 3090 with 420W TDP). unfortunately I don&amp;#39;t have the logs. Maybe this power splitter is crap though, i&amp;#39;m gonna go grab my UPS unit and put it behind that again and do more testing.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6hqi8/5090_batched_inference_performance/n4jpd2u/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753200227,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m6hqi8","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4n2zu0","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"michaelsoft__binbows","can_mod_post":false,"created_utc":1753236721,"send_replies":true,"parent_id":"t1_n4m851n","score":2,"author_fullname":"t2_iifi6ul2l","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"OK that is a good idea, silly of me not to consider it. especially since i have a pretty ready to go set of test commands for use on linux. I'll report back for science if i do go that route.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4n2zu0","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;OK that is a good idea, silly of me not to consider it. especially since i have a pretty ready to go set of test commands for use on linux. I&amp;#39;ll report back for science if i do go that route.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m6hqi8","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6hqi8/5090_batched_inference_performance/n4n2zu0/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753236721,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4nl36a","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"michaelsoft__binbows","can_mod_post":false,"created_utc":1753243712,"send_replies":true,"parent_id":"t1_n4m851n","score":2,"author_fullname":"t2_iifi6ul2l","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"based on your numbers which do look like they need some fair grains of salt, that's just under a 3x speedup going to 5090. Which would put me in agonizing dilemma mode, which is honestly par for the course. We'll see, i should be able to script up a way to run this and get some results out of it in an hour, so only a dollar spent. And failing that, yeah like you say, i couldnt imagine not being able to get it done in a full work day and thats only gonna be five bucks.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4nl36a","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;based on your numbers which do look like they need some fair grains of salt, that&amp;#39;s just under a 3x speedup going to 5090. Which would put me in agonizing dilemma mode, which is honestly par for the course. We&amp;#39;ll see, i should be able to script up a way to run this and get some results out of it in an hour, so only a dollar spent. And failing that, yeah like you say, i couldnt imagine not being able to get it done in a full work day and thats only gonna be five bucks.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m6hqi8","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6hqi8/5090_batched_inference_performance/n4nl36a/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753243712,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n4m851n","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"FullOf_Bad_Ideas","can_mod_post":false,"created_utc":1753226274,"send_replies":true,"parent_id":"t3_1m6hqi8","score":1,"author_fullname":"t2_9s7pmakgx","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"You should rent 5090s on vast for cheap and test for yourself, it would cost you just like 5 bucks. Sorry, I would have done that myself and shared results if I had the time to spare.\\n\\n5090 has FP4 and FP8 native support that 3090 doesn't have (though 3090 has INT8 support). I have more experiences with 4090 and 7B dense, and I think the output token gen speed on 3090 Ti is around 1700 t/s with FP16, around 2600 t/s with INT8, on 4090 it's around 4000 t/s with FP8, and on 5090 it should be around 5000 t/s with FP8 (rough guess). That's with batch sizes around 200 though.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4m851n","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You should rent 5090s on vast for cheap and test for yourself, it would cost you just like 5 bucks. Sorry, I would have done that myself and shared results if I had the time to spare.&lt;/p&gt;\\n\\n&lt;p&gt;5090 has FP4 and FP8 native support that 3090 doesn&amp;#39;t have (though 3090 has INT8 support). I have more experiences with 4090 and 7B dense, and I think the output token gen speed on 3090 Ti is around 1700 t/s with FP16, around 2600 t/s with INT8, on 4090 it&amp;#39;s around 4000 t/s with FP8, and on 5090 it should be around 5000 t/s with FP8 (rough guess). That&amp;#39;s with batch sizes around 200 though.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6hqi8/5090_batched_inference_performance/n4m851n/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753226274,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m6hqi8","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),i=()=>e.jsx(t,{data:a});export{i as default};
