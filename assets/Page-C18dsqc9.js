import{j as e}from"./index-BgwOAK4-.js";import{R as l}from"./RedditPostRenderer-BOBjDTFu.js";import"./index-BL22wVg5.js";const t=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Hi,\\n\\nI have a RTX 3080 with 10gb of ram, seems pretty quick with vllm running qwen2.5 coder 7b.\\n\\nI have the option to buy a 3060 but with 12gb (pretty cheap at AUD$200 I believe), I need to figure out how to fit it in (mainly power) but is it worth bothering? Anyone running one?\\n\\nAttached is what I got from copilot (sorry hard to read!), clearly not as good perf but keen for real world opinions.\\n\\nAlso, Can vllm (or ollama) run a single model across both? I’m keen to get the context window bigger for instance, but larger models would be fun too.\\n\\n","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"3060 12gb useful (pair with 3080 10gb?)","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":140,"top_awarded_type":null,"hide_score":false,"name":"t3_1m3u4rl","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.27,"author_flair_background_color":null,"ups":0,"total_awards_received":0,"media_embed":{},"thumbnail_width":140,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_100h8g","secure_media":null,"is_reddit_media_domain":true,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":0,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"https://b.thumbs.redditmedia.com/aX8H83vjKlAi1OtajLojbcn-Gk21six4tZ4tajZB9Ps.jpg","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"post_hint":"image","content_categories":null,"is_self":false,"subreddit_type":"public","created":1752925191,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"i.redd.it","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Hi,&lt;/p&gt;\\n\\n&lt;p&gt;I have a RTX 3080 with 10gb of ram, seems pretty quick with vllm running qwen2.5 coder 7b.&lt;/p&gt;\\n\\n&lt;p&gt;I have the option to buy a 3060 but with 12gb (pretty cheap at AUD$200 I believe), I need to figure out how to fit it in (mainly power) but is it worth bothering? Anyone running one?&lt;/p&gt;\\n\\n&lt;p&gt;Attached is what I got from copilot (sorry hard to read!), clearly not as good perf but keen for real world opinions.&lt;/p&gt;\\n\\n&lt;p&gt;Also, Can vllm (or ollama) run a single model across both? I’m keen to get the context window bigger for instance, but larger models would be fun too.&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"url_overridden_by_dest":"https://i.redd.it/j3ak3nhkitdf1.jpeg","view_count":null,"archived":false,"no_follow":true,"is_crosspostable":false,"pinned":false,"over_18":false,"preview":{"images":[{"source":{"url":"https://preview.redd.it/j3ak3nhkitdf1.jpeg?auto=webp&amp;s=1cfc854f73c722575f079c47de2d8263a4e7414b","width":2021,"height":2184},"resolutions":[{"url":"https://preview.redd.it/j3ak3nhkitdf1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=d67bb0dcf651cb1b9cbf5f982af5b1f1cbcff988","width":108,"height":116},{"url":"https://preview.redd.it/j3ak3nhkitdf1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=4952bba7259149d3db401826e88501b5e1430471","width":216,"height":233},{"url":"https://preview.redd.it/j3ak3nhkitdf1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=352275285735ba162801046011bf9371e6376e1f","width":320,"height":345},{"url":"https://preview.redd.it/j3ak3nhkitdf1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=0a656cb865c91b731f6a3c49f3befd1c160176b3","width":640,"height":691},{"url":"https://preview.redd.it/j3ak3nhkitdf1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=84d2865419e18797930a1b9838798c9856770662","width":960,"height":1037},{"url":"https://preview.redd.it/j3ak3nhkitdf1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e3ea4f6e89079d7a0d58f1b6f7e58108ecebcb74","width":1080,"height":1167}],"variants":{},"id":"FFwzD2AHBJbiuayF1mz2R61UhrAWnhhEI4QY2jvee70"}],"enabled":true},"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"mod_note":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"num_reports":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1m3u4rl","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"johnerp","discussion_type":null,"num_comments":10,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1m3u4rl/3060_12gb_useful_pair_with_3080_10gb/","stickied":false,"url":"https://i.redd.it/j3ak3nhkitdf1.jpeg","subreddit_subscribers":501753,"created_utc":1752925191,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3zck3x","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"johnerp","can_mod_post":false,"created_utc":1752926125,"send_replies":true,"parent_id":"t1_n3zbskl","score":1,"author_fullname":"t2_100h8g","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Ok I’ll do some research, for a couple hundred bucks I think I’ll get it any way, worst case my son gets an upgrade on his 1060!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3zck3x","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Ok I’ll do some research, for a couple hundred bucks I think I’ll get it any way, worst case my son gets an upgrade on his 1060!&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m3u4rl","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3u4rl/3060_12gb_useful_pair_with_3080_10gb/n3zck3x/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752926125,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3zbskl","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"fizzy1242","can_mod_post":false,"created_utc":1752925802,"send_replies":true,"parent_id":"t3_1m3u4rl","score":2,"author_fullname":"t2_16zcsx","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"it works, it's faster than normal ram. let's you load larger models, but expect \\"slight\\" speed hit. The speed hit is more noticable on very large models (70b+)\\n\\nI imagine vllm supports tensor-split or parallelism","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3zbskl","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;it works, it&amp;#39;s faster than normal ram. let&amp;#39;s you load larger models, but expect &amp;quot;slight&amp;quot; speed hit. The speed hit is more noticable on very large models (70b+)&lt;/p&gt;\\n\\n&lt;p&gt;I imagine vllm supports tensor-split or parallelism&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3u4rl/3060_12gb_useful_pair_with_3080_10gb/n3zbskl/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752925802,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m3u4rl","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n40j8oe","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AppearanceHeavy6724","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3zcflh","score":1,"author_fullname":"t2_uz37qfx5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"More nuanced. You can split at different ratio between the cards; you want to put as much as possible in the faster card.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n40j8oe","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;More nuanced. You can split at different ratio between the cards; you want to put as much as possible in the faster card.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m3u4rl","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3u4rl/3060_12gb_useful_pair_with_3080_10gb/n40j8oe/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752940795,"author_flair_text":null,"treatment_tags":[],"created_utc":1752940795,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3zcflh","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"johnerp","can_mod_post":false,"created_utc":1752926072,"send_replies":true,"parent_id":"t1_n3zc3eo","score":1,"author_fullname":"t2_100h8g","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Ok thx! I assume the lowest common denominator on tps is reflective, or is it more nuanced than that?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3zcflh","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Ok thx! I assume the lowest common denominator on tps is reflective, or is it more nuanced than that?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m3u4rl","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3u4rl/3060_12gb_useful_pair_with_3080_10gb/n3zcflh/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752926072,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3zc3eo","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"FriskyFennecFox","can_mod_post":false,"created_utc":1752925928,"send_replies":true,"parent_id":"t3_1m3u4rl","score":2,"author_fullname":"t2_1fqk2ehg7e","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"You'll be limited by RTX 3060's memory bandwidth if you pair it with RTX 3080! You'll run quantized 32B models, but the t/s hit is something to keep in mind.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3zc3eo","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You&amp;#39;ll be limited by RTX 3060&amp;#39;s memory bandwidth if you pair it with RTX 3080! You&amp;#39;ll run quantized 32B models, but the t/s hit is something to keep in mind.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3u4rl/3060_12gb_useful_pair_with_3080_10gb/n3zc3eo/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752925928,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m3u4rl","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"body":"Not too much X2 slower memory","subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3zi964","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Healthy-Nebula-3603","can_mod_post":false,"created_utc":1752928471,"send_replies":true,"parent_id":"t3_1m3u4rl","score":1,"author_fullname":"t2_ogjj6ebj","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"author_cakeday":true,"edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3zi964","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Not too much X2 slower memory&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3u4rl/3060_12gb_useful_pair_with_3080_10gb/n3zi964/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752928471,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m3u4rl","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3zybd6","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"InvertedVantage","can_mod_post":false,"created_utc":1752934247,"send_replies":true,"parent_id":"t3_1m3u4rl","score":1,"author_fullname":"t2_4me51","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I run this and two 3060s (1 external). Works well for loading 32b models and I usually get around 15-20 t/s which is very usable.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3zybd6","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I run this and two 3060s (1 external). Works well for loading 32b models and I usually get around 15-20 t/s which is very usable.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3u4rl/3060_12gb_useful_pair_with_3080_10gb/n3zybd6/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752934247,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m3u4rl","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n40j3qw","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AppearanceHeavy6724","can_mod_post":false,"created_utc":1752940752,"send_replies":true,"parent_id":"t3_1m3u4rl","score":1,"author_fullname":"t2_uz37qfx5","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"It will work just fine, go for it.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n40j3qw","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It will work just fine, go for it.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3u4rl/3060_12gb_useful_pair_with_3080_10gb/n40j3qw/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752940752,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m3u4rl","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n45j98s","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Unique_Judgment_1304","can_mod_post":false,"created_utc":1753013356,"send_replies":true,"parent_id":"t3_1m3u4rl","score":1,"author_fullname":"t2_1s9hyoxo94","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Since the formula for combined bandwidth under full load is a harmonic mean, the combined bandwidth would be 473 GB/s which is heavily skewed toward the slower 3060. But the good news is that for chat, RP and writing this speed is still good in this setting and should give you about 16-18 TPS under full load. You will only start feeling the 3060's relative slowness if you try adding later a 3rd or a 4th card to your rig for 30b-40b size models.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n45j98s","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Since the formula for combined bandwidth under full load is a harmonic mean, the combined bandwidth would be 473 GB/s which is heavily skewed toward the slower 3060. But the good news is that for chat, RP and writing this speed is still good in this setting and should give you about 16-18 TPS under full load. You will only start feeling the 3060&amp;#39;s relative slowness if you try adding later a 3rd or a 4th card to your rig for 30b-40b size models.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m3u4rl/3060_12gb_useful_pair_with_3080_10gb/n45j98s/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753013356,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m3u4rl","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),s=()=>e.jsx(l,{data:t});export{s as default};
