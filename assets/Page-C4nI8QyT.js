import{j as e}from"./index-BUtHYhT3.js";import{R as a}from"./RedditPostRenderer-BaN1Fn7z.js";import"./index-Cli9kp5v.js";const t=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Hello everyone,\\n\\nSo I've been working on what was initially meant to be a Claude Code clone for arbitrary LLMs over the past two weeks, [cli-agent](https://github.com/amranu/cli-agent). It has support for various APIs as well as ollama, so I felt posting here is as good idea as any.\\n\\nThe project has access to all the tools Claude Code does, such as arbitrary llm subagent support through the task tool, as well as the recently added hooks feature. I -also- recently added the ability to customize roles for your agents and subagents. This allows for some pretty dynamic behaviour changes. Because of this role feature, I was able to add the /deep-research command which allows a pseudo-deep-research with your chosen LLM. This launches 3-5 \\"researcher\\" role subagents to investigate the topic and report back, and then launches a \\"summarizer\\" role subagent to put everything together into a report. It's a pretty powerful feature! Very token hungry though. Finally, it has MCP client -and- server support. Allowing you to hook up your local LLMs to MCP servers and allowing you to make your local LLMs available over MCP through it's local mcp_server.py script. Tools -are- accessible to the LLMs over MCP.\\n\\nThe project has just made it recently to v1.2.5, so I figured I'd post it here for you all to try out. I'm especially curious if you guys find a good local LLM combination for the deep-research feature. Also, this project is only a couple weeks old, so it's still quite buggy in some places. Still, the more eyes looking at it the better I say. Cheers!","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"cli-agent - An agentic framework for arbitrary LLMs - now with hooks, roles, and deep research!","link_flair_richtext":[{"e":"text","t":"Other"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lrq827","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.8,"author_flair_background_color":null,"subreddit_type":"public","ups":6,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_3yvyd","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Other","can_mod_post":false,"score":6,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":1751669487,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"post_hint":"self","content_categories":null,"is_self":true,"mod_note":null,"created":1751654651,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Hello everyone,&lt;/p&gt;\\n\\n&lt;p&gt;So I&amp;#39;ve been working on what was initially meant to be a Claude Code clone for arbitrary LLMs over the past two weeks, &lt;a href=\\"https://github.com/amranu/cli-agent\\"&gt;cli-agent&lt;/a&gt;. It has support for various APIs as well as ollama, so I felt posting here is as good idea as any.&lt;/p&gt;\\n\\n&lt;p&gt;The project has access to all the tools Claude Code does, such as arbitrary llm subagent support through the task tool, as well as the recently added hooks feature. I -also- recently added the ability to customize roles for your agents and subagents. This allows for some pretty dynamic behaviour changes. Because of this role feature, I was able to add the /deep-research command which allows a pseudo-deep-research with your chosen LLM. This launches 3-5 &amp;quot;researcher&amp;quot; role subagents to investigate the topic and report back, and then launches a &amp;quot;summarizer&amp;quot; role subagent to put everything together into a report. It&amp;#39;s a pretty powerful feature! Very token hungry though. Finally, it has MCP client -and- server support. Allowing you to hook up your local LLMs to MCP servers and allowing you to make your local LLMs available over MCP through it&amp;#39;s local mcp_server.py script. Tools -are- accessible to the LLMs over MCP.&lt;/p&gt;\\n\\n&lt;p&gt;The project has just made it recently to v1.2.5, so I figured I&amp;#39;d post it here for you all to try out. I&amp;#39;m especially curious if you guys find a good local LLM combination for the deep-research feature. Also, this project is only a couple weeks old, so it&amp;#39;s still quite buggy in some places. Still, the more eyes looking at it the better I say. Cheers!&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"preview":{"images":[{"source":{"url":"https://external-preview.redd.it/bdLJtiVMPiAMuYWA26Aedkjth-mZiCG-flDZsN3QbGM.png?auto=webp&amp;s=ac0fd875035bc92edcaa4758d5593c6268142ebc","width":1200,"height":600},"resolutions":[{"url":"https://external-preview.redd.it/bdLJtiVMPiAMuYWA26Aedkjth-mZiCG-flDZsN3QbGM.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=031f2ce9bc32e6bd693f22f934b2c517eff29015","width":108,"height":54},{"url":"https://external-preview.redd.it/bdLJtiVMPiAMuYWA26Aedkjth-mZiCG-flDZsN3QbGM.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=7ced23d5225477f7489a2994ab614ae3c874ba45","width":216,"height":108},{"url":"https://external-preview.redd.it/bdLJtiVMPiAMuYWA26Aedkjth-mZiCG-flDZsN3QbGM.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=183485137cce6a09c37cf24a141e3fc5a4bbb177","width":320,"height":160},{"url":"https://external-preview.redd.it/bdLJtiVMPiAMuYWA26Aedkjth-mZiCG-flDZsN3QbGM.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=1bebfc2de478cfa85fe8e13513a730d925aebd39","width":640,"height":320},{"url":"https://external-preview.redd.it/bdLJtiVMPiAMuYWA26Aedkjth-mZiCG-flDZsN3QbGM.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=71e49e7d689f50adf7a88ef4b7e1556a5ce4992a","width":960,"height":480},{"url":"https://external-preview.redd.it/bdLJtiVMPiAMuYWA26Aedkjth-mZiCG-flDZsN3QbGM.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=ebdc9795638a23b3f6f75de98a330a07b768a06a","width":1080,"height":540}],"variants":{},"id":"bdLJtiVMPiAMuYWA26Aedkjth-mZiCG-flDZsN3QbGM"}],"enabled":false},"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"7a7848d2-bf8e-11ed-8c2f-765d15199f78","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#94e044","id":"1lrq827","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"amranu","discussion_type":null,"num_comments":4,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lrq827/cliagent_an_agentic_framework_for_arbitrary_llms/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lrq827/cliagent_an_agentic_framework_for_arbitrary_llms/","subreddit_subscribers":494898,"created_utc":1751654651,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1dcd95","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"amranu","can_mod_post":false,"created_utc":1751662927,"send_replies":true,"parent_id":"t1_n1dazfd","score":3,"author_fullname":"t2_3yvyd","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I've exposed this as OPENAI_BASE_URL, and it's on github now (but not PyPI yet).","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1dcd95","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;ve exposed this as OPENAI_BASE_URL, and it&amp;#39;s on github now (but not PyPI yet).&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lrq827","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrq827/cliagent_an_agentic_framework_for_arbitrary_llms/n1dcd95/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751662927,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n1dazfd","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DirectCurrent_","can_mod_post":false,"created_utc":1751662475,"send_replies":true,"parent_id":"t3_1lrq827","score":1,"author_fullname":"t2_vl1u9ir8","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"&gt;   def get_default_base_url(self) -&gt; str:\\n        return \\"https://api.openai.com/v1\\"\\n\\ncan you expose this so we can use openai compatible endpoints please, its such a simple change, i'm locally hosting using a variety of different non-ollama solutions and there's no reason for this to be hard coded","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1dazfd","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;def get_default_base_url(self) -&amp;gt; str:\\n        return &amp;quot;&lt;a href=\\"https://api.openai.com/v1\\"&gt;https://api.openai.com/v1&lt;/a&gt;&amp;quot;&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;can you expose this so we can use openai compatible endpoints please, its such a simple change, i&amp;#39;m locally hosting using a variety of different non-ollama solutions and there&amp;#39;s no reason for this to be hard coded&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrq827/cliagent_an_agentic_framework_for_arbitrary_llms/n1dazfd/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751662475,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lrq827","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1gam3c","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"amranu","can_mod_post":false,"created_utc":1751713349,"send_replies":true,"parent_id":"t1_n1g0o12","score":1,"author_fullname":"t2_3yvyd","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"No real reason, just because most people already have accounts with OpenAI and Anthropic. I also have openrouter support for instance, and it's relatively easy to add new providers.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1gam3c","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;No real reason, just because most people already have accounts with OpenAI and Anthropic. I also have openrouter support for instance, and it&amp;#39;s relatively easy to add new providers.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lrq827","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrq827/cliagent_an_agentic_framework_for_arbitrary_llms/n1gam3c/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751713349,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1g0o12","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Defenestrate_me77","can_mod_post":false,"created_utc":1751707402,"send_replies":true,"parent_id":"t3_1lrq827","score":1,"author_fullname":"t2_ec0yj42g","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"i am working on a very similar project, i have a question why did you implement individual provider support instead of using something like litellm since that abstracts a lot of this","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1g0o12","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;i am working on a very similar project, i have a question why did you implement individual provider support instead of using something like litellm since that abstracts a lot of this&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lrq827/cliagent_an_agentic_framework_for_arbitrary_llms/n1g0o12/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751707402,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lrq827","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),s=()=>e.jsx(a,{data:t});export{s as default};
