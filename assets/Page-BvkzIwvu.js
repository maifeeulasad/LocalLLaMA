import{j as e}from"./index-Bu7qcPAU.js";import{R as l}from"./RedditPostRenderer-CbHA7O5q.js";import"./index-BKgbfxhf.js";const t=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"This is just a thought experiment right now, but hear me out. \\n\\nhttps://huggingface.co/moonshotai/Kimi-K2-Instruct/tree/main the weights for Kimi K2 is about 1031GB in total. \\n\\nYou can buy 12 sticks of 96gb DDR5-6400 RAM (total 1152GB) [for about $7200](https://www.amazon.com/NEMIX-RAM-Registered-Compatible-Supermicro/dp/B0DQLVV9TK). DDR5-6400 12 channel is [614GB/sec](https://chatgpt.com/share/687a0964-60cc-8012-8b2e-d98154d79691). That's pretty close (about 75%) of the [512GB Mac Studio which has 819GB/sec](https://www.apple.com/mac-studio/specs/) memory bandwidth. \\n\\nYou just need an AMD EPYC 9005 series cpu and a compatible 12 channel RAM motherboard, which [costs around $1400 total](https://chatgpt.com/share/687a0bf0-8f00-8012-83e6-890414f2d0d1) these days. Throw in a Nvidia RTX 3090 or two, or maybe a RTX5090 (to handle the non MoE layers) and it should run even faster. With the 1152GB of DDR5 RAM combined with the GPU, you can run Kimi-K2 at a very reasonable speed for below $10k. \\n\\nDo these numbers make sense? It seems like the Mac Studio 512GB has a competitor now, at least in terms of globs of RAM. The Mac Studio 512GB is still a bit faster in terms of memory bandwidth, but having 1152GB of RAM at the same price is certainly worth considering of a tradeoff for 25% of memory bandwidth.","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Run Kimi-K2 without quantization locally for under $10k?","link_flair_richtext":[{"e":"text","t":"Discussion"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1m2xh8s","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.9,"author_flair_background_color":null,"subreddit_type":"public","ups":125,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_t6glzswk","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Discussion","can_mod_post":false,"score":125,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"post_hint":"self","content_categories":null,"is_self":true,"mod_note":null,"created":1752829800,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;This is just a thought experiment right now, but hear me out. &lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://huggingface.co/moonshotai/Kimi-K2-Instruct/tree/main\\"&gt;https://huggingface.co/moonshotai/Kimi-K2-Instruct/tree/main&lt;/a&gt; the weights for Kimi K2 is about 1031GB in total. &lt;/p&gt;\\n\\n&lt;p&gt;You can buy 12 sticks of 96gb DDR5-6400 RAM (total 1152GB) &lt;a href=\\"https://www.amazon.com/NEMIX-RAM-Registered-Compatible-Supermicro/dp/B0DQLVV9TK\\"&gt;for about $7200&lt;/a&gt;. DDR5-6400 12 channel is &lt;a href=\\"https://chatgpt.com/share/687a0964-60cc-8012-8b2e-d98154d79691\\"&gt;614GB/sec&lt;/a&gt;. That&amp;#39;s pretty close (about 75%) of the &lt;a href=\\"https://www.apple.com/mac-studio/specs/\\"&gt;512GB Mac Studio which has 819GB/sec&lt;/a&gt; memory bandwidth. &lt;/p&gt;\\n\\n&lt;p&gt;You just need an AMD EPYC 9005 series cpu and a compatible 12 channel RAM motherboard, which &lt;a href=\\"https://chatgpt.com/share/687a0bf0-8f00-8012-83e6-890414f2d0d1\\"&gt;costs around $1400 total&lt;/a&gt; these days. Throw in a Nvidia RTX 3090 or two, or maybe a RTX5090 (to handle the non MoE layers) and it should run even faster. With the 1152GB of DDR5 RAM combined with the GPU, you can run Kimi-K2 at a very reasonable speed for below $10k. &lt;/p&gt;\\n\\n&lt;p&gt;Do these numbers make sense? It seems like the Mac Studio 512GB has a competitor now, at least in terms of globs of RAM. The Mac Studio 512GB is still a bit faster in terms of memory bandwidth, but having 1152GB of RAM at the same price is certainly worth considering of a tradeoff for 25% of memory bandwidth.&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"preview":{"images":[{"source":{"url":"https://external-preview.redd.it/ZjybqN_iZigLaZxMtl0N3yFDPtiDQRo-8LU-o9LYLXQ.png?auto=webp&amp;s=78218534a59407b3e56ec5c79df38546f4efe70c","width":1200,"height":648},"resolutions":[{"url":"https://external-preview.redd.it/ZjybqN_iZigLaZxMtl0N3yFDPtiDQRo-8LU-o9LYLXQ.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=014f5215759a7ee46cc335661cfd741228ef1b1e","width":108,"height":58},{"url":"https://external-preview.redd.it/ZjybqN_iZigLaZxMtl0N3yFDPtiDQRo-8LU-o9LYLXQ.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=7a09f794f1ff77c0c16776942ad4b842977ccb84","width":216,"height":116},{"url":"https://external-preview.redd.it/ZjybqN_iZigLaZxMtl0N3yFDPtiDQRo-8LU-o9LYLXQ.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=01057b7f796b87e54735f133cbd2404a4a8425d2","width":320,"height":172},{"url":"https://external-preview.redd.it/ZjybqN_iZigLaZxMtl0N3yFDPtiDQRo-8LU-o9LYLXQ.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=65e4d917b0768ba9727a840f3e7b4ddd3fdb7ea3","width":640,"height":345},{"url":"https://external-preview.redd.it/ZjybqN_iZigLaZxMtl0N3yFDPtiDQRo-8LU-o9LYLXQ.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=fc89ea5ea896d832e6642ea7b443df8584200eab","width":960,"height":518},{"url":"https://external-preview.redd.it/ZjybqN_iZigLaZxMtl0N3yFDPtiDQRo-8LU-o9LYLXQ.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=d43168a98c7fb53d5480aa0b7e96d2c75f889729","width":1080,"height":583}],"variants":{},"id":"ZjybqN_iZigLaZxMtl0N3yFDPtiDQRo-8LU-o9LYLXQ"}],"enabled":false},"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"5f921ea4-c7bc-11ed-9c23-3a00622979b4","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#646d73","id":"1m2xh8s","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"DepthHour1669","discussion_type":null,"num_comments":149,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1m2xh8s/run_kimik2_without_quantization_locally_for_under/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1m2xh8s/run_kimik2_without_quantization_locally_for_under/","subreddit_subscribers":501752,"created_utc":1752829800,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3vhp2a","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"rbit4","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3t7oh5","score":3,"author_fullname":"t2_g4rgng0","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I am creating a similar system. 9654 instead of 9684x, do you see any specific need for for the 1152mb cache on l3 bs 384mb? Have 2 5090s and 2 4090s for the full system already to be mounted. Going for a asrock Genoa mb with 7 pcie5x16 slots. Will use it for fine tuning as well","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3vhp2a","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I am creating a similar system. 9654 instead of 9684x, do you see any specific need for for the 1152mb cache on l3 bs 384mb? Have 2 5090s and 2 4090s for the full system already to be mounted. Going for a asrock Genoa mb with 7 pcie5x16 slots. Will use it for fine tuning as well&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2xh8s","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2xh8s/run_kimik2_without_quantization_locally_for_under/n3vhp2a/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752868090,"author_flair_text":null,"treatment_tags":[],"created_utc":1752868090,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4181ca","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"couscous_sun","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3t7oh5","score":1,"author_fullname":"t2_fslqiqx57","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"But how many tokens is your input prompt? For long context generation &gt; 5k tokens, I figured Mac Studios are total trash.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n4181ca","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;But how many tokens is your input prompt? For long context generation &amp;gt; 5k tokens, I figured Mac Studios are total trash.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2xh8s","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2xh8s/run_kimik2_without_quantization_locally_for_under/n4181ca/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752948460,"author_flair_text":null,"treatment_tags":[],"created_utc":1752948460,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3t7oh5","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"tomz17","can_mod_post":false,"created_utc":1752844383,"send_replies":true,"parent_id":"t1_n3shurs","score":44,"author_fullname":"t2_1mhx5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"&gt; You will be VERY dissapointed with the speed though\\n\\nI get about 18t/s token generation on a 9684x with 12 channels of DDR5-4800 (only 384GB, though, so \\\\~3bpw weights), and offload to a single 3090.  DDR5-6400 would obviously be proportionately faster.  So nothing amazing, but definitely usable.\\n\\nThat being said, this obviously would not scale well to multiple simultaneous users.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3t7oh5","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;You will be VERY dissapointed with the speed though&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;I get about 18t/s token generation on a 9684x with 12 channels of DDR5-4800 (only 384GB, though, so ~3bpw weights), and offload to a single 3090.  DDR5-6400 would obviously be proportionately faster.  So nothing amazing, but definitely usable.&lt;/p&gt;\\n\\n&lt;p&gt;That being said, this obviously would not scale well to multiple simultaneous users.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2xh8s","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2xh8s/run_kimik2_without_quantization_locally_for_under/n3t7oh5/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752844383,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":44}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3sxplc","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"ElectronSpiderwort","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3svq24","score":13,"author_fullname":"t2_mxbu5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Your ideas are intriguing to me, and I wish to subscribe to your newsletter","edited":false,"author_flair_css_class":null,"name":"t1_n3sxplc","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Your ideas are intriguing to me, and I wish to subscribe to your newsletter&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1m2xh8s","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2xh8s/run_kimik2_without_quantization_locally_for_under/n3sxplc/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752840932,"author_flair_text":null,"collapsed":false,"created_utc":1752840932,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":13}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3ud52k","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"this-just_in","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3tbgsk","score":3,"author_fullname":"t2_kdmu4","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Push it in an Azure Storage account (fraction of a penny) in advance, then download it to the VM when it loads up.  Nuke the storage account when complete.","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n3ud52k","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Push it in an Azure Storage account (fraction of a penny) in advance, then download it to the VM when it loads up.  Nuke the storage account when complete.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1m2xh8s","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2xh8s/run_kimik2_without_quantization_locally_for_under/n3ud52k/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752856432,"author_flair_text":null,"treatment_tags":[],"created_utc":1752856432,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":6,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3vxl59","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"xanduonc","can_mod_post":false,"created_utc":1752872794,"send_replies":true,"parent_id":"t1_n3vrf9a","score":1,"author_fullname":"t2_10n3b6gg97","approved_by":null,"mod_note":null,"all_awardings":[],"body":"i dont know how it is called in AWS but in clould i'm used to you can equally attach and detach any disk to vms, there is no such thing as local storage\\n\\naccess speed and iops are good for large ssd disks as they do some kind of stripe from blocks of fixed size","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n3vxl59","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;i dont know how it is called in AWS but in clould i&amp;#39;m used to you can equally attach and detach any disk to vms, there is no such thing as local storage&lt;/p&gt;\\n\\n&lt;p&gt;access speed and iops are good for large ssd disks as they do some kind of stripe from blocks of fixed size&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1m2xh8s","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2xh8s/run_kimik2_without_quantization_locally_for_under/n3vxl59/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752872794,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":7,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3vrf9a","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"One-Employment3759","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3tbgsk","score":1,"author_fullname":"t2_1f6wnmakwr","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"don't do this, loading blocks from network storage for model weights is painful.\\n\\nat least on AWS, and for the models I was using, it was far faster to parallel download from s3 to local ephemeral NVMe on boot and use that.","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n3vrf9a","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;don&amp;#39;t do this, loading blocks from network storage for model weights is painful.&lt;/p&gt;\\n\\n&lt;p&gt;at least on AWS, and for the models I was using, it was far faster to parallel download from s3 to local ephemeral NVMe on boot and use that.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1m2xh8s","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2xh8s/run_kimik2_without_quantization_locally_for_under/n3vrf9a/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752870957,"author_flair_text":null,"treatment_tags":[],"created_utc":1752870957,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":6,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3tbgsk","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"xanduonc","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3t6vv4","score":10,"author_fullname":"t2_10n3b6gg97","approved_by":null,"mod_note":null,"all_awardings":[],"body":"You probably can download the model on network storage using cheap instance and then attach it to beefy vm for inference","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n3tbgsk","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You probably can download the model on network storage using cheap instance and then attach it to beefy vm for inference&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1m2xh8s","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2xh8s/run_kimik2_without_quantization_locally_for_under/n3tbgsk/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752845587,"author_flair_text":null,"treatment_tags":[],"created_utc":1752845587,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":10}}],"before":null}},"user_reports":[],"saved":false,"id":"n3t6vv4","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"LevianMcBirdo","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3svq24","score":10,"author_fullname":"t2_cw9f6o0r","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I mean 20$ to try it out before spending thousands of dollars seems a wise code. Question is if you have the model downloaded in that time😅","edited":false,"author_flair_css_class":null,"name":"t1_n3t6vv4","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I mean 20$ to try it out before spending thousands of dollars seems a wise code. Question is if you have the model downloaded in that time😅&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1m2xh8s","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2xh8s/run_kimik2_without_quantization_locally_for_under/n3t6vv4/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752844125,"author_flair_text":null,"collapsed":false,"created_utc":1752844125,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":10}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3u183z","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Conscious_Cut_6144","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3tb5gw","score":1,"author_fullname":"t2_9hl4ymvj","approved_by":null,"mod_note":null,"all_awardings":[],"body":"Going to be 1/2 that on a good day","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n3u183z","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Going to be 1/2 that on a good day&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1m2xh8s","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2xh8s/run_kimik2_without_quantization_locally_for_under/n3u183z/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752853044,"author_flair_text":null,"treatment_tags":[],"created_utc":1752853044,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3tb5gw","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"xanduonc","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3svq24","score":3,"author_fullname":"t2_10n3b6gg97","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Please do test your idea before investing, 20t/s seems optimistic, but would be great if confirmed","edited":false,"author_flair_css_class":null,"name":"t1_n3tb5gw","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Please do test your idea before investing, 20t/s seems optimistic, but would be great if confirmed&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1m2xh8s","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2xh8s/run_kimik2_without_quantization_locally_for_under/n3tb5gw/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752845488,"author_flair_text":null,"collapsed":false,"created_utc":1752845488,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3tkppj","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"poli-cya","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3svq24","score":1,"author_fullname":"t2_q8g93lhv4","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Can you let us know how it goes.","edited":false,"author_flair_css_class":null,"name":"t1_n3tkppj","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Can you let us know how it goes.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1m2xh8s","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2xh8s/run_kimik2_without_quantization_locally_for_under/n3tkppj/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752848392,"author_flair_text":null,"collapsed":false,"created_utc":1752848392,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3svq24","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"DepthHour1669","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3std1v","score":23,"author_fullname":"t2_t6glzswk","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Yeah, this isn't factoring in prompt processing. But that should be okay if you throw in a cheap 3090, or a 5090.\\n\\nAt least context should not be an issue, KV cache should be under 10gb since Kimi K2 uses MLA. \\n\\n\\n&gt; If you throw a couple GPUs in there and use something like ktransformers to offload the attention computations, that might help you out a bit, but I can't comment as to how much.\\n\\nI did the math on common weight size here: https://www.reddit.com/r/LocalLLaMA/comments/1m2xh8s/run_kimik2_without_quantization_locally_for_under/n3smus7/\\n\\nTL;DR you just need 11GB vram for the common weights, the rest of it is experts. And then about ~7GB vram for 128k token context (judging from Deepseek V3 architecture numbers, Kimi is probably slightly bigger), so you need a GPU with about 20GB vram. That's about it. Adding a single 5090 to the DDR5 only system would get you 25tok/sec, and an infinitely fast GPU (with the context and common weights loaded) would still only get you 29tok/sec. So I don't think there's any point in getting more than 1 GPU. \\n\\n&gt; I'd advise you to rent a cloud server for an afternoon and test it out before you drop 10K on it 🙃\\n\\nI just found out the Azure HX servers exist: \\n\\nhttps://learn.microsoft.com/en-us/azure/virtual-machines/sizes/high-performance-compute/hx-series?tabs=sizebasic\\n\\nThe Standard_HX176rs is 1408GB of ram, 780GB/sec, and $8/hour. I'm tempted to blow $20 on it and run Kimi K2 on it for fun.","edited":1752840810,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3svq24","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yeah, this isn&amp;#39;t factoring in prompt processing. But that should be okay if you throw in a cheap 3090, or a 5090.&lt;/p&gt;\\n\\n&lt;p&gt;At least context should not be an issue, KV cache should be under 10gb since Kimi K2 uses MLA. &lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;If you throw a couple GPUs in there and use something like ktransformers to offload the attention computations, that might help you out a bit, but I can&amp;#39;t comment as to how much.&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;I did the math on common weight size here: &lt;a href=\\"https://www.reddit.com/r/LocalLLaMA/comments/1m2xh8s/run_kimik2_without_quantization_locally_for_under/n3smus7/\\"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1m2xh8s/run_kimik2_without_quantization_locally_for_under/n3smus7/&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;TL;DR you just need 11GB vram for the common weights, the rest of it is experts. And then about ~7GB vram for 128k token context (judging from Deepseek V3 architecture numbers, Kimi is probably slightly bigger), so you need a GPU with about 20GB vram. That&amp;#39;s about it. Adding a single 5090 to the DDR5 only system would get you 25tok/sec, and an infinitely fast GPU (with the context and common weights loaded) would still only get you 29tok/sec. So I don&amp;#39;t think there&amp;#39;s any point in getting more than 1 GPU. &lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;I&amp;#39;d advise you to rent a cloud server for an afternoon and test it out before you drop 10K on it 🙃&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;I just found out the Azure HX servers exist: &lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://learn.microsoft.com/en-us/azure/virtual-machines/sizes/high-performance-compute/hx-series?tabs=sizebasic\\"&gt;https://learn.microsoft.com/en-us/azure/virtual-machines/sizes/high-performance-compute/hx-series?tabs=sizebasic&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;The Standard_HX176rs is 1408GB of ram, 780GB/sec, and $8/hour. I&amp;#39;m tempted to blow $20 on it and run Kimi K2 on it for fun.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2xh8s","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2xh8s/run_kimik2_without_quantization_locally_for_under/n3svq24/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752840174,"author_flair_text":null,"treatment_tags":[],"created_utc":1752840174,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":23}}],"before":null}},"user_reports":[],"saved":false,"id":"n3std1v","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"101m4n","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3skdqv","score":26,"author_fullname":"t2_p7nc2","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"That's all well and good, but compute also matters here. The longer your prompt becomes, the more that lack of compute will begin to cause you problems.\\n\\nIf you throw a couple GPUs in there and use something like ktransformers to offload the attention computations, that might help you out a bit, but I can't comment as to _how much_. It's not something I've taken the time to experiment with yet.\\n\\nI'd advise you to rent a cloud server for an afternoon and test it out before you drop 10K on it 🙃","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3std1v","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;That&amp;#39;s all well and good, but compute also matters here. The longer your prompt becomes, the more that lack of compute will begin to cause you problems.&lt;/p&gt;\\n\\n&lt;p&gt;If you throw a couple GPUs in there and use something like ktransformers to offload the attention computations, that might help you out a bit, but I can&amp;#39;t comment as to &lt;em&gt;how much&lt;/em&gt;. It&amp;#39;s not something I&amp;#39;ve taken the time to experiment with yet.&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;d advise you to rent a cloud server for an afternoon and test it out before you drop 10K on it 🙃&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2xh8s","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2xh8s/run_kimik2_without_quantization_locally_for_under/n3std1v/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752839244,"author_flair_text":null,"treatment_tags":[],"created_utc":1752839244,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":26}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3vtyyj","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DepthHour1669","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3vr2j5","score":3,"author_fullname":"t2_t6glzswk","approved_by":null,"mod_note":null,"all_awardings":[],"body":"I think I figured out why his memory bandwidth is so slow. He’s not pinning the socket to its own NUMA node. He needs to not set NPS=0 in linux.\\n \\nIf he’s doing inference and accessing ram owned by the other CPU, he’s limited to 512GB/sec across xGMI from one CPU to the other CPU. His inference speed at Q4 highly implies that he’s stuck at 512GB/sec. \\n\\nHe needs to set NPS, and set --numa-pinning in vLLM or similar. Running llama.cpp wont work: https://github.com/ggml-org/llama.cpp/discussions/11733 doesn’t look resolved yet.","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n3vtyyj","is_submitter":true,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I think I figured out why his memory bandwidth is so slow. He’s not pinning the socket to its own NUMA node. He needs to not set NPS=0 in linux.&lt;/p&gt;\\n\\n&lt;p&gt;If he’s doing inference and accessing ram owned by the other CPU, he’s limited to 512GB/sec across xGMI from one CPU to the other CPU. His inference speed at Q4 highly implies that he’s stuck at 512GB/sec. &lt;/p&gt;\\n\\n&lt;p&gt;He needs to set NPS, and set --numa-pinning in vLLM or similar. Running llama.cpp wont work: &lt;a href=\\"https://github.com/ggml-org/llama.cpp/discussions/11733\\"&gt;https://github.com/ggml-org/llama.cpp/discussions/11733&lt;/a&gt; doesn’t look resolved yet.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1m2xh8s","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2xh8s/run_kimik2_without_quantization_locally_for_under/n3vtyyj/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752871702,"author_flair_text":null,"treatment_tags":[],"created_utc":1752871702,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n3vr2j5","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"eloquentemu","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3vchhn","score":1,"author_fullname":"t2_lpdsy","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"&gt;  Get a 4090 instead of 3090 for +$1000 and you get FP8.\\n\\nCorrect.  Though what inference engine supports FP8 on CPU?  llama.cpp and vllm don't AFAICT.  (I'm genuinely curious if you know.)  I saw that gemma.cpp does (converting to bf16) but obviously not super helpful for Kimi.\\n\\n&gt; Get a 9375F with 32cores for $3k.\\n\\nFor sure, but that doesn't mean that the 9015 wasn't a bad choice so I had to say it :).  9375F is a solid choice though.  The 9355(P) could save you some money if you don't want the high boost for other applications.  The 9555 also seems to have some cheap ES/QS versions around, if you want to play that game, but I haven't looked into whether they have issues.\\n\\n&gt; doesn’t say he’s on DDR5-6400; most typical high memory build servers do not have overclocked RAM\\n\\nTrue, they definitely could be running 5600. I doubt it would be 4800 since that hasn't been meaningfully cheaper than 5200 for a while. Turin only kind of supports 6400 though... Most platforms only run at 6000 and won't let you overclock so watch for that.","edited":false,"author_flair_css_class":null,"name":"t1_n3vr2j5","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;Get a 4090 instead of 3090 for +$1000 and you get FP8.&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;Correct.  Though what inference engine supports FP8 on CPU?  llama.cpp and vllm don&amp;#39;t AFAICT.  (I&amp;#39;m genuinely curious if you know.)  I saw that gemma.cpp does (converting to bf16) but obviously not super helpful for Kimi.&lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;Get a 9375F with 32cores for $3k.&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;For sure, but that doesn&amp;#39;t mean that the 9015 wasn&amp;#39;t a bad choice so I had to say it :).  9375F is a solid choice though.  The 9355(P) could save you some money if you don&amp;#39;t want the high boost for other applications.  The 9555 also seems to have some cheap ES/QS versions around, if you want to play that game, but I haven&amp;#39;t looked into whether they have issues.&lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;doesn’t say he’s on DDR5-6400; most typical high memory build servers do not have overclocked RAM&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;True, they definitely could be running 5600. I doubt it would be 4800 since that hasn&amp;#39;t been meaningfully cheaper than 5200 for a while. Turin only kind of supports 6400 though... Most platforms only run at 6000 and won&amp;#39;t let you overclock so watch for that.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1m2xh8s","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2xh8s/run_kimik2_without_quantization_locally_for_under/n3vr2j5/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752870854,"author_flair_text":null,"collapsed":false,"created_utc":1752870854,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3vchhn","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DepthHour1669","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3uu1o7","score":2,"author_fullname":"t2_t6glzswk","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Most of what you said can be tweaked. Get a 4080 instead of 3090 and you get FP8. Get a 9375F with 32cores for $3k. Total build would be $13k ish.\\n\\nAlso, the example you linked doesn’t say he’s on DDR5-6400; most typical high memory build servers do not have overclocked RAM, as typical DDR5 tops out below that. If he bought a prebuilt typical high memory server that might be DDR5-4800 or something. He also was on 23dimms, so his memory bandwidth isn’t ideal.\\n\\nThat being said, yeah, that’s a decent chunk slower than ideal.","edited":1752885664,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3vchhn","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Most of what you said can be tweaked. Get a 4080 instead of 3090 and you get FP8. Get a 9375F with 32cores for $3k. Total build would be $13k ish.&lt;/p&gt;\\n\\n&lt;p&gt;Also, the example you linked doesn’t say he’s on DDR5-6400; most typical high memory build servers do not have overclocked RAM, as typical DDR5 tops out below that. If he bought a prebuilt typical high memory server that might be DDR5-4800 or something. He also was on 23dimms, so his memory bandwidth isn’t ideal.&lt;/p&gt;\\n\\n&lt;p&gt;That being said, yeah, that’s a decent chunk slower than ideal.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2xh8s","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2xh8s/run_kimik2_without_quantization_locally_for_under/n3vchhn/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752866537,"author_flair_text":null,"treatment_tags":[],"created_utc":1752866537,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n3uu1o7","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"eloquentemu","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3skdqv","score":3,"author_fullname":"t2_lpdsy","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"There's a lot wrong here\\n\\nEPYC 9015 is a trash CPU.  I'm not exaggerating.  With 2 CCDs it cannot utilize the full memory bandwidth because the CCDs and IODie have an interlink with finite bandwidth.  They are about ~60GBps (like ~1.5 DDR5 channels).  The CPUs with &lt;=4 CCDs will usually have two links, but even if the 9015 does, that's still only ~6ch of DDR5 worth of bandwidth.  You're wasting way more money on RAM than saving on the processor.\\n\\nAs the other posters mentioned, compute is also important and 8 cores will _definitely_ hurt you there.  I think I saw decent scaling to 32c and it still improves past there.\\n\\nYour speed math is wrong, or maybe accurately way too theoretical.  How about looking up a benchmark?  [Here's a user](https://github.com/ggml-org/llama.cpp/issues/14642#issuecomment-3071577819) with a decent Turin (9355) in a dual socket config and an RTX Pro 6000.  They get 22t/s at ~0 context length on *Q4*, and a decent percentage of that performance is from the dual socket and Pro 6000 being able to offload layers to the 96GB VRAM.  Expect ~15t/s from yours with GPU assist - well, and with a proper Epyc CPU.  Yeah, that's much lower than theoretical, but that's because you aren't just reading in model weights but also writing out intermediates and running and OS etc.  I suspect there's some optimization possible, but for now that's the reality. \\n\\nBut again, that's Q4 and you asked about \\"without quantization\\".  It's an fp8 model so we could somewhat immediately halve the expected performance (~double the bits per weight).  However there's an extra wrinkle: it's an fp8 model this won't run on the 3090 and AFAICT there's no CPU support either.  If you wanted to run lossless you'll need to use bf16 which makes it a 2TB model.  Q8 is _not_ lossless from FP8, but it is close, so you could run that.  I _think_ you can still fit non-expert weights in a 24GB GPU at Q8, but it will limit your offload further.\\n\\ntl;dr, your proposed config will get ~4t/s based on Q4 benchmarks and CPU bandwidth limitations.  Get a better processor for ~8t/s.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3uu1o7","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;There&amp;#39;s a lot wrong here&lt;/p&gt;\\n\\n&lt;p&gt;EPYC 9015 is a trash CPU.  I&amp;#39;m not exaggerating.  With 2 CCDs it cannot utilize the full memory bandwidth because the CCDs and IODie have an interlink with finite bandwidth.  They are about ~60GBps (like ~1.5 DDR5 channels).  The CPUs with &amp;lt;=4 CCDs will usually have two links, but even if the 9015 does, that&amp;#39;s still only ~6ch of DDR5 worth of bandwidth.  You&amp;#39;re wasting way more money on RAM than saving on the processor.&lt;/p&gt;\\n\\n&lt;p&gt;As the other posters mentioned, compute is also important and 8 cores will &lt;em&gt;definitely&lt;/em&gt; hurt you there.  I think I saw decent scaling to 32c and it still improves past there.&lt;/p&gt;\\n\\n&lt;p&gt;Your speed math is wrong, or maybe accurately way too theoretical.  How about looking up a benchmark?  &lt;a href=\\"https://github.com/ggml-org/llama.cpp/issues/14642#issuecomment-3071577819\\"&gt;Here&amp;#39;s a user&lt;/a&gt; with a decent Turin (9355) in a dual socket config and an RTX Pro 6000.  They get 22t/s at ~0 context length on &lt;em&gt;Q4&lt;/em&gt;, and a decent percentage of that performance is from the dual socket and Pro 6000 being able to offload layers to the 96GB VRAM.  Expect ~15t/s from yours with GPU assist - well, and with a proper Epyc CPU.  Yeah, that&amp;#39;s much lower than theoretical, but that&amp;#39;s because you aren&amp;#39;t just reading in model weights but also writing out intermediates and running and OS etc.  I suspect there&amp;#39;s some optimization possible, but for now that&amp;#39;s the reality. &lt;/p&gt;\\n\\n&lt;p&gt;But again, that&amp;#39;s Q4 and you asked about &amp;quot;without quantization&amp;quot;.  It&amp;#39;s an fp8 model so we could somewhat immediately halve the expected performance (~double the bits per weight).  However there&amp;#39;s an extra wrinkle: it&amp;#39;s an fp8 model this won&amp;#39;t run on the 3090 and AFAICT there&amp;#39;s no CPU support either.  If you wanted to run lossless you&amp;#39;ll need to use bf16 which makes it a 2TB model.  Q8 is &lt;em&gt;not&lt;/em&gt; lossless from FP8, but it is close, so you could run that.  I &lt;em&gt;think&lt;/em&gt; you can still fit non-expert weights in a 24GB GPU at Q8, but it will limit your offload further.&lt;/p&gt;\\n\\n&lt;p&gt;tl;dr, your proposed config will get ~4t/s based on Q4 benchmarks and CPU bandwidth limitations.  Get a better processor for ~8t/s.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2xh8s","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2xh8s/run_kimik2_without_quantization_locally_for_under/n3uu1o7/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752861136,"author_flair_text":null,"treatment_tags":[],"created_utc":1752861136,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3t193o","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"allenasm","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3skdqv","score":1,"author_fullname":"t2_fouwt","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Exactly! With that much ram they could run both the MOE model as well as other more precise higher parm models that would be slower but more accurate.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3t193o","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Exactly! With that much ram they could run both the MOE model as well as other more precise higher parm models that would be slower but more accurate.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2xh8s","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2xh8s/run_kimik2_without_quantization_locally_for_under/n3t193o/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752842220,"author_flair_text":null,"treatment_tags":[],"created_utc":1752842220,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3skdqv","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"DepthHour1669","can_mod_post":false,"created_utc":1752835253,"send_replies":true,"parent_id":"t1_n3shurs","score":21,"author_fullname":"t2_t6glzswk","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Running full Kimi K2 at native 8bit on DDR5-6400 12channel should result in... about 20-25tok/sec. \\n\\nAbout 66% of the weights per token are experts, and 33% are common weights. So if you put 11b on a GPU, you'll get a decent speedup on that 11b active. \\n\\nBaseline speed is about 20tok/sec without a GPU, so maybe 25tok/sec with a GPU.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3skdqv","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Running full Kimi K2 at native 8bit on DDR5-6400 12channel should result in... about 20-25tok/sec. &lt;/p&gt;\\n\\n&lt;p&gt;About 66% of the weights per token are experts, and 33% are common weights. So if you put 11b on a GPU, you&amp;#39;ll get a decent speedup on that 11b active. &lt;/p&gt;\\n\\n&lt;p&gt;Baseline speed is about 20tok/sec without a GPU, so maybe 25tok/sec with a GPU.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2xh8s","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2xh8s/run_kimik2_without_quantization_locally_for_under/n3skdqv/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752835253,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":21}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3ymyb9","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"TedditBlatherflag","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3y9vz2","score":1,"author_fullname":"t2_k5d9sjee","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"When you say parameters do you mean embedding inputs? Or do you mean nodes in the neural net?\\n\\nTrying to figure out if you mean a clever decomposition of the matrix math or a filtering of the neural net to reduce the calculations needed?","edited":false,"author_flair_css_class":null,"name":"t1_n3ymyb9","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;When you say parameters do you mean embedding inputs? Or do you mean nodes in the neural net?&lt;/p&gt;\\n\\n&lt;p&gt;Trying to figure out if you mean a clever decomposition of the matrix math or a filtering of the neural net to reduce the calculations needed?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1m2xh8s","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2xh8s/run_kimik2_without_quantization_locally_for_under/n3ymyb9/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752912652,"author_flair_text":null,"collapsed":false,"created_utc":1752912652,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3y9vz2","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"bene_42069","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3xzq8s","score":1,"author_fullname":"t2_9yo3ah1u","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Mixture of Experts. Basically only small parts of each layers (a fraction of parameters, in this case only up to 32 billion out of all 1 trillion) gets processed at a time rather than all at once in a traditional dense model. Saves a lot of compute, but may take slightly more memory and tricky to fine tune.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3y9vz2","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Mixture of Experts. Basically only small parts of each layers (a fraction of parameters, in this case only up to 32 billion out of all 1 trillion) gets processed at a time rather than all at once in a traditional dense model. Saves a lot of compute, but may take slightly more memory and tricky to fine tune.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2xh8s","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2xh8s/run_kimik2_without_quantization_locally_for_under/n3y9vz2/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752905434,"author_flair_text":null,"treatment_tags":[],"created_utc":1752905434,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3xzq8s","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"TedditBlatherflag","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3styvg","score":2,"author_fullname":"t2_k5d9sjee","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"What’s MoE?","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3xzq8s","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;What’s MoE?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2xh8s","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2xh8s/run_kimik2_without_quantization_locally_for_under/n3xzq8s/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752900334,"author_flair_text":null,"treatment_tags":[],"created_utc":1752900334,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n3styvg","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DerpageOnline","can_mod_post":false,"created_utc":1752839486,"send_replies":true,"parent_id":"t1_n3shurs","score":3,"author_fullname":"t2_jjq7n","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Since it's MoE, I don't think speed would be that bad in the end. Prompt Processing maybe, if it can't be thrown on the GPU entirely?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3styvg","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Since it&amp;#39;s MoE, I don&amp;#39;t think speed would be that bad in the end. Prompt Processing maybe, if it can&amp;#39;t be thrown on the GPU entirely?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2xh8s","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2xh8s/run_kimik2_without_quantization_locally_for_under/n3styvg/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752839486,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3ulh15","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"complains_constantly","can_mod_post":false,"created_utc":1752858761,"send_replies":true,"parent_id":"t1_n3shurs","score":1,"author_fullname":"t2_158u6z","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Maybe not. It has only 32B active params.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3ulh15","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Maybe not. It has only 32B active params.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2xh8s","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2xh8s/run_kimik2_without_quantization_locally_for_under/n3ulh15/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752858761,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3shurs","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Papabear3339","can_mod_post":false,"created_utc":1752833956,"send_replies":true,"parent_id":"t3_1m2xh8s","score":92,"author_fullname":"t2_7iw5w8ac","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Running a very large model on a server board has always been possible.\\n\\nYou will be VERY dissapointed with the speed though","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3shurs","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Running a very large model on a server board has always been possible.&lt;/p&gt;\\n\\n&lt;p&gt;You will be VERY dissapointed with the speed though&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2xh8s/run_kimik2_without_quantization_locally_for_under/n3shurs/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752833956,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m2xh8s","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":92}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3vlva5","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"eloquentemu","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3t2wi8","score":3,"author_fullname":"t2_lpdsy","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Yeah, it's actually super cool... Each CCD has its own L3 cache and GMI3 link to the IO die so it rips in doing ~16 single threaded workloads.  You can kind of think of it like having V-Cache, but without needing to share it with other cores.  Definitely a pretty specialized option but for some workloads, having a bunch of uncontested cache can be really valuable.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3vlva5","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yeah, it&amp;#39;s actually super cool... Each CCD has its own L3 cache and GMI3 link to the IO die so it rips in doing ~16 single threaded workloads.  You can kind of think of it like having V-Cache, but without needing to share it with other cores.  Definitely a pretty specialized option but for some workloads, having a bunch of uncontested cache can be really valuable.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2xh8s","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2xh8s/run_kimik2_without_quantization_locally_for_under/n3vlva5/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752869327,"author_flair_text":null,"treatment_tags":[],"created_utc":1752869327,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n3t2wi8","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"nail_nail","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3sudys","score":3,"author_fullname":"t2_quvm4","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"This 9175F is really weird. 16CCDs, really?","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3t2wi8","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;This 9175F is really weird. 16CCDs, really?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2xh8s","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2xh8s/run_kimik2_without_quantization_locally_for_under/n3t2wi8/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752842796,"author_flair_text":null,"treatment_tags":[],"created_utc":1752842796,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n3sudys","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"DepthHour1669","can_mod_post":false,"created_utc":1752839652,"send_replies":true,"parent_id":"t1_n3stjva","score":4,"author_fullname":"t2_t6glzswk","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Datasheet says the Epyc 9015 should work. \\n\\nBut worst case scenario, just buy a Epyc 9175F with 16CCDs costs about $2.5k. \\n\\nIf you're worried about warranty, put it on a corporate amex plat and use the amex warranty.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3sudys","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Datasheet says the Epyc 9015 should work. &lt;/p&gt;\\n\\n&lt;p&gt;But worst case scenario, just buy a Epyc 9175F with 16CCDs costs about $2.5k. &lt;/p&gt;\\n\\n&lt;p&gt;If you&amp;#39;re worried about warranty, put it on a corporate amex plat and use the amex warranty.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2xh8s","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2xh8s/run_kimik2_without_quantization_locally_for_under/n3sudys/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752839652,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}}],"before":null}},"user_reports":[],"saved":false,"id":"n3stjva","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"dodo13333","can_mod_post":false,"created_utc":1752839319,"send_replies":true,"parent_id":"t3_1m2xh8s","score":11,"author_fullname":"t2_fxyo2uvcw","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"It won't be under 10k. 9005 CPU with 12 CCDs will hit over 4k$ IMO. Low CCD CPU won't have enough power to feed 12 memory channels.  And you need high rank RAM to reach advertised memory bandwidth. And with that much money in question, I would not buy RAM that is not on MB manufacturer QVL...","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3stjva","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It won&amp;#39;t be under 10k. 9005 CPU with 12 CCDs will hit over 4k$ IMO. Low CCD CPU won&amp;#39;t have enough power to feed 12 memory channels.  And you need high rank RAM to reach advertised memory bandwidth. And with that much money in question, I would not buy RAM that is not on MB manufacturer QVL...&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2xh8s/run_kimik2_without_quantization_locally_for_under/n3stjva/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752839319,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m2xh8s","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":11}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3wr4bq","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"101m4n","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3tw6o7","score":1,"author_fullname":"t2_p7nc2","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"That makes sense. It's pretty common for thread over-subscription or contention to cause performance degradation past a certain point. In this case I'd bet on contention for memory bandwidth.\\n\\nI'd be interested to see what happens at lower quantisations. If the above hypothesis is correct, a lower quant would use less bandwidth and allow you to leverage more of those 96 cores.","edited":false,"author_flair_css_class":null,"name":"t1_n3wr4bq","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;That makes sense. It&amp;#39;s pretty common for thread over-subscription or contention to cause performance degradation past a certain point. In this case I&amp;#39;d bet on contention for memory bandwidth.&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;d be interested to see what happens at lower quantisations. If the above hypothesis is correct, a lower quant would use less bandwidth and allow you to leverage more of those 96 cores.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1m2xh8s","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2xh8s/run_kimik2_without_quantization_locally_for_under/n3wr4bq/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752882611,"author_flair_text":null,"collapsed":false,"created_utc":1752882611,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3tw6o7","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"tomz17","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3tglif","score":2,"author_fullname":"t2_1mhx5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Performance stops improving and/or gets worse.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3tw6o7","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Performance stops improving and/or gets worse.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2xh8s","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2xh8s/run_kimik2_without_quantization_locally_for_under/n3tw6o7/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752851637,"author_flair_text":null,"treatment_tags":[],"created_utc":1752851637,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n3tglif","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"101m4n","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3t8snz","score":2,"author_fullname":"t2_p7nc2","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Good information to have! \\n\\nJust to be clear, when you say \\"tops out\\" do you mean that it only uses that many, or that it \\"uses\\" them but performance stops improving?","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3tglif","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Good information to have! &lt;/p&gt;\\n\\n&lt;p&gt;Just to be clear, when you say &amp;quot;tops out&amp;quot; do you mean that it only uses that many, or that it &amp;quot;uses&amp;quot; them but performance stops improving?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2xh8s","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2xh8s/run_kimik2_without_quantization_locally_for_under/n3tglif/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752847167,"author_flair_text":null,"treatment_tags":[],"created_utc":1752847167,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3uoddg","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MidnightProgrammer","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3t8snz","score":1,"author_fullname":"t2_ijzb7","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Any idea what the ideal 9005 cpu would be?","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3uoddg","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Any idea what the ideal 9005 cpu would be?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2xh8s","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2xh8s/run_kimik2_without_quantization_locally_for_under/n3uoddg/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752859561,"author_flair_text":null,"treatment_tags":[],"created_utc":1752859561,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3t8snz","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"tomz17","can_mod_post":false,"created_utc":1752844739,"send_replies":true,"parent_id":"t1_n3sw9h5","score":10,"author_fullname":"t2_1mhx5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"For comparison, my 9684x (96 cores) tops out at somewhere between 32-48 threads for most models....   So I would place that as the sweet spot for purchasing a CPU for this task.  Somewhere beyond that you are just power limited for large matrix operations and simply throwing more (idle) cores at the problem doesn't help.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3t8snz","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;For comparison, my 9684x (96 cores) tops out at somewhere between 32-48 threads for most models....   So I would place that as the sweet spot for purchasing a CPU for this task.  Somewhere beyond that you are just power limited for large matrix operations and simply throwing more (idle) cores at the problem doesn&amp;#39;t help.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2xh8s","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2xh8s/run_kimik2_without_quantization_locally_for_under/n3t8snz/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752844739,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":10}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3swvan","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DepthHour1669","can_mod_post":false,"created_utc":1752840616,"send_replies":true,"parent_id":"t1_n3sw9h5","score":3,"author_fullname":"t2_t6glzswk","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"From the datasheets, it seems like the Epyc 9015 should be fine, but worst case scenario just get an Epyc 9175F for $2.5k (which will definitely work) and the build will cost $11k instead.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3swvan","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;From the datasheets, it seems like the Epyc 9015 should be fine, but worst case scenario just get an Epyc 9175F for $2.5k (which will definitely work) and the build will cost $11k instead.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2xh8s","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2xh8s/run_kimik2_without_quantization_locally_for_under/n3swvan/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752840616,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n3sw9h5","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"101m4n","can_mod_post":false,"created_utc":1752840384,"send_replies":true,"parent_id":"t3_1m2xh8s","score":20,"author_fullname":"t2_p7nc2","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Just realised that's only an 8 core CPU!\\n\\nYou aren't going to get 20T/s out of that. No chance in hell.\\n\\nEven if each core manages 8 fmas per clock cycle at 3.6GHz that's still only 230Gflops. Bear in mind that you need _more_ flops than you have parameters, and you won't manage to hit peak arithmetic throughput. Also, this is a workload with very sequential memory access patterns so SMT won't help you here either. To top it all off, it probably only has 1 or 2 ccds, so the internal busses between the iod and the CPUs probably physically aren't fast enough to carry the full 600GB/s.\\n\\nI get that these workloads are dominated by memory bandwidth, but compute still matters. You're gonna need more cores.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3sw9h5","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Just realised that&amp;#39;s only an 8 core CPU!&lt;/p&gt;\\n\\n&lt;p&gt;You aren&amp;#39;t going to get 20T/s out of that. No chance in hell.&lt;/p&gt;\\n\\n&lt;p&gt;Even if each core manages 8 fmas per clock cycle at 3.6GHz that&amp;#39;s still only 230Gflops. Bear in mind that you need &lt;em&gt;more&lt;/em&gt; flops than you have parameters, and you won&amp;#39;t manage to hit peak arithmetic throughput. Also, this is a workload with very sequential memory access patterns so SMT won&amp;#39;t help you here either. To top it all off, it probably only has 1 or 2 ccds, so the internal busses between the iod and the CPUs probably physically aren&amp;#39;t fast enough to carry the full 600GB/s.&lt;/p&gt;\\n\\n&lt;p&gt;I get that these workloads are dominated by memory bandwidth, but compute still matters. You&amp;#39;re gonna need more cores.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2xh8s/run_kimik2_without_quantization_locally_for_under/n3sw9h5/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752840384,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m2xh8s","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":20}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3wp2mc","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"eleqtriq","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3t6ilv","score":1,"author_fullname":"t2_66vte","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I mean it’s not using CPU to do the compute.  Not talking about the memory.  I conflated two different points I was trying to make.","edited":1752882366,"gildings":{},"author_flair_css_class":null,"name":"t1_n3wp2mc","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I mean it’s not using CPU to do the compute.  Not talking about the memory.  I conflated two different points I was trying to make.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1m2xh8s","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2xh8s/run_kimik2_without_quantization_locally_for_under/n3wp2mc/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752881902,"author_flair_text":null,"treatment_tags":[],"created_utc":1752881902,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":6,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3t6ilv","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"ShinyAnkleBalls","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3t2xvc","score":8,"author_fullname":"t2_2m3au2xb","approved_by":null,"mod_note":null,"all_awardings":[],"body":"Shared memory. Not pure GPU.","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n3t6ilv","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Shared memory. Not pure GPU.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1m2xh8s","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2xh8s/run_kimik2_without_quantization_locally_for_under/n3t6ilv/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752844006,"author_flair_text":null,"treatment_tags":[],"created_utc":1752844006,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":8}}],"before":null}},"user_reports":[],"saved":false,"id":"n3t2xvc","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"eleqtriq","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3sg8sv","score":-1,"author_fullname":"t2_66vte","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Except those Macs are using pure GPU.   I’m guessing the hand off is between two sequential layers so the network latency isn’t as big a factor.","edited":false,"author_flair_css_class":null,"name":"t1_n3t2xvc","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Except those Macs are using pure GPU.   I’m guessing the hand off is between two sequential layers so the network latency isn’t as big a factor.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1m2xh8s","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2xh8s/run_kimik2_without_quantization_locally_for_under/n3t2xvc/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752842809,"author_flair_text":null,"collapsed":false,"created_utc":1752842809,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":1,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3sg8sv","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"DepthHour1669","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3sg1nq","score":6,"author_fullname":"t2_t6glzswk","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"... that's running on 2 macs. Not 1 machine. \\n\\nDid you factor in network latency?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3sg8sv","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;... that&amp;#39;s running on 2 macs. Not 1 machine. &lt;/p&gt;\\n\\n&lt;p&gt;Did you factor in network latency?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2xh8s","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2xh8s/run_kimik2_without_quantization_locally_for_under/n3sg8sv/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752833103,"author_flair_text":null,"treatment_tags":[],"created_utc":1752833103,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3srqkn","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"JasperQuandary","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3sg1nq","score":1,"author_fullname":"t2_afave","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Looks fast enough to me if it’s streaming.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3srqkn","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Looks fast enough to me if it’s streaming.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2xh8s","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2xh8s/run_kimik2_without_quantization_locally_for_under/n3srqkn/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752838572,"author_flair_text":null,"treatment_tags":[],"created_utc":1752838572,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3sg1nq","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"FootballRemote4595","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3sev67","score":3,"author_fullname":"t2_1tmf4i8qmj","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I'd expect slower. 4 bit means 2x as fast and 614gb is already less than the 800 GB of M3 ultra. So it would be less than half the speed using the 8 bit ","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3sg1nq","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;d expect slower. 4 bit means 2x as fast and 614gb is already less than the 800 GB of M3 ultra. So it would be less than half the speed using the 8 bit &lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2xh8s","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2xh8s/run_kimik2_without_quantization_locally_for_under/n3sg1nq/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752832998,"author_flair_text":null,"treatment_tags":[],"created_utc":1752832998,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n3sev67","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DepthHour1669","can_mod_post":false,"created_utc":1752832366,"send_replies":true,"parent_id":"t1_n3scg4p","score":2,"author_fullname":"t2_t6glzswk","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Probably a bit faster than this:\\n\\nhttps://x.com/awnihannun/status/1943723599971443134","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3sev67","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Probably a bit faster than this:&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://x.com/awnihannun/status/1943723599971443134\\"&gt;https://x.com/awnihannun/status/1943723599971443134&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2xh8s","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2xh8s/run_kimik2_without_quantization_locally_for_under/n3sev67/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752832366,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n3scg4p","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"PreciselyWrong","can_mod_post":false,"created_utc":1752831022,"send_replies":true,"parent_id":"t3_1m2xh8s","score":6,"author_fullname":"t2_hn2ed","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"What inference speed do you expect with that?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3scg4p","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;What inference speed do you expect with that?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2xh8s/run_kimik2_without_quantization_locally_for_under/n3scg4p/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752831022,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m2xh8s","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3sata8","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Glittering-Call8746","can_mod_post":false,"created_utc":1752830081,"send_replies":true,"parent_id":"t3_1m2xh8s","score":14,"author_fullname":"t2_tqwl6sawb","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Do it .. I don't have enough money to throw around..","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3sata8","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Do it .. I don&amp;#39;t have enough money to throw around..&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2xh8s/run_kimik2_without_quantization_locally_for_under/n3sata8/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752830081,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m2xh8s","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":14}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3shq8z","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Baldur-Norddahl","can_mod_post":false,"created_utc":1752833890,"send_replies":true,"parent_id":"t3_1m2xh8s","score":11,"author_fullname":"t2_bvqb8ng0","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I would say running it at 8 bit is just stupid for the home gamer. The very large models compress well. Run it at 4 bit and get twice the TPS. Get the RAM anyway so you can have both K2 and R1 loaded at the same time.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3shq8z","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I would say running it at 8 bit is just stupid for the home gamer. The very large models compress well. Run it at 4 bit and get twice the TPS. Get the RAM anyway so you can have both K2 and R1 loaded at the same time.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2xh8s/run_kimik2_without_quantization_locally_for_under/n3shq8z/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752833890,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m2xh8s","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":11}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3sre3w","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"bullerwins","can_mod_post":false,"created_utc":1752838424,"send_replies":true,"parent_id":"t3_1m2xh8s","score":3,"author_fullname":"t2_d3wk5","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"That motherboard is interesting. I’ve been looking for a ddr5 motherboard with enough pcie slots but the MCIO2 slots should work. But I don’t have experience with those.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3sre3w","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;That motherboard is interesting. I’ve been looking for a ddr5 motherboard with enough pcie slots but the MCIO2 slots should work. But I don’t have experience with those.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2xh8s/run_kimik2_without_quantization_locally_for_under/n3sre3w/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752838424,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m2xh8s","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3sjytl","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"jfp999","can_mod_post":false,"created_utc":1752835046,"send_replies":true,"parent_id":"t3_1m2xh8s","score":4,"author_fullname":"t2_elxdc","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"You'll need a single CPU with 8 CCDs per the prior documented attempts with Deepseek R1.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3sjytl","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You&amp;#39;ll need a single CPU with 8 CCDs per the prior documented attempts with Deepseek R1.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2xh8s/run_kimik2_without_quantization_locally_for_under/n3sjytl/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752835046,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m2xh8s","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3sfne6","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"jbutlerdev","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3sfibg","score":1,"author_fullname":"t2_azse6ibv","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"oh I see now. Yeah good luck with that. Let us know how it works out","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3sfne6","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;oh I see now. Yeah good luck with that. Let us know how it works out&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2xh8s","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2xh8s/run_kimik2_without_quantization_locally_for_under/n3sfne6/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752832786,"author_flair_text":null,"treatment_tags":[],"created_utc":1752832786,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3sfibg","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DepthHour1669","can_mod_post":false,"created_utc":1752832710,"send_replies":true,"parent_id":"t1_n3sf7ow","score":2,"author_fullname":"t2_t6glzswk","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Incorrect. The motherboard is included in the $1400 price mentioned above. \\n\\nThe rest of the stuff can be easily pulled from a cheap used server.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3sfibg","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Incorrect. The motherboard is included in the $1400 price mentioned above. &lt;/p&gt;\\n\\n&lt;p&gt;The rest of the stuff can be easily pulled from a cheap used server.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2xh8s","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2xh8s/run_kimik2_without_quantization_locally_for_under/n3sfibg/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752832710,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":1,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n3sf7ow","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"jbutlerdev","can_mod_post":false,"created_utc":1752832553,"send_replies":true,"parent_id":"t3_1m2xh8s","score":3,"author_fullname":"t2_azse6ibv","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"You forgot\\n\\n* Motherboard \\n* Coolers\\n* Case\\n* PSU\\n* Case fans\\n\\nThe motherboard for these processors is not cheap","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3sf7ow","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You forgot&lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;Motherboard &lt;/li&gt;\\n&lt;li&gt;Coolers&lt;/li&gt;\\n&lt;li&gt;Case&lt;/li&gt;\\n&lt;li&gt;PSU&lt;/li&gt;\\n&lt;li&gt;Case fans&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;p&gt;The motherboard for these processors is not cheap&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2xh8s/run_kimik2_without_quantization_locally_for_under/n3sf7ow/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752832553,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m2xh8s","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3x5wbu","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"usrlocalben","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3tlbw3","score":1,"author_fullname":"t2_mzj17gy3","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"2S is better than 1S by only a small margin relative to the great additional cost. Concurrency is needed to get 2S/24x/NUMA benefits and AFAIK there's still no design (code) for this that is more effective than e.g. NPS0+ik\\\\_llama. 2S 9115 + RTX8000. [K2 IQ2\\\\_KS](https://huggingface.co/ubergarm/Kimi-K2-Instruct-GGUF) gives 90/s PP and 14/s TG. 10000 ctx.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3x5wbu","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;2S is better than 1S by only a small margin relative to the great additional cost. Concurrency is needed to get 2S/24x/NUMA benefits and AFAIK there&amp;#39;s still no design (code) for this that is more effective than e.g. NPS0+ik_llama. 2S 9115 + RTX8000. &lt;a href=\\"https://huggingface.co/ubergarm/Kimi-K2-Instruct-GGUF\\"&gt;K2 IQ2_KS&lt;/a&gt; gives 90/s PP and 14/s TG. 10000 ctx.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2xh8s","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2xh8s/run_kimik2_without_quantization_locally_for_under/n3x5wbu/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752888042,"author_flair_text":null,"treatment_tags":[],"created_utc":1752888042,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3tlbw3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Ok_Appeal8653","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3ssjv5","score":2,"author_fullname":"t2_10eo07xw69","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Dual AMD EPYC 9124 which are cheap af (a couple fo them &lt; 1000€) with a much more expensive board (some asrock for 1800€), so 24 channels of memory. Naturally a dual channel doesn't scale perfectly, so you won't get double of the performance compared to using single socket when doing inference (and not all inference engines take advantage of it), but you still enjoy 921 GBps with 4800 MHz per second (and 1075GBps with more expensive but still reasonable 5600 MHz RAM). And you can get 24 32GB ram sticks for 768BG of total system ram.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3tlbw3","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Dual AMD EPYC 9124 which are cheap af (a couple fo them &amp;lt; 1000€) with a much more expensive board (some asrock for 1800€), so 24 channels of memory. Naturally a dual channel doesn&amp;#39;t scale perfectly, so you won&amp;#39;t get double of the performance compared to using single socket when doing inference (and not all inference engines take advantage of it), but you still enjoy 921 GBps with 4800 MHz per second (and 1075GBps with more expensive but still reasonable 5600 MHz RAM). And you can get 24 32GB ram sticks for 768BG of total system ram.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2xh8s","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2xh8s/run_kimik2_without_quantization_locally_for_under/n3tlbw3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752848572,"author_flair_text":null,"treatment_tags":[],"created_utc":1752848572,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n3ssjv5","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DepthHour1669","can_mod_post":false,"created_utc":1752838911,"send_replies":true,"parent_id":"t1_n3sse3t","score":2,"author_fullname":"t2_t6glzswk","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I haven't looked into dual cpu systems. What's an example build for that? What's the memory bandwidth?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3ssjv5","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I haven&amp;#39;t looked into dual cpu systems. What&amp;#39;s an example build for that? What&amp;#39;s the memory bandwidth?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2xh8s","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2xh8s/run_kimik2_without_quantization_locally_for_under/n3ssjv5/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752838911,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3stfou","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Glittering-Call8746","can_mod_post":false,"created_utc":1752839272,"send_replies":true,"parent_id":"t1_n3sse3t","score":1,"author_fullname":"t2_tqwl6sawb","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Which memory is this?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3stfou","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Which memory is this?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2xh8s","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2xh8s/run_kimik2_without_quantization_locally_for_under/n3stfou/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752839272,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3sse3t","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Ok_Appeal8653","can_mod_post":false,"created_utc":1752838844,"send_replies":true,"parent_id":"t3_1m2xh8s","score":4,"author_fullname":"t2_10eo07xw69","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Dual CPU better. If you buy it yourself, you can slash the price and buy a complete 24 channel system (wih 4800 MHz memory) for around 8500-9000 euros. 7500€ If you buy memory in Aliexpress. And that includes 21% VAT tax. Or buy a premade server for double that. All in all, the mac studio never has made much sense for AI workloads.","edited":1752849514,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3sse3t","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Dual CPU better. If you buy it yourself, you can slash the price and buy a complete 24 channel system (wih 4800 MHz memory) for around 8500-9000 euros. 7500€ If you buy memory in Aliexpress. And that includes 21% VAT tax. Or buy a premade server for double that. All in all, the mac studio never has made much sense for AI workloads.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2xh8s/run_kimik2_without_quantization_locally_for_under/n3sse3t/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752838844,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m2xh8s","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3ss5bs","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Such_Advantage_6949","can_mod_post":false,"created_utc":1752838744,"send_replies":true,"parent_id":"t3_1m2xh8s","score":1,"author_fullname":"t2_a548b491","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Buy it and show us how well it runs. I am curious too","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3ss5bs","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Buy it and show us how well it runs. I am curious too&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2xh8s/run_kimik2_without_quantization_locally_for_under/n3ss5bs/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752838744,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m2xh8s","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3v4v42","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DepthHour1669","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3u7u21","score":2,"author_fullname":"t2_t6glzswk","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Yes, because the 3090 does like 285 TFLOPs and the CPU only does like 10 TFLOPs.\\n\\nYou’re actually able to do the compute for processing in 28 seconds. But loading the model weights from ram will take 32 seconds.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3v4v42","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yes, because the 3090 does like 285 TFLOPs and the CPU only does like 10 TFLOPs.&lt;/p&gt;\\n\\n&lt;p&gt;You’re actually able to do the compute for processing in 28 seconds. But loading the model weights from ram will take 32 seconds.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2xh8s","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2xh8s/run_kimik2_without_quantization_locally_for_under/n3v4v42/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752864289,"author_flair_text":null,"treatment_tags":[],"created_utc":1752864289,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n3u7u21","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"raysar","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3tpjn7","score":1,"author_fullname":"t2_91kj8","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Interesting, even with very low layer on 3090 speed is increase? I don't understand how to calculate it on moe 😊","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3u7u21","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Interesting, even with very low layer on 3090 speed is increase? I don&amp;#39;t understand how to calculate it on moe 😊&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2xh8s","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2xh8s/run_kimik2_without_quantization_locally_for_under/n3u7u21/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752854907,"author_flair_text":null,"treatment_tags":[],"created_utc":1752854907,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3tpjn7","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DepthHour1669","can_mod_post":false,"created_utc":1752849771,"send_replies":true,"parent_id":"t1_n3tf9xx","score":1,"author_fullname":"t2_t6glzswk","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I did the math, with a 3090 GPU added it’d be 32seconds at a context of 128k.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3tpjn7","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I did the math, with a 3090 GPU added it’d be 32seconds at a context of 128k.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2xh8s","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2xh8s/run_kimik2_without_quantization_locally_for_under/n3tpjn7/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752849771,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3tf9xx","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"raysar","can_mod_post":false,"created_utc":1752846769,"send_replies":true,"parent_id":"t3_1m2xh8s","score":1,"author_fullname":"t2_91kj8","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"What is about the prefill speed with ou without quantisation? for coding we need many input token.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3tf9xx","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;What is about the prefill speed with ou without quantisation? for coding we need many input token.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2xh8s/run_kimik2_without_quantization_locally_for_under/n3tf9xx/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752846769,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m2xh8s","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3tpx2r","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DepthHour1669","can_mod_post":false,"created_utc":1752849876,"send_replies":true,"parent_id":"t1_n3tlgx8","score":3,"author_fullname":"t2_t6glzswk","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Q4 is like 560GB, so you’ll still need 512gb","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3tpx2r","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Q4 is like 560GB, so you’ll still need 512gb&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2xh8s","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2xh8s/run_kimik2_without_quantization_locally_for_under/n3tpx2r/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752849876,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ef488598-491f-11ef-a847-9a3dd315819c","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3tzoen","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"panchovix","can_mod_post":false,"created_utc":1752852615,"send_replies":true,"parent_id":"t1_n3tlgx8","score":1,"author_fullname":"t2_j1kqr","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"For Q2 about 384GB RAM.\\n\\nFor Q4 512GB RAM.\\n\\nBoth alongside 96GB VRAM.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3tzoen","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"Llama 405B"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;For Q2 about 384GB RAM.&lt;/p&gt;\\n\\n&lt;p&gt;For Q4 512GB RAM.&lt;/p&gt;\\n\\n&lt;p&gt;Both alongside 96GB VRAM.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2xh8s","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2xh8s/run_kimik2_without_quantization_locally_for_under/n3tzoen/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752852615,"author_flair_text":"Llama 405B","treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3tlgx8","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Vusiwe","can_mod_post":false,"created_utc":1752848612,"send_replies":true,"parent_id":"t3_1m2xh8s","score":1,"author_fullname":"t2_srr01le8","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"If there was a Q2 or Q4 of Kimi and you already had 96GB VRAM, how much RAM would you need to run?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3tlgx8","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;If there was a Q2 or Q4 of Kimi and you already had 96GB VRAM, how much RAM would you need to run?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2xh8s/run_kimik2_without_quantization_locally_for_under/n3tlgx8/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752848612,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m2xh8s","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"more","data":{"count":0,"name":"t1__","id":"_","parent_id":"t1_n3webqx","depth":10,"children":[]}}],"before":null}},"user_reports":[],"saved":false,"id":"n3webqx","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"segmond","can_mod_post":false,"created_utc":1752878224,"send_replies":true,"parent_id":"t1_n3vyu7g","score":1,"author_fullname":"t2_ah13x","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"ddr4 is still expensive like crap, so doesn't mean ddr6 will drive prices down.  at this point, I don't know if it's a supply/demand thing, an exchange/interest rate thing, inflation, greed? all of the above?  i have been bidding my patience, still waiting for the payoff.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3webqx","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;ddr4 is still expensive like crap, so doesn&amp;#39;t mean ddr6 will drive prices down.  at this point, I don&amp;#39;t know if it&amp;#39;s a supply/demand thing, an exchange/interest rate thing, inflation, greed? all of the above?  i have been bidding my patience, still waiting for the payoff.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2xh8s/run_kimik2_without_quantization_locally_for_under/n3webqx/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752878224,"author_flair_text":"llama.cpp","treatment_tags":[],"link_id":"t3_1m2xh8s","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":9,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3vyu7g","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DepthHour1669","can_mod_post":false,"created_utc":1752873181,"send_replies":true,"parent_id":"t1_n3vvc6r","score":1,"author_fullname":"t2_t6glzswk","approved_by":null,"mod_note":null,"all_awardings":[],"body":"Honestly, I gave up on the idea already. \\n\\nDDR5 came out 5 years ago in July 2020. DDR5 cpus and memory came out in 2021. DDR5-4800 and 5200 came out for sale Dec 2021, DDR5-5600 came out March 2022, and DDR5-6000 came out April 2022. DDR5-6400 came out later, Feb 2023.\\n\\nIt makes more sense to wait for DDR6 at this time. DDR6 starts at DDR6-8800 and peaks at DDR6-17600. If DDR6 goes at the same pace, then if the current draft DDR6 spec is formally released this year as expected, then base tier DDR6 cpus and ram should be available late 2026. And something like DDR6-16800 should be available April 2027.\\n\\nBasically just wait 2 years for triple the memory bandwidth.","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n3vyu7g","is_submitter":true,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Honestly, I gave up on the idea already. &lt;/p&gt;\\n\\n&lt;p&gt;DDR5 came out 5 years ago in July 2020. DDR5 cpus and memory came out in 2021. DDR5-4800 and 5200 came out for sale Dec 2021, DDR5-5600 came out March 2022, and DDR5-6000 came out April 2022. DDR5-6400 came out later, Feb 2023.&lt;/p&gt;\\n\\n&lt;p&gt;It makes more sense to wait for DDR6 at this time. DDR6 starts at DDR6-8800 and peaks at DDR6-17600. If DDR6 goes at the same pace, then if the current draft DDR6 spec is formally released this year as expected, then base tier DDR6 cpus and ram should be available late 2026. And something like DDR6-16800 should be available April 2027.&lt;/p&gt;\\n\\n&lt;p&gt;Basically just wait 2 years for triple the memory bandwidth.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2xh8s/run_kimik2_without_quantization_locally_for_under/n3vyu7g/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752873181,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m2xh8s","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":8,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3vvc6r","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"segmond","can_mod_post":false,"created_utc":1752872109,"send_replies":true,"parent_id":"t1_n3vmzut","score":1,"author_fullname":"t2_ah13x","approved_by":null,"mod_note":null,"all_awardings":[],"body":"nah, they have been expensive since before, they went up about $100-$150 after the tarrif situation.   and maybe about $100 after deepseek. I have been waiting for price to come down.   i like what you posted better, granted ddr5 is not cheap, and I would need mcio/pcie cables to hookup cards.","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n3vvc6r","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;nah, they have been expensive since before, they went up about $100-$150 after the tarrif situation.   and maybe about $100 after deepseek. I have been waiting for price to come down.   i like what you posted better, granted ddr5 is not cheap, and I would need mcio/pcie cables to hookup cards.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1m2xh8s","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2xh8s/run_kimik2_without_quantization_locally_for_under/n3vvc6r/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752872109,"author_flair_text":"llama.cpp","treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":7,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3vmzut","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DepthHour1669","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3vkt61","score":1,"author_fullname":"t2_t6glzswk","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Oh yeah, wild how the chinese suppliers don’t update their pricing.\\n\\nAlthough that may be due to tariff concerns, now that i think about it.","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n3vmzut","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Oh yeah, wild how the chinese suppliers don’t update their pricing.&lt;/p&gt;\\n\\n&lt;p&gt;Although that may be due to tariff concerns, now that i think about it.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1m2xh8s","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2xh8s/run_kimik2_without_quantization_locally_for_under/n3vmzut/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752869656,"author_flair_text":null,"treatment_tags":[],"created_utc":1752869656,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":6,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3vkt61","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"segmond","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3vcn5m","score":1,"author_fullname":"t2_ah13x","approved_by":null,"mod_note":null,"all_awardings":[],"body":"I know the motherboard link you sent is new, I'm saying that used hardware from China on ebay is more expensive than this!","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n3vkt61","is_submitter":false,"collapsed":false,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I know the motherboard link you sent is new, I&amp;#39;m saying that used hardware from China on ebay is more expensive than this!&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1m2xh8s","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2xh8s/run_kimik2_without_quantization_locally_for_under/n3vkt61/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752869013,"author_flair_text":"llama.cpp","treatment_tags":[],"created_utc":1752869013,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3vcn5m","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DepthHour1669","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3v7thk","score":1,"author_fullname":"t2_t6glzswk","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Motherboard link is new! Also you can find new CPUs for about $100 more. This isn’t just used pricing.","edited":false,"author_flair_css_class":null,"name":"t1_n3vcn5m","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Motherboard link is new! Also you can find new CPUs for about $100 more. This isn’t just used pricing.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1m2xh8s","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2xh8s/run_kimik2_without_quantization_locally_for_under/n3vcn5m/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752866585,"author_flair_text":null,"collapsed":false,"created_utc":1752866585,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3v7thk","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"segmond","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3v16d1","score":1,"author_fullname":"t2_ah13x","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I feel a different kind of stupid, but I'm also very thankful and grateful that you shared.   Used combo mb/cpu on ebay from china is nuts, never realized that server board can be hard this cheap brand new too.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3v7thk","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I feel a different kind of stupid, but I&amp;#39;m also very thankful and grateful that you shared.   Used combo mb/cpu on ebay from china is nuts, never realized that server board can be hard this cheap brand new too.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2xh8s","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2xh8s/run_kimik2_without_quantization_locally_for_under/n3v7thk/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752865147,"author_flair_text":"llama.cpp","treatment_tags":[],"created_utc":1752865147,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3v16d1","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DepthHour1669","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3u272w","score":2,"author_fullname":"t2_t6glzswk","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"https://www.ebay.com/p/10074773274\\n\\nhttps://www.serverparts.pl/en/h13ssw-i9077","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3v16d1","is_submitter":true,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;&lt;a href=\\"https://www.ebay.com/p/10074773274\\"&gt;https://www.ebay.com/p/10074773274&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://www.serverparts.pl/en/h13ssw-i9077\\"&gt;https://www.serverparts.pl/en/h13ssw-i9077&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2xh8s","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2xh8s/run_kimik2_without_quantization_locally_for_under/n3v16d1/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752863206,"author_flair_text":null,"treatment_tags":[],"created_utc":1752863206,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3uikc6","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"waiting_for_zban","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3u272w","score":1,"author_fullname":"t2_13yxr6ze7l","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"&gt; rubbish, show me a link to an epyc 9005 series cpu and motherboard for $1400.\\n\\nBoth paragraphs were a quote.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3uikc6","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;rubbish, show me a link to an epyc 9005 series cpu and motherboard for $1400.&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;Both paragraphs were a quote.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2xh8s","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2xh8s/run_kimik2_without_quantization_locally_for_under/n3uikc6/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752857955,"author_flair_text":null,"treatment_tags":[],"created_utc":1752857955,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3u272w","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"segmond","can_mod_post":false,"created_utc":1752853314,"send_replies":true,"parent_id":"t1_n3tz5ss","score":1,"author_fullname":"t2_ah13x","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"show me a link to an epyc 9005 series cpu and motherboard for $1400.","edited":1752865165,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3u272w","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;show me a link to an epyc 9005 series cpu and motherboard for $1400.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2xh8s","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2xh8s/run_kimik2_without_quantization_locally_for_under/n3u272w/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752853314,"author_flair_text":"llama.cpp","treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3v0nk9","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DepthHour1669","can_mod_post":false,"created_utc":1752863052,"send_replies":true,"parent_id":"t1_n3tz5ss","score":1,"author_fullname":"t2_t6glzswk","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"False, Kimi K2 uses MLA, so you can fit 128k token context into &lt;10gb.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3v0nk9","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;False, Kimi K2 uses MLA, so you can fit 128k token context into &amp;lt;10gb.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2xh8s","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2xh8s/run_kimik2_without_quantization_locally_for_under/n3v0nk9/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752863052,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3tz5ss","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"waiting_for_zban","can_mod_post":false,"created_utc":1752852472,"send_replies":true,"parent_id":"t3_1m2xh8s","score":1,"author_fullname":"t2_13yxr6ze7l","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"&gt; You can buy 12 sticks of 96gb DDR5-6400 RAM (total 1152GB) for about $7200. DDR5-6400 12 channel is 614GB/sec. That's pretty close (about 75%) of the 512GB Mac Studio which has 819GB/sec memory bandwidth.\\n\\n&gt; You just need an AMD EPYC 9005 series cpu and a compatible 12 channel RAM motherboard, which costs around $1400 total these days. Throw in a Nvidia RTX 3090 or two, or maybe a RTX5090 (to handle the non MoE layers) and it should run even faster. With the 1152GB of DDR5 RAM combined with the GPU, you can run Kimi-K2 at a very reasonable speed for below $10k.   \\n\\n\\nOne caveat, if you want reasonably good context, you need much more ram.","edited":1752857927,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3tz5ss","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;You can buy 12 sticks of 96gb DDR5-6400 RAM (total 1152GB) for about $7200. DDR5-6400 12 channel is 614GB/sec. That&amp;#39;s pretty close (about 75%) of the 512GB Mac Studio which has 819GB/sec memory bandwidth.&lt;/p&gt;\\n\\n&lt;p&gt;You just need an AMD EPYC 9005 series cpu and a compatible 12 channel RAM motherboard, which costs around $1400 total these days. Throw in a Nvidia RTX 3090 or two, or maybe a RTX5090 (to handle the non MoE layers) and it should run even faster. With the 1152GB of DDR5 RAM combined with the GPU, you can run Kimi-K2 at a very reasonable speed for below $10k.   &lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;One caveat, if you want reasonably good context, you need much more ram.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2xh8s/run_kimik2_without_quantization_locally_for_under/n3tz5ss/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752852472,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m2xh8s","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3u22nf","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"segmond","can_mod_post":false,"created_utc":1752853279,"send_replies":true,"parent_id":"t3_1m2xh8s","score":1,"author_fullname":"t2_ah13x","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"you can buy an epyc 7000 cpu/board combo for $1200, max it out with 1tb ram.  Add a 3090, for about $5000-$6000 you can run a Q8.   maybe 7tk/sec.   Very doable.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3u22nf","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;you can buy an epyc 7000 cpu/board combo for $1200, max it out with 1tb ram.  Add a 3090, for about $5000-$6000 you can run a Q8.   maybe 7tk/sec.   Very doable.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2xh8s/run_kimik2_without_quantization_locally_for_under/n3u22nf/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752853279,"author_flair_text":"llama.cpp","treatment_tags":[],"link_id":"t3_1m2xh8s","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3v3v0f","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DepthHour1669","can_mod_post":false,"created_utc":1752863995,"send_replies":true,"parent_id":"t1_n3u2uq1","score":2,"author_fullname":"t2_t6glzswk","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Kimi K2 uses MLA.\\n\\nTotal KV cache size in bytes = L × (dc + dRh) × layers × bytes_per_weight\\n\\nFor L = 128000 tokens  \\n\\nThen 128000\\\\*960.5\\\\*61\\\\*1 = 7.0GB. \\n\\nI think we can handle 7gb of context size.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3v3v0f","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Kimi K2 uses MLA.&lt;/p&gt;\\n\\n&lt;p&gt;Total KV cache size in bytes = L × (dc + dRh) × layers × bytes_per_weight&lt;/p&gt;\\n\\n&lt;p&gt;For L = 128000 tokens  &lt;/p&gt;\\n\\n&lt;p&gt;Then 128000*960.5*61*1 = 7.0GB. &lt;/p&gt;\\n\\n&lt;p&gt;I think we can handle 7gb of context size.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2xh8s","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2xh8s/run_kimik2_without_quantization_locally_for_under/n3v3v0f/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752863995,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n3u2uq1","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"usernameplshere","can_mod_post":false,"created_utc":1752853497,"send_replies":true,"parent_id":"t3_1m2xh8s","score":1,"author_fullname":"t2_1zes6cdw","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Don't forget about the context. If you want to run 60k+ it will eat your RAM fast.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3u2uq1","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Don&amp;#39;t forget about the context. If you want to run 60k+ it will eat your RAM fast.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2xh8s/run_kimik2_without_quantization_locally_for_under/n3u2uq1/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752853497,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m2xh8s","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3v62ut","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DepthHour1669","can_mod_post":false,"created_utc":1752864641,"send_replies":true,"parent_id":"t1_n3udvcp","score":1,"author_fullname":"t2_t6glzswk","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"That’s assuming 1 user though. You can do batch size &gt; 1 and memory bandwidth requirements to load all the weights in ram to CPU stays the same. You just need a faster CPU.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3v62ut","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;That’s assuming 1 user though. You can do batch size &amp;gt; 1 and memory bandwidth requirements to load all the weights in ram to CPU stays the same. You just need a faster CPU.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2xh8s","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2xh8s/run_kimik2_without_quantization_locally_for_under/n3v62ut/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752864641,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3udvcp","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"this-just_in","can_mod_post":false,"created_utc":1752856645,"send_replies":true,"parent_id":"t3_1m2xh8s","score":1,"author_fullname":"t2_kdmu4","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Worth pointing out that even if you were generating tokens 100% of the time at 25 t/s it would only produce 2.16 million tokens in a day.  This would have cost less than $7 on Groq and taken less than 1/20th of the time (serially, much faster in parallel).\\n\\nUnless you are doing something very private or naughty the economics of this type of hardware spend make no sense for just inference.  The response generation rate negates a good bit of any value the model would otherwise provide.","edited":1752856944,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3udvcp","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Worth pointing out that even if you were generating tokens 100% of the time at 25 t/s it would only produce 2.16 million tokens in a day.  This would have cost less than $7 on Groq and taken less than 1/20th of the time (serially, much faster in parallel).&lt;/p&gt;\\n\\n&lt;p&gt;Unless you are doing something very private or naughty the economics of this type of hardware spend make no sense for just inference.  The response generation rate negates a good bit of any value the model would otherwise provide.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2xh8s/run_kimik2_without_quantization_locally_for_under/n3udvcp/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752856645,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m2xh8s","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3uuvj0","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Available_Brain6231","can_mod_post":false,"created_utc":1752861372,"send_replies":true,"parent_id":"t3_1m2xh8s","score":1,"author_fullname":"t2_wausq0yfe","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"can't you use EXO with a bunch of those mini pcs like the Minisforum MS-A2 Mini PC (assuming it can hold 128 gbs of RAM like some people said)  \\n  \\nyou can even connect even your current setup for some more ram and I also found on aliexpress a version without the default 32gb ram and no ssd for less than 800 usd, you would achieve the 1152GB of RAM with around 10k","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3uuvj0","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;can&amp;#39;t you use EXO with a bunch of those mini pcs like the Minisforum MS-A2 Mini PC (assuming it can hold 128 gbs of RAM like some people said)  &lt;/p&gt;\\n\\n&lt;p&gt;you can even connect even your current setup for some more ram and I also found on aliexpress a version without the default 32gb ram and no ssd for less than 800 usd, you would achieve the 1152GB of RAM with around 10k&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2xh8s/run_kimik2_without_quantization_locally_for_under/n3uuvj0/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752861372,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m2xh8s","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3xbl7u","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Agabeckov","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3wi0dy","score":1,"author_fullname":"t2_570qqvco","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Well, yeah, need 24 GPUs then. So it could be like 4 servers with 6 GPUs each like these: [https://www.asrockrack.com/general/productdetail.asp?Model=3U8G%2b#Specifications](https://www.asrockrack.com/general/productdetail.asp?Model=3U8G%2b#Specifications) (they are dirt cheap on eBay now) and 2 100GE/IB cards into each server for interconnect. Could be a cool project for basement homelab))","edited":1752891157,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3xbl7u","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Well, yeah, need 24 GPUs then. So it could be like 4 servers with 6 GPUs each like these: &lt;a href=\\"https://www.asrockrack.com/general/productdetail.asp?Model=3U8G%2b#Specifications\\"&gt;https://www.asrockrack.com/general/productdetail.asp?Model=3U8G%2b#Specifications&lt;/a&gt; (they are dirt cheap on eBay now) and 2 100GE/IB cards into each server for interconnect. Could be a cool project for basement homelab))&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2xh8s","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2xh8s/run_kimik2_without_quantization_locally_for_under/n3xbl7u/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752890198,"author_flair_text":null,"treatment_tags":[],"created_utc":1752890198,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3wi0dy","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DepthHour1669","can_mod_post":false,"created_utc":1752879498,"send_replies":true,"parent_id":"t1_n3w74lt","score":1,"author_fullname":"t2_t6glzswk","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"That’s only 512GB though. That won’t fit Kimi K2 Q4. \\n\\nAnd you still run into the “can’t fit all the GPUs on the motherboard” problem","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3wi0dy","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;That’s only 512GB though. That won’t fit Kimi K2 Q4. &lt;/p&gt;\\n\\n&lt;p&gt;And you still run into the “can’t fit all the GPUs on the motherboard” problem&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2xh8s","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2xh8s/run_kimik2_without_quantization_locally_for_under/n3wi0dy/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752879498,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3w74lt","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Agabeckov","can_mod_post":false,"created_utc":1752875808,"send_replies":true,"parent_id":"t3_1m2xh8s","score":1,"author_fullname":"t2_570qqvco","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Could use 16 AMD MI50s 32GB, for $250 each it's still affordable)) Also, vLLM supports distributed inference so no need to squeeze 16 GPUs into one server. Although some dude did it with 14: [https://x.com/TheAhmadOsman/status/1869841392924762168](https://x.com/TheAhmadOsman/status/1869841392924762168)","edited":1752876346,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3w74lt","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Could use 16 AMD MI50s 32GB, for $250 each it&amp;#39;s still affordable)) Also, vLLM supports distributed inference so no need to squeeze 16 GPUs into one server. Although some dude did it with 14: &lt;a href=\\"https://x.com/TheAhmadOsman/status/1869841392924762168\\"&gt;https://x.com/TheAhmadOsman/status/1869841392924762168&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2xh8s/run_kimik2_without_quantization_locally_for_under/n3w74lt/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752875808,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m2xh8s","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3xty6o","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Faintly_glowing_fish","can_mod_post":false,"created_utc":1752897717,"send_replies":true,"parent_id":"t3_1m2xh8s","score":1,"author_fullname":"t2_97avhniv","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Kimi is good, but it is way too large.  It’s not good enough to be worth it for a local deploy","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3xty6o","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Kimi is good, but it is way too large.  It’s not good enough to be worth it for a local deploy&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2xh8s/run_kimik2_without_quantization_locally_for_under/n3xty6o/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752897717,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m2xh8s","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ef488598-491f-11ef-a847-9a3dd315819c","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n42gwvx","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"numbers18","can_mod_post":false,"created_utc":1752963080,"send_replies":true,"parent_id":"t3_1m2xh8s","score":1,"author_fullname":"t2_cg6xsz9d","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Both Q\\\\_0 and Q4\\\\_K\\\\_M (Kimi K2 from Unsloth) seem to occupy the same 1TB RAM when running:\\n\\nQ8\\\\_0:  \\nMiB Mem : 2062853.+total, 408019.7 free,  17251.4 used, 1637582.+buff/cache  \\nMiB Swap:      0.0 total,      0.0 free,      0.0 used. 2036204.+avail Mem\\n\\nPID USER      PR  NI    VIRT    RES    SHR S  %CPU  %MEM     TIME+ COMMAND  \\n6533 dd        20   0 1029.5g   1.0t   1.0t R  5611  51.0  18:15.67 llama-c+\\n\\nQ4\\\\_K\\\\_M:  \\nMiB Mem : 2062853.+total,   5532.1 free, 470648.4 used, 1586672.+buff/cache  \\nMiB Swap:      0.0 total,      0.0 free,      0.0 used. 1582482.+avail Mem\\n\\nPID USER      PR  NI    VIRT    RES    SHR S  %CPU  %MEM     TIME+ COMMAND  \\n8986 dd        20   0  995.9g 994.1g 541.7g R  5480  49.3   9:47.20 llama-c+\\n\\n====\\n\\nQ4 for me runs slower, as on startup it repacks weights from Q4 to Q8 for 3 minutes, printing dots while doing it.\\n\\nload\\\\_tensors:   CPU\\\\_REPACK model buffer size = 456624.00 MiB  \\nload\\\\_tensors:          AMX model buffer size =  5841.80 MiB","edited":1752965986,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n42gwvx","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"Llama 405B"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Both Q_0 and Q4_K_M (Kimi K2 from Unsloth) seem to occupy the same 1TB RAM when running:&lt;/p&gt;\\n\\n&lt;p&gt;Q8_0:&lt;br/&gt;\\nMiB Mem : 2062853.+total, 408019.7 free,  17251.4 used, 1637582.+buff/cache&lt;br/&gt;\\nMiB Swap:      0.0 total,      0.0 free,      0.0 used. 2036204.+avail Mem&lt;/p&gt;\\n\\n&lt;p&gt;PID USER      PR  NI    VIRT    RES    SHR S  %CPU  %MEM     TIME+ COMMAND&lt;br/&gt;\\n6533 dd        20   0 1029.5g   1.0t   1.0t R  5611  51.0  18:15.67 llama-c+&lt;/p&gt;\\n\\n&lt;p&gt;Q4_K_M:&lt;br/&gt;\\nMiB Mem : 2062853.+total,   5532.1 free, 470648.4 used, 1586672.+buff/cache&lt;br/&gt;\\nMiB Swap:      0.0 total,      0.0 free,      0.0 used. 1582482.+avail Mem&lt;/p&gt;\\n\\n&lt;p&gt;PID USER      PR  NI    VIRT    RES    SHR S  %CPU  %MEM     TIME+ COMMAND&lt;br/&gt;\\n8986 dd        20   0  995.9g 994.1g 541.7g R  5480  49.3   9:47.20 llama-c+&lt;/p&gt;\\n\\n&lt;h1&gt;&lt;/h1&gt;\\n\\n&lt;p&gt;Q4 for me runs slower, as on startup it repacks weights from Q4 to Q8 for 3 minutes, printing dots while doing it.&lt;/p&gt;\\n\\n&lt;p&gt;load_tensors:   CPU_REPACK model buffer size = 456624.00 MiB&lt;br/&gt;\\nload_tensors:          AMX model buffer size =  5841.80 MiB&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2xh8s/run_kimik2_without_quantization_locally_for_under/n42gwvx/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752963080,"author_flair_text":"Llama 405B","treatment_tags":[],"link_id":"t3_1m2xh8s","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n43b62c","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Every_Bathroom_119","can_mod_post":false,"created_utc":1752974046,"send_replies":true,"parent_id":"t3_1m2xh8s","score":1,"author_fullname":"t2_t39ypn0v","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Hi guys,\\nAnyone really tried this? I had a 8×H20 server, I want to run Kimi-K2 without quantization locally, what is solution for this? Is it possible to run with ktransformers?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n43b62c","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Hi guys,\\nAnyone really tried this? I had a 8×H20 server, I want to run Kimi-K2 without quantization locally, what is solution for this? Is it possible to run with ktransformers?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2xh8s/run_kimik2_without_quantization_locally_for_under/n43b62c/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752974046,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m2xh8s","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"total_awards_received":0,"approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"ups":1,"removal_reason":null,"link_id":"t3_1m2xh8s","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3synuj","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DepthHour1669","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3ssxfg","score":1,"author_fullname":"t2_t6glzswk","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"If you assume the CPU is GMI limited, then you'll need more CCDs, but in that case an Epyc 9175F would work. It'll cost you $2k more though. But that's still reasonable for a build with 1152GB of ram.","edited":false,"author_flair_css_class":null,"name":"t1_n3synuj","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;If you assume the CPU is GMI limited, then you&amp;#39;ll need more CCDs, but in that case an Epyc 9175F would work. It&amp;#39;ll cost you $2k more though. But that&amp;#39;s still reasonable for a build with 1152GB of ram.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1m2xh8s","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2xh8s/run_kimik2_without_quantization_locally_for_under/n3synuj/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752841284,"author_flair_text":null,"collapsed":false,"created_utc":1752841284,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3ssxfg","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Glittering-Call8746","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3sjnmq","score":1,"author_fullname":"t2_tqwl6sawb","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"So how much does 32 core 12 channel cpu costs..","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3ssxfg","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;So how much does 32 core 12 channel cpu costs..&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2xh8s","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2xh8s/run_kimik2_without_quantization_locally_for_under/n3ssxfg/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752839067,"author_flair_text":null,"treatment_tags":[],"created_utc":1752839067,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3sjnmq","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"timmytimmy01","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3sfxay","score":4,"author_fullname":"t2_vi73k","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"9015 is not enough. To fully use 12 channel ddr5 6400 bandwidth, you need at least 32 or 48 core 9005 cpu per socket","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3sjnmq","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;9015 is not enough. To fully use 12 channel ddr5 6400 bandwidth, you need at least 32 or 48 core 9005 cpu per socket&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2xh8s","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2xh8s/run_kimik2_without_quantization_locally_for_under/n3sjnmq/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752834886,"author_flair_text":null,"treatment_tags":[],"created_utc":1752834886,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3sz2cq","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"No_Afternoon_4260","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3sxwhs","score":1,"author_fullname":"t2_cj9kap4bx","approved_by":null,"mod_note":null,"all_awardings":[],"body":"Idk for the speed really, I feel 20tk/s for kimi is really ambitious, it all depends of your quant of course, may be a Q1 or Q2 🤷\\nFor what I've gathered here and there there seems to be a sweet spot price/performance around 32 cores.\\nThe genoa 9004 needed at least 8CCDs to hope for around 80% of the theoretical ram bandwidth where the change in architecture on turin 9005 brought you around 90%.\\nSo yeah may be a 9375F because the F has faster clock, with more money you can probably buy a 9475F. Idk really I gave you some key concepts to deepen your research","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n3sz2cq","is_submitter":false,"collapsed":false,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Idk for the speed really, I feel 20tk/s for kimi is really ambitious, it all depends of your quant of course, may be a Q1 or Q2 🤷\\nFor what I&amp;#39;ve gathered here and there there seems to be a sweet spot price/performance around 32 cores.\\nThe genoa 9004 needed at least 8CCDs to hope for around 80% of the theoretical ram bandwidth where the change in architecture on turin 9005 brought you around 90%.\\nSo yeah may be a 9375F because the F has faster clock, with more money you can probably buy a 9475F. Idk really I gave you some key concepts to deepen your research&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1m2xh8s","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2xh8s/run_kimik2_without_quantization_locally_for_under/n3sz2cq/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752841430,"author_flair_text":"llama.cpp","treatment_tags":[],"created_utc":1752841430,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3sxwhs","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MidnightProgrammer","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3sx6q0","score":1,"author_fullname":"t2_ijzb7","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"What epyc would be the ideal for Kimi to try to get around 20 tokens/sec?  \\n\\nI was looking at the 9015, 9175F, 9355, 9655P, and 9375F as options.","edited":false,"author_flair_css_class":null,"name":"t1_n3sxwhs","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;What epyc would be the ideal for Kimi to try to get around 20 tokens/sec?  &lt;/p&gt;\\n\\n&lt;p&gt;I was looking at the 9015, 9175F, 9355, 9655P, and 9375F as options.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1m2xh8s","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2xh8s/run_kimik2_without_quantization_locally_for_under/n3sxwhs/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752841003,"author_flair_text":null,"collapsed":false,"created_utc":1752841003,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3sx6q0","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"No_Afternoon_4260","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3sjgc9","score":1,"author_fullname":"t2_cj9kap4bx","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Really bad it doesn't have the same number of CCDs so poor ram bandwidth, they platform have challenges with numa node and you lack compute power on these low end cpu","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3sx6q0","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Really bad it doesn&amp;#39;t have the same number of CCDs so poor ram bandwidth, they platform have challenges with numa node and you lack compute power on these low end cpu&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2xh8s","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2xh8s/run_kimik2_without_quantization_locally_for_under/n3sx6q0/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752840735,"author_flair_text":"llama.cpp","treatment_tags":[],"created_utc":1752840735,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"more","data":{"count":0,"name":"t1__","id":"_","parent_id":"t1_n3uz837","depth":10,"children":[]}}],"before":null}},"user_reports":[],"saved":false,"id":"n3uz837","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DepthHour1669","can_mod_post":false,"created_utc":1752862633,"send_replies":true,"parent_id":"t1_n3ttb5l","score":1,"author_fullname":"t2_t6glzswk","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"256 was the headline AVX 8 bit number I found.\\n\\n\\n4.2\\\\*16\\\\*16 / 32 = 33.6 tokens per sec then.","edited":1752863666,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3uz837","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;256 was the headline AVX 8 bit number I found.&lt;/p&gt;\\n\\n&lt;p&gt;4.2*16*16 / 32 = 33.6 tokens per sec then.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2xh8s/run_kimik2_without_quantization_locally_for_under/n3uz837/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752862633,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m2xh8s","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":9,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3ttb5l","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"101m4n","can_mod_post":false,"created_utc":1752850826,"send_replies":true,"parent_id":"t1_n3toks7","score":1,"author_fullname":"t2_p7nc2","approved_by":null,"mod_note":null,"all_awardings":[],"body":"Hmm. Where are you getting the 256 ops per cycle from?\\n\\nMy understanding is that with avx512 it's more like 16 f32 fma operations per cycle per core. However that's peak throughput. Realistically you're going to get less than that.\\n\\nAlso, minor detail, but you don't need 2 ops per weight. Most of the weights are just used in big matrix multiplies, so each weight becomes a single fma operation, generally considered to be 1 flop (unless you're in marketing).","edited":1752851192,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n3ttb5l","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Hmm. Where are you getting the 256 ops per cycle from?&lt;/p&gt;\\n\\n&lt;p&gt;My understanding is that with avx512 it&amp;#39;s more like 16 f32 fma operations per cycle per core. However that&amp;#39;s peak throughput. Realistically you&amp;#39;re going to get less than that.&lt;/p&gt;\\n\\n&lt;p&gt;Also, minor detail, but you don&amp;#39;t need 2 ops per weight. Most of the weights are just used in big matrix multiplies, so each weight becomes a single fma operation, generally considered to be 1 flop (unless you&amp;#39;re in marketing).&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2xh8s/run_kimik2_without_quantization_locally_for_under/n3ttb5l/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752850826,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m2xh8s","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":8,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3v7g9c","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DepthHour1669","can_mod_post":false,"created_utc":1752865040,"send_replies":true,"parent_id":"t1_n3tu4fd","score":1,"author_fullname":"t2_t6glzswk","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"https://en.wikipedia.org/wiki/Template:AMD_Epyc_9005_series\\n\\nYou want 8 CCDs and 16 or 32 cores. 16 works in the ideal theoretical case, but I think 32 would be a safer bet since you’ll probably get less than ideal cpu speeds. \\n\\nSo probably the 9375F.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3v7g9c","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;&lt;a href=\\"https://en.wikipedia.org/wiki/Template:AMD_Epyc_9005_series\\"&gt;https://en.wikipedia.org/wiki/Template:AMD_Epyc_9005_series&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;You want 8 CCDs and 16 or 32 cores. 16 works in the ideal theoretical case, but I think 32 would be a safer bet since you’ll probably get less than ideal cpu speeds. &lt;/p&gt;\\n\\n&lt;p&gt;So probably the 9375F.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2xh8s/run_kimik2_without_quantization_locally_for_under/n3v7g9c/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752865040,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m2xh8s","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":9,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3tu4fd","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MidnightProgrammer","can_mod_post":false,"created_utc":1752851058,"send_replies":true,"parent_id":"t1_n3toks7","score":1,"author_fullname":"t2_ijzb7","approved_by":null,"mod_note":null,"all_awardings":[],"body":"How do you figure out how many CCD a cpu has?  I'm looking at TechPowerup and it doesn't specify.","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n3tu4fd","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;How do you figure out how many CCD a cpu has?  I&amp;#39;m looking at TechPowerup and it doesn&amp;#39;t specify.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2xh8s/run_kimik2_without_quantization_locally_for_under/n3tu4fd/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752851058,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m2xh8s","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":8,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3ys2xf","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Baldur-Norddahl","can_mod_post":false,"created_utc":1752915572,"send_replies":true,"parent_id":"t1_n3toks7","score":1,"author_fullname":"t2_bvqb8ng0","approved_by":null,"mod_note":null,"all_awardings":[],"body":"I tried researching this flops thing a little and ended up with more questions than answers. Seems to me that we will only get a proper use of the CPU if the work is very optimized. They should be using AVX512 for matrix multiplication. But what if they are not unpacking q4 efficiently? Doesn't take much to throw the whole thing off. Could even be that fp8 is faster than q4 because they are more likely to use the proper avx512 upscale instructions for that.\\n\\nTo me it seems clear that something is going on. The tps numbers reported are too low.","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n3ys2xf","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I tried researching this flops thing a little and ended up with more questions than answers. Seems to me that we will only get a proper use of the CPU if the work is very optimized. They should be using AVX512 for matrix multiplication. But what if they are not unpacking q4 efficiently? Doesn&amp;#39;t take much to throw the whole thing off. Could even be that fp8 is faster than q4 because they are more likely to use the proper avx512 upscale instructions for that.&lt;/p&gt;\\n\\n&lt;p&gt;To me it seems clear that something is going on. The tps numbers reported are too low.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2xh8s/run_kimik2_without_quantization_locally_for_under/n3ys2xf/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752915572,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m2xh8s","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":8,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3toks7","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DepthHour1669","can_mod_post":false,"created_utc":1752849498,"send_replies":true,"parent_id":"t1_n3taaef","score":0,"author_fullname":"t2_t6glzswk","approved_by":null,"mod_note":null,"all_awardings":[],"body":"/u/midnightprogrammer this is the information you need\\n\\nFor inference, we only care about read, so let’s go with 96GB/CCD. Then we need 6.4 CCDs to saturate 619GB/sec.\\n\\nOn the compute side, for token generation you need 2x32b flops per token, and assuming 256 ops per cycle, you can do 146tok/sec compute. So you should be still just memory bandwidth limited.\\n\\nTL;DR you need a cpu with at least 8 CCDs and core count doesn’t really matter.","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n3toks7","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;&lt;a href=\\"/u/midnightprogrammer\\"&gt;/u/midnightprogrammer&lt;/a&gt; this is the information you need&lt;/p&gt;\\n\\n&lt;p&gt;For inference, we only care about read, so let’s go with 96GB/CCD. Then we need 6.4 CCDs to saturate 619GB/sec.&lt;/p&gt;\\n\\n&lt;p&gt;On the compute side, for token generation you need 2x32b flops per token, and assuming 256 ops per cycle, you can do 146tok/sec compute. So you should be still just memory bandwidth limited.&lt;/p&gt;\\n\\n&lt;p&gt;TL;DR you need a cpu with at least 8 CCDs and core count doesn’t really matter.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1m2xh8s","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2xh8s/run_kimik2_without_quantization_locally_for_under/n3toks7/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752849498,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":7,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}},"user_reports":[],"saved":false,"id":"n3taaef","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"101m4n","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3sx00h","score":2,"author_fullname":"t2_p7nc2","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I did a little digging and it looks like the zen 4 fabric clock is 1:1 with the memory clock up to about ddr5 6000. So 3000MHz. The width of the link is 256 bits (per ccd) for reads and 128 bits for writes. So that's 96GB/s per CCD for read and 48GB/s for writes. This bandwidth is also shared for IO. \\n\\nSo the 9015 with only 2 ccds will top out at a theoretical max of 192GB/s. In practice it will be 10-20% lower than that due to various overheads. \\n\\nThe 9175F is weird as hell. 16 1 core CCDs 🤨 (when gemini told me that I thought it was hallucinating!). So it can (maybe?) push the bandwidth? At least the links between the ccds and the IO die aren't the limiting factor. Though I feel expecting a single core to push 40GB/s is a bit of a stretch.\\n\\nI think an 8 4 core CCD SKU is probably a more sensible bet.","edited":1752846731,"gildings":{},"author_flair_css_class":null,"name":"t1_n3taaef","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I did a little digging and it looks like the zen 4 fabric clock is 1:1 with the memory clock up to about ddr5 6000. So 3000MHz. The width of the link is 256 bits (per ccd) for reads and 128 bits for writes. So that&amp;#39;s 96GB/s per CCD for read and 48GB/s for writes. This bandwidth is also shared for IO. &lt;/p&gt;\\n\\n&lt;p&gt;So the 9015 with only 2 ccds will top out at a theoretical max of 192GB/s. In practice it will be 10-20% lower than that due to various overheads. &lt;/p&gt;\\n\\n&lt;p&gt;The 9175F is weird as hell. 16 1 core CCDs 🤨 (when gemini told me that I thought it was hallucinating!). So it can (maybe?) push the bandwidth? At least the links between the ccds and the IO die aren&amp;#39;t the limiting factor. Though I feel expecting a single core to push 40GB/s is a bit of a stretch.&lt;/p&gt;\\n\\n&lt;p&gt;I think an 8 4 core CCD SKU is probably a more sensible bet.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1m2xh8s","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2xh8s/run_kimik2_without_quantization_locally_for_under/n3taaef/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752845214,"author_flair_text":null,"treatment_tags":[],"created_utc":1752845214,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":6,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n3sx00h","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DepthHour1669","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3swnta","score":1,"author_fullname":"t2_t6glzswk","approved_by":null,"mod_note":null,"all_awardings":[],"body":"I'll take your word for that, I haven't done the compute math yet, just the memory bandwidth math. \\n\\nBut still, from what I can tell, drop another $2k on a Epyc 9175F and you'll get a $11k machine that should get you 20tok/sec.","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n3sx00h","is_submitter":true,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;ll take your word for that, I haven&amp;#39;t done the compute math yet, just the memory bandwidth math. &lt;/p&gt;\\n\\n&lt;p&gt;But still, from what I can tell, drop another $2k on a Epyc 9175F and you&amp;#39;ll get a $11k machine that should get you 20tok/sec.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1m2xh8s","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2xh8s/run_kimik2_without_quantization_locally_for_under/n3sx00h/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752840664,"author_flair_text":null,"treatment_tags":[],"created_utc":1752840664,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3swnta","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"101m4n","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3sjo2s","score":2,"author_fullname":"t2_p7nc2","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I'm sorry, but you're dead wrong here.\\n\\nYou do need a lot more memory bandwidth than compute, but you still need _enough_ compute. And that's to say nothing of context processing.\\n\\n8 cores are absolutely not going to cut it.","edited":false,"author_flair_css_class":null,"name":"t1_n3swnta","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;m sorry, but you&amp;#39;re dead wrong here.&lt;/p&gt;\\n\\n&lt;p&gt;You do need a lot more memory bandwidth than compute, but you still need &lt;em&gt;enough&lt;/em&gt; compute. And that&amp;#39;s to say nothing of context processing.&lt;/p&gt;\\n\\n&lt;p&gt;8 cores are absolutely not going to cut it.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1m2xh8s","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2xh8s/run_kimik2_without_quantization_locally_for_under/n3swnta/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752840538,"author_flair_text":null,"collapsed":false,"created_utc":1752840538,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3sujm1","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DepthHour1669","can_mod_post":false,"created_utc":1752839713,"send_replies":true,"parent_id":"t1_n3snqtd","score":1,"author_fullname":"t2_t6glzswk","approved_by":null,"mod_note":null,"all_awardings":[],"body":"&gt; Would two 5090's or even a 6000 Pro improve it much?\\n\\nNo, not at all. That would have the same memory bandwidth as a single regular 5090. \\n\\n&gt; I was thinking the 9375F as it has the fastest core speed, but the 9015 would be a massive savings.\\n\\nI'm not 100% the 9015 would work, some people are questioning it. I think the GMI3-wide links would be a bottleneck. \\n\\nBut worst case scenario, buy a 9175F, that should work at full speed.","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n3sujm1","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;Would two 5090&amp;#39;s or even a 6000 Pro improve it much?&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;No, not at all. That would have the same memory bandwidth as a single regular 5090. &lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;I was thinking the 9375F as it has the fastest core speed, but the 9015 would be a massive savings.&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;I&amp;#39;m not 100% the 9015 would work, some people are questioning it. I think the GMI3-wide links would be a bottleneck. &lt;/p&gt;\\n\\n&lt;p&gt;But worst case scenario, buy a 9175F, that should work at full speed.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1m2xh8s","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2xh8s/run_kimik2_without_quantization_locally_for_under/n3sujm1/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752839713,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":7,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3snqtd","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MidnightProgrammer","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3smus7","score":1,"author_fullname":"t2_ijzb7","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Would two 5090's or even a 6000 Pro improve it much?  \\nFrom what I have tested, unless you get to about 75% of the VRAM on GPU, the GPU improvement is very unimpressive, outside of a single one to improve prompt processing.\\n\\nI am looking at setting up a Q4 or Q8 setup that can run kimi.  I have a 3090 lying around, but I was considering a 5090 or maybe even a 6000 pro.\\n\\nMy goal is to hit at least 20 tokens/sec and as much as the context window as possible.  I was thinking the 9375F as it has the fastest core speed, but the 9015 would be a massive savings.","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n3snqtd","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Would two 5090&amp;#39;s or even a 6000 Pro improve it much?&lt;br/&gt;\\nFrom what I have tested, unless you get to about 75% of the VRAM on GPU, the GPU improvement is very unimpressive, outside of a single one to improve prompt processing.&lt;/p&gt;\\n\\n&lt;p&gt;I am looking at setting up a Q4 or Q8 setup that can run kimi.  I have a 3090 lying around, but I was considering a 5090 or maybe even a 6000 pro.&lt;/p&gt;\\n\\n&lt;p&gt;My goal is to hit at least 20 tokens/sec and as much as the context window as possible.  I was thinking the 9375F as it has the fastest core speed, but the 9015 would be a massive savings.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1m2xh8s","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2xh8s/run_kimik2_without_quantization_locally_for_under/n3snqtd/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752836831,"author_flair_text":null,"treatment_tags":[],"created_utc":1752836831,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":6,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3t2ph0","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"nail_nail","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3smus7","score":1,"author_fullname":"t2_quvm4","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I think prompt processing will be very slow though, no?","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n3t2ph0","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I think prompt processing will be very slow though, no?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1m2xh8s","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2xh8s/run_kimik2_without_quantization_locally_for_under/n3t2ph0/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752842729,"author_flair_text":null,"treatment_tags":[],"created_utc":1752842729,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":6,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3smus7","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DepthHour1669","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3sk647","score":2,"author_fullname":"t2_t6glzswk","approved_by":null,"mod_note":null,"all_awardings":[],"body":"Ok so, mathematically, Kimi K2 uses a SwiGLU feed‑forward (three weight matrices), so each expert is 44 M params. This means it has a 66/34 weight distribution experts/common. So that means you can load about 11b weights in vram. \\n\\n619/(32b-11b)= 29.476tok/sec, so this is the max speed you can hit with an infinitely fast GPU due to amdahl's law. The minimum speed with no GPU is 19.3tok/sec. \\n\\nSo with a 3090Ti (I'm picking the Ti since it's easier to round to 1000GB/sec bandwidth), you'll see 33.9ms for the expert weights and 11ms for the common weights, leading to 22.3tokens/sec. \\n\\nWith a 5090, you'll see 33.9ms for the expert weights and 6.1ms for the common weights, leading to 24.98tokens/sec. Basically 25tok/sec exactly.","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n3smus7","is_submitter":true,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Ok so, mathematically, Kimi K2 uses a SwiGLU feed‑forward (three weight matrices), so each expert is 44 M params. This means it has a 66/34 weight distribution experts/common. So that means you can load about 11b weights in vram. &lt;/p&gt;\\n\\n&lt;p&gt;619/(32b-11b)= 29.476tok/sec, so this is the max speed you can hit with an infinitely fast GPU due to amdahl&amp;#39;s law. The minimum speed with no GPU is 19.3tok/sec. &lt;/p&gt;\\n\\n&lt;p&gt;So with a 3090Ti (I&amp;#39;m picking the Ti since it&amp;#39;s easier to round to 1000GB/sec bandwidth), you&amp;#39;ll see 33.9ms for the expert weights and 11ms for the common weights, leading to 22.3tokens/sec. &lt;/p&gt;\\n\\n&lt;p&gt;With a 5090, you&amp;#39;ll see 33.9ms for the expert weights and 6.1ms for the common weights, leading to 24.98tokens/sec. Basically 25tok/sec exactly.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1m2xh8s","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2xh8s/run_kimik2_without_quantization_locally_for_under/n3smus7/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752836425,"author_flair_text":null,"treatment_tags":[],"created_utc":1752836425,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n3sk647","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MidnightProgrammer","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3sjo2s","score":1,"author_fullname":"t2_ijzb7","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"You think this build would hit 20 tokens/sec?  How much would a 3090/5090 improve it?","edited":false,"author_flair_css_class":null,"name":"t1_n3sk647","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You think this build would hit 20 tokens/sec?  How much would a 3090/5090 improve it?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1m2xh8s","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2xh8s/run_kimik2_without_quantization_locally_for_under/n3sk647/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752835148,"author_flair_text":null,"collapsed":false,"created_utc":1752835148,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3sjo2s","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DepthHour1669","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3sjgc9","score":0,"author_fullname":"t2_t6glzswk","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Basically identical. I don't think compute is the limiting factor at all, just memory bandwidth.\\n\\nI wonder if larger batch sizes are possible with a faster CPU... but I haven't done the math for that yet.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3sjo2s","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Basically identical. I don&amp;#39;t think compute is the limiting factor at all, just memory bandwidth.&lt;/p&gt;\\n\\n&lt;p&gt;I wonder if larger batch sizes are possible with a faster CPU... but I haven&amp;#39;t done the math for that yet.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2xh8s","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2xh8s/run_kimik2_without_quantization_locally_for_under/n3sjo2s/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752834892,"author_flair_text":null,"treatment_tags":[],"created_utc":1752834892,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}},"user_reports":[],"saved":false,"id":"n3sjgc9","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MidnightProgrammer","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3sfxay","score":1,"author_fullname":"t2_ijzb7","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"How would the 9015 perform compared to say the 9375F in terms of token/sec?  \\nAny idea what a system you described would get token/sec on Q8 Kimi?","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3sjgc9","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;How would the 9015 perform compared to say the 9375F in terms of token/sec?&lt;br/&gt;\\nAny idea what a system you described would get token/sec on Q8 Kimi?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2xh8s","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2xh8s/run_kimik2_without_quantization_locally_for_under/n3sjgc9/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752834782,"author_flair_text":null,"treatment_tags":[],"created_utc":1752834782,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3sfxay","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DepthHour1669","can_mod_post":false,"created_utc":1752832933,"send_replies":true,"parent_id":"t1_n3sdo33","score":4,"author_fullname":"t2_t6glzswk","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"False. The AMD 9015 cpu supports 12 channel DDR5-6400 with the Supermicro H13SSW or H13SSL‑N motherboard (6000 speeds on slower motherboard), and the cpu costs about $600. The motherboard costs about $800 new. \\n\\nhttps://www.techpowerup.com/cpu-specs/epyc-9015.c3903\\n\\n&gt; Memory Bus: \\tTwelve-channel \\n\\n&gt; Rated Speed: \\t6000 MT/s \\n\\n&gt; AMD's \\"Turin\\" CPUs can be configured for DDR5 6400 MT/s with 1 DIMM per channel (1DPC) in specific scenarios","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3sfxay","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;False. The AMD 9015 cpu supports 12 channel DDR5-6400 with the Supermicro H13SSW or H13SSL‑N motherboard (6000 speeds on slower motherboard), and the cpu costs about $600. The motherboard costs about $800 new. &lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://www.techpowerup.com/cpu-specs/epyc-9015.c3903\\"&gt;https://www.techpowerup.com/cpu-specs/epyc-9015.c3903&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;Memory Bus:   Twelve-channel &lt;/p&gt;\\n\\n&lt;p&gt;Rated Speed:  6000 MT/s &lt;/p&gt;\\n\\n&lt;p&gt;AMD&amp;#39;s &amp;quot;Turin&amp;quot; CPUs can be configured for DDR5 6400 MT/s with 1 DIMM per channel (1DPC) in specific scenarios&lt;/p&gt;\\n&lt;/blockquote&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2xh8s","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2xh8s/run_kimik2_without_quantization_locally_for_under/n3sfxay/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752832933,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}}],"before":null}},"user_reports":[],"saved":false,"id":"n3sdo33","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":"DELETED","no_follow":true,"author":"[deleted]","can_mod_post":false,"send_replies":true,"parent_id":"t3_1m2xh8s","score":1,"approved_by":null,"report_reasons":null,"all_awardings":[],"subreddit_id":"t5_81eyvm","body":"[deleted]","edited":false,"downs":0,"author_flair_css_class":null,"collapsed":true,"is_submitter":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;[deleted]&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"associated_award":null,"stickied":false,"subreddit_type":"public","can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":"dark","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2xh8s/run_kimik2_without_quantization_locally_for_under/n3sdo33/","num_reports":null,"locked":false,"name":"t1_n3sdo33","created":1752831717,"subreddit":"LocalLLaMA","author_flair_text":null,"treatment_tags":[],"created_utc":1752831717,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"","collapsed_because_crowd_control":null,"mod_reports":[],"mod_note":null,"distinguished":null}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3t0xro","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"holchansg","can_mod_post":false,"created_utc":1752842109,"send_replies":true,"parent_id":"t3_1m2xh8s","score":1,"author_fullname":"t2_ppu9v","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Without quant? But why? Ok, just 4fun...\\n\\nEpyc + Ram + 5090 offloading? Would be my go to. So yeah, we are aligned.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3t0xro","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Without quant? But why? Ok, just 4fun...&lt;/p&gt;\\n\\n&lt;p&gt;Epyc + Ram + 5090 offloading? Would be my go to. So yeah, we are aligned.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2xh8s/run_kimik2_without_quantization_locally_for_under/n3t0xro/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752842109,"author_flair_text":"llama.cpp","treatment_tags":[],"link_id":"t3_1m2xh8s","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3sj2l7","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"a_beautiful_rhind","can_mod_post":false,"created_utc":1752834586,"send_replies":true,"parent_id":"t3_1m2xh8s","score":-1,"author_fullname":"t2_h5utwre7","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"It will run, but I wouldn't pay 10k for this model.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3sj2l7","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It will run, but I wouldn&amp;#39;t pay 10k for this model.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2xh8s/run_kimik2_without_quantization_locally_for_under/n3sj2l7/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752834586,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m2xh8s","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":1,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-1}},{"kind":"t1","data":{"total_awards_received":0,"approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"ups":-3,"removal_reason":null,"link_id":"t3_1m2xh8s","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3se2uk","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"dodiyeztr","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3scxfm","score":2,"author_fullname":"t2_17a5ts","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Tgere are bare metal instances that overcome most of what you listed","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3se2uk","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Tgere are bare metal instances that overcome most of what you listed&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2xh8s","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2xh8s/run_kimik2_without_quantization_locally_for_under/n3se2uk/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752831940,"author_flair_text":null,"treatment_tags":[],"created_utc":1752831940,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3t1mh4","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"RazzmatazzReal4129","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3soyka","score":1,"author_fullname":"t2_flbicsbbj","approved_by":null,"mod_note":null,"all_awardings":[],"body":"If you want to rent an actual physical server, you could try dedicated host as well: [https://azure.microsoft.com/en-us/pricing/details/virtual-machines/dedicated-host/](https://azure.microsoft.com/en-us/pricing/details/virtual-machines/dedicated-host/)","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n3t1mh4","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;If you want to rent an actual physical server, you could try dedicated host as well: &lt;a href=\\"https://azure.microsoft.com/en-us/pricing/details/virtual-machines/dedicated-host/\\"&gt;https://azure.microsoft.com/en-us/pricing/details/virtual-machines/dedicated-host/&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1m2xh8s","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2xh8s/run_kimik2_without_quantization_locally_for_under/n3t1mh4/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752842349,"author_flair_text":null,"treatment_tags":[],"created_utc":1752842349,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3soyka","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DepthHour1669","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3si89z","score":-1,"author_fullname":"t2_t6glzswk","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"&gt; But what I am Talking about is CPU to CPU communication\\n\\nYou do realize, worst case scenario during inference, you don't transfer the params of layers across cpu cores? You just transfer the token (in latent space)? Each token is 16kb for deepseekMoE architecture. That's it. You NEVER need to transfer the entire layer across. \\n\\nNo inference engine is transferring a whole layer of a few gb of weights to another cpu. You only need to transfer the latent space representation, that's it. \\n\\n&gt;  your VPS could literally be sliced up between Several System\\n\\nDid you even look at the link? All Azure HBv4 instances [are the same price](https://costcalc.cloudoptimo.com/azure-pricing-calculator/vm/Standard-HB176-24rs-v4#region=eastus&amp;os=linux&amp;tier-type=standard), and the HBv4 Standard_HB176rs_v4 **is 1 user to 1 bare metal server**. Dumbass. It's running on actual hardware AMD EPYC Genoa-X CPUs with 176 physical cores, and you get 176 cores. What magical \\"other shared user\\" is running on this machine with 0 cpu cores?","edited":false,"author_flair_css_class":null,"name":"t1_n3soyka","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;But what I am Talking about is CPU to CPU communication&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;You do realize, worst case scenario during inference, you don&amp;#39;t transfer the params of layers across cpu cores? You just transfer the token (in latent space)? Each token is 16kb for deepseekMoE architecture. That&amp;#39;s it. You NEVER need to transfer the entire layer across. &lt;/p&gt;\\n\\n&lt;p&gt;No inference engine is transferring a whole layer of a few gb of weights to another cpu. You only need to transfer the latent space representation, that&amp;#39;s it. &lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;your VPS could literally be sliced up between Several System&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;Did you even look at the link? All Azure HBv4 instances &lt;a href=\\"https://costcalc.cloudoptimo.com/azure-pricing-calculator/vm/Standard-HB176-24rs-v4#region=eastus&amp;amp;os=linux&amp;amp;tier-type=standard\\"&gt;are the same price&lt;/a&gt;, and the HBv4 Standard_HB176rs_v4 &lt;strong&gt;is 1 user to 1 bare metal server&lt;/strong&gt;. Dumbass. It&amp;#39;s running on actual hardware AMD EPYC Genoa-X CPUs with 176 physical cores, and you get 176 cores. What magical &amp;quot;other shared user&amp;quot; is running on this machine with 0 cpu cores?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1m2xh8s","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2xh8s/run_kimik2_without_quantization_locally_for_under/n3soyka/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752837372,"author_flair_text":null,"collapsed":false,"created_utc":1752837372,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3si89z","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"GeekyBit","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3sh6ez","score":-1,"author_fullname":"t2_zq180","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Having access to that speed doesn't mean running at that speed OMG... So they have interconnects to directly talk to more than one processor See here this Tech specs by AMD\\n\\n[https://www.amd.com/content/dam/amd/en/documents/epyc-technical-docs/user-guides/58462\\\\_amd-epyc-9005-tg-architecture-overview.pdf](https://www.amd.com/content/dam/amd/en/documents/epyc-technical-docs/user-guides/58462_amd-epyc-9005-tg-architecture-overview.pdf)\\n\\nIt seems their misunderstanding on your part. Ram speed test show ram speed, But what I am Talking about is CPU to CPU communication. It isn't as fast as 12 channel ddr5 in fact there are up to  4 32GB/s  connections for a speed of 128GB/s and That is assuming it is Bi direction, if it is like Intels interconnect it is one direction so the bandwidth suffers so if that were the case it would be a bottleneck of 64GB/s\\n\\n[https://forums.servethehome.com/index.php?threads/how-important-it-is-the-extra-4-xgmi-link-socket-to-socket.36764/](https://forums.servethehome.com/index.php?threads/how-important-it-is-the-extra-4-xgmi-link-socket-to-socket.36764/)\\n\\nSo yes the memory GO bur... but if you share a part of a System with someone else and some of your cores are on CPU1 and some of your Cores are on CPU0 then the actual speed will only be as fast as those processors talk to each other...\\n\\nHow do I know this I have several older DRR4 Systems where that is the actual bottleneck on both AMD and Intel.\\n\\nThat is all to say You are only saying I am \\"Wrong\\" based on Community posts about Microsoft's Azure, but my post talks about both Azure and AWS as you did.\\n\\nyour reply also doesn't address many Cloud solutions work more like clusters and your VPS could literally be sliced up between Several System... Which is even slower than CPU to CPU interconnects, dependent on how many systems your resources are pulled from.\\n\\nThese are actual facts.\\n\\nAgain Yes Memoery bandwidth fast ... no one is disagreeing on that, but to actually use it in the cloud you don't know the topplogy that Azure or AWS will go with... In Fact AWS doesn't even use Epyc on mass they use ARM... so who know what solution they are working with. Which again is my whole point... You don't know if you will get those speeds.\\n\\n  \\nEDIT: I feel I need to point this out too... Cloud VPS services aren't buying single CPU systems ... in fact if it was something that was available they would use 4 CPUs per Server board but it looks like there only dual CPU solutions... This is not CPU Cores, but how many physical CPUs used in the server. \\n\\nFrom what I could find Network interconnects have a max speed of 1000Gb/s and most run at 800Gb/s that is to say 125GB/s to 100GB/s dependent on network. There is also some kind of special sauce that lets the CPU interconnect have up to 2 of the internal Interconnects used in a network interface for Epyc 900X which would mean max speeds of 64GB/s assuming by directional speeds. Which is to say very slow compared again to ram... but a common Topology in servers when using several servers for shared resources.","edited":1752835155,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3si89z","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Having access to that speed doesn&amp;#39;t mean running at that speed OMG... So they have interconnects to directly talk to more than one processor See here this Tech specs by AMD&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://www.amd.com/content/dam/amd/en/documents/epyc-technical-docs/user-guides/58462_amd-epyc-9005-tg-architecture-overview.pdf\\"&gt;https://www.amd.com/content/dam/amd/en/documents/epyc-technical-docs/user-guides/58462_amd-epyc-9005-tg-architecture-overview.pdf&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;It seems their misunderstanding on your part. Ram speed test show ram speed, But what I am Talking about is CPU to CPU communication. It isn&amp;#39;t as fast as 12 channel ddr5 in fact there are up to  4 32GB/s  connections for a speed of 128GB/s and That is assuming it is Bi direction, if it is like Intels interconnect it is one direction so the bandwidth suffers so if that were the case it would be a bottleneck of 64GB/s&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://forums.servethehome.com/index.php?threads/how-important-it-is-the-extra-4-xgmi-link-socket-to-socket.36764/\\"&gt;https://forums.servethehome.com/index.php?threads/how-important-it-is-the-extra-4-xgmi-link-socket-to-socket.36764/&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;So yes the memory GO bur... but if you share a part of a System with someone else and some of your cores are on CPU1 and some of your Cores are on CPU0 then the actual speed will only be as fast as those processors talk to each other...&lt;/p&gt;\\n\\n&lt;p&gt;How do I know this I have several older DRR4 Systems where that is the actual bottleneck on both AMD and Intel.&lt;/p&gt;\\n\\n&lt;p&gt;That is all to say You are only saying I am &amp;quot;Wrong&amp;quot; based on Community posts about Microsoft&amp;#39;s Azure, but my post talks about both Azure and AWS as you did.&lt;/p&gt;\\n\\n&lt;p&gt;your reply also doesn&amp;#39;t address many Cloud solutions work more like clusters and your VPS could literally be sliced up between Several System... Which is even slower than CPU to CPU interconnects, dependent on how many systems your resources are pulled from.&lt;/p&gt;\\n\\n&lt;p&gt;These are actual facts.&lt;/p&gt;\\n\\n&lt;p&gt;Again Yes Memoery bandwidth fast ... no one is disagreeing on that, but to actually use it in the cloud you don&amp;#39;t know the topplogy that Azure or AWS will go with... In Fact AWS doesn&amp;#39;t even use Epyc on mass they use ARM... so who know what solution they are working with. Which again is my whole point... You don&amp;#39;t know if you will get those speeds.&lt;/p&gt;\\n\\n&lt;p&gt;EDIT: I feel I need to point this out too... Cloud VPS services aren&amp;#39;t buying single CPU systems ... in fact if it was something that was available they would use 4 CPUs per Server board but it looks like there only dual CPU solutions... This is not CPU Cores, but how many physical CPUs used in the server. &lt;/p&gt;\\n\\n&lt;p&gt;From what I could find Network interconnects have a max speed of 1000Gb/s and most run at 800Gb/s that is to say 125GB/s to 100GB/s dependent on network. There is also some kind of special sauce that lets the CPU interconnect have up to 2 of the internal Interconnects used in a network interface for Epyc 900X which would mean max speeds of 64GB/s assuming by directional speeds. Which is to say very slow compared again to ram... but a common Topology in servers when using several servers for shared resources.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2xh8s","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2xh8s/run_kimik2_without_quantization_locally_for_under/n3si89z/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752834148,"author_flair_text":null,"treatment_tags":[],"created_utc":1752834148,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3sh6ez","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DepthHour1669","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3scxfm","score":2,"author_fullname":"t2_t6glzswk","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Nope, you're wrong. \\n\\nhttps://learn.microsoft.com/en-us/azure/virtual-machines/sizes/high-performance-compute/hbv4-series?tabs=sizebasic\\n\\nAdvertised 780 GB/s memory bandwidth from a dual CPU system, about 700GB/sec in STREAM triad tests: https://techcommunity.microsoft.com/blog/azurehighperformancecomputingblog/performance--scalability-of-hbv4-and-hx-series-vms-with-genoa-x-cpus/3846766\\n\\nPic: https://techcommunity.microsoft.com/t5/s/gxcuf89792/images/bS0zODQ2NzY2LTQ3OTc3Nmk2MTg0MEM2QzMxMDYwOENG?revision=7\\n\\nSo clearly, it can do 700GB/sec copying 9.6gb of data.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3sh6ez","is_submitter":true,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Nope, you&amp;#39;re wrong. &lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://learn.microsoft.com/en-us/azure/virtual-machines/sizes/high-performance-compute/hbv4-series?tabs=sizebasic\\"&gt;https://learn.microsoft.com/en-us/azure/virtual-machines/sizes/high-performance-compute/hbv4-series?tabs=sizebasic&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;Advertised 780 GB/s memory bandwidth from a dual CPU system, about 700GB/sec in STREAM triad tests: &lt;a href=\\"https://techcommunity.microsoft.com/blog/azurehighperformancecomputingblog/performance--scalability-of-hbv4-and-hx-series-vms-with-genoa-x-cpus/3846766\\"&gt;https://techcommunity.microsoft.com/blog/azurehighperformancecomputingblog/performance--scalability-of-hbv4-and-hx-series-vms-with-genoa-x-cpus/3846766&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;Pic: &lt;a href=\\"https://techcommunity.microsoft.com/t5/s/gxcuf89792/images/bS0zODQ2NzY2LTQ3OTc3Nmk2MTg0MEM2QzMxMDYwOENG?revision=7\\"&gt;https://techcommunity.microsoft.com/t5/s/gxcuf89792/images/bS0zODQ2NzY2LTQ3OTc3Nmk2MTg0MEM2QzMxMDYwOENG?revision=7&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;So clearly, it can do 700GB/sec copying 9.6gb of data.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2xh8s","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2xh8s/run_kimik2_without_quantization_locally_for_under/n3sh6ez/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752833602,"author_flair_text":null,"treatment_tags":[],"created_utc":1752833602,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n3scxfm","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"GeekyBit","can_mod_post":false,"created_utc":1752831299,"send_replies":true,"parent_id":"t1_n3sc40z","score":1,"author_fullname":"t2_zq180","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Not really many of those solutions segment things so the ram will not run in 12 channel DDR-5 even if it can. You don't get access to a whole system... you get access to a segmented chuck of a system.\\n\\nThen their are multi CPU systems, And what happens if your vCPU cores come dynamically from several of the real CPUs in the system but then the ram only comes from one Physical CPU ... Then it will be very slow as slow as the CPU interconnect.\\n\\nAmazon Might not even be using 12 Channel DDR5, because they have their own custom Arm solution for their data centers.\\n\\nAlso to make matters worst if you need lots of ram, they span your VPS across several physical systems, you think the CPU interconnect is slow... that will be way slower.\\n\\n  \\nEDIT: All that is to say, Just because you can envision a decently fast topology doesn't mean that is how it will work in a VPS cloud based solution.","edited":1752831508,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3scxfm","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Not really many of those solutions segment things so the ram will not run in 12 channel DDR-5 even if it can. You don&amp;#39;t get access to a whole system... you get access to a segmented chuck of a system.&lt;/p&gt;\\n\\n&lt;p&gt;Then their are multi CPU systems, And what happens if your vCPU cores come dynamically from several of the real CPUs in the system but then the ram only comes from one Physical CPU ... Then it will be very slow as slow as the CPU interconnect.&lt;/p&gt;\\n\\n&lt;p&gt;Amazon Might not even be using 12 Channel DDR5, because they have their own custom Arm solution for their data centers.&lt;/p&gt;\\n\\n&lt;p&gt;Also to make matters worst if you need lots of ram, they span your VPS across several physical systems, you think the CPU interconnect is slow... that will be way slower.&lt;/p&gt;\\n\\n&lt;p&gt;EDIT: All that is to say, Just because you can envision a decently fast topology doesn&amp;#39;t mean that is how it will work in a VPS cloud based solution.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2xh8s","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2xh8s/run_kimik2_without_quantization_locally_for_under/n3scxfm/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752831299,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3sc40z","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":"DELETED","no_follow":true,"author":"[deleted]","can_mod_post":false,"send_replies":true,"parent_id":"t3_1m2xh8s","score":-3,"approved_by":null,"report_reasons":null,"all_awardings":[],"subreddit_id":"t5_81eyvm","body":"[deleted]","edited":false,"downs":0,"author_flair_css_class":null,"collapsed":true,"is_submitter":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;[deleted]&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"associated_award":null,"stickied":false,"subreddit_type":"public","can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":"dark","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2xh8s/run_kimik2_without_quantization_locally_for_under/n3sc40z/","num_reports":null,"locked":false,"name":"t1_n3sc40z","created":1752830828,"subreddit":"LocalLLaMA","author_flair_text":null,"treatment_tags":[],"created_utc":1752830828,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":1,"depth":0,"author_flair_background_color":"","collapsed_because_crowd_control":null,"mod_reports":[],"mod_note":null,"distinguished":null}},{"kind":"t1","data":{"total_awards_received":0,"approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"ups":-3,"removal_reason":null,"link_id":"t3_1m2xh8s","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3sfvy3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Baldur-Norddahl","can_mod_post":false,"created_utc":1752832913,"send_replies":true,"parent_id":"t1_n3sdi5d","score":5,"author_fullname":"t2_bvqb8ng0","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"K2 is MoE with 32b active parameters. That is about 20 tps theoretically. 614/32.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3sfvy3","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;K2 is MoE with 32b active parameters. That is about 20 tps theoretically. 614/32.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2xh8s","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2xh8s/run_kimik2_without_quantization_locally_for_under/n3sfvy3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752832913,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"body":"It's 32B active, so roughly 20 TPS at 8bit quant, which is decent. I think the bigger issue with CPU only is the KV cache size and prompt processing speeds. How fast could the cpu pre-fill with full context? So then I was looking at having a GPU for pre fill only , but even that needs like 109GB of vram just for the kv cache at 8bit quant. When is the RTX 7000 PRO with 256GB vram coming out? \\\\*sigh\\\\*","subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"body":"Oh that's awesome, I didn't know about MLA. This could be a pretty viable approach with 12 channel ddr5.","subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3sj13p","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"joninco","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3sh1p3","score":1,"author_fullname":"t2_8e8y0","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"author_cakeday":true,"edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3sj13p","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Oh that&amp;#39;s awesome, I didn&amp;#39;t know about MLA. This could be a pretty viable approach with 12 channel ddr5.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2xh8s","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2xh8s/run_kimik2_without_quantization_locally_for_under/n3sj13p/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752834564,"author_flair_text":null,"treatment_tags":[],"created_utc":1752834564,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3sh1p3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DepthHour1669","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3sgb9r","score":3,"author_fullname":"t2_t6glzswk","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Kimi K2 is 8bit native; it's not a 8 bit quant. \\n\\nAnd Kimi K2 is on deepseekV3MOE architecture with MLA, so 128k context should have a ~7gb KV cache. \\n\\nIf you buy a $700 RTX 3090 and throw it in there, you can probably get ~250tok/sec prompt processing. That's based off of ~500tok/sec prompt processing for a 4bit 32b model on a 3090.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3sh1p3","is_submitter":true,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Kimi K2 is 8bit native; it&amp;#39;s not a 8 bit quant. &lt;/p&gt;\\n\\n&lt;p&gt;And Kimi K2 is on deepseekV3MOE architecture with MLA, so 128k context should have a ~7gb KV cache. &lt;/p&gt;\\n\\n&lt;p&gt;If you buy a $700 RTX 3090 and throw it in there, you can probably get ~250tok/sec prompt processing. That&amp;#39;s based off of ~500tok/sec prompt processing for a 4bit 32b model on a 3090.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2xh8s","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2xh8s/run_kimik2_without_quantization_locally_for_under/n3sh1p3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752833532,"author_flair_text":null,"treatment_tags":[],"created_utc":1752833532,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n3sgb9r","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"joninco","can_mod_post":false,"created_utc":1752833141,"send_replies":true,"parent_id":"t1_n3sdi5d","score":4,"author_fullname":"t2_8e8y0","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"author_cakeday":true,"edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3sgb9r","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It&amp;#39;s 32B active, so roughly 20 TPS at 8bit quant, which is decent. I think the bigger issue with CPU only is the KV cache size and prompt processing speeds. How fast could the cpu pre-fill with full context? So then I was looking at having a GPU for pre fill only , but even that needs like 109GB of vram just for the kv cache at 8bit quant. When is the RTX 7000 PRO with 256GB vram coming out? *sigh*&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2xh8s","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2xh8s/run_kimik2_without_quantization_locally_for_under/n3sgb9r/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752833141,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3sgiij","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"JustinPooDough","can_mod_post":false,"created_utc":1752833248,"send_replies":true,"parent_id":"t1_n3sdi5d","score":1,"author_fullname":"t2_4kns99rz","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"No your best option is to run it in the cloud most likely. Unless privacy concerns","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3sgiij","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;No your best option is to run it in the cloud most likely. Unless privacy concerns&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2xh8s","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2xh8s/run_kimik2_without_quantization_locally_for_under/n3sgiij/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752833248,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3sdi5d","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":"DELETED","no_follow":true,"author":"[deleted]","can_mod_post":false,"send_replies":true,"parent_id":"t3_1m2xh8s","score":-3,"approved_by":null,"report_reasons":null,"all_awardings":[],"subreddit_id":"t5_81eyvm","body":"[deleted]","edited":false,"downs":0,"author_flair_css_class":null,"collapsed":true,"is_submitter":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;[deleted]&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"associated_award":null,"stickied":false,"subreddit_type":"public","can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":"dark","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2xh8s/run_kimik2_without_quantization_locally_for_under/n3sdi5d/","num_reports":null,"locked":false,"name":"t1_n3sdi5d","created":1752831626,"subreddit":"LocalLLaMA","author_flair_text":null,"treatment_tags":[],"created_utc":1752831626,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":1,"depth":0,"author_flair_background_color":"","collapsed_because_crowd_control":null,"mod_reports":[],"mod_note":null,"distinguished":null}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3sdgmc","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"GPTrack_ai","can_mod_post":false,"created_utc":1752831603,"send_replies":true,"parent_id":"t3_1m2xh8s","score":-1,"author_fullname":"t2_1tpuoj72sa","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"FP4 quantization does not result in significant quality loss. If you are on a budget you NEED to run it in FP4!!!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3sdgmc","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;FP4 quantization does not result in significant quality loss. If you are on a budget you NEED to run it in FP4!!!&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2xh8s/run_kimik2_without_quantization_locally_for_under/n3sdgmc/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752831603,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m2xh8s","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3sj3p3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"complead","can_mod_post":false,"created_utc":1752834602,"send_replies":true,"parent_id":"t3_1m2xh8s","score":0,"author_fullname":"t2_uzn88fhh","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Before jumping in, you might want to explore custom-built or specialized options tailored for deep learning, which could potentially optimize performance for models like Kimi-K2, without overspending. Some server providers offer configurations that balance cost and efficiency better—worth checking those specs against your setup.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3sj3p3","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Before jumping in, you might want to explore custom-built or specialized options tailored for deep learning, which could potentially optimize performance for models like Kimi-K2, without overspending. Some server providers offer configurations that balance cost and efficiency better—worth checking those specs against your setup.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2xh8s/run_kimik2_without_quantization_locally_for_under/n3sj3p3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752834602,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m2xh8s","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3v6alk","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DepthHour1669","can_mod_post":false,"created_utc":1752864704,"send_replies":true,"parent_id":"t1_n3uq1ah","score":1,"author_fullname":"t2_t6glzswk","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Kimi K2 is 8 bit for the base model, not 16 bit.\\n\\nI even point it out in the post: the official Kimi K2 model is 1031GB in size. That’s 8bit.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3v6alk","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Kimi K2 is 8 bit for the base model, not 16 bit.&lt;/p&gt;\\n\\n&lt;p&gt;I even point it out in the post: the official Kimi K2 model is 1031GB in size. That’s 8bit.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2xh8s","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2xh8s/run_kimik2_without_quantization_locally_for_under/n3v6alk/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752864704,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3uq1ah","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"complains_constantly","can_mod_post":false,"created_utc":1752860028,"send_replies":true,"parent_id":"t3_1m2xh8s","score":0,"author_fullname":"t2_158u6z","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"This would basically be impossible, however it *might* be possible under 25k. You should at least consider doing FP8, since this is pretty much indistinguishable from the base model in all cases. I think K2 was trained natively in FP8 though, so this might not even be a consideration.\\n\\nIf you do FP8, then you can pretty easily calculate it as roughly 1 GB of VRAM for every billon params, so we're looking at just under a terabyte here. Add in some room for KV caching and context size, and you're looking to get something with 1 TB of VRAM and some change.\\n\\nYou'll want to go with the specialized systems that load up on memory (either VRAM or unified memory) compared to processing power. This is pretty much either Apple's Mac Studio variants, or Nividia's DGX Spark (which still hasn't been released). Neither will get you under 10k, but they will get you the cheapest version of what you're asking for.\\n\\nThe actual cheapest option here would be 2 M3 Ultra Mac Studio's, both upgraded with the 512 GB of unified memory. These would cost $9,499.00 each, plus tax. So a little over 20k.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3uq1ah","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;This would basically be impossible, however it &lt;em&gt;might&lt;/em&gt; be possible under 25k. You should at least consider doing FP8, since this is pretty much indistinguishable from the base model in all cases. I think K2 was trained natively in FP8 though, so this might not even be a consideration.&lt;/p&gt;\\n\\n&lt;p&gt;If you do FP8, then you can pretty easily calculate it as roughly 1 GB of VRAM for every billon params, so we&amp;#39;re looking at just under a terabyte here. Add in some room for KV caching and context size, and you&amp;#39;re looking to get something with 1 TB of VRAM and some change.&lt;/p&gt;\\n\\n&lt;p&gt;You&amp;#39;ll want to go with the specialized systems that load up on memory (either VRAM or unified memory) compared to processing power. This is pretty much either Apple&amp;#39;s Mac Studio variants, or Nividia&amp;#39;s DGX Spark (which still hasn&amp;#39;t been released). Neither will get you under 10k, but they will get you the cheapest version of what you&amp;#39;re asking for.&lt;/p&gt;\\n\\n&lt;p&gt;The actual cheapest option here would be 2 M3 Ultra Mac Studio&amp;#39;s, both upgraded with the 512 GB of unified memory. These would cost $9,499.00 each, plus tax. So a little over 20k.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2xh8s/run_kimik2_without_quantization_locally_for_under/n3uq1ah/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752860028,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m2xh8s","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3uzqqd","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Fox-Lopsided","can_mod_post":false,"created_utc":1752862785,"send_replies":true,"parent_id":"t3_1m2xh8s","score":0,"author_fullname":"t2_7ivwbs3t","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Impossible.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3uzqqd","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Impossible.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2xh8s/run_kimik2_without_quantization_locally_for_under/n3uzqqd/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752862785,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m2xh8s","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3vh3qi","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"nivvis","can_mod_post":false,"created_utc":1752867917,"send_replies":true,"parent_id":"t3_1m2xh8s","score":0,"author_fullname":"t2_39blx","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"When you can run Kimi on groq .. and then still get tired of it not being Sonnet, Gemini pro .. ah it’s hard to go back to my local models for general use.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3vh3qi","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;When you can run Kimi on groq .. and then still get tired of it not being Sonnet, Gemini pro .. ah it’s hard to go back to my local models for general use.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2xh8s/run_kimik2_without_quantization_locally_for_under/n3vh3qi/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752867917,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m2xh8s","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3y43fa","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Square-Onion-1825","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3xz2sz","score":0,"author_fullname":"t2_1mkh7x2yxn","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"i think you gonna run into inference speed bottlenecks","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3y43fa","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;i think you gonna run into inference speed bottlenecks&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2xh8s","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2xh8s/run_kimik2_without_quantization_locally_for_under/n3y43fa/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752902443,"author_flair_text":null,"treatment_tags":[],"created_utc":1752902443,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}},"user_reports":[],"saved":false,"id":"n3xz2sz","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DepthHour1669","can_mod_post":false,"created_utc":1752900035,"send_replies":true,"parent_id":"t1_n3xwbu7","score":2,"author_fullname":"t2_t6glzswk","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"It’s 2/3 the speed of a 3090","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3xz2sz","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It’s 2/3 the speed of a 3090&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2xh8s","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2xh8s/run_kimik2_without_quantization_locally_for_under/n3xz2sz/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752900035,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n3xwbu7","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Square-Onion-1825","can_mod_post":false,"created_utc":1752898780,"send_replies":true,"parent_id":"t3_1m2xh8s","score":0,"author_fullname":"t2_1mkh7x2yxn","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"makes no sense because you need to GPU VRAM to run the model for speed.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3xwbu7","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;makes no sense because you need to GPU VRAM to run the model for speed.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2xh8s/run_kimik2_without_quantization_locally_for_under/n3xwbu7/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752898780,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m2xh8s","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3voo80","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"lakySK","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3sutaz","score":2,"author_fullname":"t2_y9y2q","approved_by":null,"mod_note":null,"all_awardings":[],"body":"Even if you find a way to run 30 GPUs on a motherboard, good luck powering them with those many thousands of watts. For running at home, I feel like that’s the biggest issue I keep running into. ","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n3voo80","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Even if you find a way to run 30 GPUs on a motherboard, good luck powering them with those many thousands of watts. For running at home, I feel like that’s the biggest issue I keep running into. &lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1m2xh8s","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2xh8s/run_kimik2_without_quantization_locally_for_under/n3voo80/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752870150,"author_flair_text":null,"treatment_tags":[],"created_utc":1752870150,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n3t9rmm","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DepthHour1669","can_mod_post":false,"created_utc":1752845049,"send_replies":true,"parent_id":"t1_n3t2u4r","score":-1,"author_fullname":"t2_t6glzswk","approved_by":null,"mod_note":null,"all_awardings":[],"body":"Max tokens = 128 000  ￼  \\nTotal FLOPs = 2 FLOPs/param × 32b params × tokens = 64 × 10⁹ FLOPs × 128 000 = 8.192 × 10¹⁵ FLOPs  ￼\\nTime = Total FLOPs / RTX3090 FLOPS = 8.192 × 10¹⁵ / (284 × 10¹²) ≈ 28.9 s  ￼\\n\\nIt would take me 28.9secs to do prefill at max 128k context length. Meh. Good enough. \\n\\n64k context would be ~1/4 the time.","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n3t9rmm","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Max tokens = 128 000  ￼&lt;br/&gt;\\nTotal FLOPs = 2 FLOPs/param × 32b params × tokens = 64 × 10⁹ FLOPs × 128 000 = 8.192 × 10¹⁵ FLOPs  ￼\\nTime = Total FLOPs / RTX3090 FLOPS = 8.192 × 10¹⁵ / (284 × 10¹²) ≈ 28.9 s  ￼&lt;/p&gt;\\n\\n&lt;p&gt;It would take me 28.9secs to do prefill at max 128k context length. Meh. Good enough. &lt;/p&gt;\\n\\n&lt;p&gt;64k context would be ~1/4 the time.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1m2xh8s","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2xh8s/run_kimik2_without_quantization_locally_for_under/n3t9rmm/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752845049,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":7,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3t2u4r","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"cantgetthistowork","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3swnp9","score":-2,"author_fullname":"t2_j1i0o","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Expensive is better than unusable. If you're trying to use the full context you'll have unbearable performance halfway through.","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n3t2u4r","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Expensive is better than unusable. If you&amp;#39;re trying to use the full context you&amp;#39;ll have unbearable performance halfway through.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1m2xh8s","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2xh8s/run_kimik2_without_quantization_locally_for_under/n3t2u4r/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752842773,"author_flair_text":null,"treatment_tags":[],"created_utc":1752842773,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":6,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-2}}],"before":null}},"user_reports":[],"saved":false,"id":"n3swnp9","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DepthHour1669","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3sutaz","score":0,"author_fullname":"t2_t6glzswk","approved_by":null,"mod_note":null,"all_awardings":[],"body":"&gt; so you can easily fit 30 of them with the right risers\\n\\nBut you need 43 of them, so that won't work. And again, you're looking at $40k+ in costs.","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n3swnp9","is_submitter":true,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;so you can easily fit 30 of them with the right risers&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;But you need 43 of them, so that won&amp;#39;t work. And again, you&amp;#39;re looking at $40k+ in costs.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1m2xh8s","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2xh8s/run_kimik2_without_quantization_locally_for_under/n3swnp9/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752840536,"author_flair_text":null,"treatment_tags":[],"created_utc":1752840536,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}},"user_reports":[],"saved":false,"id":"n3sutaz","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"cantgetthistowork","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3styt9","score":-4,"author_fullname":"t2_j1i0o","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"You're forgetting the prompt processing is crippled with CPU offload. When you have PCIe 5.0 lanes you can run each card at x4 at close to full speeds. There are boards that already support 15x GPUs at  x8 so you can easily fit 30 of them with the right risers.","edited":false,"author_flair_css_class":null,"name":"t1_n3sutaz","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You&amp;#39;re forgetting the prompt processing is crippled with CPU offload. When you have PCIe 5.0 lanes you can run each card at x4 at close to full speeds. There are boards that already support 15x GPUs at  x8 so you can easily fit 30 of them with the right risers.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1m2xh8s","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2xh8s/run_kimik2_without_quantization_locally_for_under/n3sutaz/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752839820,"author_flair_text":null,"collapsed":false,"created_utc":1752839820,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-4}}],"before":null}},"user_reports":[],"saved":false,"id":"n3styt9","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DepthHour1669","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3ssp92","score":-1,"author_fullname":"t2_t6glzswk","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"That won't work. 3090s are a terrible option for big models.\\n\\nFor one, do you know how much a motherboard that can support all those 3090s costs? \\n\\nSecondly, the 3090 has 936GB/sec bandwidth. So even if you somehow fit the model into 43 RTX 3090s (which will cost you at least $30k), at full speed Kimi K2 will run... 936/32b= **29.25tokens per sec... for over $30k**. \\n\\nThe 12channel DDR5 system I'm describing is 619/32b= 19.3tok/sec at a minimum. Kimi K2 uses a SwiGLU feed‑forward (three weight matrices), so each expert is 44 M params. This means it has a 66/34 weight distribution experts/common. So that means you can load about 11b weights in vram. \\n\\nWith a single 3090Ti (I'm picking the Ti since it's easier to round to 1000GB/sec bandwidth), you'll see 33.9ms for the expert weights and 11ms for the common weights, leading to **22.3tokens/sec for under $10k**.\\n\\nWith a 5090, you'll see 33.9ms for the expert weights and 6.1ms for the common weights, leading to **24.98tokens/sec. Basically 25tok/sec exactly, for about $12k**.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3styt9","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;That won&amp;#39;t work. 3090s are a terrible option for big models.&lt;/p&gt;\\n\\n&lt;p&gt;For one, do you know how much a motherboard that can support all those 3090s costs? &lt;/p&gt;\\n\\n&lt;p&gt;Secondly, the 3090 has 936GB/sec bandwidth. So even if you somehow fit the model into 43 RTX 3090s (which will cost you at least $30k), at full speed Kimi K2 will run... 936/32b= &lt;strong&gt;29.25tokens per sec... for over $30k&lt;/strong&gt;. &lt;/p&gt;\\n\\n&lt;p&gt;The 12channel DDR5 system I&amp;#39;m describing is 619/32b= 19.3tok/sec at a minimum. Kimi K2 uses a SwiGLU feed‑forward (three weight matrices), so each expert is 44 M params. This means it has a 66/34 weight distribution experts/common. So that means you can load about 11b weights in vram. &lt;/p&gt;\\n\\n&lt;p&gt;With a single 3090Ti (I&amp;#39;m picking the Ti since it&amp;#39;s easier to round to 1000GB/sec bandwidth), you&amp;#39;ll see 33.9ms for the expert weights and 11ms for the common weights, leading to &lt;strong&gt;22.3tokens/sec for under $10k&lt;/strong&gt;.&lt;/p&gt;\\n\\n&lt;p&gt;With a 5090, you&amp;#39;ll see 33.9ms for the expert weights and 6.1ms for the common weights, leading to &lt;strong&gt;24.98tokens/sec. Basically 25tok/sec exactly, for about $12k&lt;/strong&gt;.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2xh8s","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2xh8s/run_kimik2_without_quantization_locally_for_under/n3styt9/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752839486,"author_flair_text":null,"treatment_tags":[],"created_utc":1752839486,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-1}}],"before":null}},"user_reports":[],"saved":false,"id":"n3ssp92","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"cantgetthistowork","can_mod_post":false,"send_replies":true,"parent_id":"t1_n3slh3v","score":-3,"author_fullname":"t2_j1i0o","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"You should offload minimal layers onto CPU. You can offload only up layers. 16x3090s is 384GB and costs slightly over 10k. Fill the rest with GPUs. The speeds will be miles ahead.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n3ssp92","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You should offload minimal layers onto CPU. You can offload only up layers. 16x3090s is 384GB and costs slightly over 10k. Fill the rest with GPUs. The speeds will be miles ahead.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2xh8s","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2xh8s/run_kimik2_without_quantization_locally_for_under/n3ssp92/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752838974,"author_flair_text":null,"treatment_tags":[],"created_utc":1752838974,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-3}}],"before":null}},"user_reports":[],"saved":false,"id":"n3slh3v","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"DepthHour1669","can_mod_post":false,"created_utc":1752835783,"send_replies":true,"parent_id":"t1_n3sk8oc","score":6,"author_fullname":"t2_t6glzswk","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Ok, you tell me how much it would cost to load the 4bit 547GB Kimi K2 onto GPU vram.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3slh3v","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Ok, you tell me how much it would cost to load the 4bit 547GB Kimi K2 onto GPU vram.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m2xh8s","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2xh8s/run_kimik2_without_quantization_locally_for_under/n3slh3v/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752835783,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}}],"before":null}},"user_reports":[],"saved":false,"id":"n3sk8oc","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":"LOW_SCORE","no_follow":true,"author":"cantgetthistowork","can_mod_post":false,"created_utc":1752835184,"send_replies":true,"parent_id":"t3_1m2xh8s","score":-5,"author_fullname":"t2_j1i0o","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":true,"body":"GPUs make more sense","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n3sk8oc","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;GPUs make more sense&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":"comment score below threshold","distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m2xh8s/run_kimik2_without_quantization_locally_for_under/n3sk8oc/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752835184,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m2xh8s","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-5}}],"before":null}}]`),n=()=>e.jsx(l,{data:t});export{n as default};
