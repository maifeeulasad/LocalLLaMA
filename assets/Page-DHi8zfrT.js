import{j as e}from"./index-BOnf-UhU.js";import{R as t}from"./RedditPostRenderer-Ce39qICS.js";import"./index-CDase6VD.js";const a=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"I need the highest quality I can get for a price point below $1000 in training and $1/M tokens inference. I would prefer to do full finetuning on a base model. It's for a continuation task (writing with long range dependency) so I don't actually need or want chat or instruct style. I need context 32K.\\n\\nI have about 200M tokens of finetuning data which I can augment to 1B easily by doing different variations.\\n\\nMy opinions are:\\n1. Finetune Gemini Flash 2.0. They're using a LoRA. It'll cost $800, but then I can infer for $0.30/M on batch.\\n2. Finetune Qwen2.5 or Llama 3.3 either 70B or 32B. Might cost a bit more. Inference could be cheaper if I use 4bit quantization, otherwise probably a slightly more expensive, and a lot more difficult to maintain.\\n\\nBut ultimately in the end I care about the quality output. I don't really want to test both because of the time and money it would take to do so.\\nWhich do you think would give the better output?\\n\\nI'm torn. It seems to me I'd be able to train it better if I train the full base model on 1B tokens. That would probably be a bit expensive to train.\\nYet Gemini might just be a better model in the first place. It's hard to tell because Gemini Flash 2.0 is absolutely amazing at some things, stuff that none of the Open Source can do like editing a massive block of text and actually responsing with the entire thing every time instead of secretly deleting sentences here and there. Then some other stuff it doesn't do so well. So it *might* actually be a small model that's really really well trained (or 100 tiny experts), in which case a LoRA on that might not be able to keep my task up for 32K tokens.\\n\\nSince I'm only training one task (actually 2 but they're related) I don't need or want experts, or thinking.\\n\\nOn the other hand it's cheaper and easier to train Flash 2.0 by a lot.\\n\\nDoes anyone have any personal insight into my dilemma?","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Should I do finetuning on Gemini or on open source models?","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1m7e8d0","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.75,"author_flair_background_color":null,"subreddit_type":"public","ups":2,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_8jhue7k0","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":2,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1753288860,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;I need the highest quality I can get for a price point below $1000 in training and $1/M tokens inference. I would prefer to do full finetuning on a base model. It&amp;#39;s for a continuation task (writing with long range dependency) so I don&amp;#39;t actually need or want chat or instruct style. I need context 32K.&lt;/p&gt;\\n\\n&lt;p&gt;I have about 200M tokens of finetuning data which I can augment to 1B easily by doing different variations.&lt;/p&gt;\\n\\n&lt;p&gt;My opinions are:\\n1. Finetune Gemini Flash 2.0. They&amp;#39;re using a LoRA. It&amp;#39;ll cost $800, but then I can infer for $0.30/M on batch.\\n2. Finetune Qwen2.5 or Llama 3.3 either 70B or 32B. Might cost a bit more. Inference could be cheaper if I use 4bit quantization, otherwise probably a slightly more expensive, and a lot more difficult to maintain.&lt;/p&gt;\\n\\n&lt;p&gt;But ultimately in the end I care about the quality output. I don&amp;#39;t really want to test both because of the time and money it would take to do so.\\nWhich do you think would give the better output?&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;m torn. It seems to me I&amp;#39;d be able to train it better if I train the full base model on 1B tokens. That would probably be a bit expensive to train.\\nYet Gemini might just be a better model in the first place. It&amp;#39;s hard to tell because Gemini Flash 2.0 is absolutely amazing at some things, stuff that none of the Open Source can do like editing a massive block of text and actually responsing with the entire thing every time instead of secretly deleting sentences here and there. Then some other stuff it doesn&amp;#39;t do so well. So it &lt;em&gt;might&lt;/em&gt; actually be a small model that&amp;#39;s really really well trained (or 100 tiny experts), in which case a LoRA on that might not be able to keep my task up for 32K tokens.&lt;/p&gt;\\n\\n&lt;p&gt;Since I&amp;#39;m only training one task (actually 2 but they&amp;#39;re related) I don&amp;#39;t need or want experts, or thinking.&lt;/p&gt;\\n\\n&lt;p&gt;On the other hand it&amp;#39;s cheaper and easier to train Flash 2.0 by a lot.&lt;/p&gt;\\n\\n&lt;p&gt;Does anyone have any personal insight into my dilemma?&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1m7e8d0","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"Pan000","discussion_type":null,"num_comments":4,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1m7e8d0/should_i_do_finetuning_on_gemini_or_on_open/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1m7e8d0/should_i_do_finetuning_on_gemini_or_on_open/","subreddit_subscribers":503518,"created_utc":1753288860,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4r80xf","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"a_slay_nub","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4r1qhe","score":1,"author_fullname":"t2_u8o4d","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"It will never be just one training run, I would budget for multiple failures.\\n\\nThe real cost though is your time, I assume you work for a business, your cost to learn how to utilize runpod, set up the training, manage the training, and everything will greatly exceed any compute costs you have. My time costs the company ~$160/hr, if I have to take just 8 extra hours to do something on runpod vs just calling Google, that's the cost difference there.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n4r80xf","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It will never be just one training run, I would budget for multiple failures.&lt;/p&gt;\\n\\n&lt;p&gt;The real cost though is your time, I assume you work for a business, your cost to learn how to utilize runpod, set up the training, manage the training, and everything will greatly exceed any compute costs you have. My time costs the company ~$160/hr, if I have to take just 8 extra hours to do something on runpod vs just calling Google, that&amp;#39;s the cost difference there.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m7e8d0","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7e8d0/should_i_do_finetuning_on_gemini_or_on_open/n4r80xf/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753294114,"author_flair_text":null,"treatment_tags":[],"created_utc":1753294114,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4r1qhe","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Pan000","can_mod_post":false,"created_utc":1753292398,"send_replies":true,"parent_id":"t1_n4qwy73","score":1,"author_fullname":"t2_8jhue7k0","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Thanks for your reply. Why do you say orders of magnitude? From my research I can train it on let's say 8x H200 in a day or two. The setup for that is in theory straightforward, though I know something always goes wrong. That's around $1000 on Runpod. For inference I could pack it into a couple of 5090s with 4-bit quantization, or a single decent GPU in 8bit. I'd need to run it for probably 2 hours a day, so I could in theory sort out a Docker for it and rent a GPU for a couple hours every day for $10 a day. That's roughly equivalent in price to Gemini, though a lot more work. Like I said, it's really quality that my decision is about.\\n\\nI wonder how much of the difference between open and closed source is to do with poor quality finetuning data..? If I'm training for a single task with examples for literally everything it's doing, and it only has to infer the subject matter... that's why I'm thinking it might work better doing a full finetune on a base model. I don't know how much of the magic of the closed source models is just quality of the training data but I suspect it's a large part of it.\\n\\nIf anybody can shed any light on this I'd appreciate it.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4r1qhe","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thanks for your reply. Why do you say orders of magnitude? From my research I can train it on let&amp;#39;s say 8x H200 in a day or two. The setup for that is in theory straightforward, though I know something always goes wrong. That&amp;#39;s around $1000 on Runpod. For inference I could pack it into a couple of 5090s with 4-bit quantization, or a single decent GPU in 8bit. I&amp;#39;d need to run it for probably 2 hours a day, so I could in theory sort out a Docker for it and rent a GPU for a couple hours every day for $10 a day. That&amp;#39;s roughly equivalent in price to Gemini, though a lot more work. Like I said, it&amp;#39;s really quality that my decision is about.&lt;/p&gt;\\n\\n&lt;p&gt;I wonder how much of the difference between open and closed source is to do with poor quality finetuning data..? If I&amp;#39;m training for a single task with examples for literally everything it&amp;#39;s doing, and it only has to infer the subject matter... that&amp;#39;s why I&amp;#39;m thinking it might work better doing a full finetune on a base model. I don&amp;#39;t know how much of the magic of the closed source models is just quality of the training data but I suspect it&amp;#39;s a large part of it.&lt;/p&gt;\\n\\n&lt;p&gt;If anybody can shed any light on this I&amp;#39;d appreciate it.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m7e8d0","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7e8d0/should_i_do_finetuning_on_gemini_or_on_open/n4r1qhe/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753292398,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4qwy73","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"a_slay_nub","can_mod_post":false,"created_utc":1753291130,"send_replies":true,"parent_id":"t3_1m7e8d0","score":2,"author_fullname":"t2_u8o4d","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"This is r/localllama so our default answer is local all the way.\\n\\nThat said, if you're okay with using SAS, the answer is almost always SAS unless you're a huge org. Gemini Flash 2.0 is a much better model than Llama 3.3 or Qwen 2.5 and it will be cheaper and easier for you to fine-tune. Note that both qwen and llama struggle with long context as well.\\n\\nIn addition, once the model is trained, Google manages the deployment for you and is reasonably cheap. You'd be looking at 1-2 orders of magnitude more in cost to set this up yourself.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4qwy73","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;This is &lt;a href=\\"/r/localllama\\"&gt;r/localllama&lt;/a&gt; so our default answer is local all the way.&lt;/p&gt;\\n\\n&lt;p&gt;That said, if you&amp;#39;re okay with using SAS, the answer is almost always SAS unless you&amp;#39;re a huge org. Gemini Flash 2.0 is a much better model than Llama 3.3 or Qwen 2.5 and it will be cheaper and easier for you to fine-tune. Note that both qwen and llama struggle with long context as well.&lt;/p&gt;\\n\\n&lt;p&gt;In addition, once the model is trained, Google manages the deployment for you and is reasonably cheap. You&amp;#39;d be looking at 1-2 orders of magnitude more in cost to set this up yourself.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7e8d0/should_i_do_finetuning_on_gemini_or_on_open/n4qwy73/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753291130,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m7e8d0","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4r72xu","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"my_name_isnt_clever","can_mod_post":false,"created_utc":1753293847,"send_replies":true,"parent_id":"t3_1m7e8d0","score":1,"author_fullname":"t2_5o567","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I just want to add, if you rely on a closed source model and then Google makes some poor choices that affect your use case you don't have many options. With an open weights model it's fully in your own control and will never change unless you decide to change it.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4r72xu","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I just want to add, if you rely on a closed source model and then Google makes some poor choices that affect your use case you don&amp;#39;t have many options. With an open weights model it&amp;#39;s fully in your own control and will never change unless you decide to change it.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7e8d0/should_i_do_finetuning_on_gemini_or_on_open/n4r72xu/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753293847,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m7e8d0","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),i=()=>e.jsx(t,{data:a});export{i as default};
