import{j as e}from"./index-CWmJdUH_.js";import{R as l}from"./RedditPostRenderer-D2iunoQ9.js";import"./index-BCg9RP6g.js";const a=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Many AI models are built for lower CUDA versions, mostly 12.1-12.2 \\nWhy wouldn't I just buy 2x3090 that will end up with pretty much same speed with bigger vRAM?","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Why 5090 for inference if min CUDA is 12.9","link_flair_richtext":[{"e":"text","t":"Discussion"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lsm0ua","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.38,"author_flair_background_color":null,"subreddit_type":"public","ups":0,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_qrnpi","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Discussion","can_mod_post":false,"score":0,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1751754862,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Many AI models are built for lower CUDA versions, mostly 12.1-12.2 \\nWhy wouldn&amp;#39;t I just buy 2x3090 that will end up with pretty much same speed with bigger vRAM?&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":true,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"5f921ea4-c7bc-11ed-9c23-3a00622979b4","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#646d73","id":"1lsm0ua","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"VihmaVillu","discussion_type":null,"num_comments":3,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lsm0ua/why_5090_for_inference_if_min_cuda_is_129/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lsm0ua/why_5090_for_inference_if_min_cuda_is_129/","subreddit_subscribers":494897,"created_utc":1751754862,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1jpn3f","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"TheRealMasonMac","can_mod_post":false,"created_utc":1751756526,"send_replies":true,"parent_id":"t3_1lsm0ua","score":3,"author_fullname":"t2_101haj","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"LLM models are not tied tied to any GPU. They're just data to be processed, like a photo or text file.\\n\\nAs an example, a new android phones might release with a higher android version than what a specific gallery app supports. It doesn't mean you're unable to see the photos. It just means you either wait for the app to update or use a different one.\\n\\n\\nI'm also pretty sure CUDA 12.9 is supported by the major inference frameworks by now.","edited":1751756711,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1jpn3f","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;LLM models are not tied tied to any GPU. They&amp;#39;re just data to be processed, like a photo or text file.&lt;/p&gt;\\n\\n&lt;p&gt;As an example, a new android phones might release with a higher android version than what a specific gallery app supports. It doesn&amp;#39;t mean you&amp;#39;re unable to see the photos. It just means you either wait for the app to update or use a different one.&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;m also pretty sure CUDA 12.9 is supported by the major inference frameworks by now.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lsm0ua/why_5090_for_inference_if_min_cuda_is_129/n1jpn3f/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751756526,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lsm0ua","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1jsdp2","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Conscious_Cut_6144","can_mod_post":false,"created_utc":1751757529,"send_replies":true,"parent_id":"t3_1lsm0ua","score":2,"author_fullname":"t2_9hl4ymvj","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"The cuda version is not relevant (and also the 5090 requires 12.8+, but that's beside the point)    \\nYou can build llama.cpp with 12.1 or 12.8 or whatever you want.    \\nSame goes for VLLM and basically every inference tool.\\n\\nNow for the dual 3090 vs 5090 comparison it's a bit more complicated.    \\n\\\\-You need a more powerful PSU, a larger case, and a more expensive motherboard to support dual 3090's.    \\n\\\\-Video/Image stuff doesn't scale well at all to additional GPU's    \\n\\\\-llama.cpp also doesn't scale well to additional GPU's.    \\nIf the model fits in 32GB, the 5090 will be significantly faster than 2 3090's on llama.cpp\\n\\nCounter points:    \\nvllm with tensor parallel you are likely to get similar speeds between the 3090's and a 5090.    \\nIf your model doesn't fit on 32GB (70b class models for one) then you are much better off with the 3090's","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1jsdp2","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The cuda version is not relevant (and also the 5090 requires 12.8+, but that&amp;#39;s beside the point)&lt;br/&gt;\\nYou can build llama.cpp with 12.1 or 12.8 or whatever you want.&lt;br/&gt;\\nSame goes for VLLM and basically every inference tool.&lt;/p&gt;\\n\\n&lt;p&gt;Now for the dual 3090 vs 5090 comparison it&amp;#39;s a bit more complicated.&lt;br/&gt;\\n-You need a more powerful PSU, a larger case, and a more expensive motherboard to support dual 3090&amp;#39;s.&lt;br/&gt;\\n-Video/Image stuff doesn&amp;#39;t scale well at all to additional GPU&amp;#39;s&lt;br/&gt;\\n-llama.cpp also doesn&amp;#39;t scale well to additional GPU&amp;#39;s.&lt;br/&gt;\\nIf the model fits in 32GB, the 5090 will be significantly faster than 2 3090&amp;#39;s on llama.cpp&lt;/p&gt;\\n\\n&lt;p&gt;Counter points:&lt;br/&gt;\\nvllm with tensor parallel you are likely to get similar speeds between the 3090&amp;#39;s and a 5090.&lt;br/&gt;\\nIf your model doesn&amp;#39;t fit on 32GB (70b class models for one) then you are much better off with the 3090&amp;#39;s&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lsm0ua/why_5090_for_inference_if_min_cuda_is_129/n1jsdp2/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751757529,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lsm0ua","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1jqmq7","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ieatdownvotes4food","can_mod_post":false,"created_utc":1751756893,"send_replies":true,"parent_id":"t3_1lsm0ua","score":1,"author_fullname":"t2_12fv96rb","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Ive been using 12.8, but not too hard to get things up to speed in general. Blackwell has been out for a minute","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1jqmq7","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Ive been using 12.8, but not too hard to get things up to speed in general. Blackwell has been out for a minute&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lsm0ua/why_5090_for_inference_if_min_cuda_is_129/n1jqmq7/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751756893,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lsm0ua","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),s=()=>e.jsx(l,{data:a});export{s as default};
