import{j as e}from"./index-Cd3v0jxz.js";import{R as t}from"./RedditPostRenderer-cI96YLhy.js";import"./index-DGBoKyQm.js";const a=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"I'm new to running LLM's locally and have been working on a new project that has an \\"AI powered\\" requirement... I've learned a ton in the process but feel like I'm missing something.\\n\\nThe idea is to take a large csv that has been aggregated and formatted from various other sources, then feed that to an LLM that can identify trends, flag items that need attention, allow queries etc... but it can't use 3rd party API's\\n\\nI'm using self hosted Open Web UI API as my backend with Ollama and Mistral behind it all running on a 64GB AWS EC2 instance CPU only.   \\n  \\nThe file is too large to fit into the context window alone so I tried using the Files / Knowledge / RAG functionality that comes with OpenWebUI but that seems to really struggle to understand the entire dataset. \\n\\nFor example it's unable to tell me how many lines are in the file, or which item ID appears most often. \\n\\nJust curious if I'm going about this all wrong. Is this even realistic?\\n\\n","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Analyzing CSV and structured data - RAG, MCP, tools, or plain old scripting?","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1m7mu6e","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.75,"author_flair_background_color":null,"subreddit_type":"public","ups":2,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_jlnyy","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":2,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1753308541,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;m new to running LLM&amp;#39;s locally and have been working on a new project that has an &amp;quot;AI powered&amp;quot; requirement... I&amp;#39;ve learned a ton in the process but feel like I&amp;#39;m missing something.&lt;/p&gt;\\n\\n&lt;p&gt;The idea is to take a large csv that has been aggregated and formatted from various other sources, then feed that to an LLM that can identify trends, flag items that need attention, allow queries etc... but it can&amp;#39;t use 3rd party API&amp;#39;s&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;m using self hosted Open Web UI API as my backend with Ollama and Mistral behind it all running on a 64GB AWS EC2 instance CPU only.   &lt;/p&gt;\\n\\n&lt;p&gt;The file is too large to fit into the context window alone so I tried using the Files / Knowledge / RAG functionality that comes with OpenWebUI but that seems to really struggle to understand the entire dataset. &lt;/p&gt;\\n\\n&lt;p&gt;For example it&amp;#39;s unable to tell me how many lines are in the file, or which item ID appears most often. &lt;/p&gt;\\n\\n&lt;p&gt;Just curious if I&amp;#39;m going about this all wrong. Is this even realistic?&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1m7mu6e","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"Tactical_Chicken","discussion_type":null,"num_comments":4,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1m7mu6e/analyzing_csv_and_structured_data_rag_mcp_tools/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1m7mu6e/analyzing_csv_and_structured_data_rag_mcp_tools/","subreddit_subscribers":503759,"created_utc":1753308541,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4ux2e5","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Tactical_Chicken","can_mod_post":false,"created_utc":1753340829,"send_replies":true,"parent_id":"t1_n4tauha","score":1,"author_fullname":"t2_jlnyy","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"But how does something like google sheets or even chat gpt deliver reliable results when asking those same kind of questions? I understand they have a lot more resources and still not always reliable ;)\\n\\nBut do they look at queries and if they seem to fit a specific pattern they call a tool or script?\\n\\nFor example if a user asks \\"Whats the sum of B\\" or \\"how many times has B happened\\" does it look to see if the query matches a certain pattern, pass the params off to a tool called \\"sum\\\\_column.sh\\" and then inject the result into the response?","edited":1753341525,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4ux2e5","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;But how does something like google sheets or even chat gpt deliver reliable results when asking those same kind of questions? I understand they have a lot more resources and still not always reliable ;)&lt;/p&gt;\\n\\n&lt;p&gt;But do they look at queries and if they seem to fit a specific pattern they call a tool or script?&lt;/p&gt;\\n\\n&lt;p&gt;For example if a user asks &amp;quot;Whats the sum of B&amp;quot; or &amp;quot;how many times has B happened&amp;quot; does it look to see if the query matches a certain pattern, pass the params off to a tool called &amp;quot;sum_column.sh&amp;quot; and then inject the result into the response?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m7mu6e","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7mu6e/analyzing_csv_and_structured_data_rag_mcp_tools/n4ux2e5/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753340829,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4tauha","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"ttkciar","can_mod_post":false,"created_utc":1753316514,"send_replies":true,"parent_id":"t3_1m7mu6e","score":2,"author_fullname":"t2_cpegz","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Some of those things (like counting lines and how many IDs appear most often) are tasks for which LLMs are not very good, but are simple and easy to do with scripting.\\n\\nAs a general rule, if something is easy and obvious to do with scripting, you should go ahead and script it.  It will work more reliably, orders of magnitude more quickly, and with a lot less compute and RAM.  If a task is too vague, fluid, or hard to define to allow for an obvious scripting solution, try LLM inference.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4tauha","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Some of those things (like counting lines and how many IDs appear most often) are tasks for which LLMs are not very good, but are simple and easy to do with scripting.&lt;/p&gt;\\n\\n&lt;p&gt;As a general rule, if something is easy and obvious to do with scripting, you should go ahead and script it.  It will work more reliably, orders of magnitude more quickly, and with a lot less compute and RAM.  If a task is too vague, fluid, or hard to define to allow for an obvious scripting solution, try LLM inference.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7mu6e/analyzing_csv_and_structured_data_rag_mcp_tools/n4tauha/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753316514,"author_flair_text":"llama.cpp","treatment_tags":[],"link_id":"t3_1m7mu6e","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4tmsie","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"HistorianPotential48","can_mod_post":false,"created_utc":1753320680,"send_replies":true,"parent_id":"t3_1m7mu6e","score":2,"author_fullname":"t2_4dzthia7","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I'd suggest storing this csv into a database, then tell LLM what does schema looks like and it can write queries to check things out. Then this becomes a summarization task after multiple queries and agent's own messages. You'll need to clarify requirements to LLM - what's the condition for the LLM to decide if it already looked enough for seeking trends? What's the condition of items needing attention?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4tmsie","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;d suggest storing this csv into a database, then tell LLM what does schema looks like and it can write queries to check things out. Then this becomes a summarization task after multiple queries and agent&amp;#39;s own messages. You&amp;#39;ll need to clarify requirements to LLM - what&amp;#39;s the condition for the LLM to decide if it already looked enough for seeking trends? What&amp;#39;s the condition of items needing attention?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m7mu6e/analyzing_csv_and_structured_data_rag_mcp_tools/n4tmsie/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753320680,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m7mu6e","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}}]`),o=()=>e.jsx(t,{data:a});export{o as default};
