import{j as e}from"./index-BlGsFJYy.js";import{R as l}from"./RedditPostRenderer-B6uvq_Zl.js";import"./index-DDvVtNwD.js";const t=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Are there any local models that have high precision integer or decimal when doing calculations. The paid big guys do,  claude sonnet 4 , gemini 2.5 pro, Chatgpt. . But can't find any downloadable ones (tried up to 70b) . Anything above 24 or 32 digits they just give incorrect results.  ","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"High Precision","link_flair_richtext":[{"e":"text","t":"Discussion"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lv5uie","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.67,"author_flair_background_color":null,"subreddit_type":"public","ups":2,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_1gzvdilba8","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Discussion","can_mod_post":false,"score":2,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1752023315,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Are there any local models that have high precision integer or decimal when doing calculations. The paid big guys do,  claude sonnet 4 , gemini 2.5 pro, Chatgpt. . But can&amp;#39;t find any downloadable ones (tried up to 70b) . Anything above 24 or 32 digits they just give incorrect results.  &lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"5f921ea4-c7bc-11ed-9c23-3a00622979b4","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#646d73","id":"1lv5uie","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"AlgorithmicMuse","discussion_type":null,"num_comments":14,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lv5uie/high_precision/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lv5uie/high_precision/","subreddit_subscribers":496591,"created_utc":1752023315,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n243c40","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AlgorithmicMuse","can_mod_post":false,"send_replies":true,"parent_id":"t1_n23z3bs","score":2,"author_fullname":"t2_1gzvdilba8","approved_by":null,"mod_note":null,"all_awardings":[],"body":"Thank you , that looks super helpful","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n243c40","is_submitter":true,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thank you , that looks super helpful&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lv5uie","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lv5uie/high_precision/n243c40/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752032523,"author_flair_text":null,"treatment_tags":[],"created_utc":1752032523,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n23z3bs","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DinoAmino","can_mod_post":false,"send_replies":true,"parent_id":"t1_n23y50a","score":1,"author_fullname":"t2_j1v7f","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"If you are looking for the best performing tool/function calling models there is a leaderboard for that. Unsurprisingly, the best ones are fine-tuned for this purpose\\n\\nhttps://gorilla.cs.berkeley.edu/leaderboard.html","edited":false,"author_flair_css_class":null,"name":"t1_n23z3bs","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;If you are looking for the best performing tool/function calling models there is a leaderboard for that. Unsurprisingly, the best ones are fine-tuned for this purpose&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://gorilla.cs.berkeley.edu/leaderboard.html\\"&gt;https://gorilla.cs.berkeley.edu/leaderboard.html&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lv5uie","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lv5uie/high_precision/n23z3bs/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752030859,"author_flair_text":null,"collapsed":false,"created_utc":1752030859,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n23y50a","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AlgorithmicMuse","can_mod_post":false,"send_replies":true,"parent_id":"t1_n23wc9m","score":0,"author_fullname":"t2_1gzvdilba8","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Yes, im only looking for some model names vs testing the plethora of models available.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n23y50a","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yes, im only looking for some model names vs testing the plethora of models available.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lv5uie","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lv5uie/high_precision/n23y50a/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752030498,"author_flair_text":null,"treatment_tags":[],"created_utc":1752030498,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}},"user_reports":[],"saved":false,"id":"n23wc9m","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DinoAmino","can_mod_post":false,"send_replies":true,"parent_id":"t1_n23ng7d","score":1,"author_fullname":"t2_j1v7f","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Are you familiar with how tool calling works? The LLM does not do the calculation - it \\"hands it off\\" to local code by the inferencing engine. And the model is made aware of the tools that you have configured in the system prompt. So yeah, you *do* need to write the tool code that needs to execute. Or find the MCP that works for you.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n23wc9m","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Are you familiar with how tool calling works? The LLM does not do the calculation - it &amp;quot;hands it off&amp;quot; to local code by the inferencing engine. And the model is made aware of the tools that you have configured in the system prompt. So yeah, you &lt;em&gt;do&lt;/em&gt; need to write the tool code that needs to execute. Or find the MCP that works for you.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lv5uie","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lv5uie/high_precision/n23wc9m/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752029833,"author_flair_text":null,"treatment_tags":[],"created_utc":1752029833,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n23ng7d","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AlgorithmicMuse","can_mod_post":false,"created_utc":1752026708,"send_replies":true,"parent_id":"t1_n23fq6b","score":2,"author_fullname":"t2_1gzvdilba8","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":" Correct, the big guys use internal , Claude uses Javascripts BigInt for integers to write its code to generate solutions) gemini does the same thing but won't show what it's using for high precision . Simple prompting just Calculate using high precision, always worked so far\\n\\n That does not work using local, I was able to tell deepseek -r1:70b using ollama to use pythons math.factorial and it got correct 90 digit precision, but had to lead it to the correct output.  All seems a waste of time to use local if I have to lead them to get the correct solutions, faster just rolling my own code.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n23ng7d","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Correct, the big guys use internal , Claude uses Javascripts BigInt for integers to write its code to generate solutions) gemini does the same thing but won&amp;#39;t show what it&amp;#39;s using for high precision . Simple prompting just Calculate using high precision, always worked so far&lt;/p&gt;\\n\\n&lt;p&gt;That does not work using local, I was able to tell deepseek -r1:70b using ollama to use pythons math.factorial and it got correct 90 digit precision, but had to lead it to the correct output.  All seems a waste of time to use local if I have to lead them to get the correct solutions, faster just rolling my own code.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lv5uie","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lv5uie/high_precision/n23ng7d/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752026708,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n23fq6b","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"zoupishness7","can_mod_post":false,"created_utc":1752024104,"send_replies":true,"parent_id":"t3_1lv5uie","score":7,"author_fullname":"t2_disot","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Getting accurate calculations out of an LLM requires tool use. It needs to write code that it can then execute. So you need an agentic system. Something like [Open Interpreter](https://github.com/OpenInterpreter/open-interpreter) might work for you, since your use case is fairly simple and straightforward.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n23fq6b","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Getting accurate calculations out of an LLM requires tool use. It needs to write code that it can then execute. So you need an agentic system. Something like &lt;a href=\\"https://github.com/OpenInterpreter/open-interpreter\\"&gt;Open Interpreter&lt;/a&gt; might work for you, since your use case is fairly simple and straightforward.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lv5uie/high_precision/n23fq6b/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752024104,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lv5uie","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":7}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n23p0ge","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AlgorithmicMuse","can_mod_post":false,"created_utc":1752027239,"send_replies":true,"parent_id":"t1_n23f5bz","score":3,"author_fullname":"t2_1gzvdilba8","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I will be the mcps and roll my own, seems easiest solution","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n23p0ge","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I will be the mcps and roll my own, seems easiest solution&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lv5uie","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lv5uie/high_precision/n23p0ge/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752027239,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n23f5bz","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"International_Air700","can_mod_post":false,"created_utc":1752023906,"send_replies":true,"parent_id":"t3_1lv5uie","score":5,"author_fullname":"t2_aml42hwr","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Get some mcps for your model, LLMs are not designed for that kind of jobs","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n23f5bz","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Get some mcps for your model, LLMs are not designed for that kind of jobs&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lv5uie/high_precision/n23f5bz/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752023906,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lv5uie","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n24gk3r","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AlgorithmicMuse","can_mod_post":false,"created_utc":1752038286,"send_replies":true,"parent_id":"t1_n248qok","score":1,"author_fullname":"t2_1gzvdilba8","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I just tried qwen3:32b.  Gave it a factorial calculation that was 90 digits. It used pythons math.factorial to obtainthe answer. Once it did that its no longer predicting  anything and it chose the correct method.   That was the good news. Bad was that after 6 tries it was getting better,  but only  got to the first  30 correct digits the next 60 incorrect .  I coded it as well and python gave the correct answer to 90 digits. . Not sure why qwen kept getting it incorrect .  Seemed it was either in using math.factorial wrong  or potentially getting the correct answer and outputting it wrong.  Anyway all interesting in testing it.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n24gk3r","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I just tried qwen3:32b.  Gave it a factorial calculation that was 90 digits. It used pythons math.factorial to obtainthe answer. Once it did that its no longer predicting  anything and it chose the correct method.   That was the good news. Bad was that after 6 tries it was getting better,  but only  got to the first  30 correct digits the next 60 incorrect .  I coded it as well and python gave the correct answer to 90 digits. . Not sure why qwen kept getting it incorrect .  Seemed it was either in using math.factorial wrong  or potentially getting the correct answer and outputting it wrong.  Anyway all interesting in testing it.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lv5uie","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lv5uie/high_precision/n24gk3r/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752038286,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n248qok","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ArsNeph","can_mod_post":false,"created_utc":1752034756,"send_replies":true,"parent_id":"t3_1lv5uie","score":1,"author_fullname":"t2_vt0xkv60d","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"The best models at math are the ones that score highly on the MATH benchmark, so namely the larger Qwen 3 models. That said, no matter how good it is, it is just a token prediction algorithm, it cannot be trusted to reliably do math by itself. You should use tool calling with a calculator MCP.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n248qok","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The best models at math are the ones that score highly on the MATH benchmark, so namely the larger Qwen 3 models. That said, no matter how good it is, it is just a token prediction algorithm, it cannot be trusted to reliably do math by itself. You should use tool calling with a calculator MCP.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lv5uie/high_precision/n248qok/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752034756,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lv5uie","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n23xv7j","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AlgorithmicMuse","can_mod_post":false,"send_replies":true,"parent_id":"t1_n23u5n0","score":2,"author_fullname":"t2_1gzvdilba8","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I was only looking for the names vs the plethora of locals available","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n23xv7j","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I was only looking for the names vs the plethora of locals available&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lv5uie","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lv5uie/high_precision/n23xv7j/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752030397,"author_flair_text":null,"treatment_tags":[],"created_utc":1752030397,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n23u5n0","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DinoAmino","can_mod_post":false,"send_replies":true,"parent_id":"t1_n23q6y3","score":1,"author_fullname":"t2_j1v7f","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Oh, ok. Yeah, so tool calling is a capability that many new local models have - just like the commercial models do. And like most things, smaller parameter models aren't particularly great at it. But it's generally available.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n23u5n0","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Oh, ok. Yeah, so tool calling is a capability that many new local models have - just like the commercial models do. And like most things, smaller parameter models aren&amp;#39;t particularly great at it. But it&amp;#39;s generally available.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lv5uie","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lv5uie/high_precision/n23u5n0/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752029040,"author_flair_text":null,"treatment_tags":[],"created_utc":1752029040,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n23q6y3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AlgorithmicMuse","can_mod_post":false,"created_utc":1752027641,"send_replies":true,"parent_id":"t1_n23o4y7","score":2,"author_fullname":"t2_1gzvdilba8","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"You lost me, i dont expect llm to do anything other thsn being another tool in a toolbox. I'm not burning any tokens. The difference is the big boys follow prompts for high precision to generate their own code to get the result.  Are those the tools you are talking about.  The locals don't seem to do that , but I was able to guide deepseek into doing it but not any others so far. But still a pain to guide it","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n23q6y3","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You lost me, i dont expect llm to do anything other thsn being another tool in a toolbox. I&amp;#39;m not burning any tokens. The difference is the big boys follow prompts for high precision to generate their own code to get the result.  Are those the tools you are talking about.  The locals don&amp;#39;t seem to do that , but I was able to guide deepseek into doing it but not any others so far. But still a pain to guide it&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lv5uie","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lv5uie/high_precision/n23q6y3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752027641,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n23o4y7","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"DinoAmino","can_mod_post":false,"created_utc":1752026941,"send_replies":true,"parent_id":"t3_1lv5uie","score":1,"author_fullname":"t2_j1v7f","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Too many people unreasonably expect LLMs to be a replacement for just about everything. Language is LLM's middle name. They are text generators. I'm sure the Big boy cloud models use basic calculator tools like a boss in order to get that sort of precision. Why burn tokens on a GPU for that if you don't have to?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n23o4y7","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Too many people unreasonably expect LLMs to be a replacement for just about everything. Language is LLM&amp;#39;s middle name. They are text generators. I&amp;#39;m sure the Big boy cloud models use basic calculator tools like a boss in order to get that sort of precision. Why burn tokens on a GPU for that if you don&amp;#39;t have to?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lv5uie/high_precision/n23o4y7/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752026941,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lv5uie","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),n=()=>e.jsx(l,{data:t});export{n as default};
