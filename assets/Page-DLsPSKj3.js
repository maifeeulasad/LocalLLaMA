import{j as e}from"./index-Dh2YTDbC.js";import{R as t}from"./RedditPostRenderer-BwWe7STC.js";import"./index-D7FMfiLd.js";const a=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Excited to share Arch-Router, our research and model for LLM routing. Routing to the right LLM is still an elusive problem, riddled with nuance and gotchas. For example:\\n\\nâ€œEmbedding-basedâ€ (or simple intent-classifier) routers sound good on paperâ€”label each prompt via embeddings as â€œsupport,â€ â€œSQL,â€ â€œmath,â€ then hand it to the matching modelâ€”but real chats donâ€™t stay in their lanes. Users bounce between topics, task boundaries blur, and any new feature means retraining the classifier. The result is brittle routing that canâ€™t keep up with multi-turn conversations or fast-moving product requirements.\\n\\n\\"Performance-based\\" routers swing the other way, picking models by benchmark or cost curves. They rack up points on MMLU or MT-Bench yet miss the human tests that matter in production: â€œWill Legal accept this clause?â€ â€œDoes our support tone still feel right?â€ Because these decisions are subjective and domain-specific, benchmark-driven black-box routers often send the wrong model when it counts.\\n\\n**Arch-Router skips both pitfalls by routing on** ***preferences you write in plain language.*** Drop rules like â€œcontract clauses â†’ GPT-4oâ€ or â€œquick travel tips â†’ Gemini-Flash,â€ and our 1.5B auto-regressive router model maps prompt along with the context to your routing policiesâ€”no retraining, no sprawling rules that are encoded in if/else statements. Co-designed with Twilio and Atlassian, it adapts to intent drift, lets you swap in new models with a one-liner, and keeps routing logic in sync with the way you actually judge quality.\\n\\n**Specs**\\n\\n* **Tiny footprint** â€“ 1.5 B params â†’ runs on one modern GPU (or CPU while you play).\\n* **Plug-n-play** â€“ points at any mix of LLM endpoints; adding models needs *zero* retraining.\\n* **SOTA query-to-policy matching** â€“ beats bigger closed models on conversational datasets.\\n* **Cost / latency smart** â€“ push heavy stuff to premium models, everyday queries to the fast ones.\\n\\nExclusively available in Arch (the AI-native proxy for agents): [https://github.com/katanemo/archgw](https://github.com/katanemo/archgw)  \\nðŸ”— Model + code: [https://huggingface.co/katanemo/Arch-Router-1.5B](https://huggingface.co/katanemo/Arch-Router-1.5B)  \\nðŸ“„ Paper / longer read: [https://arxiv.org/abs/2506.16655](https://arxiv.org/abs/2506.16655)","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Arch-Router: The first (and fastest) LLM router that can align to your usage preferences.","link_flair_richtext":[{"e":"text","t":"Resources"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":75,"top_awarded_type":null,"hide_score":false,"name":"t3_1lm3jvm","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.92,"author_flair_background_color":null,"ups":59,"total_awards_received":0,"media_embed":{},"thumbnail_width":140,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_gwq7fd01b","secure_media":null,"is_reddit_media_domain":true,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Resources","can_mod_post":false,"score":59,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"https://external-preview.redd.it/0msLUcdAMY2ZykAPTe2kLzuqXfC343fgyje108UksZ4.png?width=140&amp;height=75&amp;crop=140:75,smart&amp;auto=webp&amp;s=6b2dbcefb96bf7207bdb87ee792de5f0e000c3ac","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"post_hint":"image","content_categories":null,"is_self":false,"subreddit_type":"public","created":1751054437,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"i.redd.it","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Excited to share Arch-Router, our research and model for LLM routing. Routing to the right LLM is still an elusive problem, riddled with nuance and gotchas. For example:&lt;/p&gt;\\n\\n&lt;p&gt;â€œEmbedding-basedâ€ (or simple intent-classifier) routers sound good on paperâ€”label each prompt via embeddings as â€œsupport,â€ â€œSQL,â€ â€œmath,â€ then hand it to the matching modelâ€”but real chats donâ€™t stay in their lanes. Users bounce between topics, task boundaries blur, and any new feature means retraining the classifier. The result is brittle routing that canâ€™t keep up with multi-turn conversations or fast-moving product requirements.&lt;/p&gt;\\n\\n&lt;p&gt;&amp;quot;Performance-based&amp;quot; routers swing the other way, picking models by benchmark or cost curves. They rack up points on MMLU or MT-Bench yet miss the human tests that matter in production: â€œWill Legal accept this clause?â€ â€œDoes our support tone still feel right?â€ Because these decisions are subjective and domain-specific, benchmark-driven black-box routers often send the wrong model when it counts.&lt;/p&gt;\\n\\n&lt;p&gt;&lt;strong&gt;Arch-Router skips both pitfalls by routing on&lt;/strong&gt; &lt;strong&gt;&lt;em&gt;preferences you write in plain language.&lt;/em&gt;&lt;/strong&gt; Drop rules like â€œcontract clauses â†’ GPT-4oâ€ or â€œquick travel tips â†’ Gemini-Flash,â€ and our 1.5B auto-regressive router model maps prompt along with the context to your routing policiesâ€”no retraining, no sprawling rules that are encoded in if/else statements. Co-designed with Twilio and Atlassian, it adapts to intent drift, lets you swap in new models with a one-liner, and keeps routing logic in sync with the way you actually judge quality.&lt;/p&gt;\\n\\n&lt;p&gt;&lt;strong&gt;Specs&lt;/strong&gt;&lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;&lt;strong&gt;Tiny footprint&lt;/strong&gt; â€“ 1.5 B params â†’ runs on one modern GPU (or CPU while you play).&lt;/li&gt;\\n&lt;li&gt;&lt;strong&gt;Plug-n-play&lt;/strong&gt; â€“ points at any mix of LLM endpoints; adding models needs &lt;em&gt;zero&lt;/em&gt; retraining.&lt;/li&gt;\\n&lt;li&gt;&lt;strong&gt;SOTA query-to-policy matching&lt;/strong&gt; â€“ beats bigger closed models on conversational datasets.&lt;/li&gt;\\n&lt;li&gt;&lt;strong&gt;Cost / latency smart&lt;/strong&gt; â€“ push heavy stuff to premium models, everyday queries to the fast ones.&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;p&gt;Exclusively available in Arch (the AI-native proxy for agents): &lt;a href=\\"https://github.com/katanemo/archgw\\"&gt;https://github.com/katanemo/archgw&lt;/a&gt;&lt;br/&gt;\\nðŸ”— Model + code: &lt;a href=\\"https://huggingface.co/katanemo/Arch-Router-1.5B\\"&gt;https://huggingface.co/katanemo/Arch-Router-1.5B&lt;/a&gt;&lt;br/&gt;\\nðŸ“„ Paper / longer read: &lt;a href=\\"https://arxiv.org/abs/2506.16655\\"&gt;https://arxiv.org/abs/2506.16655&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"url_overridden_by_dest":"https://i.redd.it/6zqw0rkhzi9f1.png","view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"preview":{"images":[{"source":{"url":"https://preview.redd.it/6zqw0rkhzi9f1.png?auto=webp&amp;s=36e5d972417dd422fef185e975da2e90265f93ac","width":1630,"height":882},"resolutions":[{"url":"https://preview.redd.it/6zqw0rkhzi9f1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=b712f804f1db5610e134be3d9e50702d5eb9f53d","width":108,"height":58},{"url":"https://preview.redd.it/6zqw0rkhzi9f1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=42abe055b279227f32cb6b27c2df2d76b3436723","width":216,"height":116},{"url":"https://preview.redd.it/6zqw0rkhzi9f1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=16da199d5baef424528bda8a666f0e3a5d79e117","width":320,"height":173},{"url":"https://preview.redd.it/6zqw0rkhzi9f1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=b84cb94dea055e799bbb2285e64e2b597538da36","width":640,"height":346},{"url":"https://preview.redd.it/6zqw0rkhzi9f1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=5e902e0b83cd2768d0d7354d059ab1996beee55e","width":960,"height":519},{"url":"https://preview.redd.it/6zqw0rkhzi9f1.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=c2b0fa4a5a89dc4b9aeaa4b1b590658beb016845","width":1080,"height":584}],"variants":{},"id":"0msLUcdAMY2ZykAPTe2kLzuqXfC343fgyje108UksZ4"}],"enabled":true},"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"ab9120c4-bf8e-11ed-ae5e-2eb8b7c7e10b","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"mod_note":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"num_reports":null,"removal_reason":null,"link_flair_background_color":"#ccac2b","id":"1lm3jvm","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"AdditionalWeb107","discussion_type":null,"num_comments":12,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lm3jvm/archrouter_the_first_and_fastest_llm_router_that/","stickied":false,"url":"https://i.redd.it/6zqw0rkhzi9f1.png","subreddit_subscribers":492232,"created_utc":1751054437,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n07ys6b","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Saegifu","can_mod_post":false,"created_utc":1751107452,"send_replies":true,"parent_id":"t1_n05h6up","score":1,"author_fullname":"t2_8qkiqip","approved_by":null,"mod_note":null,"all_awardings":[],"body":"This conversation is so wholesome. Pure camaraderie","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n07ys6b","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;This conversation is so wholesome. Pure camaraderie&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lm3jvm","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lm3jvm/archrouter_the_first_and_fastest_llm_router_that/n07ys6b/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751107452,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":7,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n05h6up","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"SomeOddCodeGuy","can_mod_post":false,"send_replies":true,"parent_id":"t1_n05gmvj","score":6,"author_fullname":"t2_kle75fbd6","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I'll do both right now. And I'll definitely take a peek to see if I can help with Arch in any way! Routing and workflows, especially, are something I'm quite passionate about. Some of the choices you've made in your project are really cool, so I'll definitely see if there's somewhere I can help out at. While Wilmer is just a little hobby project, Arch has real viability for large scale.","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n05h6up","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;ll do both right now. And I&amp;#39;ll definitely take a peek to see if I can help with Arch in any way! Routing and workflows, especially, are something I&amp;#39;m quite passionate about. Some of the choices you&amp;#39;ve made in your project are really cool, so I&amp;#39;ll definitely see if there&amp;#39;s somewhere I can help out at. While Wilmer is just a little hobby project, Arch has real viability for large scale.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lm3jvm","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lm3jvm/archrouter_the_first_and_fastest_llm_router_that/n05h6up/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751065824,"author_flair_text":null,"treatment_tags":[],"created_utc":1751065824,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":6,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}}],"before":null}},"user_reports":[],"saved":false,"id":"n05gmvj","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AdditionalWeb107","can_mod_post":false,"send_replies":true,"parent_id":"t1_n05fym9","score":4,"author_fullname":"t2_gwq7fd01b","approved_by":null,"mod_note":null,"all_awardings":[],"body":"You are kind - would love for you to find ways to contribute to our OSS efforts if you are willing and inclined. Would love for you to watch/star our project as I just did Wilmer as we support our efforts in the open.","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n05gmvj","is_submitter":true,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You are kind - would love for you to find ways to contribute to our OSS efforts if you are willing and inclined. Would love for you to watch/star our project as I just did Wilmer as we support our efforts in the open.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lm3jvm","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lm3jvm/archrouter_the_first_and_fastest_llm_router_that/n05gmvj/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751065629,"author_flair_text":null,"treatment_tags":[],"created_utc":1751065629,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}}],"before":null}},"user_reports":[],"saved":false,"id":"n05fym9","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"SomeOddCodeGuy","can_mod_post":false,"send_replies":true,"parent_id":"t1_n05fk7l","score":6,"author_fullname":"t2_kle75fbd6","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"&gt;We've built the first LLM router model that can handle this better than any foundational model over turn, span and conversation. So I should say \\"first LLM router model\\" - not say its the first approach - that might be more precise?\\n\\nI agree with this all around. Both in the fact that I don't know another router model that does it as well, and also the fact that this will be more precise at less cost, both in terms of resources and speed. Wilmer is clunky; it relies heavily on large models to get routing right. Your trained model likely can produce the same results I require a 32b to do, but with only 1.5b.\\n\\nBy and large, I expect that with the work you've put into your project, your routing is simply *better* all around.","edited":false,"author_flair_css_class":null,"name":"t1_n05fym9","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;We&amp;#39;ve built the first LLM router model that can handle this better than any foundational model over turn, span and conversation. So I should say &amp;quot;first LLM router model&amp;quot; - not say its the first approach - that might be more precise?&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;I agree with this all around. Both in the fact that I don&amp;#39;t know another router model that does it as well, and also the fact that this will be more precise at less cost, both in terms of resources and speed. Wilmer is clunky; it relies heavily on large models to get routing right. Your trained model likely can produce the same results I require a 32b to do, but with only 1.5b.&lt;/p&gt;\\n\\n&lt;p&gt;By and large, I expect that with the work you&amp;#39;ve put into your project, your routing is simply &lt;em&gt;better&lt;/em&gt; all around.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lm3jvm","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lm3jvm/archrouter_the_first_and_fastest_llm_router_that/n05fym9/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751065397,"author_flair_text":null,"collapsed":false,"created_utc":1751065397,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}}],"before":null}},"user_reports":[],"saved":false,"id":"n05fk7l","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AdditionalWeb107","can_mod_post":false,"send_replies":true,"parent_id":"t1_n05epkb","score":2,"author_fullname":"t2_gwq7fd01b","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I think the key is: LLM of your choice. We've built the first LLM router model that can handle this better than any foundational model over turn, span and conversation. So I should say \\"first LLM router model\\" - not say its the first approach - that might be more precise?\\n\\nAnd Wilmer should get all the credit that its due to it. Innovators and builders like you are what we need here. I will update the post with this now.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n05fk7l","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I think the key is: LLM of your choice. We&amp;#39;ve built the first LLM router model that can handle this better than any foundational model over turn, span and conversation. So I should say &amp;quot;first LLM router model&amp;quot; - not say its the first approach - that might be more precise?&lt;/p&gt;\\n\\n&lt;p&gt;And Wilmer should get all the credit that its due to it. Innovators and builders like you are what we need here. I will update the post with this now.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lm3jvm","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lm3jvm/archrouter_the_first_and_fastest_llm_router_that/n05fk7l/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751065259,"author_flair_text":null,"treatment_tags":[],"created_utc":1751065259,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n05epkb","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"SomeOddCodeGuy","can_mod_post":false,"send_replies":true,"parent_id":"t1_n05d9u1","score":7,"author_fullname":"t2_kle75fbd6","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"So the way routing works in Wilmer\\n\\n**First**\\\\- [In your routing config](https://github.com/SomeOddCodeGuy/WilmerAI/blob/master/Public/Configs/Routing/assistantMultiModelCategoriesConfig.json), you can specify labels and descriptions. Both get sent the LLM you define as your routing LLM, using a[ customizable categorization workflow](https://github.com/SomeOddCodeGuy/WilmerAI/blob/master/Public/Configs/Workflows/assistant-single-model/CustomCategorizationWorkflow.json) that you can use to help it determine which of the routes you want it to take. Each route can specify a different LLM. So, your case:\\n\\n&gt;Â Drop rules like â€œcontract clauses â†’ GPT-4oâ€ or â€œquick travel tips â†’ Gemini-Flash,â€ and our 1.5B auto-regressive router model maps prompt along with the context to your routing policiesâ€”no retraining, no sprawling rules that are encoded in if/else statements.\\n\\nYour config would look like this:\\n\\n    {\\n      \\"CONTRACT\\": {\\n        \\"description\\": \\"The user is wanting to do stuff with contract clauses\\",\\n        \\"workflow\\": \\"Contract-Workflow-That-Uses-GPT-4o\\"\\n      },\\n      \\"TRIPS\\": {\\n        \\"description\\": \\"The user asked for Quick Travel Tips\\",\\n        \\"workflow\\": \\"Trips-Workflow-That-Uses-Gemini-Flash\\"\\n      }\\n    }\\n\\nThen, an LLM of your choice will do the categorization. In your case, you'd select the 1.5b routing LLM you trained.\\n\\nOnce it picks the route, it sends you to the workflow you specified; it could call just 1 node that goes to chatgpt, or it could call 10 or 12 nodes, each hitting a different LLM.\\n\\nBasically, routing like this was the very core of Wilmer\\n\\nEDIT: Again- I think that arch is bigger, better, faster, and better supported. Way more popular. There just weren't many things like Wilmer when it came out, and I was proud to have been able to do that, so it hurts my feelings a bit when others who came later claims the \\"first\\" label as well, just kind of writing the rest of us out.","edited":1751065214,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n05epkb","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;So the way routing works in Wilmer&lt;/p&gt;\\n\\n&lt;p&gt;&lt;strong&gt;First&lt;/strong&gt;- &lt;a href=\\"https://github.com/SomeOddCodeGuy/WilmerAI/blob/master/Public/Configs/Routing/assistantMultiModelCategoriesConfig.json\\"&gt;In your routing config&lt;/a&gt;, you can specify labels and descriptions. Both get sent the LLM you define as your routing LLM, using a&lt;a href=\\"https://github.com/SomeOddCodeGuy/WilmerAI/blob/master/Public/Configs/Workflows/assistant-single-model/CustomCategorizationWorkflow.json\\"&gt; customizable categorization workflow&lt;/a&gt; that you can use to help it determine which of the routes you want it to take. Each route can specify a different LLM. So, your case:&lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;Â Drop rules like â€œcontract clauses â†’ GPT-4oâ€ or â€œquick travel tips â†’ Gemini-Flash,â€ and our 1.5B auto-regressive router model maps prompt along with the context to your routing policiesâ€”no retraining, no sprawling rules that are encoded in if/else statements.&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;Your config would look like this:&lt;/p&gt;\\n\\n&lt;pre&gt;&lt;code&gt;{\\n  &amp;quot;CONTRACT&amp;quot;: {\\n    &amp;quot;description&amp;quot;: &amp;quot;The user is wanting to do stuff with contract clauses&amp;quot;,\\n    &amp;quot;workflow&amp;quot;: &amp;quot;Contract-Workflow-That-Uses-GPT-4o&amp;quot;\\n  },\\n  &amp;quot;TRIPS&amp;quot;: {\\n    &amp;quot;description&amp;quot;: &amp;quot;The user asked for Quick Travel Tips&amp;quot;,\\n    &amp;quot;workflow&amp;quot;: &amp;quot;Trips-Workflow-That-Uses-Gemini-Flash&amp;quot;\\n  }\\n}\\n&lt;/code&gt;&lt;/pre&gt;\\n\\n&lt;p&gt;Then, an LLM of your choice will do the categorization. In your case, you&amp;#39;d select the 1.5b routing LLM you trained.&lt;/p&gt;\\n\\n&lt;p&gt;Once it picks the route, it sends you to the workflow you specified; it could call just 1 node that goes to chatgpt, or it could call 10 or 12 nodes, each hitting a different LLM.&lt;/p&gt;\\n\\n&lt;p&gt;Basically, routing like this was the very core of Wilmer&lt;/p&gt;\\n\\n&lt;p&gt;EDIT: Again- I think that arch is bigger, better, faster, and better supported. Way more popular. There just weren&amp;#39;t many things like Wilmer when it came out, and I was proud to have been able to do that, so it hurts my feelings a bit when others who came later claims the &amp;quot;first&amp;quot; label as well, just kind of writing the rest of us out.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lm3jvm","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lm3jvm/archrouter_the_first_and_fastest_llm_router_that/n05epkb/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751064969,"author_flair_text":null,"treatment_tags":[],"created_utc":1751064969,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":7}}],"before":null}},"user_reports":[],"saved":false,"id":"n05d9u1","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AdditionalWeb107","can_mod_post":false,"created_utc":1751064476,"send_replies":true,"parent_id":"t1_n05b8gf","score":1,"author_fullname":"t2_gwq7fd01b","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I am sorry and just digging in.   \\n  \\nAt fist glance you can't describe usage patterns more granular in nature like \\"understand and explain existing code snippets, functions, or libraries\\" or \\"generating new code snippets, functions, or boilerplate based on user prompts or requirements\\". Wilmer feels like try a traditional classifier, while we are an auto-regressive router that generates usage labels based on the full context contextual history of the prompt.  It supports granular usage patterns that reflect real-world application scenarios\\n\\nPlus we've built a model with a technical report showing performance gains over foundational models. With a full research study that shows our approach in more detail.   \\n  \\nPlease correct me if my understanding is wrong.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n05d9u1","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I am sorry and just digging in.   &lt;/p&gt;\\n\\n&lt;p&gt;At fist glance you can&amp;#39;t describe usage patterns more granular in nature like &amp;quot;understand and explain existing code snippets, functions, or libraries&amp;quot; or &amp;quot;generating new code snippets, functions, or boilerplate based on user prompts or requirements&amp;quot;. Wilmer feels like try a traditional classifier, while we are an auto-regressive router that generates usage labels based on the full context contextual history of the prompt.  It supports granular usage patterns that reflect real-world application scenarios&lt;/p&gt;\\n\\n&lt;p&gt;Plus we&amp;#39;ve built a model with a technical report showing performance gains over foundational models. With a full research study that shows our approach in more detail.   &lt;/p&gt;\\n\\n&lt;p&gt;Please correct me if my understanding is wrong.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lm3jvm","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lm3jvm/archrouter_the_first_and_fastest_llm_router_that/n05d9u1/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751064476,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n05b8gf","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"SomeOddCodeGuy","can_mod_post":false,"created_utc":1751063772,"send_replies":true,"parent_id":"t3_1lm3jvm","score":15,"author_fullname":"t2_kle75fbd6","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I take a little offense to the \\"first\\", since this is exactly[ what Wilmer does lol](https://github.com/SomeOddCodeGuy/WilmerAI). Wilmer was ported to Github in May of 2024, two months before Arch kicked off in July; it's not fair to those of us who have also done this to try to just write them out of history.\\n\\nI don't doubt that Arch is bigger or faster and better, and it's a really cool project, but do be kind on the \\"first\\" claims.","edited":1751064274,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n05b8gf","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I take a little offense to the &amp;quot;first&amp;quot;, since this is exactly&lt;a href=\\"https://github.com/SomeOddCodeGuy/WilmerAI\\"&gt; what Wilmer does lol&lt;/a&gt;. Wilmer was ported to Github in May of 2024, two months before Arch kicked off in July; it&amp;#39;s not fair to those of us who have also done this to try to just write them out of history.&lt;/p&gt;\\n\\n&lt;p&gt;I don&amp;#39;t doubt that Arch is bigger or faster and better, and it&amp;#39;s a really cool project, but do be kind on the &amp;quot;first&amp;quot; claims.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lm3jvm/archrouter_the_first_and_fastest_llm_router_that/n05b8gf/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751063772,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lm3jvm","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":15}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n055vrd","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"AdditionalWeb107","can_mod_post":false,"created_utc":1751061953,"send_replies":true,"parent_id":"t1_n052uml","score":7,"author_fullname":"t2_gwq7fd01b","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"You can call it that - but its really an auto regressive usage label generator acting as an intent classifier. The performance over context is listed as tables in the paper. Here is a quick screenshot of our performance across turn, span and conversation.\\n\\nhttps://preview.redd.it/nai94z57mj9f1.jpeg?width=1290&amp;format=pjpg&amp;auto=webp&amp;s=91e5cb70003c80001d012ac6f57af0781b565d7d","edited":1751064546,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n055vrd","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You can call it that - but its really an auto regressive usage label generator acting as an intent classifier. The performance over context is listed as tables in the paper. Here is a quick screenshot of our performance across turn, span and conversation.&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://preview.redd.it/nai94z57mj9f1.jpeg?width=1290&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=91e5cb70003c80001d012ac6f57af0781b565d7d\\"&gt;https://preview.redd.it/nai94z57mj9f1.jpeg?width=1290&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=91e5cb70003c80001d012ac6f57af0781b565d7d&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lm3jvm","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lm3jvm/archrouter_the_first_and_fastest_llm_router_that/n055vrd/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751061953,"media_metadata":{"nai94z57mj9f1":{"status":"valid","e":"Image","m":"image/jpeg","p":[{"y":127,"x":108,"u":"https://preview.redd.it/nai94z57mj9f1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=6d9d71ff567fdbce3b57b2c466069661821cf894"},{"y":254,"x":216,"u":"https://preview.redd.it/nai94z57mj9f1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=c49148f395e2e405185455b293dba2c795dc90ce"},{"y":376,"x":320,"u":"https://preview.redd.it/nai94z57mj9f1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=3a623e0627cf2beb27d9cc419bd9eba1506665a1"},{"y":753,"x":640,"u":"https://preview.redd.it/nai94z57mj9f1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=7df369521d517bd0758fd78b79db8406467455b0"},{"y":1129,"x":960,"u":"https://preview.redd.it/nai94z57mj9f1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=262490dd2c48e5389c6c16b606a60c555d6d480b"},{"y":1270,"x":1080,"u":"https://preview.redd.it/nai94z57mj9f1.jpeg?width=1080&amp;crop=smart&amp;auto=webp&amp;s=6160c970357cb637659eaa5419ff01731394fdb3"}],"s":{"y":1518,"x":1290,"u":"https://preview.redd.it/nai94z57mj9f1.jpeg?width=1290&amp;format=pjpg&amp;auto=webp&amp;s=91e5cb70003c80001d012ac6f57af0781b565d7d"},"id":"nai94z57mj9f1"}},"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":7}}],"before":null}},"user_reports":[],"saved":false,"id":"n052uml","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"DeepInEvil","can_mod_post":false,"created_utc":1751060956,"send_replies":true,"parent_id":"t3_1lm3jvm","score":6,"author_fullname":"t2_163rrd","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"So this is a powerful intent classifier? How good/bad it understands the context of the underlying data/content wrt to the task?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n052uml","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;So this is a powerful intent classifier? How good/bad it understands the context of the underlying data/content wrt to the task?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lm3jvm/archrouter_the_first_and_fastest_llm_router_that/n052uml/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751060956,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lm3jvm","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n07fzf8","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"gwyngwynsituation","can_mod_post":false,"created_utc":1751096430,"send_replies":true,"parent_id":"t3_1lm3jvm","score":1,"author_fullname":"t2_5hyfmu1b","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"will it correctly detect and route NSFW requests? or is it censored in any way? it looks cool thanks!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n07fzf8","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;will it correctly detect and route NSFW requests? or is it censored in any way? it looks cool thanks!&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lm3jvm/archrouter_the_first_and_fastest_llm_router_that/n07fzf8/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751096430,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lm3jvm","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),n=()=>e.jsx(t,{data:a});export{n as default};
