import{j as e}from"./index-Cd3v0jxz.js";import{R as l}from"./RedditPostRenderer-cI96YLhy.js";import"./index-DGBoKyQm.js";const t=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Cards like 3090, 4090, 5090 has very high electric consumption. Isn't it possible to make 24,32gb cards with like 5060 level electric consumption?","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"+24GB VRAM with low electric consumption","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1m6hzf0","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.67,"author_flair_background_color":null,"subreddit_type":"public","ups":4,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_d9gk5hdlt","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":4,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1753200005,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Cards like 3090, 4090, 5090 has very high electric consumption. Isn&amp;#39;t it possible to make 24,32gb cards with like 5060 level electric consumption?&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1m6hzf0","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"narca_hakan","discussion_type":null,"num_comments":55,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1m6hzf0/24gb_vram_with_low_electric_consumption/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1m6hzf0/24gb_vram_with_low_electric_consumption/","subreddit_subscribers":503254,"created_utc":1753200005,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4jtcxb","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"brown2green","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4jqxfm","score":15,"author_fullname":"t2_f010l","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"It is, in a way. Alternatively, you can also roughly halve memory and core frequency for similar results (and considerably lower PSU-hammering current spikes) with \`nvidia-smi\`; by doing so you could easily get a 3090 below 200W of power during both prompt processing inference, without even touching the power limit (350~400W depending on the model).\\n\\nIt is not being done by default because the GPUs with lots of VRAM and decent enough bandwidth are expensive (many memory chips, wide bus width, large GPU cores, etc.) and buyers spending that much money are looking for performance first and foremost, not efficiency.","edited":1753205145,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n4jtcxb","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It is, in a way. Alternatively, you can also roughly halve memory and core frequency for similar results (and considerably lower PSU-hammering current spikes) with &lt;code&gt;nvidia-smi&lt;/code&gt;; by doing so you could easily get a 3090 below 200W of power during both prompt processing inference, without even touching the power limit (350~400W depending on the model).&lt;/p&gt;\\n\\n&lt;p&gt;It is not being done by default because the GPUs with lots of VRAM and decent enough bandwidth are expensive (many memory chips, wide bus width, large GPU cores, etc.) and buyers spending that much money are looking for performance first and foremost, not efficiency.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m6hzf0","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6hzf0/24gb_vram_with_low_electric_consumption/n4jtcxb/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753201353,"author_flair_text":null,"treatment_tags":[],"created_utc":1753201353,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":15}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4lxrmk","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"nukesrb","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4jqxfm","score":1,"author_fullname":"t2_6wo60vj","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Why? It'll be slower, but a 3090 at 175w still gives you the VRAM, and it'll be faster than running those layers on the CPU.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n4lxrmk","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Why? It&amp;#39;ll be slower, but a 3090 at 175w still gives you the VRAM, and it&amp;#39;ll be faster than running those layers on the CPU.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m6hzf0","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6hzf0/24gb_vram_with_low_electric_consumption/n4lxrmk/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753222925,"author_flair_text":null,"treatment_tags":[],"created_utc":1753222925,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4jucns","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Important_Concept967","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4jqxfm","score":0,"author_fullname":"t2_10htrrqujv","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Why, you can easily lower the power draw of any GPU using afterburner, do you want to pay nvidia to do that for you?","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n4jucns","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Why, you can easily lower the power draw of any GPU using afterburner, do you want to pay nvidia to do that for you?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m6hzf0","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6hzf0/24gb_vram_with_low_electric_consumption/n4jucns/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753201625,"author_flair_text":null,"treatment_tags":[],"created_utc":1753201625,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}},"user_reports":[],"saved":false,"id":"n4jqxfm","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":"LOW_SCORE","no_follow":true,"author":"narca_hakan","can_mod_post":false,"created_utc":1753200672,"send_replies":true,"parent_id":"t1_n4jpgg0","score":-28,"author_fullname":"t2_d9gk5hdlt","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"This this not the answer I am looking for","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4jqxfm","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;This this not the answer I am looking for&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":"comment score below threshold","distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m6hzf0","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6hzf0/24gb_vram_with_low_electric_consumption/n4jqxfm/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753200672,"author_flair_text":null,"treatment_tags":[],"collapsed":true,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-28}}],"before":null}},"user_reports":[],"saved":false,"id":"n4jpgg0","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"nukesrb","can_mod_post":false,"created_utc":1753200254,"send_replies":true,"parent_id":"t3_1m6hzf0","score":21,"author_fullname":"t2_6wo60vj","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Lower the power limit to 175 or 150w?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4jpgg0","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Lower the power limit to 175 or 150w?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6hzf0/24gb_vram_with_low_electric_consumption/n4jpgg0/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753200254,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m6hzf0","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":21}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4jy5j5","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Cergorach","can_mod_post":false,"created_utc":1753202690,"send_replies":true,"parent_id":"t3_1m6hzf0","score":8,"author_fullname":"t2_cs4w88d2l","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Under full load, a 5060 is still consuming \\\\~140W of power. Also keep in mind that a 5060 is (less then) half the speed of a 3090/4090 and 1/4th of the speed of a 5090.\\n\\nIf you want less power usage, but more memory, look at Apple's line up of modern Mac Mini's and Mac Studio's with M4 (Pro/Max). Keep in mind that they are less fast, the M4 Max only having the speed of a 5060, but comes with 36GB-128GB of unified memory (similar to VRAM). The rest is even slower. But I can run a 70b model on my Mac Mini M4 pro (20c GPU) 64GB and it draws 70w max, 7w while typing this (with keyboard and mouse attached).\\n\\nThere is no perfect solution, it's either speed, power or price, choose two.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4jy5j5","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Under full load, a 5060 is still consuming ~140W of power. Also keep in mind that a 5060 is (less then) half the speed of a 3090/4090 and 1/4th of the speed of a 5090.&lt;/p&gt;\\n\\n&lt;p&gt;If you want less power usage, but more memory, look at Apple&amp;#39;s line up of modern Mac Mini&amp;#39;s and Mac Studio&amp;#39;s with M4 (Pro/Max). Keep in mind that they are less fast, the M4 Max only having the speed of a 5060, but comes with 36GB-128GB of unified memory (similar to VRAM). The rest is even slower. But I can run a 70b model on my Mac Mini M4 pro (20c GPU) 64GB and it draws 70w max, 7w while typing this (with keyboard and mouse attached).&lt;/p&gt;\\n\\n&lt;p&gt;There is no perfect solution, it&amp;#39;s either speed, power or price, choose two.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6hzf0/24gb_vram_with_low_electric_consumption/n4jy5j5/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753202690,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m6hzf0","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":8}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4ld41g","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"narca_hakan","can_mod_post":false,"created_utc":1753216827,"send_replies":true,"parent_id":"t1_n4k7hzs","score":1,"author_fullname":"t2_d9gk5hdlt","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I think so, that's why we need more competition.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4ld41g","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I think so, that&amp;#39;s why we need more competition.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m6hzf0","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6hzf0/24gb_vram_with_low_electric_consumption/n4ld41g/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753216827,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4k7hzs","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"LostTheElectrons","can_mod_post":false,"created_utc":1753205200,"send_replies":true,"parent_id":"t3_1m6hzf0","score":6,"author_fullname":"t2_4andtwhf","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Yes it's possible, but companies won't make them (yet) because that would eat up their high margin AI card market.\\n\\nThe best we get is buying those 3090s and power limiting them.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4k7hzs","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yes it&amp;#39;s possible, but companies won&amp;#39;t make them (yet) because that would eat up their high margin AI card market.&lt;/p&gt;\\n\\n&lt;p&gt;The best we get is buying those 3090s and power limiting them.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6hzf0/24gb_vram_with_low_electric_consumption/n4k7hzs/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753205200,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m6hzf0","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4lcze7","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"narca_hakan","can_mod_post":false,"created_utc":1753216792,"send_replies":true,"parent_id":"t1_n4kaoyf","score":1,"author_fullname":"t2_d9gk5hdlt","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Thanks for the answer.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4lcze7","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thanks for the answer.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m6hzf0","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6hzf0/24gb_vram_with_low_electric_consumption/n4lcze7/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753216792,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4kaoyf","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"fallingdowndizzyvr","can_mod_post":false,"created_utc":1753206060,"send_replies":true,"parent_id":"t3_1m6hzf0","score":3,"author_fullname":"t2_o65i6kx","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"What you want is the upcoming Intel B60.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4kaoyf","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;What you want is the upcoming Intel B60.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6hzf0/24gb_vram_with_low_electric_consumption/n4kaoyf/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753206060,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m6hzf0","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4obkm0","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AppearanceHeavy6724","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4o93iw","score":1,"author_fullname":"t2_uz37qfx5","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Did you read that post at all? Mofo did not use flash attention and got sour ass slow down with CUDA. With CUDA It should have produced 100t/s, like commenters in that thread pointed out. \\n\\n&gt; Vulkan has gotten even faster since then. You'd know that if you had actual experience and not just the BS between your ears.\\n\\nVulkan is _still slower_ than CUDA, moron. Checks the graphs in the post you've linked yourself: https://github.com/ggml-org/llama.cpp/discussions/10879","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n4obkm0","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Did you read that post at all? Mofo did not use flash attention and got sour ass slow down with CUDA. With CUDA It should have produced 100t/s, like commenters in that thread pointed out. &lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;Vulkan has gotten even faster since then. You&amp;#39;d know that if you had actual experience and not just the BS between your ears.&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;Vulkan is &lt;em&gt;still slower&lt;/em&gt; than CUDA, moron. Checks the graphs in the post you&amp;#39;ve linked yourself: &lt;a href=\\"https://github.com/ggml-org/llama.cpp/discussions/10879\\"&gt;https://github.com/ggml-org/llama.cpp/discussions/10879&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1m6hzf0","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6hzf0/24gb_vram_with_low_electric_consumption/n4obkm0/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753257167,"author_flair_text":null,"treatment_tags":[],"created_utc":1753257167,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":6,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4o93iw","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"fallingdowndizzyvr","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4o2ye9","score":1,"author_fullname":"t2_o65i6kx","approved_by":null,"mod_note":null,"all_awardings":[],"body":"&gt; Asshole, it is Vulkan. notoriously slow, no one runs Nvidia with Vulkan. You need to use cuda for reliable numbers.\\n\\nWow. The BS never stops flowing from your orifices. All of them.\\n\\n\\"VULKAN is faster tan CUDA\\"\\n\\nhttps://www.reddit.com/r/LocalLLaMA/comments/1kabje8/vulkan_is_faster_tan_cuda_currently_with_llamacpp/\\n\\nVulkan has gotten even faster since then. You'd know that if you had *actual* experience and not just the BS between your ears.","edited":1753256081,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n4o93iw","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;Asshole, it is Vulkan. notoriously slow, no one runs Nvidia with Vulkan. You need to use cuda for reliable numbers.&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;Wow. The BS never stops flowing from your orifices. All of them.&lt;/p&gt;\\n\\n&lt;p&gt;&amp;quot;VULKAN is faster tan CUDA&amp;quot;&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://www.reddit.com/r/LocalLLaMA/comments/1kabje8/vulkan_is_faster_tan_cuda_currently_with_llamacpp/\\"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1kabje8/vulkan_is_faster_tan_cuda_currently_with_llamacpp/&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;Vulkan has gotten even faster since then. You&amp;#39;d know that if you had &lt;em&gt;actual&lt;/em&gt; experience and not just the BS between your ears.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1m6hzf0","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6hzf0/24gb_vram_with_low_electric_consumption/n4o93iw/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753255745,"author_flair_text":null,"treatment_tags":[],"created_utc":1753255745,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4o2ye9","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AppearanceHeavy6724","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4n1a62","score":1,"author_fullname":"t2_uz37qfx5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Asshole, it is Vulkan. notoriously slow, no one runs Nvidia with Vulkan. You need to use cuda for reliable numbers. You've also never addressed the point that 2x3060 (op wants 24gib at least) is extremely inefficient compared to single 3090. You also need to test with bigger models, like Gemma 3 12b, not puny 7b model; also power needs to be capped on both cards, at 130W and 260W respectively.\\n\\nEven with with that braindead vulkan bennchmark, prompt processing on 3090 is 2.5 times faster than on 3060. Lack of compute on 3060 will cause massive speed degradation with growth of context.","edited":false,"author_flair_css_class":null,"name":"t1_n4o2ye9","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Asshole, it is Vulkan. notoriously slow, no one runs Nvidia with Vulkan. You need to use cuda for reliable numbers. You&amp;#39;ve also never addressed the point that 2x3060 (op wants 24gib at least) is extremely inefficient compared to single 3090. You also need to test with bigger models, like Gemma 3 12b, not puny 7b model; also power needs to be capped on both cards, at 130W and 260W respectively.&lt;/p&gt;\\n\\n&lt;p&gt;Even with with that braindead vulkan bennchmark, prompt processing on 3090 is 2.5 times faster than on 3060. Lack of compute on 3060 will cause massive speed degradation with growth of context.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1m6hzf0","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6hzf0/24gb_vram_with_low_electric_consumption/n4o2ye9/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753252372,"author_flair_text":null,"collapsed":false,"created_utc":1753252372,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4n1a62","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"fallingdowndizzyvr","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4kcrk4","score":1,"author_fullname":"t2_o65i6kx","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"&gt; I wonder - what are you smoking?\\n\\nI'm smoking the truth.\\n\\nYou are smoking BS. Or more likely BS just oozes out of every orifice and pore you have like a smelly spring.\\n\\n\\"Nvidia RTX 3060 \\t64.76 ± 3.20\\"\\n\\n\\"Nvidia RTX 3090 \\t133.63 ± 4.50\\"\\n\\nhttps://github.com/ggml-org/llama.cpp/discussions/10879\\n\\nSince you seem to be math challenged.....\\n\\n64.76/133.63 = .48 = 48% = \\"roughly half\\". OK. More like exact than rough.","edited":1753236950,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4n1a62","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;I wonder - what are you smoking?&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;I&amp;#39;m smoking the truth.&lt;/p&gt;\\n\\n&lt;p&gt;You are smoking BS. Or more likely BS just oozes out of every orifice and pore you have like a smelly spring.&lt;/p&gt;\\n\\n&lt;p&gt;&amp;quot;Nvidia RTX 3060    64.76 ± 3.20&amp;quot;&lt;/p&gt;\\n\\n&lt;p&gt;&amp;quot;Nvidia RTX 3090    133.63 ± 4.50&amp;quot;&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://github.com/ggml-org/llama.cpp/discussions/10879\\"&gt;https://github.com/ggml-org/llama.cpp/discussions/10879&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;Since you seem to be math challenged.....&lt;/p&gt;\\n\\n&lt;p&gt;64.76/133.63 = .48 = 48% = &amp;quot;roughly half&amp;quot;. OK. More like exact than rough.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m6hzf0","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6hzf0/24gb_vram_with_low_electric_consumption/n4n1a62/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753236128,"author_flair_text":null,"treatment_tags":[],"created_utc":1753236128,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4n28sb","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"fallingdowndizzyvr","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4lypko","score":2,"author_fullname":"t2_o65i6kx","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"And only a 70% speed up. Far short of what that \\"triple the memory bandwidth, and triple the memory.\\" implies.\\n\\nBy the way, it's not \\"triple the memory bandwidth\\", more like double.\\n\\n\\n\\n\\"Nvidia RTX 3070 \\t78.71 ± 0.13\\"\\n\\n\\"Nvidia RTX 3090 \\t133.63 ± 4.50\\"\\n\\nhttps://github.com/ggml-org/llama.cpp/discussions/10879","edited":false,"author_flair_css_class":null,"name":"t1_n4n28sb","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;And only a 70% speed up. Far short of what that &amp;quot;triple the memory bandwidth, and triple the memory.&amp;quot; implies.&lt;/p&gt;\\n\\n&lt;p&gt;By the way, it&amp;#39;s not &amp;quot;triple the memory bandwidth&amp;quot;, more like double.&lt;/p&gt;\\n\\n&lt;p&gt;&amp;quot;Nvidia RTX 3070    78.71 ± 0.13&amp;quot;&lt;/p&gt;\\n\\n&lt;p&gt;&amp;quot;Nvidia RTX 3090    133.63 ± 4.50&amp;quot;&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://github.com/ggml-org/llama.cpp/discussions/10879\\"&gt;https://github.com/ggml-org/llama.cpp/discussions/10879&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1m6hzf0","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6hzf0/24gb_vram_with_low_electric_consumption/n4n28sb/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753236459,"author_flair_text":null,"collapsed":false,"created_utc":1753236459,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n4lypko","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"nukesrb","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4kcrk4","score":1,"author_fullname":"t2_6wo60vj","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"This. I went from 3070 to 3090 and it's triple the memory bandwidth, and triple the memory.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4lypko","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;This. I went from 3070 to 3090 and it&amp;#39;s triple the memory bandwidth, and triple the memory.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m6hzf0","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6hzf0/24gb_vram_with_low_electric_consumption/n4lypko/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753223225,"author_flair_text":null,"treatment_tags":[],"created_utc":1753223225,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4kcrk4","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"AppearanceHeavy6724","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4k9h6o","score":3,"author_fullname":"t2_uz37qfx5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"&gt; A 3060 is roughly half the speed of a 3090 and uses a third the power.\\n\\nI wonder - what are you smoking? Power limited 3060 at 130W is more than twice slower than 3090 capped at 260W, due to awful memory bandwidth. And if we consider large models, like 24b or 32b 2x3060 is abysmal in terms of energy efficiency compared to single 3090.\\n\\nEDIT: if you also take into account prompt processing speed , 3090 is many times (3x? 4x?) faster than 3060.","edited":1753206849,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n4kcrk4","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;A 3060 is roughly half the speed of a 3090 and uses a third the power.&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;I wonder - what are you smoking? Power limited 3060 at 130W is more than twice slower than 3090 capped at 260W, due to awful memory bandwidth. And if we consider large models, like 24b or 32b 2x3060 is abysmal in terms of energy efficiency compared to single 3090.&lt;/p&gt;\\n\\n&lt;p&gt;EDIT: if you also take into account prompt processing speed , 3090 is many times (3x? 4x?) faster than 3060.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m6hzf0","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6hzf0/24gb_vram_with_low_electric_consumption/n4kcrk4/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753206621,"author_flair_text":null,"treatment_tags":[],"created_utc":1753206621,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n4k9h6o","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"fallingdowndizzyvr","can_mod_post":false,"created_utc":1753205733,"send_replies":true,"parent_id":"t1_n4ju07i","score":4,"author_fullname":"t2_o65i6kx","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"&gt; Lower consuming cards are less economical as t/s per watt are worse on 5060ti than on 3090 or 4090.\\n\\nIt's the exact opposite of that. Lower wattage cards are more efficient than high wattage cards. Since the ramp is not linear. The watts you pour in doesn't give you a linear ramp up in t/s. For high performing cards, the power you pour into it only gives you a marginal increase in performance. A 3060 is roughly half the speed of a 3090 and uses a third the power.","edited":1753205944,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4k9h6o","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;Lower consuming cards are less economical as t/s per watt are worse on 5060ti than on 3090 or 4090.&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;It&amp;#39;s the exact opposite of that. Lower wattage cards are more efficient than high wattage cards. Since the ramp is not linear. The watts you pour in doesn&amp;#39;t give you a linear ramp up in t/s. For high performing cards, the power you pour into it only gives you a marginal increase in performance. A 3060 is roughly half the speed of a 3090 and uses a third the power.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m6hzf0","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6hzf0/24gb_vram_with_low_electric_consumption/n4k9h6o/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753205733,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4p3737","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"narca_hakan","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4lol5v","score":1,"author_fullname":"t2_d9gk5hdlt","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I can't run 24b 27b models on my 8gb 3060ti card. Sharing with CPU makes them extremely slow / unusable. I believe if my card had 24gb VRAM without any other upgrade it would be usable enough to run those models. 5t/s generation with 15k context would be enough. \\n\\nI am wondering why they don't produce cheaper high VRAM cards. And I was asking if it is not possible to produce but people offer me buy and 3090 and power limit. I want something cheaper like 5060ti 24gb version.","edited":false,"author_flair_css_class":null,"name":"t1_n4p3737","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I can&amp;#39;t run 24b 27b models on my 8gb 3060ti card. Sharing with CPU makes them extremely slow / unusable. I believe if my card had 24gb VRAM without any other upgrade it would be usable enough to run those models. 5t/s generation with 15k context would be enough. &lt;/p&gt;\\n\\n&lt;p&gt;I am wondering why they don&amp;#39;t produce cheaper high VRAM cards. And I was asking if it is not possible to produce but people offer me buy and 3090 and power limit. I want something cheaper like 5060ti 24gb version.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1m6hzf0","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6hzf0/24gb_vram_with_low_electric_consumption/n4p3737/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753271444,"author_flair_text":null,"collapsed":false,"created_utc":1753271444,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4lol5v","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AppearanceHeavy6724","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4lcuax","score":-1,"author_fullname":"t2_uz37qfx5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"The dude is giving you bad advice. The cards he is advising are expensive and extremely, uncomfortably slow. If you clarify why would you want lower power cards - either if you have low power psu or if you want to save on energy bill (you will not with lower power cards) then we could give you a better advice.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4lol5v","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The dude is giving you bad advice. The cards he is advising are expensive and extremely, uncomfortably slow. If you clarify why would you want lower power cards - either if you have low power psu or if you want to save on energy bill (you will not with lower power cards) then we could give you a better advice.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m6hzf0","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6hzf0/24gb_vram_with_low_electric_consumption/n4lol5v/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753220107,"author_flair_text":null,"treatment_tags":[],"created_utc":1753220107,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4lcuax","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"narca_hakan","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4l10qa","score":1,"author_fullname":"t2_d9gk5hdlt","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Thanks I'll check Intel, I agree with you about t/ps. I am not looking for super speed token generation.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n4lcuax","is_submitter":true,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thanks I&amp;#39;ll check Intel, I agree with you about t/ps. I am not looking for super speed token generation.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m6hzf0","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6hzf0/24gb_vram_with_low_electric_consumption/n4lcuax/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753216752,"author_flair_text":null,"treatment_tags":[],"created_utc":1753216752,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4o4wqv","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AppearanceHeavy6724","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4m4wyn","score":0,"author_fullname":"t2_uz37qfx5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"&gt; The point I was making is that practically speaking perf/watt stops making sense as a metric when perf exceeds what you practically need or can benefit from for your usecase.\\n\\n_You still pay energy bill_. This is the _single most important reason_ why would you want as much t/s per watt as possible, not because you want it fast; card can be slow or fast but independently efficiency can be good or bad. Besides for _coding_, speed is **never enough**.\\n\\n&gt; A single A2000 12GB can run a 14B model at q4. 2 is unnecessary (and I personally don't even consider multi-gpu for my use case)\\n\\nYou were giving an unusable advise to the OP who wants to have 24GiB+ VRAM.  Why? My point was even if you can run 14b at Q4 on A2000 marginally tolerable, with 24b model and 2xA2000 to accomodate the model the speed will be unusable, like 8 ts.\\n\\nAlso poor compute on A2000 will cause rapid speed degradation with growth of context.","edited":false,"author_flair_css_class":null,"name":"t1_n4o4wqv","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;The point I was making is that practically speaking perf/watt stops making sense as a metric when perf exceeds what you practically need or can benefit from for your usecase.&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;&lt;em&gt;You still pay energy bill&lt;/em&gt;. This is the &lt;em&gt;single most important reason&lt;/em&gt; why would you want as much t/s per watt as possible, not because you want it fast; card can be slow or fast but independently efficiency can be good or bad. Besides for &lt;em&gt;coding&lt;/em&gt;, speed is &lt;strong&gt;never enough&lt;/strong&gt;.&lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;A single A2000 12GB can run a 14B model at q4. 2 is unnecessary (and I personally don&amp;#39;t even consider multi-gpu for my use case)&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;You were giving an unusable advise to the OP who wants to have 24GiB+ VRAM.  Why? My point was even if you can run 14b at Q4 on A2000 marginally tolerable, with 24b model and 2xA2000 to accomodate the model the speed will be unusable, like 8 ts.&lt;/p&gt;\\n\\n&lt;p&gt;Also poor compute on A2000 will cause rapid speed degradation with growth of context.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1m6hzf0","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6hzf0/24gb_vram_with_low_electric_consumption/n4o4wqv/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753253429,"author_flair_text":null,"collapsed":false,"created_utc":1753253429,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}},"user_reports":[],"saved":false,"id":"n4m4wyn","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"redoubt515","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4lnsp9","score":2,"author_fullname":"t2_kehp8nb59","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"&gt;\\\\&gt; blaming me for...\\n\\nI'm not \\"*blaming you*\\" for anything, there is no need to be argumentative. I wasn't even really disagreeing with you. I think you are taking something very personally that was not meant that way\\n\\n(and also fundamentally misunderstanding what was said) nowhere did I tell you not to use Tk/s per watt as a unit. Its a useful unit of measurement in some contexts, and a unit I use myself. But it's too blunt of a measurement to provide a full picture or to be a goal in and of itself in all contexts, *especially personal use.*\\n\\nThe point I was making is that practically speaking perf/watt stops making sense as a metric when perf exceeds what you practically need or can benefit from for your usecase. You can get better perf/watt, but if that perf is unneeded its not a practical efficiency gain.\\n\\n&gt;\\\\&gt; Idle power also matters BTW and afaik a4000 has high idle as it has no p8 state.\\n\\nIdle is extremely important to me, if what you say about idle is true, that would lead me look away from the a\\\\_\\\\_\\\\_ series. But I am under the impression that the a\\\\_\\\\_\\\\_ and the rtx \\\\_\\\\_\\\\_\\\\_ ada are specifically designed with power consumption as a priority. [Benchmarks show A2000 idles aaround 7W](https://www.techpowerup.com/review/nvidia-rtx-a2000/35.html). I can't find idle power benchmarks for the other 3 GPU's I mentioned, but a redditor reports [6W idle with the RTX 2000 ADA](https://www.reddit.com/r/homelab/comments/1f7wmmr/comment/lladg01/?utm_source=share&amp;utm_medium=web3x&amp;utm_name=web3xcss&amp;utm_term=1&amp;utm_content=share_button)\\n\\nAs I (and you) have already mentioned, none of these cards have the minimum vram OP is looking for.\\n\\n&gt;\\\\&gt; 2xA2000 is completely unusable for running anything bigger than 14b models\\n\\nA single *A2000 12GB* can run a [14B model](https://huggingface.co/unsloth/Qwen3-14B-GGUF) at q4. 2 is unnecessary (and I personally don't even consider multi-gpu for my use case)\\n\\n&gt;\\\\&gt; I \\\\[think your advice is\\\\] well intended but incompetent.\\n\\nI won't rule that out.","edited":1753225665,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4m4wyn","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;&amp;gt; blaming me for...&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;I&amp;#39;m not &amp;quot;&lt;em&gt;blaming you&lt;/em&gt;&amp;quot; for anything, there is no need to be argumentative. I wasn&amp;#39;t even really disagreeing with you. I think you are taking something very personally that was not meant that way&lt;/p&gt;\\n\\n&lt;p&gt;(and also fundamentally misunderstanding what was said) nowhere did I tell you not to use Tk/s per watt as a unit. Its a useful unit of measurement in some contexts, and a unit I use myself. But it&amp;#39;s too blunt of a measurement to provide a full picture or to be a goal in and of itself in all contexts, &lt;em&gt;especially personal use.&lt;/em&gt;&lt;/p&gt;\\n\\n&lt;p&gt;The point I was making is that practically speaking perf/watt stops making sense as a metric when perf exceeds what you practically need or can benefit from for your usecase. You can get better perf/watt, but if that perf is unneeded its not a practical efficiency gain.&lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;&amp;gt; Idle power also matters BTW and afaik a4000 has high idle as it has no p8 state.&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;Idle is extremely important to me, if what you say about idle is true, that would lead me look away from the a___ series. But I am under the impression that the a___ and the rtx ____ ada are specifically designed with power consumption as a priority. &lt;a href=\\"https://www.techpowerup.com/review/nvidia-rtx-a2000/35.html\\"&gt;Benchmarks show A2000 idles aaround 7W&lt;/a&gt;. I can&amp;#39;t find idle power benchmarks for the other 3 GPU&amp;#39;s I mentioned, but a redditor reports &lt;a href=\\"https://www.reddit.com/r/homelab/comments/1f7wmmr/comment/lladg01/?utm_source=share&amp;amp;utm_medium=web3x&amp;amp;utm_name=web3xcss&amp;amp;utm_term=1&amp;amp;utm_content=share_button\\"&gt;6W idle with the RTX 2000 ADA&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;As I (and you) have already mentioned, none of these cards have the minimum vram OP is looking for.&lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;&amp;gt; 2xA2000 is completely unusable for running anything bigger than 14b models&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;A single &lt;em&gt;A2000 12GB&lt;/em&gt; can run a &lt;a href=\\"https://huggingface.co/unsloth/Qwen3-14B-GGUF\\"&gt;14B model&lt;/a&gt; at q4. 2 is unnecessary (and I personally don&amp;#39;t even consider multi-gpu for my use case)&lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;&amp;gt; I [think your advice is] well intended but incompetent.&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;I won&amp;#39;t rule that out.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m6hzf0","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6hzf0/24gb_vram_with_low_electric_consumption/n4m4wyn/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753225232,"author_flair_text":null,"treatment_tags":[],"created_utc":1753225232,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n4lnsp9","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AppearanceHeavy6724","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4l10qa","score":-1,"author_fullname":"t2_uz37qfx5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"You are contradicting yourself when you blaming me for using impractical units for measuring efficiency,but using exactly same units yourself, albeit spelled differently, as bandwidth per watt. There is only one way to measure efficiency and it is tok/sec per watt. Idle power also matters BTW and afaik a4000 has high idle as it has no p8 state. Besides you still blatantly missing the point that 2xa4000, minimal will be much slower than single 3090 at running larger models yet consuming twice as much power. 2xA2000 is completely unusable for running anything bigger than 14b models as 32b model even at q4 would run at miserable 6-7 t/s.\\n\\n\\nI do not think you really have used the cards you are offering in your well intended but incompetent advice.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n4lnsp9","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You are contradicting yourself when you blaming me for using impractical units for measuring efficiency,but using exactly same units yourself, albeit spelled differently, as bandwidth per watt. There is only one way to measure efficiency and it is tok/sec per watt. Idle power also matters BTW and afaik a4000 has high idle as it has no p8 state. Besides you still blatantly missing the point that 2xa4000, minimal will be much slower than single 3090 at running larger models yet consuming twice as much power. 2xA2000 is completely unusable for running anything bigger than 14b models as 32b model even at q4 would run at miserable 6-7 t/s.&lt;/p&gt;\\n\\n&lt;p&gt;I do not think you really have used the cards you are offering in your well intended but incompetent advice.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m6hzf0","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6hzf0/24gb_vram_with_low_electric_consumption/n4lnsp9/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753219873,"author_flair_text":null,"treatment_tags":[],"created_utc":1753219873,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4l10qa","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"redoubt515","can_mod_post":false,"created_utc":1753213471,"send_replies":true,"parent_id":"t1_n4ju07i","score":3,"author_fullname":"t2_kehp8nb59","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"That calculus is very relevant if you *need/want* more TPS, but for many of us using LLMs for personal use, there is a flattening of the curve where more TPS don't equal more units of goodness in practice. In those contexts, it'd be nice to have some power optimized cards geared more towards power efficiency in relative terms not just in (tk per second per watt).\\n\\nI think these cards do exist, but maybe not with the amount of memory OP needs. For example the [RTX A2000](https://www.techpowerup.com/gpu-specs/rtx-2000-ada-generation.c4199) or [RTX 2000 ADA](https://www.techpowerup.com/gpu-specs/rtx-2000-ada-generation.c4199) have TDPs of only 70W, 12-16GB VRAM, relatively modest bandwith (\\\\~250 GB/s). The [A4000](https://www.techpowerup.com/gpu-specs/rtx-a4000.c3756) and [4000 ADA](https://www.techpowerup.com/gpu-specs/rtx-4000-ada-generation.c4171), have TDPs of \\\\~130W, 16-20GB, and a bit better bandwidth 360-450 GB/s). Intel's [upcoming B50 and B60](https://www.servethehome.com/intel-arc-pro-b50-and-b60-for-lower-cost-pro-gpus-and-18a-panther-lake-shown-at-computex-2025/) are kind of in the same ballpark as well.\\n\\nu/narca_hakan take a look at some of these options \\\\^ none achieve your 24GB requirement except the B60, but they are all more tuned towards efficiency from what I know.","edited":1753213728,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4l10qa","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;That calculus is very relevant if you &lt;em&gt;need/want&lt;/em&gt; more TPS, but for many of us using LLMs for personal use, there is a flattening of the curve where more TPS don&amp;#39;t equal more units of goodness in practice. In those contexts, it&amp;#39;d be nice to have some power optimized cards geared more towards power efficiency in relative terms not just in (tk per second per watt).&lt;/p&gt;\\n\\n&lt;p&gt;I think these cards do exist, but maybe not with the amount of memory OP needs. For example the &lt;a href=\\"https://www.techpowerup.com/gpu-specs/rtx-2000-ada-generation.c4199\\"&gt;RTX A2000&lt;/a&gt; or &lt;a href=\\"https://www.techpowerup.com/gpu-specs/rtx-2000-ada-generation.c4199\\"&gt;RTX 2000 ADA&lt;/a&gt; have TDPs of only 70W, 12-16GB VRAM, relatively modest bandwith (~250 GB/s). The &lt;a href=\\"https://www.techpowerup.com/gpu-specs/rtx-a4000.c3756\\"&gt;A4000&lt;/a&gt; and &lt;a href=\\"https://www.techpowerup.com/gpu-specs/rtx-4000-ada-generation.c4171\\"&gt;4000 ADA&lt;/a&gt;, have TDPs of ~130W, 16-20GB, and a bit better bandwidth 360-450 GB/s). Intel&amp;#39;s &lt;a href=\\"https://www.servethehome.com/intel-arc-pro-b50-and-b60-for-lower-cost-pro-gpus-and-18a-panther-lake-shown-at-computex-2025/\\"&gt;upcoming B50 and B60&lt;/a&gt; are kind of in the same ballpark as well.&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"/u/narca_hakan\\"&gt;u/narca_hakan&lt;/a&gt; take a look at some of these options ^ none achieve your 24GB requirement except the B60, but they are all more tuned towards efficiency from what I know.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m6hzf0","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6hzf0/24gb_vram_with_low_electric_consumption/n4l10qa/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753213471,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n4ju07i","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AppearanceHeavy6724","can_mod_post":false,"created_utc":1753201530,"send_replies":true,"parent_id":"t3_1m6hzf0","score":4,"author_fullname":"t2_uz37qfx5","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Lower consuming cards are less economical as t/s per watt are worse on 5060ti than on 3090 or 4090. Essentially fools errand to chase low power cards, unless the difference is substantial like on Mac vs 3090.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4ju07i","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Lower consuming cards are less economical as t/s per watt are worse on 5060ti than on 3090 or 4090. Essentially fools errand to chase low power cards, unless the difference is substantial like on Mac vs 3090.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6hzf0/24gb_vram_with_low_electric_consumption/n4ju07i/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753201530,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m6hzf0","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":1,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4jzu6n","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"_xulion","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4jrfbs","score":2,"author_fullname":"t2_a1dvxm4d","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"It's an ADA gen gpu by nvidia. Very expensive\\n\\n[L4 Tensor Core GPU for AI &amp; Graphics | NVIDIA](https://www.nvidia.com/en-us/data-center/l4/)\\n\\n75W TDP with 24G vram","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n4jzu6n","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It&amp;#39;s an ADA gen gpu by nvidia. Very expensive&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://www.nvidia.com/en-us/data-center/l4/\\"&gt;L4 Tensor Core GPU for AI &amp;amp; Graphics | NVIDIA&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;75W TDP with 24G vram&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m6hzf0","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6hzf0/24gb_vram_with_low_electric_consumption/n4jzu6n/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753203149,"author_flair_text":null,"treatment_tags":[],"created_utc":1753203149,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4kcsbp","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Herr_Drosselmeyer","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4k7rdy","score":1,"author_fullname":"t2_1zr9gwsn","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"&gt;I mean cheaper than 5090 but with same VRAM. \\n\\nDoesn't exist.","edited":false,"author_flair_css_class":null,"name":"t1_n4kcsbp","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;I mean cheaper than 5090 but with same VRAM. &lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;Doesn&amp;#39;t exist.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1m6hzf0","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6hzf0/24gb_vram_with_low_electric_consumption/n4kcsbp/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753206627,"author_flair_text":null,"collapsed":false,"created_utc":1753206627,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4lp1cu","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AppearanceHeavy6724","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4k7rdy","score":1,"author_fullname":"t2_uz37qfx5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Just add used 3060 to your 3060ti and you are good.","edited":false,"author_flair_css_class":null,"name":"t1_n4lp1cu","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Just add used 3060 to your 3060ti and you are good.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1m6hzf0","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6hzf0/24gb_vram_with_low_electric_consumption/n4lp1cu/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753220239,"author_flair_text":null,"collapsed":false,"created_utc":1753220239,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4k7rdy","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"narca_hakan","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4jvmhm","score":1,"author_fullname":"t2_d9gk5hdlt","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I mean cheaper than 5090 but with same VRAM. Performance will be worse than 5090 but cheaper. I believe higher VRAM is enough upgrade for local LLM no need to extra power consumption and extra raw performance. I have 3060ti 8gb. I am sure it would performance much better if it had 24gb VRAM to run Mistral small.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4k7rdy","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I mean cheaper than 5090 but with same VRAM. Performance will be worse than 5090 but cheaper. I believe higher VRAM is enough upgrade for local LLM no need to extra power consumption and extra raw performance. I have 3060ti 8gb. I am sure it would performance much better if it had 24gb VRAM to run Mistral small.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m6hzf0","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6hzf0/24gb_vram_with_low_electric_consumption/n4k7rdy/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753205271,"author_flair_text":null,"treatment_tags":[],"created_utc":1753205271,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4jvmhm","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Direspark","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4jrfbs","score":1,"author_fullname":"t2_675qc","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"&gt;I am imagining a cheaper card with less power consumption.\\n\\nNewer data center cards are faster per watt, but they are also extremely expensive. You aren't going to find a high performance, power efficient gpu with lots of VRAM for cheap. It doesn't exist.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n4jvmhm","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;I am imagining a cheaper card with less power consumption.&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;Newer data center cards are faster per watt, but they are also extremely expensive. You aren&amp;#39;t going to find a high performance, power efficient gpu with lots of VRAM for cheap. It doesn&amp;#39;t exist.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m6hzf0","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6hzf0/24gb_vram_with_low_electric_consumption/n4jvmhm/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753201978,"author_flair_text":null,"treatment_tags":[],"created_utc":1753201978,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4jrfbs","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"narca_hakan","can_mod_post":false,"created_utc":1753200811,"send_replies":true,"parent_id":"t1_n4jqd8z","score":0,"author_fullname":"t2_d9gk5hdlt","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"What is L4 and I wonder if it is technically possible or not. I wouldn't want to spend a lot of money, on the contrary, I am imagining a cheaper card with less power consumption.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4jrfbs","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;What is L4 and I wonder if it is technically possible or not. I wouldn&amp;#39;t want to spend a lot of money, on the contrary, I am imagining a cheaper card with less power consumption.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m6hzf0","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6hzf0/24gb_vram_with_low_electric_consumption/n4jrfbs/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753200811,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}},"user_reports":[],"saved":false,"id":"n4jqd8z","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"_xulion","can_mod_post":false,"created_utc":1753200514,"send_replies":true,"parent_id":"t3_1m6hzf0","score":1,"author_fullname":"t2_a1dvxm4d","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"L4 is the one.... but you may not want to spend that money","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4jqd8z","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;L4 is the one.... but you may not want to spend that money&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6hzf0/24gb_vram_with_low_electric_consumption/n4jqd8z/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753200514,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m6hzf0","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4jqmrn","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"getpodapp","can_mod_post":false,"created_utc":1753200589,"send_replies":true,"parent_id":"t3_1m6hzf0","score":1,"author_fullname":"t2_v2x4wxet","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"2x a2000 sff or a a4000 sff (close enough)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4jqmrn","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;2x a2000 sff or a a4000 sff (close enough)&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6hzf0/24gb_vram_with_low_electric_consumption/n4jqmrn/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753200589,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m6hzf0","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4la9mc","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Dry-Influence9","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4l3sjb","score":1,"author_fullname":"t2_vep8zxxd","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"3090 founders idles at like 10-15w and 4090 founders goes down to 5-10w. It has to be founders, all other models take more power. It doesnt get better than that for 24gb+ cards as far as I know.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n4la9mc","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;3090 founders idles at like 10-15w and 4090 founders goes down to 5-10w. It has to be founders, all other models take more power. It doesnt get better than that for 24gb+ cards as far as I know.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m6hzf0","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6hzf0/24gb_vram_with_low_electric_consumption/n4la9mc/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753216045,"author_flair_text":null,"treatment_tags":[],"created_utc":1753216045,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4l3sjb","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"redoubt515","can_mod_post":false,"created_utc":1753214246,"send_replies":true,"parent_id":"t1_n4ju1nn","score":1,"author_fullname":"t2_kehp8nb59","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Not OP, but I have similar priorities. Personally I'm mostly interested in the lowest possible *idle* power consumption. \\n\\nPower consumption under load only matters to me to the extent that I want my little itx system to run as cool/quiet as possible, and just generally be efficient. But it's just a personal machine, so it will be sitting idle the vast majority of the time that I'm not actively using an LLM or possibly gaming.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4l3sjb","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Not OP, but I have similar priorities. Personally I&amp;#39;m mostly interested in the lowest possible &lt;em&gt;idle&lt;/em&gt; power consumption. &lt;/p&gt;\\n\\n&lt;p&gt;Power consumption under load only matters to me to the extent that I want my little itx system to run as cool/quiet as possible, and just generally be efficient. But it&amp;#39;s just a personal machine, so it will be sitting idle the vast majority of the time that I&amp;#39;m not actively using an LLM or possibly gaming.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m6hzf0","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6hzf0/24gb_vram_with_low_electric_consumption/n4l3sjb/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753214246,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4ju1nn","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Dry-Influence9","can_mod_post":false,"created_utc":1753201541,"send_replies":true,"parent_id":"t3_1m6hzf0","score":1,"author_fullname":"t2_vep8zxxd","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"are you looking to lower idle power consumption or load?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4ju1nn","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;are you looking to lower idle power consumption or load?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6hzf0/24gb_vram_with_low_electric_consumption/n4ju1nn/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753201541,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m6hzf0","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4jukl9","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"LA_rent_Aficionado","can_mod_post":false,"created_utc":1753201686,"send_replies":true,"parent_id":"t3_1m6hzf0","score":1,"author_fullname":"t2_t8zbiflk","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Server grade cards generally accomplish this and that is one of the reasons why the are magnitudes more expensive than their consumer counterparts.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4jukl9","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Server grade cards generally accomplish this and that is one of the reasons why the are magnitudes more expensive than their consumer counterparts.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6hzf0/24gb_vram_with_low_electric_consumption/n4jukl9/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753201686,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m6hzf0","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4jwqan","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"fp4guru","can_mod_post":false,"created_utc":1753202291,"send_replies":true,"parent_id":"t3_1m6hzf0","score":1,"author_fullname":"t2_1tp8zldw5g","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"If you are doing hybrid serving, you would realize that your GPUs are not even close to 100%.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4jwqan","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;If you are doing hybrid serving, you would realize that your GPUs are not even close to 100%.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6hzf0/24gb_vram_with_low_electric_consumption/n4jwqan/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753202291,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m6hzf0","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4l6am1","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"redoubt515","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4l46w6","score":1,"author_fullname":"t2_kehp8nb59","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Much appreciated!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4l6am1","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Much appreciated!&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m6hzf0","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6hzf0/24gb_vram_with_low_electric_consumption/n4l6am1/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753214937,"author_flair_text":null,"treatment_tags":[],"created_utc":1753214937,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4l46w6","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"sersoniko","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4l3wxf","score":1,"author_fullname":"t2_22pmb26r","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I’ll let you know tomorrow if I remember to check\\n\\nEdit: see other comment","edited":1753257926,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n4l46w6","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I’ll let you know tomorrow if I remember to check&lt;/p&gt;\\n\\n&lt;p&gt;Edit: see other comment&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m6hzf0","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6hzf0/24gb_vram_with_low_electric_consumption/n4l46w6/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753214354,"author_flair_text":null,"treatment_tags":[],"created_utc":1753214354,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4m3vbh","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MaruluVR","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4l3wxf","score":1,"author_fullname":"t2_10ryluzwb5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"20W if you used pstated otherwise 40\\\\~60W\\n\\n[https://github.com/sasha0552/nvidia-pstated](https://github.com/sasha0552/nvidia-pstated)","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n4m3vbh","is_submitter":false,"collapsed":false,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;20W if you used pstated otherwise 40~60W&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://github.com/sasha0552/nvidia-pstated\\"&gt;https://github.com/sasha0552/nvidia-pstated&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m6hzf0","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6hzf0/24gb_vram_with_low_electric_consumption/n4m3vbh/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753224895,"author_flair_text":"llama.cpp","treatment_tags":[],"created_utc":1753224895,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4p3bcs","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"sersoniko","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4odhzz","score":1,"author_fullname":"t2_22pmb26r","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I've been wondering that myself, I think it has to do with how llama.cpp handles the power states of the GPU to reduce latency but I never looked into it","edited":false,"author_flair_css_class":null,"name":"t1_n4p3bcs","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;ve been wondering that myself, I think it has to do with how llama.cpp handles the power states of the GPU to reduce latency but I never looked into it&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1m6hzf0","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6hzf0/24gb_vram_with_low_electric_consumption/n4p3bcs/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753271492,"author_flair_text":null,"collapsed":false,"created_utc":1753271492,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4odhzz","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"redoubt515","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4octgm","score":1,"author_fullname":"t2_kehp8nb59","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Thanks so much for checking. That is actually surprising good! (unloaded)\\n\\nI wonder why loading it into vram causes that much of an increase, in consumption, I wouldn't think just sitting their loading in vram would cause much of a bump in consumption if not being actively used.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4odhzz","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thanks so much for checking. That is actually surprising good! (unloaded)&lt;/p&gt;\\n\\n&lt;p&gt;I wonder why loading it into vram causes that much of an increase, in consumption, I wouldn&amp;#39;t think just sitting their loading in vram would cause much of a bump in consumption if not being actively used.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m6hzf0","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6hzf0/24gb_vram_with_low_electric_consumption/n4odhzz/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753258273,"author_flair_text":null,"treatment_tags":[],"created_utc":1753258273,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4octgm","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"sersoniko","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4l3wxf","score":1,"author_fullname":"t2_22pmb26r","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Idle is 9 W, while if you have the weights loaded into memory it’s 50 W with Ollama/llama.cpp","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n4octgm","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Idle is 9 W, while if you have the weights loaded into memory it’s 50 W with Ollama/llama.cpp&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m6hzf0","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6hzf0/24gb_vram_with_low_electric_consumption/n4octgm/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753257879,"author_flair_text":null,"treatment_tags":[],"created_utc":1753257879,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4l3wxf","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"redoubt515","can_mod_post":false,"created_utc":1753214279,"send_replies":true,"parent_id":"t1_n4jwvxv","score":1,"author_fullname":"t2_kehp8nb59","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"What about idle power consumption?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4l3wxf","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;What about idle power consumption?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m6hzf0","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6hzf0/24gb_vram_with_low_electric_consumption/n4l3wxf/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753214279,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4jwvxv","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"sersoniko","can_mod_post":false,"created_utc":1753202335,"send_replies":true,"parent_id":"t3_1m6hzf0","score":1,"author_fullname":"t2_22pmb26r","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"The Nvidia Tesla P40 has 24GB of VRAM and is capped to 250 W, usually it draws around 200-180 W","edited":1753206036,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4jwvxv","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The Nvidia Tesla P40 has 24GB of VRAM and is capped to 250 W, usually it draws around 200-180 W&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6hzf0/24gb_vram_with_low_electric_consumption/n4jwvxv/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753202335,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m6hzf0","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4jwxd8","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"GeekyBit","can_mod_post":false,"created_utc":1753202346,"send_replies":true,"parent_id":"t3_1m6hzf0","score":1,"author_fullname":"t2_zq180","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"You could get a M1 Ultra 64gb Mac which for LLMs can have simliar performance and a very small power envelop size...\\n\\nIf money isn't an issue the M3  Ultra 96gb isn't to bad , but if you are going to do that may as well go for the 256gb model","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4jwxd8","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You could get a M1 Ultra 64gb Mac which for LLMs can have simliar performance and a very small power envelop size...&lt;/p&gt;\\n\\n&lt;p&gt;If money isn&amp;#39;t an issue the M3  Ultra 96gb isn&amp;#39;t to bad , but if you are going to do that may as well go for the 256gb model&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6hzf0/24gb_vram_with_low_electric_consumption/n4jwxd8/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753202346,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m6hzf0","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"total_awards_received":0,"approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"ups":1,"removal_reason":null,"link_id":"t3_1m6hzf0","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4jxiqn","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"[deleted]","can_mod_post":false,"send_replies":true,"parent_id":"t3_1m6hzf0","score":1,"approved_by":null,"report_reasons":null,"all_awardings":[],"subreddit_id":"t5_81eyvm","body":"not a card, but nvidia dgx spark and ryzen ai max mini pcs have lower electric consumption (ram is shared)","edited":false,"downs":0,"author_flair_css_class":null,"collapsed":false,"is_submitter":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;not a card, but nvidia dgx spark and ryzen ai max mini pcs have lower electric consumption (ram is shared)&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"associated_award":null,"stickied":false,"subreddit_type":"public","can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":"dark","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6hzf0/24gb_vram_with_low_electric_consumption/n4jxiqn/","num_reports":null,"locked":false,"name":"t1_n4jxiqn","created":1753202513,"subreddit":"LocalLLaMA","author_flair_text":null,"treatment_tags":[],"created_utc":1753202513,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"","collapsed_because_crowd_control":null,"mod_reports":[],"mod_note":null,"distinguished":null}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4k667o","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Rich_Artist_8327","can_mod_post":false,"created_utc":1753204847,"send_replies":true,"parent_id":"t3_1m6hzf0","score":1,"author_fullname":"t2_1jk2ep8a52","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"What do you mean by power consumption? During idle or during full load? 7900 XTX 24gb consumes idle 15W and during inference around 260-320W. 3090 is more power hungry during idle, and might be similarly efficient during inference. 4090 and 5099 are more erfficient even they may consume over 330W during inference, but cos they do it faster, they consume less power in total","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4k667o","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;What do you mean by power consumption? During idle or during full load? 7900 XTX 24gb consumes idle 15W and during inference around 260-320W. 3090 is more power hungry during idle, and might be similarly efficient during inference. 4090 and 5099 are more erfficient even they may consume over 330W during inference, but cos they do it faster, they consume less power in total&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6hzf0/24gb_vram_with_low_electric_consumption/n4k667o/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753204847,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m6hzf0","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4ld6zy","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"narca_hakan","can_mod_post":false,"created_utc":1753216849,"send_replies":true,"parent_id":"t1_n4k7yqc","score":1,"author_fullname":"t2_d9gk5hdlt","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Sorry I didn't understand.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4ld6zy","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Sorry I didn&amp;#39;t understand.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m6hzf0","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6hzf0/24gb_vram_with_low_electric_consumption/n4ld6zy/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753216849,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4k7yqc","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Ninja_Weedle","can_mod_post":false,"created_utc":1753205325,"send_replies":true,"parent_id":"t3_1m6hzf0","score":1,"author_fullname":"t2_smvqlry","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"the quadros are essentially this","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4k7yqc","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;the quadros are essentially this&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6hzf0/24gb_vram_with_low_electric_consumption/n4k7yqc/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753205325,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m6hzf0","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4mzk2s","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Monkey_1505","can_mod_post":false,"created_utc":1753235531,"send_replies":true,"parent_id":"t3_1m6hzf0","score":1,"author_fullname":"t2_7qrmh9n9","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"AMD Radeon PRO V710 has 38gb at 178W TPD. I believe there's also an rtx ada with 20gb. \\n\\nOtherwise you could get something close and power limit it by half.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4mzk2s","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;AMD Radeon PRO V710 has 38gb at 178W TPD. I believe there&amp;#39;s also an rtx ada with 20gb. &lt;/p&gt;\\n\\n&lt;p&gt;Otherwise you could get something close and power limit it by half.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6hzf0/24gb_vram_with_low_electric_consumption/n4mzk2s/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753235531,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m6hzf0","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),n=()=>e.jsx(l,{data:t});export{n as default};
