import{j as e}from"./index-BlGsFJYy.js";import{R as l}from"./RedditPostRenderer-B6uvq_Zl.js";import"./index-DDvVtNwD.js";const a=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"How bad are laptops for running LLM’s? I am going to get a laptop this August and would love to run a 5b-7B local LLM. How feasible is this? \\n\\nAny serious hardware suggestions here would be much appreciated. Also how much I amount to spend here? Haha ","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Local LLM on laptop?","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lxx4sb","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.75,"author_flair_background_color":null,"subreddit_type":"public","ups":2,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_21pp8tew","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":2,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":1752316171,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1752315729,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;How bad are laptops for running LLM’s? I am going to get a laptop this August and would love to run a 5b-7B local LLM. How feasible is this? &lt;/p&gt;\\n\\n&lt;p&gt;Any serious hardware suggestions here would be much appreciated. Also how much I amount to spend here? Haha &lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1lxx4sb","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"ontologicalmemes","discussion_type":null,"num_comments":12,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lxx4sb/local_llm_on_laptop/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lxx4sb/local_llm_on_laptop/","subreddit_subscribers":498345,"created_utc":1752315729,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2whgbx","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ontologicalmemes","can_mod_post":false,"created_utc":1752416533,"send_replies":true,"parent_id":"t1_n2puctf","score":1,"author_fullname":"t2_21pp8tew","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I’ve heard mixed things about qwen. How are do you like the model? Any use cases you’d recommend avoiding with qwen?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2whgbx","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I’ve heard mixed things about qwen. How are do you like the model? Any use cases you’d recommend avoiding with qwen?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lxx4sb","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxx4sb/local_llm_on_laptop/n2whgbx/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752416533,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2puctf","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"ArchdukeofHyperbole","can_mod_post":false,"created_utc":1752323260,"send_replies":true,"parent_id":"t3_1lxx4sb","score":4,"author_fullname":"t2_1p41v97q5d","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Depends on how fast you want it to run. You could fully offload a q4 quant of a 5-7B model on like a 6GB gpu and that could run at 20-40 tokens per second depending on context length. \\n\\nOr if you want something a little smarter, you could run qwen3 30B moe that has 3B active parameters. Its runs on my six year old gaming pc, partially offloaded to gpu, at 9 tokens per second. And without gpu, it runs at about 5 tokens per second. You'd just need to make sure you have enough ram to hold the model. i think that one is around 17GB file size for the q4 quant.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2puctf","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Depends on how fast you want it to run. You could fully offload a q4 quant of a 5-7B model on like a 6GB gpu and that could run at 20-40 tokens per second depending on context length. &lt;/p&gt;\\n\\n&lt;p&gt;Or if you want something a little smarter, you could run qwen3 30B moe that has 3B active parameters. Its runs on my six year old gaming pc, partially offloaded to gpu, at 9 tokens per second. And without gpu, it runs at about 5 tokens per second. You&amp;#39;d just need to make sure you have enough ram to hold the model. i think that one is around 17GB file size for the q4 quant.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxx4sb/local_llm_on_laptop/n2puctf/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752323260,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lxx4sb","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2r47h2","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ComposerGen","can_mod_post":false,"created_utc":1752338711,"send_replies":true,"parent_id":"t3_1lxx4sb","score":2,"author_fullname":"t2_p5kjlq8uk","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Get a usaged M2 Max 96GB or M3 Max 128GB if you are on a budget. Or go max to M4 Max","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2r47h2","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Get a usaged M2 Max 96GB or M3 Max 128GB if you are on a budget. Or go max to M4 Max&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxx4sb/local_llm_on_laptop/n2r47h2/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752338711,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lxx4sb","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2pjfpn","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Baldur-Norddahl","can_mod_post":false,"created_utc":1752318228,"send_replies":true,"parent_id":"t3_1lxx4sb","score":3,"author_fullname":"t2_bvqb8ng0","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"M4 Max MacBook Pro is the king for LLM on a laptop. Nothing really compares. As much memory as you can afford. With 128 GB you can even run some serious models such as Qwen3 235b q3 and get 20 tps. But even with the minimum memory of 36 GB you will be able to run 32b models at reasonable quantisation.\\n\\nSecond choice if Mac is not on the table, would be a laptop with the new AMD AI 395 CPU with unified memory.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2pjfpn","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;M4 Max MacBook Pro is the king for LLM on a laptop. Nothing really compares. As much memory as you can afford. With 128 GB you can even run some serious models such as Qwen3 235b q3 and get 20 tps. But even with the minimum memory of 36 GB you will be able to run 32b models at reasonable quantisation.&lt;/p&gt;\\n\\n&lt;p&gt;Second choice if Mac is not on the table, would be a laptop with the new AMD AI 395 CPU with unified memory.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxx4sb/local_llm_on_laptop/n2pjfpn/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752318228,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lxx4sb","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2pk0pi","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"serious_minor","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2pidyz","score":3,"author_fullname":"t2_2jmq5kxg","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Laptops with mobile 4090's have 16gb of vram, the new 5090's have 24.  Not sure about new 5080s.  24B models at Q4 - GGUF work fine.  16\\" gaming or dell precision laptops (still showing ada versions) are relatively portable.  Some are even linux certified.","edited":1752318705,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2pk0pi","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Laptops with mobile 4090&amp;#39;s have 16gb of vram, the new 5090&amp;#39;s have 24.  Not sure about new 5080s.  24B models at Q4 - GGUF work fine.  16&amp;quot; gaming or dell precision laptops (still showing ada versions) are relatively portable.  Some are even linux certified.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":true,"can_gild":false,"link_id":"t3_1lxx4sb","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxx4sb/local_llm_on_laptop/n2pk0pi/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752318524,"author_flair_text":null,"treatment_tags":[],"created_utc":1752318524,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2r822q","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"im_not_here_","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2pp1k9","score":1,"author_fullname":"t2_9l428","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Contextually could easily be not close to the best option. If money is unlimited and/or you have specific requirements to match, then Macbook can be the best option.","edited":1752340178,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2r822q","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Contextually could easily be not close to the best option. If money is unlimited and/or you have specific requirements to match, then Macbook can be the best option.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lxx4sb","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxx4sb/local_llm_on_laptop/n2r822q/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752339883,"author_flair_text":null,"treatment_tags":[],"created_utc":1752339883,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2pp1k9","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"xmBQWugdxjaA","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2pidyz","score":1,"author_fullname":"t2_nyyscwdgr","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"If you really want to do it on a laptop, then the best Macbook is probably the best option - but they're very expensive.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2pp1k9","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;If you really want to do it on a laptop, then the best Macbook is probably the best option - but they&amp;#39;re very expensive.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lxx4sb","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxx4sb/local_llm_on_laptop/n2pp1k9/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752320952,"author_flair_text":null,"treatment_tags":[],"created_utc":1752320952,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2pidyz","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ontologicalmemes","can_mod_post":false,"created_utc":1752317686,"send_replies":true,"parent_id":"t1_n2pfe7k","score":1,"author_fullname":"t2_21pp8tew","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Thanks for the feedback, do you have any specific laptop suggestions?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2pidyz","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thanks for the feedback, do you have any specific laptop suggestions?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lxx4sb","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxx4sb/local_llm_on_laptop/n2pidyz/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752317686,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2pfe7k","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"xmBQWugdxjaA","can_mod_post":false,"created_utc":1752316039,"send_replies":true,"parent_id":"t3_1lxx4sb","score":2,"author_fullname":"t2_nyyscwdgr","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"It's totally feasible for that size, but you'll need a \\"gamer\\" laptop with a big, heavy, power-hungry GPU.\\n\\nOr a Macbook, with a slight hit to inference speed.\\n\\nI'd look at mini-PCs / desktops tbh (Mac Mini or GPUs with lots of VRAM), and then just send the queries over the network (it's also easier to leave on overnight if you want to do fine-tuning). But I hate big, heavy laptops.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2pfe7k","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It&amp;#39;s totally feasible for that size, but you&amp;#39;ll need a &amp;quot;gamer&amp;quot; laptop with a big, heavy, power-hungry GPU.&lt;/p&gt;\\n\\n&lt;p&gt;Or a Macbook, with a slight hit to inference speed.&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;d look at mini-PCs / desktops tbh (Mac Mini or GPUs with lots of VRAM), and then just send the queries over the network (it&amp;#39;s also easier to leave on overnight if you want to do fine-tuning). But I hate big, heavy laptops.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxx4sb/local_llm_on_laptop/n2pfe7k/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752316039,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lxx4sb","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2po5xz","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MHTMakerspace","can_mod_post":false,"created_utc":1752320549,"send_replies":true,"parent_id":"t3_1lxx4sb","score":1,"author_fullname":"t2_dv0swue7","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I just [posted about my build earlier today](https://www.reddit.com/r/ASUSROG/comments/1lxy7ah/repurposed_older_rog_strix_to_run_ai_workload/), it is definitely doable with the right laptop (e.g. ROG Strix).\\n\\n&gt;Any serious hardware suggestions here would be much appreciated. Also how much I amount to spend here? Haha\\n\\nIf you're serious about running larger models on a laptop, there are newer models with massive VRAM, and prices to match.\\n\\nWith unlimited budget, i'd buy HP **Omnibook X** or maybe the newest **EliteBook Ultra.**","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2po5xz","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I just &lt;a href=\\"https://www.reddit.com/r/ASUSROG/comments/1lxy7ah/repurposed_older_rog_strix_to_run_ai_workload/\\"&gt;posted about my build earlier today&lt;/a&gt;, it is definitely doable with the right laptop (e.g. ROG Strix).&lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;Any serious hardware suggestions here would be much appreciated. Also how much I amount to spend here? Haha&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;If you&amp;#39;re serious about running larger models on a laptop, there are newer models with massive VRAM, and prices to match.&lt;/p&gt;\\n\\n&lt;p&gt;With unlimited budget, i&amp;#39;d buy HP &lt;strong&gt;Omnibook X&lt;/strong&gt; or maybe the newest &lt;strong&gt;EliteBook Ultra.&lt;/strong&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxx4sb/local_llm_on_laptop/n2po5xz/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752320549,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lxx4sb","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2u2j1u","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Red_Redditor_Reddit","can_mod_post":false,"created_utc":1752374780,"send_replies":true,"parent_id":"t3_1lxx4sb","score":1,"author_fullname":"t2_8eelmfjg","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"You could run a 4q 7B model even on modest CPU only hardware.  I run 7B models on my PI....  Now, the bigger models are going to be better, but you can easily run 7B models on pretty much anything.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2u2j1u","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You could run a 4q 7B model even on modest CPU only hardware.  I run 7B models on my PI....  Now, the bigger models are going to be better, but you can easily run 7B models on pretty much anything.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxx4sb/local_llm_on_laptop/n2u2j1u/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752374780,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lxx4sb","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2vwkmw","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Intelligent-Gift4519","can_mod_post":false,"created_utc":1752408762,"send_replies":true,"parent_id":"t3_1lxx4sb","score":1,"author_fullname":"t2_dxkuu99m","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I run LLMs up to 22B on my Surface Laptop 7 with Qualcomm Snapdragon. An 8B runs at about 18-20 t/s.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2vwkmw","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I run LLMs up to 22B on my Surface Laptop 7 with Qualcomm Snapdragon. An 8B runs at about 18-20 t/s.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lxx4sb/local_llm_on_laptop/n2vwkmw/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752408762,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lxx4sb","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),n=()=>e.jsx(l,{data:a});export{n as default};
