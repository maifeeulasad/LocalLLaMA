import{j as e}from"./index-BOnf-UhU.js";import{R as t}from"./RedditPostRenderer-Ce39qICS.js";import"./index-CDase6VD.js";const l=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Here is something of a hiccup I find myself running into a lot. I type up a prompt, often very elaborate of course, and RIGHT AFTER sending the prompt I realize that I have one more parting thought that could change everything.\\n\\nIt occurs to me that an LLM just flows all previously generated tokens through as it generates the next tokens. The way that thinking models are able to hack around the inherent inaccuracies at counting or arithmetic (for example) in purely one-shot fashion is (near as i can tell) just having them trained deeply on making a good call on how much to keep going back over the response and re-working it until it's confident it can move forward. Which is to say, that if you ask a modern thinking LLM to do math, it's going to work on it in drafts over and over and eventually decide on its own that it's satisfied before emitting the answer, and it's a LOT more likely to be correct.  \\n\\nThat gives me the idea that we should be able to slap in like a \\"BREAKING NEWS: User has offered up this ADDITIONAL THOUGHT that you should consider: &lt;additional prompt&gt;\\" and the thinking process should definitely be able to integrate the added information. In fact based on how I see it work on problems I expect it to ramble on for \\n\\nI doubt a modern LLM even needs much training on this stuff to respond usefully to it. So it seems like a pure frontend engineering question. The timing of the new input is pretty critical since if it doesnt come in fast enough (e.g. before end of thinking) then we kinda don't want to send it in. I also think it could even be possible to feed in the keystrokes in realtime to the LLM while it is inferencing. Why not? ","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Does LLM architecture allow for injecting some more input tokens in the middle of token generation?","link_flair_richtext":[{"e":"text","t":"Discussion"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1m4hfy0","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.67,"author_flair_background_color":null,"subreddit_type":"public","ups":9,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_iifi6ul2l","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Discussion","can_mod_post":false,"score":9,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":1752991468,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1752990962,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Here is something of a hiccup I find myself running into a lot. I type up a prompt, often very elaborate of course, and RIGHT AFTER sending the prompt I realize that I have one more parting thought that could change everything.&lt;/p&gt;\\n\\n&lt;p&gt;It occurs to me that an LLM just flows all previously generated tokens through as it generates the next tokens. The way that thinking models are able to hack around the inherent inaccuracies at counting or arithmetic (for example) in purely one-shot fashion is (near as i can tell) just having them trained deeply on making a good call on how much to keep going back over the response and re-working it until it&amp;#39;s confident it can move forward. Which is to say, that if you ask a modern thinking LLM to do math, it&amp;#39;s going to work on it in drafts over and over and eventually decide on its own that it&amp;#39;s satisfied before emitting the answer, and it&amp;#39;s a LOT more likely to be correct.  &lt;/p&gt;\\n\\n&lt;p&gt;That gives me the idea that we should be able to slap in like a &amp;quot;BREAKING NEWS: User has offered up this ADDITIONAL THOUGHT that you should consider: &amp;lt;additional prompt&amp;gt;&amp;quot; and the thinking process should definitely be able to integrate the added information. In fact based on how I see it work on problems I expect it to ramble on for &lt;/p&gt;\\n\\n&lt;p&gt;I doubt a modern LLM even needs much training on this stuff to respond usefully to it. So it seems like a pure frontend engineering question. The timing of the new input is pretty critical since if it doesnt come in fast enough (e.g. before end of thinking) then we kinda don&amp;#39;t want to send it in. I also think it could even be possible to feed in the keystrokes in realtime to the LLM while it is inferencing. Why not? &lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"5f921ea4-c7bc-11ed-9c23-3a00622979b4","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#646d73","id":"1m4hfy0","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"michaelsoft__binbows","discussion_type":null,"num_comments":28,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1m4hfy0/does_llm_architecture_allow_for_injecting_some/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1m4hfy0/does_llm_architecture_allow_for_injecting_some/","subreddit_subscribers":502274,"created_utc":1752990962,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n44pcwo","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Capable-Ad-7494","can_mod_post":false,"send_replies":true,"parent_id":"t1_n44hz8i","score":6,"author_fullname":"t2_9so78ol2","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Why not just use cache tokens? way cheaper for multi turn conversations, less ‘after the fact’ cost punishment as well in this instance also.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n44pcwo","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Why not just use cache tokens? way cheaper for multi turn conversations, less ‘after the fact’ cost punishment as well in this instance also.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m4hfy0","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4hfy0/does_llm_architecture_allow_for_injecting_some/n44pcwo/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752996864,"author_flair_text":null,"treatment_tags":[],"created_utc":1752996864,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n44qxja","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"FunnyAsparagus1253","can_mod_post":false,"send_replies":true,"parent_id":"t1_n44hz8i","score":6,"author_fullname":"t2_i6c8tay3w","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"It’s a good idea. There are a bunch of improvements that can be made on the input side, imo. Like why does it wait until you’ve typed your whole thing and hit ‘send’ before it starts processing? Just because it’s simpler. There’s no technical reason it can’t do the prompt processing in real time and be already ready to generate when you hit ‘send’.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n44qxja","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It’s a good idea. There are a bunch of improvements that can be made on the input side, imo. Like why does it wait until you’ve typed your whole thing and hit ‘send’ before it starts processing? Just because it’s simpler. There’s no technical reason it can’t do the prompt processing in real time and be already ready to generate when you hit ‘send’.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m4hfy0","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4hfy0/does_llm_architecture_allow_for_injecting_some/n44qxja/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752997760,"author_flair_text":null,"treatment_tags":[],"created_utc":1752997760,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n47mquo","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"michaelsoft__binbows","can_mod_post":false,"send_replies":true,"parent_id":"t1_n44vmrk","score":1,"author_fullname":"t2_iifi6ul2l","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I wonder if sglang does. It gives unheard of speeds for me and I'm just getting started. But it's more of an experimental/production runtime and hard to find people sharing tips on it.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n47mquo","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I wonder if sglang does. It gives unheard of speeds for me and I&amp;#39;m just getting started. But it&amp;#39;s more of an experimental/production runtime and hard to find people sharing tips on it.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m4hfy0","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4hfy0/does_llm_architecture_allow_for_injecting_some/n47mquo/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753037808,"author_flair_text":null,"treatment_tags":[],"created_utc":1753037808,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n44vmrk","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"berni8k","can_mod_post":false,"send_replies":true,"parent_id":"t1_n44hz8i","score":2,"author_fullname":"t2_hfyjp","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Use a local LLM runtime that has an input token cache. You can re-feed it a input context that is just slightly modified and it will reuse most of the cached input tokens, making it start generating near instantly even if the input is 50k tokens long.\\n\\nLM Studio has a cache by default. The built in UI also has options to stop generation at any time, slightly edit the context (be it your input or the LLMs output) and then resume generation. I find that useful if the generation starts off good but then goes wrong, i can just re gerate the last 1/4 of a response by deleting that part and resuming generation","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n44vmrk","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Use a local LLM runtime that has an input token cache. You can re-feed it a input context that is just slightly modified and it will reuse most of the cached input tokens, making it start generating near instantly even if the input is 50k tokens long.&lt;/p&gt;\\n\\n&lt;p&gt;LM Studio has a cache by default. The built in UI also has options to stop generation at any time, slightly edit the context (be it your input or the LLMs output) and then resume generation. I find that useful if the generation starts off good but then goes wrong, i can just re gerate the last 1/4 of a response by deleting that part and resuming generation&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m4hfy0","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4hfy0/does_llm_architecture_allow_for_injecting_some/n44vmrk/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753000465,"author_flair_text":null,"treatment_tags":[],"created_utc":1753000465,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n44hz8i","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"michaelsoft__binbows","can_mod_post":false,"created_utc":1752992785,"send_replies":true,"parent_id":"t1_n44g2ad","score":2,"author_fullname":"t2_iifi6ul2l","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yeah i do do that often if i decide that the new information is of critical importance. But the cost of input tokens (tons of code context etc) is already burned and i will need to restart the next prompt by re-sending the whole context again so this burns the whole input token cost which is usually the larger cost.\\n\\nI should have emphasized it in my OP but the \\"innovation\\" here would be that injecting the new straggler prompt in a JIT fashion would allow the original input tokens not to be \\"wasted\\" in order to insert the new information. that's the have your cake and eat it too aspect of this.\\n\\nThen again though maybe in some/many cases token/prompt caching could make this kinda work seamlessly.","edited":1752993045,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n44hz8i","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yeah i do do that often if i decide that the new information is of critical importance. But the cost of input tokens (tons of code context etc) is already burned and i will need to restart the next prompt by re-sending the whole context again so this burns the whole input token cost which is usually the larger cost.&lt;/p&gt;\\n\\n&lt;p&gt;I should have emphasized it in my OP but the &amp;quot;innovation&amp;quot; here would be that injecting the new straggler prompt in a JIT fashion would allow the original input tokens not to be &amp;quot;wasted&amp;quot; in order to insert the new information. that&amp;#39;s the have your cake and eat it too aspect of this.&lt;/p&gt;\\n\\n&lt;p&gt;Then again though maybe in some/many cases token/prompt caching could make this kinda work seamlessly.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m4hfy0","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4hfy0/does_llm_architecture_allow_for_injecting_some/n44hz8i/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752992785,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":1,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n44g2ad","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"cybran3","can_mod_post":false,"created_utc":1752991755,"send_replies":true,"parent_id":"t3_1m4hfy0","score":14,"author_fullname":"t2_41gmkw5z","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Just interrupt the generation when you want to insert new tokens?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n44g2ad","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Just interrupt the generation when you want to insert new tokens?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4hfy0/does_llm_architecture_allow_for_injecting_some/n44g2ad/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752991755,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m4hfy0","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":14}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n46iz5k","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Pedalnomica","can_mod_post":false,"created_utc":1753025868,"send_replies":true,"parent_id":"t1_n44ez6y","score":1,"author_fullname":"t2_b0d7j6x9","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Lol, this has strong malicious compliance energy","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n46iz5k","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Lol, this has strong malicious compliance energy&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m4hfy0","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4hfy0/does_llm_architecture_allow_for_injecting_some/n46iz5k/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753025868,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n44ez6y","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Equivalent_Cut_5845","can_mod_post":false,"created_utc":1752991180,"send_replies":true,"parent_id":"t3_1m4hfy0","score":3,"author_fullname":"t2_1oy2v7xti6","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Limit max token generation to 1 then add your stuff whenever you want to.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n44ez6y","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Limit max token generation to 1 then add your stuff whenever you want to.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4hfy0/does_llm_architecture_allow_for_injecting_some/n44ez6y/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752991180,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m4hfy0","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n455uc2","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"blepcoin","can_mod_post":false,"created_utc":1753006508,"send_replies":true,"parent_id":"t3_1m4hfy0","score":3,"author_fullname":"t2_dg5ivdb","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Take it one step further and remove the send part completely. As you start typing the LLM starts responding (including predicting your question perhaps) immediately. Completing the question and/or modifying it is incorporated into the current llm thoughts rather than resetting every keystroke. You could then tweak and fix as you watch the llm thought process go awry due to that typo or gotcha you should’ve included.\\n\\nInteresting academic challenge to make the training for this work.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n455uc2","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Take it one step further and remove the send part completely. As you start typing the LLM starts responding (including predicting your question perhaps) immediately. Completing the question and/or modifying it is incorporated into the current llm thoughts rather than resetting every keystroke. You could then tweak and fix as you watch the llm thought process go awry due to that typo or gotcha you should’ve included.&lt;/p&gt;\\n\\n&lt;p&gt;Interesting academic challenge to make the training for this work.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4hfy0/does_llm_architecture_allow_for_injecting_some/n455uc2/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753006508,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m4hfy0","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n44iykl","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"bjodah","can_mod_post":false,"send_replies":true,"parent_id":"t1_n44hqk4","score":6,"author_fullname":"t2_atvy2","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"you just cancel the previous request, if whatever frontend you are using does not allow for that: switch.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n44iykl","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;you just cancel the previous request, if whatever frontend you are using does not allow for that: switch.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m4hfy0","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4hfy0/does_llm_architecture_allow_for_injecting_some/n44iykl/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752993322,"author_flair_text":null,"treatment_tags":[],"created_utc":1752993322,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n44iv07","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"kneeanderthul","can_mod_post":false,"send_replies":true,"parent_id":"t1_n44hqk4","score":6,"author_fullname":"t2_nz7b4uav","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"My intent wasn’t to dismiss your idea — just to offer a practical workaround based on how I understand current models work.\\nAs far as I know, LLMs must complete their processing before you can introduce new tokens. Pausing and injecting mid-inference isn’t currently how the architecture works — even the idea of a 'pause' is really just canceling and re-prompting.\\n\\nThat said, if you do find a way to inject in real time, you’d be breaking new ground. It would fundamentally change how we think about dynamic interaction with LLMs. I genuinely hope you push it forward — would be amazing to see","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n44iv07","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;My intent wasn’t to dismiss your idea — just to offer a practical workaround based on how I understand current models work.\\nAs far as I know, LLMs must complete their processing before you can introduce new tokens. Pausing and injecting mid-inference isn’t currently how the architecture works — even the idea of a &amp;#39;pause&amp;#39; is really just canceling and re-prompting.&lt;/p&gt;\\n\\n&lt;p&gt;That said, if you do find a way to inject in real time, you’d be breaking new ground. It would fundamentally change how we think about dynamic interaction with LLMs. I genuinely hope you push it forward — would be amazing to see&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m4hfy0","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4hfy0/does_llm_architecture_allow_for_injecting_some/n44iv07/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752993267,"author_flair_text":null,"treatment_tags":[],"created_utc":1752993267,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}}],"before":null}},"user_reports":[],"saved":false,"id":"n44hqk4","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":"LOW_SCORE","no_follow":true,"author":"michaelsoft__binbows","can_mod_post":false,"created_utc":1752992654,"send_replies":true,"parent_id":"t1_n44evwh","score":-5,"author_fullname":"t2_iifi6ul2l","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"no... you're not getting it. i send the prompt. It's going to crunch for a total of 60 seconds, 45 of which is done in thinking mode. My thought comes in at t=2s and i am able to type it by t=7s. There is still time.\\n\\nYou are suggesting I wait the full 60 seconds and issue a new prompt waiting another (expected to be) 60s. Which amounts to two full prompts and responses worth of consumed tokens. \\n\\nI'm talking about something pretty low level and (if the stars align on timing) making more efficient use of time and resources. You're just dismissing the idea by wilfully not considering what i'm trying to describe.","edited":1752992957,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n44hqk4","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;no... you&amp;#39;re not getting it. i send the prompt. It&amp;#39;s going to crunch for a total of 60 seconds, 45 of which is done in thinking mode. My thought comes in at t=2s and i am able to type it by t=7s. There is still time.&lt;/p&gt;\\n\\n&lt;p&gt;You are suggesting I wait the full 60 seconds and issue a new prompt waiting another (expected to be) 60s. Which amounts to two full prompts and responses worth of consumed tokens. &lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;m talking about something pretty low level and (if the stars align on timing) making more efficient use of time and resources. You&amp;#39;re just dismissing the idea by wilfully not considering what i&amp;#39;m trying to describe.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":"comment score below threshold","distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m4hfy0","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4hfy0/does_llm_architecture_allow_for_injecting_some/n44hqk4/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752992654,"author_flair_text":null,"treatment_tags":[],"collapsed":true,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-5}}],"before":null}},"user_reports":[],"saved":false,"id":"n44evwh","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"kneeanderthul","can_mod_post":false,"created_utc":1752991132,"send_replies":true,"parent_id":"t3_1m4hfy0","score":9,"author_fullname":"t2_nz7b4uav","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"A simple \\n\\n“Reconsider previous prompt with this new info:” \\n\\nAnd you’re done.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n44evwh","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;A simple &lt;/p&gt;\\n\\n&lt;p&gt;“Reconsider previous prompt with this new info:” &lt;/p&gt;\\n\\n&lt;p&gt;And you’re done.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4hfy0/does_llm_architecture_allow_for_injecting_some/n44evwh/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752991132,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m4hfy0","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":9}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n46imcr","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Pedalnomica","can_mod_post":false,"created_utc":1753025761,"send_replies":true,"parent_id":"t3_1m4hfy0","score":1,"author_fullname":"t2_b0d7j6x9","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Yes and maybe.\\n\\n\\nYes in that the models are previous tokens in, next token(s) out, repeat unless stopped. There is no LLM architectural reason you couldn't pause it at a repeat, add more tokens of your own, and resume. \\n\\nI say maybe in that they aren't really trained for this to happen in the middle of the thinking tokens like you seem to want, and I haven't seen any evals on any tasks like that. (Plus, I haven't seen any tooling set up to make that easy.) Whether it would work well, even with some type of additional training, is an empirical question.\\n\\nMy gut says you're better off just waiting or restarting than investing time in answering that question... but I've been wrong before!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n46imcr","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yes and maybe.&lt;/p&gt;\\n\\n&lt;p&gt;Yes in that the models are previous tokens in, next token(s) out, repeat unless stopped. There is no LLM architectural reason you couldn&amp;#39;t pause it at a repeat, add more tokens of your own, and resume. &lt;/p&gt;\\n\\n&lt;p&gt;I say maybe in that they aren&amp;#39;t really trained for this to happen in the middle of the thinking tokens like you seem to want, and I haven&amp;#39;t seen any evals on any tasks like that. (Plus, I haven&amp;#39;t seen any tooling set up to make that easy.) Whether it would work well, even with some type of additional training, is an empirical question.&lt;/p&gt;\\n\\n&lt;p&gt;My gut says you&amp;#39;re better off just waiting or restarting than investing time in answering that question... but I&amp;#39;ve been wrong before!&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4hfy0/does_llm_architecture_allow_for_injecting_some/n46imcr/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753025761,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m4hfy0","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n49q705","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Awwtifishal","can_mod_post":false,"send_replies":true,"parent_id":"t1_n47ocn3","score":1,"author_fullname":"t2_1d96a8k10t","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Just in case I was misinterpreted: KV cache reuses the context from the beginning up to the point where something changed. That means that any later tokens can't be reused and have to be reprocessed. In practice you rarely need to change anything that is not near the end of the used context.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n49q705","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Just in case I was misinterpreted: KV cache reuses the context from the beginning up to the point where something changed. That means that any later tokens can&amp;#39;t be reused and have to be reprocessed. In practice you rarely need to change anything that is not near the end of the used context.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m4hfy0","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4hfy0/does_llm_architecture_allow_for_injecting_some/n49q705/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753063338,"author_flair_text":null,"treatment_tags":[],"created_utc":1753063338,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n47ocn3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"michaelsoft__binbows","can_mod_post":false,"created_utc":1753038306,"send_replies":true,"parent_id":"t1_n47de3s","score":1,"author_fullname":"t2_iifi6ul2l","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Thank you. I'm using sglang currently and found this: https://github.com/sgl-project/sglang/issues/906#issuecomment-2267222733\\n\\nWill pay attention to this info next time.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n47ocn3","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thank you. I&amp;#39;m using sglang currently and found this: &lt;a href=\\"https://github.com/sgl-project/sglang/issues/906#issuecomment-2267222733\\"&gt;https://github.com/sgl-project/sglang/issues/906#issuecomment-2267222733&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;Will pay attention to this info next time.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m4hfy0","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4hfy0/does_llm_architecture_allow_for_injecting_some/n47ocn3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753038306,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n47de3s","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Awwtifishal","can_mod_post":false,"created_utc":1753034927,"send_replies":true,"parent_id":"t3_1m4hfy0","score":1,"author_fullname":"t2_1d96a8k10t","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"With KV cache it costs basically zero to use the part of the context that has not changed. With a local LLM (or with a self deployed one) you get that by default.  \\n  \\nSome LLM services does offer cached input tokens, that may be more expensive the first time but free the next time if it's within a specific amount of time, so it's worth it in the vast majority of cases.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n47de3s","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;With KV cache it costs basically zero to use the part of the context that has not changed. With a local LLM (or with a self deployed one) you get that by default.  &lt;/p&gt;\\n\\n&lt;p&gt;Some LLM services does offer cached input tokens, that may be more expensive the first time but free the next time if it&amp;#39;s within a specific amount of time, so it&amp;#39;s worth it in the vast majority of cases.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4hfy0/does_llm_architecture_allow_for_injecting_some/n47de3s/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753034927,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m4hfy0","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n49lb25","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Imaginary_Bench_7294","can_mod_post":false,"created_utc":1753061521,"send_replies":true,"parent_id":"t3_1m4hfy0","score":1,"author_fullname":"t2_fling4kx","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"So, at least with the way that you described it, what you want to do is not compatible with current LLM architecture.\\n\\nIt mostly boils down to the attention mechanism.\\n\\nSo, when an input is submitted to the model for processing, it has to compare the Q, K, and V values of all tokens to all tokens. This is how the model knows what relates to what, which bits to emphasize or depreciate, etc. This is essentially the higher order \\"thinking\\" that is internal to the model that allows it to decide which things in the prompt are important.\\n\\nSo, if you were to say, submit a prompt and then pause the generation and modify it, the model only has two choices:\\n\\n**_Continue generation with the previous attention matrices, ignoring any and all alterations._**\\n\\n**_Reprocess the entire sequence to incorporate the new data into the attention matrices._**\\n\\nThis is one of the downsides to the attention mechanism. It is an all or nothing design for the most part. Either the LLM is provided the full prompt at runtime, or it doesn't know its own ass from a hole in the wall.\\n\\nThere are some caveats:\\n- Sliding window attention models\\n- Attention free models such as RWKV\\n- Sparse attention models\\n- More exotic designs like Hyena\\n\\nIn all of those designs, there are drawbacks.\\n\\n**Sliding window attention** processes the sequence in overlapping or sequential chunks (depends on the particular implementation)  called windows. It sacrifices global relational capabilities for localized relational capabilities. Basically, it may no longer be able to relate the first paragraph of a book to one that is 10 pages in, but it should be quite good within the paragraph. If implemented properly, this would make it so only the windows that overlap the changed data need to be recomputed.\\n\\n**RWKV** technically has infinite context but suffers from forgetting over time (iirc) and must process things mostly sequentially. This architecture you could probably do what you're talking about - injecting new tokens mid processing. I haven't kept up with this project, though, so I don't know the current capabilities.\\n \\n **Sparse attention** still has to recompute the attention matrices, but it uses some method to select tokens to drop from the matrix. So, it's still attention, just with fewer stored values.\\n\\nHyena and other \\"exotic\\" architectures... well, to be honest, I don't know enough about them to give you much detail or to know if they're capable of doing what you want.\\n\\nSo, unfortunately, at least with the way that you described what you want to do, it is not viable with the current mainstream architectures. They need all of the prompt data, all at the same time, otherwise they don't know how to integrate the data.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n49lb25","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;So, at least with the way that you described it, what you want to do is not compatible with current LLM architecture.&lt;/p&gt;\\n\\n&lt;p&gt;It mostly boils down to the attention mechanism.&lt;/p&gt;\\n\\n&lt;p&gt;So, when an input is submitted to the model for processing, it has to compare the Q, K, and V values of all tokens to all tokens. This is how the model knows what relates to what, which bits to emphasize or depreciate, etc. This is essentially the higher order &amp;quot;thinking&amp;quot; that is internal to the model that allows it to decide which things in the prompt are important.&lt;/p&gt;\\n\\n&lt;p&gt;So, if you were to say, submit a prompt and then pause the generation and modify it, the model only has two choices:&lt;/p&gt;\\n\\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;Continue generation with the previous attention matrices, ignoring any and all alterations.&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;\\n\\n&lt;p&gt;&lt;strong&gt;&lt;em&gt;Reprocess the entire sequence to incorporate the new data into the attention matrices.&lt;/em&gt;&lt;/strong&gt;&lt;/p&gt;\\n\\n&lt;p&gt;This is one of the downsides to the attention mechanism. It is an all or nothing design for the most part. Either the LLM is provided the full prompt at runtime, or it doesn&amp;#39;t know its own ass from a hole in the wall.&lt;/p&gt;\\n\\n&lt;p&gt;There are some caveats:\\n- Sliding window attention models\\n- Attention free models such as RWKV\\n- Sparse attention models\\n- More exotic designs like Hyena&lt;/p&gt;\\n\\n&lt;p&gt;In all of those designs, there are drawbacks.&lt;/p&gt;\\n\\n&lt;p&gt;&lt;strong&gt;Sliding window attention&lt;/strong&gt; processes the sequence in overlapping or sequential chunks (depends on the particular implementation)  called windows. It sacrifices global relational capabilities for localized relational capabilities. Basically, it may no longer be able to relate the first paragraph of a book to one that is 10 pages in, but it should be quite good within the paragraph. If implemented properly, this would make it so only the windows that overlap the changed data need to be recomputed.&lt;/p&gt;\\n\\n&lt;p&gt;&lt;strong&gt;RWKV&lt;/strong&gt; technically has infinite context but suffers from forgetting over time (iirc) and must process things mostly sequentially. This architecture you could probably do what you&amp;#39;re talking about - injecting new tokens mid processing. I haven&amp;#39;t kept up with this project, though, so I don&amp;#39;t know the current capabilities.&lt;/p&gt;\\n\\n&lt;p&gt;&lt;strong&gt;Sparse attention&lt;/strong&gt; still has to recompute the attention matrices, but it uses some method to select tokens to drop from the matrix. So, it&amp;#39;s still attention, just with fewer stored values.&lt;/p&gt;\\n\\n&lt;p&gt;Hyena and other &amp;quot;exotic&amp;quot; architectures... well, to be honest, I don&amp;#39;t know enough about them to give you much detail or to know if they&amp;#39;re capable of doing what you want.&lt;/p&gt;\\n\\n&lt;p&gt;So, unfortunately, at least with the way that you described what you want to do, it is not viable with the current mainstream architectures. They need all of the prompt data, all at the same time, otherwise they don&amp;#39;t know how to integrate the data.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4hfy0/does_llm_architecture_allow_for_injecting_some/n49lb25/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753061521,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m4hfy0","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4ah0gp","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"elbiot","can_mod_post":false,"created_utc":1753074585,"send_replies":true,"parent_id":"t3_1m4hfy0","score":1,"author_fullname":"t2_55ir3","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Of course LLMs do. The web chat interface doesn't","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4ah0gp","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Of course LLMs do. The web chat interface doesn&amp;#39;t&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4hfy0/does_llm_architecture_allow_for_injecting_some/n4ah0gp/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753074585,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m4hfy0","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n47ncug","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"michaelsoft__binbows","can_mod_post":false,"send_replies":true,"parent_id":"t1_n45zd6d","score":1,"author_fullname":"t2_iifi6ul2l","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Ok this is interesting but seems primitive compared to the other suggestion of blowing the doors off the \\"prompt submission flow\\" entirely and allowing inference to proceed based off of what's been typed in real time.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n47ncug","is_submitter":true,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Ok this is interesting but seems primitive compared to the other suggestion of blowing the doors off the &amp;quot;prompt submission flow&amp;quot; entirely and allowing inference to proceed based off of what&amp;#39;s been typed in real time.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m4hfy0","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4hfy0/does_llm_architecture_allow_for_injecting_some/n47ncug/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753037997,"author_flair_text":null,"treatment_tags":[],"created_utc":1753037997,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n45zd6d","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"kneeanderthul","can_mod_post":false,"created_utc":1753019547,"send_replies":true,"parent_id":"t1_n44l81p","score":2,"author_fullname":"t2_nz7b4uav","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Thank you for sharing Review Gate! I’ve been diving deep into MCPs lately, so I was genuinely excited when you mentioned there might be one doing something groundbreaking here. I took a closer look, and here’s what I found:\\n\\n* You type a prompt (let’s call it A), and instead of immediately sending it to the model, Review Gate pauses and opens a local terminal.\\n* You’re then invited to add more input (B, C, etc.) while the request is still “on hold.”\\n* Once you signal you’re done, it bundles everything you wrote (A + B + C…) and sends **a single request** to the model.\\n\\nIn other words, the model only ever sees **one complete prompt**, sent once you give the green light. There’s no live injection, no mid-thread augmentation — just a helpful pause before sending.\\n\\nThat doesn’t make it any less valuable! Personally, I’ve burned more tokens than I care to admit by sending too fast — so I love tools that help slow me down. Even just having a separate terminal pop up changes the *feel* of the moment. That bit of friction gives your brain a second wind, and that’s powerful.\\n\\nBut to be clear: this isn’t a memory trick or a runtime prompt extender. It’s more like a **staging area** — a space to collect your thoughts before you hit “send.” Helpful? Absolutely. The magic isn’t in what the model sees — it’s in how it helps you think before you send. And that part is very real.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n45zd6d","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thank you for sharing Review Gate! I’ve been diving deep into MCPs lately, so I was genuinely excited when you mentioned there might be one doing something groundbreaking here. I took a closer look, and here’s what I found:&lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;You type a prompt (let’s call it A), and instead of immediately sending it to the model, Review Gate pauses and opens a local terminal.&lt;/li&gt;\\n&lt;li&gt;You’re then invited to add more input (B, C, etc.) while the request is still “on hold.”&lt;/li&gt;\\n&lt;li&gt;Once you signal you’re done, it bundles everything you wrote (A + B + C…) and sends &lt;strong&gt;a single request&lt;/strong&gt; to the model.&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;p&gt;In other words, the model only ever sees &lt;strong&gt;one complete prompt&lt;/strong&gt;, sent once you give the green light. There’s no live injection, no mid-thread augmentation — just a helpful pause before sending.&lt;/p&gt;\\n\\n&lt;p&gt;That doesn’t make it any less valuable! Personally, I’ve burned more tokens than I care to admit by sending too fast — so I love tools that help slow me down. Even just having a separate terminal pop up changes the &lt;em&gt;feel&lt;/em&gt; of the moment. That bit of friction gives your brain a second wind, and that’s powerful.&lt;/p&gt;\\n\\n&lt;p&gt;But to be clear: this isn’t a memory trick or a runtime prompt extender. It’s more like a &lt;strong&gt;staging area&lt;/strong&gt; — a space to collect your thoughts before you hit “send.” Helpful? Absolutely. The magic isn’t in what the model sees — it’s in how it helps you think before you send. And that part is very real.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m4hfy0","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4hfy0/does_llm_architecture_allow_for_injecting_some/n45zd6d/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753019547,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n44l81p","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"derdigga","can_mod_post":false,"created_utc":1752994552,"send_replies":true,"parent_id":"t3_1m4hfy0","score":1,"author_fullname":"t2_i8i51","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"There is a mcp for that, check on reviewgate","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n44l81p","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;There is a mcp for that, check on reviewgate&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4hfy0/does_llm_architecture_allow_for_injecting_some/n44l81p/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752994552,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m4hfy0","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n44n0mw","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AFruitShopOwner","can_mod_post":false,"created_utc":1752995540,"send_replies":true,"parent_id":"t3_1m4hfy0","score":1,"author_fullname":"t2_h15f0","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Doesn't claude code let you do this already?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n44n0mw","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Doesn&amp;#39;t claude code let you do this already?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4hfy0/does_llm_architecture_allow_for_injecting_some/n44n0mw/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752995540,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m4hfy0","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n47nrf7","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"michaelsoft__binbows","can_mod_post":false,"send_replies":true,"parent_id":"t1_n45pjhy","score":1,"author_fullname":"t2_iifi6ul2l","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"That's a nice optimization, in terms of agentic I haven't considered wanting to pause and resume with data fetched from an async request. That's a pretty interesting approach if it could work.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n47nrf7","is_submitter":true,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;That&amp;#39;s a nice optimization, in terms of agentic I haven&amp;#39;t considered wanting to pause and resume with data fetched from an async request. That&amp;#39;s a pretty interesting approach if it could work.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m4hfy0","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4hfy0/does_llm_architecture_allow_for_injecting_some/n47nrf7/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753038122,"author_flair_text":null,"treatment_tags":[],"created_utc":1753038122,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n45pjhy","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"absolooot1","can_mod_post":false,"created_utc":1753015932,"send_replies":true,"parent_id":"t1_n44fsqe","score":1,"author_fullname":"t2_1pr7hwh6t5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I'm pretty sure that the large proprietary LLM vendors that offer their models with agentic/tool calling abilities do exactly what you're proposing: the model outputs some tokens, realizes a tool call is needed, issues the tool call as part of the ongoing response, generation is paused for the tool call results to come back from the serving software, results are inserted in the response, the generation continues.\\n\\nI don't know if any of the usual local LLM serving software like vLLM or llama.cpp offer this functionality, but I think it is available in the Hugging Face transformers library. But that's not a very speedy way of running an LLM... It may be worth experimenting to at least learn how to implement the injection.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n45pjhy","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;m pretty sure that the large proprietary LLM vendors that offer their models with agentic/tool calling abilities do exactly what you&amp;#39;re proposing: the model outputs some tokens, realizes a tool call is needed, issues the tool call as part of the ongoing response, generation is paused for the tool call results to come back from the serving software, results are inserted in the response, the generation continues.&lt;/p&gt;\\n\\n&lt;p&gt;I don&amp;#39;t know if any of the usual local LLM serving software like vLLM or llama.cpp offer this functionality, but I think it is available in the Hugging Face transformers library. But that&amp;#39;s not a very speedy way of running an LLM... It may be worth experimenting to at least learn how to implement the injection.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m4hfy0","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4hfy0/does_llm_architecture_allow_for_injecting_some/n45pjhy/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753015932,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n44fsqe","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"michaelsoft__binbows","can_mod_post":false,"created_utc":1752991615,"send_replies":true,"parent_id":"t3_1m4hfy0","score":0,"author_fullname":"t2_iifi6ul2l","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"i think this is pretty interesting to think about, there is a parallel here with the nuances of carrying on a spoken conversation. Emphasis on nuance. How to make a judgement call based off of a given burst of sound waves to make the call on whether we should stop talking and listen or carry on. It's wildly difficult.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n44fsqe","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;i think this is pretty interesting to think about, there is a parallel here with the nuances of carrying on a spoken conversation. Emphasis on nuance. How to make a judgement call based off of a given burst of sound waves to make the call on whether we should stop talking and listen or carry on. It&amp;#39;s wildly difficult.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4hfy0/does_llm_architecture_allow_for_injecting_some/n44fsqe/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752991615,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m4hfy0","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"c07aa42e-51fe-11f0-afcc-462aad931709","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n46bt37","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"entsnack","can_mod_post":false,"created_utc":1753023659,"send_replies":true,"parent_id":"t3_1m4hfy0","score":0,"author_fullname":"t2_1a48h7vf","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"It's called multi-turn reinforcement fine-tuning. Check out the Verifiers library by Will on Github.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n46bt37","is_submitter":false,"downs":0,"author_flair_richtext":[{"a":":X:","u":"https://emoji.redditmedia.com/tbgegafk739f1_t5_81eyvm/X","e":"emoji"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It&amp;#39;s called multi-turn reinforcement fine-tuning. Check out the Verifiers library by Will on Github.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"dark","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m4hfy0/does_llm_architecture_allow_for_injecting_some/n46bt37/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753023659,"author_flair_text":":X:","treatment_tags":[],"link_id":"t3_1m4hfy0","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"transparent","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}}]`),r=()=>e.jsx(t,{data:l});export{r as default};
