import{j as e}from"./index-Cd3v0jxz.js";import{R as t}from"./RedditPostRenderer-cI96YLhy.js";import"./index-DGBoKyQm.js";const l=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"You guys might've seen my earlier posts about the models I downloaded spitting out their chat template, looping around it, etc etc. I fixed it and I really appreciate the comments.\\n\\nNow, this next issue is something I couldn't fix. I only have 16GB of RAM, no dGPU, on a mobile CPU. I managed to run Gemma-3 4B-Q4-K-XL for a bit but it hit rock bottom when it complained about context window being too big for it. I tried to search about it and how to fix it but I came up with nothing, basically.\\n\\nI'm making this post to get help for me and others who might encounter the same issue in the future.","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Running AIs Locally without a GPU: Context Window","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1m42c2q","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.67,"author_flair_background_color":null,"subreddit_type":"public","ups":2,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_cw6v7ot8","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":2,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":1752952138,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1752947215,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;You guys might&amp;#39;ve seen my earlier posts about the models I downloaded spitting out their chat template, looping around it, etc etc. I fixed it and I really appreciate the comments.&lt;/p&gt;\\n\\n&lt;p&gt;Now, this next issue is something I couldn&amp;#39;t fix. I only have 16GB of RAM, no dGPU, on a mobile CPU. I managed to run Gemma-3 4B-Q4-K-XL for a bit but it hit rock bottom when it complained about context window being too big for it. I tried to search about it and how to fix it but I came up with nothing, basically.&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;m making this post to get help for me and others who might encounter the same issue in the future.&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1m42c2q","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"Leather_Flan5071","discussion_type":null,"num_comments":6,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1m42c2q/running_ais_locally_without_a_gpu_context_window/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1m42c2q/running_ais_locally_without_a_gpu_context_window/","subreddit_subscribers":502030,"created_utc":1752947215,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n41t9lu","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Leather_Flan5071","can_mod_post":false,"send_replies":true,"parent_id":"t1_n41od3d","score":1,"author_fullname":"t2_cw6v7ot8","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I'm thinking some sort of http expiration cuz I am dealing with http and stuff. The model took a while to generate a response so it could be that","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n41t9lu","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;m thinking some sort of http expiration cuz I am dealing with http and stuff. The model took a while to generate a response so it could be that&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m42c2q","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m42c2q/running_ais_locally_without_a_gpu_context_window/n41t9lu/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752955301,"author_flair_text":null,"treatment_tags":[],"created_utc":1752955301,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n41od3d","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"eloquentemu","can_mod_post":false,"send_replies":true,"parent_id":"t1_n41jlq5","score":2,"author_fullname":"t2_lpdsy","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Hrm.  I haven't used OpenWebUI much. Where did you see:\\n\\n&gt; it complained about context window being too big for it\\n\\nIs it possible it hits the context limit after generating the 615 tokens and you're seeing a sort of \\"inference terminated early\\" error?","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n41od3d","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Hrm.  I haven&amp;#39;t used OpenWebUI much. Where did you see:&lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;it complained about context window being too big for it&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;Is it possible it hits the context limit after generating the 615 tokens and you&amp;#39;re seeing a sort of &amp;quot;inference terminated early&amp;quot; error?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m42c2q","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m42c2q/running_ais_locally_without_a_gpu_context_window/n41od3d/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752953714,"author_flair_text":null,"treatment_tags":[],"created_utc":1752953714,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n41jlq5","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Leather_Flan5071","can_mod_post":false,"created_utc":1752952176,"send_replies":true,"parent_id":"t1_n41evn1","score":1,"author_fullname":"t2_cw6v7ot8","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Well, I found out right after posting this. I do get the model to generate something but it doesn't print anything on the OpenWebUI interface.\\n\\n    slot print_timing: id  0 | task 0 | \\n    prompt eval time = 1302678.24 ms / 31293 tokens (   41.63 ms per token,    24.02 tokens per second)\\n           eval time =   89470.04 ms /   615 tokens (  145.48 ms per token,     6.87 tokens per second)\\n          total time = 1392148.28 ms / 31908 tokens\\n    srv  update_slots: all slots are idle\\n\\nhttps://preview.redd.it/t9ldjr4vqvdf1.png?width=1005&amp;format=png&amp;auto=webp&amp;s=cf900a8f47eb626874a215c9c626f7eb315f499e","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n41jlq5","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Well, I found out right after posting this. I do get the model to generate something but it doesn&amp;#39;t print anything on the OpenWebUI interface.&lt;/p&gt;\\n\\n&lt;pre&gt;&lt;code&gt;slot print_timing: id  0 | task 0 | \\nprompt eval time = 1302678.24 ms / 31293 tokens (   41.63 ms per token,    24.02 tokens per second)\\n       eval time =   89470.04 ms /   615 tokens (  145.48 ms per token,     6.87 tokens per second)\\n      total time = 1392148.28 ms / 31908 tokens\\nsrv  update_slots: all slots are idle\\n&lt;/code&gt;&lt;/pre&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://preview.redd.it/t9ldjr4vqvdf1.png?width=1005&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cf900a8f47eb626874a215c9c626f7eb315f499e\\"&gt;https://preview.redd.it/t9ldjr4vqvdf1.png?width=1005&amp;amp;format=png&amp;amp;auto=webp&amp;amp;s=cf900a8f47eb626874a215c9c626f7eb315f499e&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m42c2q","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m42c2q/running_ais_locally_without_a_gpu_context_window/n41jlq5/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752952176,"media_metadata":{"t9ldjr4vqvdf1":{"status":"valid","e":"Image","m":"image/png","p":[{"y":23,"x":108,"u":"https://preview.redd.it/t9ldjr4vqvdf1.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=0728d2cdb5941b0a579c335a2c4df8fccf51fe86"},{"y":47,"x":216,"u":"https://preview.redd.it/t9ldjr4vqvdf1.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=cf0b9b8c3f93310b51a4eb4f204d4cf85f93130f"},{"y":70,"x":320,"u":"https://preview.redd.it/t9ldjr4vqvdf1.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=ed2749e18281fd1c0648fc8e8604ee45bf698b37"},{"y":140,"x":640,"u":"https://preview.redd.it/t9ldjr4vqvdf1.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=1a1861f0023170543f2c29d08e36b1c02ce4e0dc"},{"y":210,"x":960,"u":"https://preview.redd.it/t9ldjr4vqvdf1.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=e7d3644569388b56db5cdc5fa0b28a773429b849"}],"s":{"y":220,"x":1005,"u":"https://preview.redd.it/t9ldjr4vqvdf1.png?width=1005&amp;format=png&amp;auto=webp&amp;s=cf900a8f47eb626874a215c9c626f7eb315f499e"},"id":"t9ldjr4vqvdf1"}},"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n41evn1","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"eloquentemu","can_mod_post":false,"created_utc":1752950654,"send_replies":true,"parent_id":"t3_1m42c2q","score":2,"author_fullname":"t2_lpdsy","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"How are you running it?  Context size is configurable, so basically you just need to configure a smaller context.  For llama.cpp this is the \`-c\` (or \`--ctx-size\`) argument.  So you'd do something like \`llama-cli -m Gemma-3-4B-Q4-K-XL.gguf -c 1000\`.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n41evn1","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;How are you running it?  Context size is configurable, so basically you just need to configure a smaller context.  For llama.cpp this is the &lt;code&gt;-c&lt;/code&gt; (or &lt;code&gt;--ctx-size&lt;/code&gt;) argument.  So you&amp;#39;d do something like &lt;code&gt;llama-cli -m Gemma-3-4B-Q4-K-XL.gguf -c 1000&lt;/code&gt;.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m42c2q/running_ais_locally_without_a_gpu_context_window/n41evn1/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752950654,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m42c2q","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n41sykt","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Leather_Flan5071","can_mod_post":false,"created_utc":1752955201,"send_replies":true,"parent_id":"t1_n41rkx3","score":1,"author_fullname":"t2_cw6v7ot8","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I didn't set any context size.\\n\\n./llama-server -m ../../models/gemma-3-4b-it-UD-Q4\\\\_K\\\\_XL.gguf --host [0.0.0.0](http://0.0.0.0) \\\\--port 11343 --chat-template-file ../../models/gemma-3-4b-it-UD-IQ1\\\\_M.gguf.jinja\\n\\nI just ran this before discovering that I can set the context size via -c or --ctx-size\\n\\nalso the problem wasn't that I had too much context, it was that I didn't have enough. The Model couldn't exactly process the imported chat I gave it","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n41sykt","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I didn&amp;#39;t set any context size.&lt;/p&gt;\\n\\n&lt;p&gt;./llama-server -m ../../models/gemma-3-4b-it-UD-Q4_K_XL.gguf --host &lt;a href=\\"http://0.0.0.0\\"&gt;0.0.0.0&lt;/a&gt; --port 11343 --chat-template-file ../../models/gemma-3-4b-it-UD-IQ1_M.gguf.jinja&lt;/p&gt;\\n\\n&lt;p&gt;I just ran this before discovering that I can set the context size via -c or --ctx-size&lt;/p&gt;\\n\\n&lt;p&gt;also the problem wasn&amp;#39;t that I had too much context, it was that I didn&amp;#39;t have enough. The Model couldn&amp;#39;t exactly process the imported chat I gave it&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m42c2q","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m42c2q/running_ais_locally_without_a_gpu_context_window/n41sykt/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752955201,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n41rkx3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Marksta","can_mod_post":false,"created_utc":1752954760,"send_replies":true,"parent_id":"t3_1m42c2q","score":1,"author_fullname":"t2_559a1","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Post your full llama.cpp command or people can't really help. I think default context window is full size so if you didn't set one, yea its going to be 128K which is going to take up all your system's memory. Set it to 32K or 16K with -c","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n41rkx3","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Post your full llama.cpp command or people can&amp;#39;t really help. I think default context window is full size so if you didn&amp;#39;t set one, yea its going to be 128K which is going to take up all your system&amp;#39;s memory. Set it to 32K or 16K with -c&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m42c2q/running_ais_locally_without_a_gpu_context_window/n41rkx3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752954760,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m42c2q","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),r=()=>e.jsx(t,{data:l});export{r as default};
