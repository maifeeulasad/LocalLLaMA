import{j as e}from"./index-Bu7qcPAU.js";import{R as l}from"./RedditPostRenderer-CbHA7O5q.js";import"./index-BKgbfxhf.js";const t=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Hey everyone,\\n\\nI'm looking to set up a local code assistant/agentic LLM on my own VPS and could use your recommendations!\\n\\n**Specs:**\\n\\n* 8 vCores (VPS)\\n* 16 GB RAM\\n* 480 GB NVMe SSD\\n\\nI plan to run everything with **Ollama** (Dockerized), mainly for local privacy and performance reasons.\\n\\n**My goals:**\\n\\n* **Agentic coding**: Not just code completion, but autonomous code changes, repo analysis, bug fixing, etc.\\n* Integrate it into my workflow, ideally via **VS Code** (extension).\\n\\n**What I've tried so far:**\\n\\n* I’ve already tried using the **Cline** extension in VS Code (with Ollama as backend and models like Qwen2.5-Coder:7b/14b).\\n* Unfortunately, **everything freezes up as soon as I start an agentic coding task or send an API call**. Cline doesn’t respond, and the model never replies (even with enough RAM, etc.).\\n\\n**Questions:**\\n\\n1. **Which local LLM would you recommend for my specs?** (Qwen2.5-Coder, Deepseek Coder, Llama-3, etc. — ideally with “agent” features or good reasoning/coding performance)\\n2. **Which VS Code extension works best for local Ollama models** (agent-style coding, not just chat)? I know about “Continue” and “Cline” — but Cline seems unstable for me. Any real-world feedback, or others to consider?\\n\\n**Bonus:**  \\nIf you’ve actually run “agentic” workflows (like multi-step code changes, repo understanding, etc.) with a local model and VS Code, please share your experiences!\\n\\nThanks in advance!","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Best Local LLM for Agentic Coding on Ollama (8 vCore, 16 GB RAM VPS)? + VS Code Extension Recommendation","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lvjtc4","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.6,"author_flair_background_color":null,"subreddit_type":"public","ups":2,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_eiet8zs3","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":2,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1752070321,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Hey everyone,&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;m looking to set up a local code assistant/agentic LLM on my own VPS and could use your recommendations!&lt;/p&gt;\\n\\n&lt;p&gt;&lt;strong&gt;Specs:&lt;/strong&gt;&lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;8 vCores (VPS)&lt;/li&gt;\\n&lt;li&gt;16 GB RAM&lt;/li&gt;\\n&lt;li&gt;480 GB NVMe SSD&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;p&gt;I plan to run everything with &lt;strong&gt;Ollama&lt;/strong&gt; (Dockerized), mainly for local privacy and performance reasons.&lt;/p&gt;\\n\\n&lt;p&gt;&lt;strong&gt;My goals:&lt;/strong&gt;&lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;&lt;strong&gt;Agentic coding&lt;/strong&gt;: Not just code completion, but autonomous code changes, repo analysis, bug fixing, etc.&lt;/li&gt;\\n&lt;li&gt;Integrate it into my workflow, ideally via &lt;strong&gt;VS Code&lt;/strong&gt; (extension).&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;p&gt;&lt;strong&gt;What I&amp;#39;ve tried so far:&lt;/strong&gt;&lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;I’ve already tried using the &lt;strong&gt;Cline&lt;/strong&gt; extension in VS Code (with Ollama as backend and models like Qwen2.5-Coder:7b/14b).&lt;/li&gt;\\n&lt;li&gt;Unfortunately, &lt;strong&gt;everything freezes up as soon as I start an agentic coding task or send an API call&lt;/strong&gt;. Cline doesn’t respond, and the model never replies (even with enough RAM, etc.).&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;p&gt;&lt;strong&gt;Questions:&lt;/strong&gt;&lt;/p&gt;\\n\\n&lt;ol&gt;\\n&lt;li&gt;&lt;strong&gt;Which local LLM would you recommend for my specs?&lt;/strong&gt; (Qwen2.5-Coder, Deepseek Coder, Llama-3, etc. — ideally with “agent” features or good reasoning/coding performance)&lt;/li&gt;\\n&lt;li&gt;&lt;strong&gt;Which VS Code extension works best for local Ollama models&lt;/strong&gt; (agent-style coding, not just chat)? I know about “Continue” and “Cline” — but Cline seems unstable for me. Any real-world feedback, or others to consider?&lt;/li&gt;\\n&lt;/ol&gt;\\n\\n&lt;p&gt;&lt;strong&gt;Bonus:&lt;/strong&gt;&lt;br/&gt;\\nIf you’ve actually run “agentic” workflows (like multi-step code changes, repo understanding, etc.) with a local model and VS Code, please share your experiences!&lt;/p&gt;\\n\\n&lt;p&gt;Thanks in advance!&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":true,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1lvjtc4","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"HeislPeda","discussion_type":null,"num_comments":3,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lvjtc4/best_local_llm_for_agentic_coding_on_ollama_8/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lvjtc4/best_local_llm_for_agentic_coding_on_ollama_8/","subreddit_subscribers":497025,"created_utc":1752070321,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n26ze9i","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Candid_Highlight_116","can_mod_post":false,"created_utc":1752075705,"send_replies":true,"parent_id":"t3_1lvjtc4","score":2,"author_fullname":"t2_k9mydxdm3","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"so you're willing to wait few years for your agent to finish writing up plans for hello world in C?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n26ze9i","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;so you&amp;#39;re willing to wait few years for your agent to finish writing up plans for hello world in C?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lvjtc4/best_local_llm_for_agentic_coding_on_ollama_8/n26ze9i/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752075705,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lvjtc4","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n27bgkv","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"FieldProgrammable","can_mod_post":false,"created_utc":1752079016,"send_replies":true,"parent_id":"t3_1lvjtc4","score":2,"author_fullname":"t2_moet0t","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"That hardware sounds like a non-starter. Three options:\\n\\n1. GPU with reasonable VRAM and supported by llama.cpp.\\n2. Server class CPU and MB with eight RAM channels (pure x86 CPU inference).\\n3. Mac or Strix Halo type platform with shared high bandwidth RAM.\\n\\nFor the average desktop user, upgrading to option 1 is the most cost effective. A typical local agentic LLM would be DevStral 24b or Qwen3 32b, just getting these to load at a useful quant and context within 16GB of memory would be asking a lot, let alone running inference at a practical speed.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n27bgkv","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;That hardware sounds like a non-starter. Three options:&lt;/p&gt;\\n\\n&lt;ol&gt;\\n&lt;li&gt;GPU with reasonable VRAM and supported by llama.cpp.&lt;/li&gt;\\n&lt;li&gt;Server class CPU and MB with eight RAM channels (pure x86 CPU inference).&lt;/li&gt;\\n&lt;li&gt;Mac or Strix Halo type platform with shared high bandwidth RAM.&lt;/li&gt;\\n&lt;/ol&gt;\\n\\n&lt;p&gt;For the average desktop user, upgrading to option 1 is the most cost effective. A typical local agentic LLM would be DevStral 24b or Qwen3 32b, just getting these to load at a useful quant and context within 16GB of memory would be asking a lot, let alone running inference at a practical speed.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lvjtc4/best_local_llm_for_agentic_coding_on_ollama_8/n27bgkv/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752079016,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lvjtc4","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n27ch1o","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"vasileer","can_mod_post":false,"created_utc":1752079294,"send_replies":true,"parent_id":"t3_1lvjtc4","score":1,"author_fullname":"t2_730bgdulm","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"\\"performance reasons\\" with a VPS (=CPU)?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n27ch1o","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;&amp;quot;performance reasons&amp;quot; with a VPS (=CPU)?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lvjtc4/best_local_llm_for_agentic_coding_on_ollama_8/n27ch1o/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752079294,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lvjtc4","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),r=()=>e.jsx(l,{data:t});export{r as default};
