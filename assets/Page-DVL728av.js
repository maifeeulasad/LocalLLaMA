import{j as e}from"./index-cvG704yx.js";import{R as l}from"./RedditPostRenderer-CBthLTAH.js";import"./index-D-GavSZU.js";const t=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Got tired of waiting for k2 ggufs and found this guy:  \\n[https://huggingface.co/fastllm/Kimi-K2-Instruct-INT4MIX/tree/main](https://huggingface.co/fastllm/Kimi-K2-Instruct-INT4MIX/tree/main)\\n\\nThere is a typo in the commands but it seems to work great, and really easy to get going:  \\npip install ftllm  \\nftllm server fastllm/Kimi-K2-Instruct-INT4MIX -t 40\\n\\nand just like that I'm getting 7-10T/s on my 5090 + DDR5 Xeon machine","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Never seen fastllm mentioned here, anyone using it? (kimi k2 local)","link_flair_richtext":[{"e":"text","t":"Discussion"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lyyhwz","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.93,"author_flair_background_color":null,"subreddit_type":"public","ups":55,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_9hl4ymvj","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Discussion","can_mod_post":false,"score":55,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":1752428193,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"post_hint":"self","content_categories":null,"is_self":true,"mod_note":null,"created":1752427662,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Got tired of waiting for k2 ggufs and found this guy:&lt;br/&gt;\\n&lt;a href=\\"https://huggingface.co/fastllm/Kimi-K2-Instruct-INT4MIX/tree/main\\"&gt;https://huggingface.co/fastllm/Kimi-K2-Instruct-INT4MIX/tree/main&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;There is a typo in the commands but it seems to work great, and really easy to get going:&lt;br/&gt;\\npip install ftllm&lt;br/&gt;\\nftllm server fastllm/Kimi-K2-Instruct-INT4MIX -t 40&lt;/p&gt;\\n\\n&lt;p&gt;and just like that I&amp;#39;m getting 7-10T/s on my 5090 + DDR5 Xeon machine&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"preview":{"images":[{"source":{"url":"https://external-preview.redd.it/IHmG84k97laJH2U80Nq7hMuZfDwRJ7BBdwRt7MKEcMw.png?auto=webp&amp;s=bae6f5c013ad93b7ca44d907a27215b9cd031d97","width":1200,"height":648},"resolutions":[{"url":"https://external-preview.redd.it/IHmG84k97laJH2U80Nq7hMuZfDwRJ7BBdwRt7MKEcMw.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=1ae33582b4b0826302a3cd3ed7609af7df200f8d","width":108,"height":58},{"url":"https://external-preview.redd.it/IHmG84k97laJH2U80Nq7hMuZfDwRJ7BBdwRt7MKEcMw.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=4c75bfa0830517e198d439f93aa7ff4a96c4340e","width":216,"height":116},{"url":"https://external-preview.redd.it/IHmG84k97laJH2U80Nq7hMuZfDwRJ7BBdwRt7MKEcMw.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=eca931e08f88125c81e49d17938fac710ad11893","width":320,"height":172},{"url":"https://external-preview.redd.it/IHmG84k97laJH2U80Nq7hMuZfDwRJ7BBdwRt7MKEcMw.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=f6743d4fea5b1dddfd1c329d591498a0f9454d56","width":640,"height":345},{"url":"https://external-preview.redd.it/IHmG84k97laJH2U80Nq7hMuZfDwRJ7BBdwRt7MKEcMw.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=b4b0fb1bec8773b706d6c25c2eff96884f1c19a3","width":960,"height":518},{"url":"https://external-preview.redd.it/IHmG84k97laJH2U80Nq7hMuZfDwRJ7BBdwRt7MKEcMw.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=f63241ebc2da005589f7190f60055879ec108c46","width":1080,"height":583}],"variants":{},"id":"IHmG84k97laJH2U80Nq7hMuZfDwRJ7BBdwRt7MKEcMw"}],"enabled":false},"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"5f921ea4-c7bc-11ed-9c23-3a00622979b4","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#646d73","id":"1lyyhwz","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"Conscious_Cut_6144","discussion_type":null,"num_comments":21,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lyyhwz/never_seen_fastllm_mentioned_here_anyone_using_it/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lyyhwz/never_seen_fastllm_mentioned_here_anyone_using_it/","subreddit_subscribers":499297,"created_utc":1752427662,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2xvp3u","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"a_beautiful_rhind","can_mod_post":false,"created_utc":1752431550,"send_replies":true,"parent_id":"t3_1lyyhwz","score":30,"author_fullname":"t2_h5utwre7","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Half a terabyte of weights is too rich for my blood. They'd have to make an Int2 for me. \\n\\nInference engine looks cool and supports numa but main docs in CN. Does it need FP8, FP4? What instructions for CPU?\\n\\nCould this shit beat out ik_llama? It supports multiple GPU as well. He claims really high tp/s. Sounds more interesting than kimi itself.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2xvp3u","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Half a terabyte of weights is too rich for my blood. They&amp;#39;d have to make an Int2 for me. &lt;/p&gt;\\n\\n&lt;p&gt;Inference engine looks cool and supports numa but main docs in CN. Does it need FP8, FP4? What instructions for CPU?&lt;/p&gt;\\n\\n&lt;p&gt;Could this shit beat out ik_llama? It supports multiple GPU as well. He claims really high tp/s. Sounds more interesting than kimi itself.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyyhwz/never_seen_fastllm_mentioned_here_anyone_using_it/n2xvp3u/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752431550,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lyyhwz","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":30}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2y723n","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"-Kebob-","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2y3xqu","score":6,"author_fullname":"t2_qycj4py2","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I'll write up a guide and see if I can create a reproducible build with Docker. I had to make some changes to the build files to actually get it to work.  It sounds like llama.cpp support is getting close though: https://github.com/ggml-org/llama.cpp/issues/14642.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2y723n","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;ll write up a guide and see if I can create a reproducible build with Docker. I had to make some changes to the build files to actually get it to work.  It sounds like llama.cpp support is getting close though: &lt;a href=\\"https://github.com/ggml-org/llama.cpp/issues/14642\\"&gt;https://github.com/ggml-org/llama.cpp/issues/14642&lt;/a&gt;.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lyyhwz","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyyhwz/never_seen_fastllm_mentioned_here_anyone_using_it/n2y723n/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752434965,"author_flair_text":null,"treatment_tags":[],"created_utc":1752434965,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n30nf4n","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"-Kebob-","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2y3xqu","score":6,"author_fullname":"t2_qycj4py2","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Here's a fork with the build fixes and updated Dockerfile: https://github.com/KebobZ/ktransformers. It should just work for you with the defaults I set. Take a look at the top of the README for instructions. Hopefully it's helpful - the build will take about 15-20 minutes.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n30nf4n","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Here&amp;#39;s a fork with the build fixes and updated Dockerfile: &lt;a href=\\"https://github.com/KebobZ/ktransformers\\"&gt;https://github.com/KebobZ/ktransformers&lt;/a&gt;. It should just work for you with the defaults I set. Take a look at the top of the README for instructions. Hopefully it&amp;#39;s helpful - the build will take about 15-20 minutes.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lyyhwz","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyyhwz/never_seen_fastllm_mentioned_here_anyone_using_it/n30nf4n/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752465009,"author_flair_text":null,"treatment_tags":[],"created_utc":1752465009,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}}],"before":null}},"user_reports":[],"saved":false,"id":"n2y3xqu","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Conscious_Cut_6144","can_mod_post":false,"created_utc":1752434014,"send_replies":true,"parent_id":"t1_n2xwdt9","score":7,"author_fullname":"t2_9hl4ymvj","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Oh nice work, I always have the hardest time with Ktransformers lol","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2y3xqu","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Oh nice work, I always have the hardest time with Ktransformers lol&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lyyhwz","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyyhwz/never_seen_fastllm_mentioned_here_anyone_using_it/n2y3xqu/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752434014,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":7}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n30qu8w","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"-Kebob-","can_mod_post":false,"send_replies":true,"parent_id":"t1_n305mhb","score":3,"author_fullname":"t2_qycj4py2","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Guilty, lol. I have a pair of QYFS.","edited":false,"author_flair_css_class":null,"name":"t1_n30qu8w","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Guilty, lol. I have a pair of QYFS.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lyyhwz","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyyhwz/never_seen_fastllm_mentioned_here_anyone_using_it/n30qu8w/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752466499,"author_flair_text":null,"collapsed":false,"created_utc":1752466499,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n305mhb","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Conscious_Cut_6144","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2zcyny","score":1,"author_fullname":"t2_9hl4ymvj","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Oh dang me too. You don’t have engineering sample CPUs too do you?!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n305mhb","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Oh dang me too. You don’t have engineering sample CPUs too do you?!&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lyyhwz","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyyhwz/never_seen_fastllm_mentioned_here_anyone_using_it/n305mhb/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752458091,"author_flair_text":null,"treatment_tags":[],"created_utc":1752458091,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2zcyny","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"-Kebob-","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2ynkyh","score":2,"author_fullname":"t2_qycj4py2","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Gigabyte MS73-HB1","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2zcyny","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Gigabyte MS73-HB1&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lyyhwz","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyyhwz/never_seen_fastllm_mentioned_here_anyone_using_it/n2zcyny/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752448001,"author_flair_text":null,"treatment_tags":[],"created_utc":1752448001,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n2ynkyh","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ii_social","can_mod_post":false,"created_utc":1752439855,"send_replies":true,"parent_id":"t1_n2xwdt9","score":1,"author_fullname":"t2_tohvxz80x","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Cool what motherboard do you have?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2ynkyh","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Cool what motherboard do you have?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lyyhwz","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyyhwz/never_seen_fastllm_mentioned_here_anyone_using_it/n2ynkyh/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752439855,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2xwdt9","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"-Kebob-","can_mod_post":false,"created_utc":1752431754,"send_replies":true,"parent_id":"t3_1lyyhwz","score":15,"author_fullname":"t2_qycj4py2","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Looks like we have a similar setup (Dual 4th gen scalable Xeon and 5090). I just got ktransformers working with https://huggingface.co/KVCache-ai/Kimi-K2-Instruct-GGUF, and I'm getting about 80 t/s PP and 10 t/s TG at low context (1k tokens). It was a pain to get working, so thanks for posting about fastllm - I hadn't seen this before. I'll give it a try later.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2xwdt9","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Looks like we have a similar setup (Dual 4th gen scalable Xeon and 5090). I just got ktransformers working with &lt;a href=\\"https://huggingface.co/KVCache-ai/Kimi-K2-Instruct-GGUF\\"&gt;https://huggingface.co/KVCache-ai/Kimi-K2-Instruct-GGUF&lt;/a&gt;, and I&amp;#39;m getting about 80 t/s PP and 10 t/s TG at low context (1k tokens). It was a pain to get working, so thanks for posting about fastllm - I hadn&amp;#39;t seen this before. I&amp;#39;ll give it a try later.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyyhwz/never_seen_fastllm_mentioned_here_anyone_using_it/n2xwdt9/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752431754,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lyyhwz","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":15}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2y76ep","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Longjumpingfish0403","can_mod_post":false,"created_utc":1752435003,"send_replies":true,"parent_id":"t3_1lyyhwz","score":10,"author_fullname":"t2_jarttha4","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"For those with limited resources, integrating non-uniform memory access (NUMA) can enhance performance when using fastllm. If your system lacks high RAM capacity, optimizing parallel processing with CPU instructions might help. Also, if you're coding for extended periods, you'd want to monitor resource usage closely to ensure stability.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2y76ep","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;For those with limited resources, integrating non-uniform memory access (NUMA) can enhance performance when using fastllm. If your system lacks high RAM capacity, optimizing parallel processing with CPU instructions might help. Also, if you&amp;#39;re coding for extended periods, you&amp;#39;d want to monitor resource usage closely to ensure stability.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyyhwz/never_seen_fastllm_mentioned_here_anyone_using_it/n2y76ep/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752435003,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lyyhwz","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":10}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n316txu","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Conscious_Cut_6144","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2ybd0d","score":1,"author_fullname":"t2_9hl4ymvj","approved_by":null,"mod_note":null,"all_awardings":[],"body":"My 7763 + 4060Ti just finished downloading, getting similar speeds as my ddr5 system.","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n316txu","is_submitter":true,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;My 7763 + 4060Ti just finished downloading, getting similar speeds as my ddr5 system.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lyyhwz","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyyhwz/never_seen_fastllm_mentioned_here_anyone_using_it/n316txu/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752474445,"author_flair_text":null,"treatment_tags":[],"created_utc":1752474445,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2ybd0d","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"segmond","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2yave5","score":1,"author_fullname":"t2_ah13x","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"nice, i'm thinking of getting a bunch of 64gb sticks DDR4 2400mhz, just trying to figure out what my performance is going to be on an epyc, looks like about half 4tk/sec, might not be too bad. I'm waiting to see if the verdict on kimi k2 is the real deal, so far deepseek is keeping up for me.","edited":false,"author_flair_css_class":null,"name":"t1_n2ybd0d","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;nice, i&amp;#39;m thinking of getting a bunch of 64gb sticks DDR4 2400mhz, just trying to figure out what my performance is going to be on an epyc, looks like about half 4tk/sec, might not be too bad. I&amp;#39;m waiting to see if the verdict on kimi k2 is the real deal, so far deepseek is keeping up for me.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lyyhwz","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":"light","treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyyhwz/never_seen_fastllm_mentioned_here_anyone_using_it/n2ybd0d/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752436255,"author_flair_text":"llama.cpp","collapsed":false,"created_utc":1752436255,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n31jl5g","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"CommunityTough1","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2yave5","score":1,"author_fullname":"t2_1iuzpxw7eg","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"You might only be using 7GB of GPU because you may need to set a parameter for how many layers to offload to the GPU. Llama.cpp default is usually very low, for example. I think the param is \`-ngl\`; set it to however many layers Kimi has. It'll put as many as possible into the GPU then and automatically offload the rest to system RAM. This should boost your performance some.","edited":false,"author_flair_css_class":null,"name":"t1_n31jl5g","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You might only be using 7GB of GPU because you may need to set a parameter for how many layers to offload to the GPU. Llama.cpp default is usually very low, for example. I think the param is &lt;code&gt;-ngl&lt;/code&gt;; set it to however many layers Kimi has. It&amp;#39;ll put as many as possible into the GPU then and automatically offload the rest to system RAM. This should boost your performance some.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lyyhwz","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyyhwz/never_seen_fastllm_mentioned_here_anyone_using_it/n31jl5g/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752481750,"author_flair_text":null,"collapsed":false,"created_utc":1752481750,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2yave5","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Conscious_Cut_6144","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2y713p","score":3,"author_fullname":"t2_9hl4ymvj","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I normally run 8 channel 5200 DDR5 - 48GB ea\\nThis model required installing my second cpu and another 4 sticks.\\nNot an optimal setup really.\\n\\nI bet single cpu with 8x 64g sticks would be faster.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2yave5","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I normally run 8 channel 5200 DDR5 - 48GB ea\\nThis model required installing my second cpu and another 4 sticks.\\nNot an optimal setup really.&lt;/p&gt;\\n\\n&lt;p&gt;I bet single cpu with 8x 64g sticks would be faster.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lyyhwz","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyyhwz/never_seen_fastllm_mentioned_here_anyone_using_it/n2yave5/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752436109,"author_flair_text":null,"treatment_tags":[],"created_utc":1752436109,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n2y713p","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"segmond","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2xk4ze","score":2,"author_fullname":"t2_ah13x","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"any idea, why it's only using 7gb out of 32gb of vram?\\n\\nwhat's the speed of your system ram?","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2y713p","is_submitter":false,"collapsed":false,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;any idea, why it&amp;#39;s only using 7gb out of 32gb of vram?&lt;/p&gt;\\n\\n&lt;p&gt;what&amp;#39;s the speed of your system ram?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lyyhwz","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyyhwz/never_seen_fastllm_mentioned_here_anyone_using_it/n2y713p/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752434957,"author_flair_text":"llama.cpp","treatment_tags":[],"created_utc":1752434957,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2yeskj","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Lissanro","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2xowlp","score":3,"author_fullname":"t2_fpfao9g","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"You will need at least half TB of RAM to run it well. And for practical usage much more VRAM too, at least enough to hold the whole context cache for fast prompt processing. This is because even though Kimi K2 has just 30B active parameters, it is still heavy 1T model.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2yeskj","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You will need at least half TB of RAM to run it well. And for practical usage much more VRAM too, at least enough to hold the whole context cache for fast prompt processing. This is because even though Kimi K2 has just 30B active parameters, it is still heavy 1T model.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lyyhwz","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyyhwz/never_seen_fastllm_mentioned_here_anyone_using_it/n2yeskj/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752437267,"author_flair_text":null,"treatment_tags":[],"created_utc":1752437267,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n2xowlp","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":"LOW_SCORE","no_follow":true,"author":"Expensive-Spirit9118","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2xk4ze","score":-7,"author_fullname":"t2_axxw1n0s","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I have a 12GB RTX 3060 and 16GB of ram. I imagine it must run well. But what about coding for many hours?","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2xowlp","is_submitter":false,"collapsed":true,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I have a 12GB RTX 3060 and 16GB of ram. I imagine it must run well. But what about coding for many hours?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":"comment score below threshold","distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lyyhwz","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyyhwz/never_seen_fastllm_mentioned_here_anyone_using_it/n2xowlp/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752429525,"author_flair_text":null,"treatment_tags":[],"created_utc":1752429525,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-7}}],"before":null}},"user_reports":[],"saved":false,"id":"n2xk4ze","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Conscious_Cut_6144","can_mod_post":false,"created_utc":1752428158,"send_replies":true,"parent_id":"t1_n2xjal5","score":13,"author_fullname":"t2_9hl4ymvj","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"It appears to be another inference tool just like llama.cpp or vllm  \\nMy OpenWebUi connected right to it same as those other tools.\\n\\nAs for the machine the you need a ton of ram,  \\nMy machine is showing 7GB used on the GPU and 494GB of System ram in use.","edited":1752428396,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2xk4ze","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It appears to be another inference tool just like llama.cpp or vllm&lt;br/&gt;\\nMy OpenWebUi connected right to it same as those other tools.&lt;/p&gt;\\n\\n&lt;p&gt;As for the machine the you need a ton of ram,&lt;br/&gt;\\nMy machine is showing 7GB used on the GPU and 494GB of System ram in use.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lyyhwz","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyyhwz/never_seen_fastllm_mentioned_here_anyone_using_it/n2xk4ze/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752428158,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":13}}],"before":null}},"user_reports":[],"saved":false,"id":"n2xjal5","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Expensive-Spirit9118","can_mod_post":false,"created_utc":1752427919,"send_replies":true,"parent_id":"t3_1lyyhwz","score":4,"author_fullname":"t2_axxw1n0s","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Is it for running Kimi K2 locally? Is it for coding? And what machine do you need to make it run well?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2xjal5","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Is it for running Kimi K2 locally? Is it for coding? And what machine do you need to make it run well?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyyhwz/never_seen_fastllm_mentioned_here_anyone_using_it/n2xjal5/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752427919,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lyyhwz","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}}],"before":null}}]`),o=()=>e.jsx(l,{data:t});export{o as default};
