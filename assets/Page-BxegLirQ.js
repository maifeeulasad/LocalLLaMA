import{j as e}from"./index-Bu7qcPAU.js";import{R as l}from"./RedditPostRenderer-CbHA7O5q.js";import"./index-BKgbfxhf.js";const t=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Anyone who actually codes with local LLM on their laptops, what's your setup and are you happy with the quality and speed?  Should I even bother trying to code with an LLM that fits on a laptop GPU, or just tether back to my beefier home server or openrouter?","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Laptop GPU for Agentic Coding -- Worth it?","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lyen05","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.91,"author_flair_background_color":null,"subreddit_type":"public","ups":9,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_4nw3v","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":9,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1752364130,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Anyone who actually codes with local LLM on their laptops, what&amp;#39;s your setup and are you happy with the quality and speed?  Should I even bother trying to code with an LLM that fits on a laptop GPU, or just tether back to my beefier home server or openrouter?&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1lyen05","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"randomqhacker","discussion_type":null,"num_comments":29,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lyen05/laptop_gpu_for_agentic_coding_worth_it/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lyen05/laptop_gpu_for_agentic_coding_worth_it/","subreddit_subscribers":498344,"created_utc":1752364130,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2u4kcy","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"giant3","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2u30rc","score":1,"author_fullname":"t2_82esi","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Max is 32GB. I don't know about throughout. \\n\\n\\nI am just curious whether anyone has used them. ","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2u4kcy","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Max is 32GB. I don&amp;#39;t know about throughout. &lt;/p&gt;\\n\\n&lt;p&gt;I am just curious whether anyone has used them. &lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lyen05","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyen05/laptop_gpu_for_agentic_coding_worth_it/n2u4kcy/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752375603,"author_flair_text":null,"treatment_tags":[],"created_utc":1752375603,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2u30rc","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"randomqhacker","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2tn2vf","score":1,"author_fullname":"t2_4nw3v","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"How much RAM and GB/s throughput though? I thought most of those LPDDR5x APUs topped out around 80-120 GB/s.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2u30rc","is_submitter":true,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;How much RAM and GB/s throughput though? I thought most of those LPDDR5x APUs topped out around 80-120 GB/s.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lyen05","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyen05/laptop_gpu_for_agentic_coding_worth_it/n2u30rc/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752374977,"author_flair_text":null,"treatment_tags":[],"created_utc":1752374977,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2tn2vf","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"giant3","can_mod_post":false,"created_utc":1752368891,"send_replies":true,"parent_id":"t1_n2tevu2","score":1,"author_fullname":"t2_82esi","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"How about Arc 140V on the Lunar Lake Intel SoCs? They claim 48 TOPS(INT8) just on the NPU. Combined with CPU+GPU, I think it is around 120 TOPS?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2tn2vf","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;How about Arc 140V on the Lunar Lake Intel SoCs? They claim 48 TOPS(INT8) just on the NPU. Combined with CPU+GPU, I think it is around 120 TOPS?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lyen05","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyen05/laptop_gpu_for_agentic_coding_worth_it/n2tn2vf/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752368891,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2u2oe2","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"randomqhacker","can_mod_post":false,"created_utc":1752374840,"send_replies":true,"parent_id":"t1_n2tevu2","score":1,"author_fullname":"t2_4nw3v","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yeah, was wondering about the mobile 5090 with 24GB, but the cost is so high I don't think it'd be worth it given the quality of the models I've tested that would fit. I haven't tried the new Devstral yet though.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2u2oe2","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yeah, was wondering about the mobile 5090 with 24GB, but the cost is so high I don&amp;#39;t think it&amp;#39;d be worth it given the quality of the models I&amp;#39;ve tested that would fit. I haven&amp;#39;t tried the new Devstral yet though.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lyen05","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyen05/laptop_gpu_for_agentic_coding_worth_it/n2u2oe2/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752374840,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2tevu2","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"RedBoxSquare","can_mod_post":false,"created_utc":1752365806,"send_replies":true,"parent_id":"t3_1lyen05","score":11,"author_fullname":"t2_1jmp1x4av3","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Unless you have a macbook with sufficient RAM, no, you wouldn't be running LLM locally with your laptop GPU. Many recent laptops GPUs are capable chips, but they are held back by low VRAM amounts compared to desktop.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2tevu2","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Unless you have a macbook with sufficient RAM, no, you wouldn&amp;#39;t be running LLM locally with your laptop GPU. Many recent laptops GPUs are capable chips, but they are held back by low VRAM amounts compared to desktop.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyen05/laptop_gpu_for_agentic_coding_worth_it/n2tevu2/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752365806,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lyen05","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":11}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2u28ib","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"randomqhacker","can_mod_post":false,"created_utc":1752374662,"send_replies":true,"parent_id":"t1_n2tuppt","score":1,"author_fullname":"t2_4nw3v","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Thanks for describing your setup. I've had those issues too (lots of retries). \\n\\n\\nAny other local models up there with Deepseek for you?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2u28ib","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thanks for describing your setup. I&amp;#39;ve had those issues too (lots of retries). &lt;/p&gt;\\n\\n&lt;p&gt;Any other local models up there with Deepseek for you?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lyen05","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyen05/laptop_gpu_for_agentic_coding_worth_it/n2u28ib/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752374662,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2v8gst","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"CommunityTough1","can_mod_post":false,"created_utc":1752395702,"send_replies":true,"parent_id":"t1_n2tuppt","score":1,"author_fullname":"t2_1iuzpxw7eg","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I found the same. Finally broke down and paid the $20 for Claude Pro so I can use Claude Code. The limits are really good and I really haven't come up against them, and whenever I have, usually it's about to reset anyways because they reset the limits on Claude every few hours. Also Google is giving away $300 in Gemini API credits, so there's that too. I used it pretty heavily for like a week and only used like $12 of them. But I think Claude Sonnet 4 is better than Gemini, and $20 for Claude Pro guarantees you'll never get surprises (Gemini would cost about $50/mo if you used $12/wk like I did).","edited":1752395896,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2v8gst","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I found the same. Finally broke down and paid the $20 for Claude Pro so I can use Claude Code. The limits are really good and I really haven&amp;#39;t come up against them, and whenever I have, usually it&amp;#39;s about to reset anyways because they reset the limits on Claude every few hours. Also Google is giving away $300 in Gemini API credits, so there&amp;#39;s that too. I used it pretty heavily for like a week and only used like $12 of them. But I think Claude Sonnet 4 is better than Gemini, and $20 for Claude Pro guarantees you&amp;#39;ll never get surprises (Gemini would cost about $50/mo if you used $12/wk like I did).&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lyen05","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyen05/laptop_gpu_for_agentic_coding_worth_it/n2v8gst/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752395702,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2tuppt","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Lissanro","can_mod_post":false,"created_utc":1752371761,"send_replies":true,"parent_id":"t3_1lyen05","score":3,"author_fullname":"t2_fpfao9g","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"The main issue with agentic coding that it does not work that great with small models - I tried quite a few of them hoping for speed for at least simple tasks, but each time ended up spending more time than I would with a bigger model due to many errors and retries the smaller models typically end up having.\\n\\nFor agentic coding I use workstation with 1 TB RAM + 96 GB VRAM, and that's barely enough to run 671B model - I have to offload most of the weight to CPU, but at least can keep whole context cache in VRAM and have 100K context length. Cline for example often goes beyond 64K, so it is a necessity especially considering that it also needs to include the output buffer.\\n\\nWhen I need to do something on my mobile phone for example, like you mentioned, connecting to the home server is the simplest solution. Or if privacy is not an issue, using paid API providers may be another alternative.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2tuppt","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The main issue with agentic coding that it does not work that great with small models - I tried quite a few of them hoping for speed for at least simple tasks, but each time ended up spending more time than I would with a bigger model due to many errors and retries the smaller models typically end up having.&lt;/p&gt;\\n\\n&lt;p&gt;For agentic coding I use workstation with 1 TB RAM + 96 GB VRAM, and that&amp;#39;s barely enough to run 671B model - I have to offload most of the weight to CPU, but at least can keep whole context cache in VRAM and have 100K context length. Cline for example often goes beyond 64K, so it is a necessity especially considering that it also needs to include the output buffer.&lt;/p&gt;\\n\\n&lt;p&gt;When I need to do something on my mobile phone for example, like you mentioned, connecting to the home server is the simplest solution. Or if privacy is not an issue, using paid API providers may be another alternative.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyen05/laptop_gpu_for_agentic_coding_worth_it/n2tuppt/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752371761,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lyen05","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2vfigw","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"admajic","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2v6zry","score":1,"author_fullname":"t2_60b9farf","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"It uses the latest llama.cpp i compared it and it's fast. \\nYou can use it free for personal use.","edited":false,"author_flair_css_class":null,"name":"t1_n2vfigw","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It uses the latest llama.cpp i compared it and it&amp;#39;s fast. \\nYou can use it free for personal use.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lyen05","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyen05/laptop_gpu_for_agentic_coding_worth_it/n2vfigw/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752399879,"author_flair_text":null,"collapsed":false,"created_utc":1752399879,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2v6zry","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"spaceman_","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2v5e5p","score":2,"author_fullname":"t2_9neub","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Kind of annoyed LM Studio isn't open source. Not sure what their long term intentions are keeping it closed, so I'd rather not depend it.\\n\\n\\nDo you know if LM Studio uses vLLM or Llama.cpp under the hood? ","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2v6zry","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Kind of annoyed LM Studio isn&amp;#39;t open source. Not sure what their long term intentions are keeping it closed, so I&amp;#39;d rather not depend it.&lt;/p&gt;\\n\\n&lt;p&gt;Do you know if LM Studio uses vLLM or Llama.cpp under the hood? &lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lyen05","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyen05/laptop_gpu_for_agentic_coding_worth_it/n2v6zry/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752394824,"author_flair_text":null,"treatment_tags":[],"created_utc":1752394824,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n2v5e5p","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"admajic","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2utzov","score":1,"author_fullname":"t2_60b9farf","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Lmstudio I got the unsloth q4 model I think 4k_m then in lmstudio you set it to share the model via api set kv cache to q4 (both of them) context to max turn on flash attention. \\n\\nIn roocode pick lmstudio and the model set 0 temperature and you're on your way","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2v5e5p","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Lmstudio I got the unsloth q4 model I think 4k_m then in lmstudio you set it to share the model via api set kv cache to q4 (both of them) context to max turn on flash attention. &lt;/p&gt;\\n\\n&lt;p&gt;In roocode pick lmstudio and the model set 0 temperature and you&amp;#39;re on your way&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lyen05","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyen05/laptop_gpu_for_agentic_coding_worth_it/n2v5e5p/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752393887,"author_flair_text":null,"treatment_tags":[],"created_utc":1752393887,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2utzov","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"spaceman_","can_mod_post":false,"created_utc":1752387419,"send_replies":true,"parent_id":"t1_n2u28xg","score":2,"author_fullname":"t2_9neub","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Could you document you setup (or give us some pointers) ?\\n\\n\\nAre you using Llama.cpp to run the model? ","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2utzov","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Could you document you setup (or give us some pointers) ?&lt;/p&gt;\\n\\n&lt;p&gt;Are you using Llama.cpp to run the model? &lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lyen05","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyen05/laptop_gpu_for_agentic_coding_worth_it/n2utzov/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752387419,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2ua575","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"admajic","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2u4s50","score":2,"author_fullname":"t2_60b9farf","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"All q4 the largest q4 model. It seems designed to fit in 24gb vram","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2ua575","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;All q4 the largest q4 model. It seems designed to fit in 24gb vram&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lyen05","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyen05/laptop_gpu_for_agentic_coding_worth_it/n2ua575/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752377945,"author_flair_text":null,"treatment_tags":[],"created_utc":1752377945,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n2u4s50","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"randomqhacker","can_mod_post":false,"created_utc":1752375692,"send_replies":true,"parent_id":"t1_n2u28xg","score":1,"author_fullname":"t2_4nw3v","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Ah thanks, I heard it got better with other frameworks! I will have to try it with Aider!  You using q4 or higher?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2u4s50","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Ah thanks, I heard it got better with other frameworks! I will have to try it with Aider!  You using q4 or higher?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lyen05","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyen05/laptop_gpu_for_agentic_coding_worth_it/n2u4s50/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752375692,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2u28xg","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"admajic","can_mod_post":false,"created_utc":1752374667,"send_replies":true,"parent_id":"t3_1lyen05","score":3,"author_fullname":"t2_60b9farf","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I'm using the latest devstral that came out the other day. Fits 132k context on 24gb vram. Very good at tool calling. Is it a good as deepseek-r1 or other 600b model no. But it's very capable.\\n\\nI'm using it in roocode.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2u28xg","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;m using the latest devstral that came out the other day. Fits 132k context on 24gb vram. Very good at tool calling. Is it a good as deepseek-r1 or other 600b model no. But it&amp;#39;s very capable.&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;m using it in roocode.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyen05/laptop_gpu_for_agentic_coding_worth_it/n2u28xg/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752374667,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lyen05","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2uqcvd","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Nixellion","can_mod_post":false,"created_utc":1752385502,"send_replies":true,"parent_id":"t3_1lyen05","score":2,"author_fullname":"t2_12fajr","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I have a laptop with 16GB 3080. The largest model you can load there at 4bit is 14B. 20-30 might fit in 1-2bpw but I never consider it. Especially for agentic coding, you need larger context.\\n\\nSo far the best models to work as agents are Qwen3, Gemma3 and Codestral (22B). At 14B none of them really are very useful in agentic coding. \\n\\n30B qwen and gemma are where they start to work, for example I was able to get Qwen3 32B to generate a good documentation for a Unity script, which involved looking at many files in the project to figure out dependencies and context. \\n\\nWhat you CAN use your laptop GPU for is to run a completion model. Up to 7B at 3-4bpw, nextcoder or qwen or something like that works quite well and is quite fast. You can use tweeny and ollama for autocompletion, and tweeny also can be used as old non agentic AI chat which is helpful to ask small questions to AI that even a 7B can answer (like about syntax of some API)\\n\\nEdit: yeah, worth mentioning that nothing in local LLMs comes close for agentic tasks to Claude models or even Deepseek V3. Anything else you are probably better of doing yourself.\\n\\nHowever the fact that a 30B can analyze code and provide documentation for a component with complex depndencies and figure out what its doing is in itself useful. Even if it hallucinates it can be a good starting point when figuring out how something works.","edited":1752387438,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2uqcvd","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I have a laptop with 16GB 3080. The largest model you can load there at 4bit is 14B. 20-30 might fit in 1-2bpw but I never consider it. Especially for agentic coding, you need larger context.&lt;/p&gt;\\n\\n&lt;p&gt;So far the best models to work as agents are Qwen3, Gemma3 and Codestral (22B). At 14B none of them really are very useful in agentic coding. &lt;/p&gt;\\n\\n&lt;p&gt;30B qwen and gemma are where they start to work, for example I was able to get Qwen3 32B to generate a good documentation for a Unity script, which involved looking at many files in the project to figure out dependencies and context. &lt;/p&gt;\\n\\n&lt;p&gt;What you CAN use your laptop GPU for is to run a completion model. Up to 7B at 3-4bpw, nextcoder or qwen or something like that works quite well and is quite fast. You can use tweeny and ollama for autocompletion, and tweeny also can be used as old non agentic AI chat which is helpful to ask small questions to AI that even a 7B can answer (like about syntax of some API)&lt;/p&gt;\\n\\n&lt;p&gt;Edit: yeah, worth mentioning that nothing in local LLMs comes close for agentic tasks to Claude models or even Deepseek V3. Anything else you are probably better of doing yourself.&lt;/p&gt;\\n\\n&lt;p&gt;However the fact that a 30B can analyze code and provide documentation for a component with complex depndencies and figure out what its doing is in itself useful. Even if it hallucinates it can be a good starting point when figuring out how something works.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyen05/laptop_gpu_for_agentic_coding_worth_it/n2uqcvd/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752385502,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lyen05","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2uttlj","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"spaceman_","can_mod_post":false,"created_utc":1752387329,"send_replies":true,"parent_id":"t3_1lyen05","score":2,"author_fullname":"t2_9neub","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I have tried with Devstral on a Ryzen Max 395+ with 64GB of memory. It's... OK, not great quality or speed but usable.\\n\\n\\nSoftware support seems lacking though. ","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2uttlj","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I have tried with Devstral on a Ryzen Max 395+ with 64GB of memory. It&amp;#39;s... OK, not great quality or speed but usable.&lt;/p&gt;\\n\\n&lt;p&gt;Software support seems lacking though. &lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyen05/laptop_gpu_for_agentic_coding_worth_it/n2uttlj/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752387329,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lyen05","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2tfgmg","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"SlowFail2433","can_mod_post":false,"created_utc":1752366020,"send_replies":true,"parent_id":"t1_n2tcwz3","score":1,"author_fullname":"t2_131eezppgs","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yes agentic coding is frontier task.\\n\\n\\nYou can do it with smaller LLMs if you do a distill/fine-tune/RL pass using your own data. Off the shelf not so much","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2tfgmg","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yes agentic coding is frontier task.&lt;/p&gt;\\n\\n&lt;p&gt;You can do it with smaller LLMs if you do a distill/fine-tune/RL pass using your own data. Off the shelf not so much&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lyen05","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyen05/laptop_gpu_for_agentic_coding_worth_it/n2tfgmg/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752366020,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2u847r","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"offlinesir","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2u3t70","score":2,"author_fullname":"t2_jn5ft2le","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"GLM 4 is what I would have recomended, at least! Since agentic coding is more of a focus now than it was just a year ago, it's possible local models will get better!","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2u847r","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;GLM 4 is what I would have recomended, at least! Since agentic coding is more of a focus now than it was just a year ago, it&amp;#39;s possible local models will get better!&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lyen05","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyen05/laptop_gpu_for_agentic_coding_worth_it/n2u847r/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752377085,"author_flair_text":null,"treatment_tags":[],"created_utc":1752377085,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n2u3t70","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"randomqhacker","can_mod_post":false,"created_utc":1752375295,"send_replies":true,"parent_id":"t1_n2tcwz3","score":1,"author_fullname":"t2_4nw3v","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Thanks. I use Qwen3-32b and GLM4 locally on my server, but mostly for spot edits, or to make an initial prototype of something.  Was hoping maybe someone had a good model I'd overlooked...","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2u3t70","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thanks. I use Qwen3-32b and GLM4 locally on my server, but mostly for spot edits, or to make an initial prototype of something.  Was hoping maybe someone had a good model I&amp;#39;d overlooked...&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lyen05","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyen05/laptop_gpu_for_agentic_coding_worth_it/n2u3t70/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752375295,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2tcwz3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"offlinesir","can_mod_post":false,"created_utc":1752365086,"send_replies":true,"parent_id":"t3_1lyen05","score":4,"author_fullname":"t2_jn5ft2le","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"You say \\"Agentic Coding,\\" which is only really possible with large LLM's (think Qwen 235B, Deepseek at 671B, or Google Gemini 2.5 Pro, Claude 4, o4 mini). I'm assuming the Agentic Coding you are talking about is using an \\"agent\\" style mode, where you ask it to adjust the code and it does so across multiple files. While that works well for large models, this **can't** be run on a laptop OR desktop gpu, as small models (32B, 7B, 13B, etc) suffer in Agentic Coding tasks and can often be too slow. If you want to stay local, you could make a really expensive home server or get a mac studio to run those models, or you can spend money on openrouter. I would also recomend [Gemini CLI](https://blog.google/technology/developers/introducing-gemini-cli-open-source-ai-agent/) (which is in no way local).\\n\\nTLDR: to answer your question in the title, **no**, becuase a laptop (or desktop) couldn't run it.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2tcwz3","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You say &amp;quot;Agentic Coding,&amp;quot; which is only really possible with large LLM&amp;#39;s (think Qwen 235B, Deepseek at 671B, or Google Gemini 2.5 Pro, Claude 4, o4 mini). I&amp;#39;m assuming the Agentic Coding you are talking about is using an &amp;quot;agent&amp;quot; style mode, where you ask it to adjust the code and it does so across multiple files. While that works well for large models, this &lt;strong&gt;can&amp;#39;t&lt;/strong&gt; be run on a laptop OR desktop gpu, as small models (32B, 7B, 13B, etc) suffer in Agentic Coding tasks and can often be too slow. If you want to stay local, you could make a really expensive home server or get a mac studio to run those models, or you can spend money on openrouter. I would also recomend &lt;a href=\\"https://blog.google/technology/developers/introducing-gemini-cli-open-source-ai-agent/\\"&gt;Gemini CLI&lt;/a&gt; (which is in no way local).&lt;/p&gt;\\n\\n&lt;p&gt;TLDR: to answer your question in the title, &lt;strong&gt;no&lt;/strong&gt;, becuase a laptop (or desktop) couldn&amp;#39;t run it.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyen05/laptop_gpu_for_agentic_coding_worth_it/n2tcwz3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752365086,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lyen05","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2tfups","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ChrisZavadil","can_mod_post":false,"created_utc":1752366168,"send_replies":true,"parent_id":"t1_n2tfn0v","score":1,"author_fullname":"t2_71knjqyi","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Sorry for missing some context, use Local LLM Node, and N8N.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2tfups","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Sorry for missing some context, use Local LLM Node, and N8N.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lyen05","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyen05/laptop_gpu_for_agentic_coding_worth_it/n2tfups/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752366168,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2tfn0v","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ChrisZavadil","can_mod_post":false,"created_utc":1752366087,"send_replies":true,"parent_id":"t3_1lyen05","score":1,"author_fullname":"t2_71knjqyi","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"If you can tune the temp down, and your laptop is newer you should easily be able to handle Local llm coding.  \\nKeep the context window short, build out with cuda!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2tfn0v","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;If you can tune the temp down, and your laptop is newer you should easily be able to handle Local llm coding.&lt;br/&gt;\\nKeep the context window short, build out with cuda!&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyen05/laptop_gpu_for_agentic_coding_worth_it/n2tfn0v/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752366087,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lyen05","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2va2cc","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"fooo12gh","can_mod_post":false,"created_utc":1752396642,"send_replies":true,"parent_id":"t3_1lyen05","score":2,"author_fullname":"t2_3vvvbmh","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"if the coding task is not very complicated, you can give it a shot.\\n\\ni've had positive experience with my task. i've coded some simple python scripts on laptop (8845hs, 96gb ram, 4060 8gb vram) using vscode and continue.dev. taking into account really limited resources, my main models were:  \\n\\\\- qwen2.5 coder 7b q4\\\\_k\\\\_m - for autocomplete  \\n\\\\- qwen3 30b a3b  q4\\\\_k\\\\_m - for chat\\n\\nthough it was in python, which is probably simple enough and has good coverage in models. overall i have impression that smaller models are not that bad, and not reaching the top of the benchmark dashboard doesn't mean they are useless. i didn't like that laptop using dGPU was pretty much loud, so needed to work in noise cancelling headphones. overall it's more pleasure to use copilot (at work), so maybe copilot pro with 10$/month (100$/year) doesn't look that bad - less noise, less electricity consumption, better than local models, no need to invest in expensive rig\\n\\non the other hand why don't you give it a try and share your experience?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2va2cc","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;if the coding task is not very complicated, you can give it a shot.&lt;/p&gt;\\n\\n&lt;p&gt;i&amp;#39;ve had positive experience with my task. i&amp;#39;ve coded some simple python scripts on laptop (8845hs, 96gb ram, 4060 8gb vram) using vscode and continue.dev. taking into account really limited resources, my main models were:&lt;br/&gt;\\n- qwen2.5 coder 7b q4_k_m - for autocomplete&lt;br/&gt;\\n- qwen3 30b a3b  q4_k_m - for chat&lt;/p&gt;\\n\\n&lt;p&gt;though it was in python, which is probably simple enough and has good coverage in models. overall i have impression that smaller models are not that bad, and not reaching the top of the benchmark dashboard doesn&amp;#39;t mean they are useless. i didn&amp;#39;t like that laptop using dGPU was pretty much loud, so needed to work in noise cancelling headphones. overall it&amp;#39;s more pleasure to use copilot (at work), so maybe copilot pro with 10$/month (100$/year) doesn&amp;#39;t look that bad - less noise, less electricity consumption, better than local models, no need to invest in expensive rig&lt;/p&gt;\\n\\n&lt;p&gt;on the other hand why don&amp;#39;t you give it a try and share your experience?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyen05/laptop_gpu_for_agentic_coding_worth_it/n2va2cc/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752396642,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lyen05","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"1c60b73a-72f2-11ee-bdcc-8e2a7b94d6a0","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2vdbsn","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"StableLlama","can_mod_post":false,"created_utc":1752398578,"send_replies":true,"parent_id":"t3_1lyen05","score":1,"author_fullname":"t2_pyjm83bcg","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"All LLMs have disappointed me for coding - until Gemini Pro 2.5 came along. But that's too heavy to run locally - and it's not available for that.\\n\\nBased on that: not it's not worth it.\\n\\nBut laptops get quicker and smaller models more powerful. So in one years time the answer might be different.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2vdbsn","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"textgen web UI"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;All LLMs have disappointed me for coding - until Gemini Pro 2.5 came along. But that&amp;#39;s too heavy to run locally - and it&amp;#39;s not available for that.&lt;/p&gt;\\n\\n&lt;p&gt;Based on that: not it&amp;#39;s not worth it.&lt;/p&gt;\\n\\n&lt;p&gt;But laptops get quicker and smaller models more powerful. So in one years time the answer might be different.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyen05/laptop_gpu_for_agentic_coding_worth_it/n2vdbsn/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752398578,"author_flair_text":"textgen web UI","treatment_tags":[],"link_id":"t3_1lyen05","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2v30a8","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"randomqhacker","can_mod_post":false,"created_utc":1752392531,"send_replies":true,"parent_id":"t1_n2te1lt","score":3,"author_fullname":"t2_4nw3v","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I do live in a rainforest... (But we have 4G!)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2v30a8","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I do live in a rainforest... (But we have 4G!)&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lyen05","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyen05/laptop_gpu_for_agentic_coding_worth_it/n2v30a8/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752392531,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n2te1lt","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"a_beautiful_rhind","can_mod_post":false,"created_utc":1752365496,"send_replies":true,"parent_id":"t3_1lyen05","score":2,"author_fullname":"t2_h5utwre7","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"You're not going to be coding in the middle of the amazon so you can connect from a crappy laptop to your server or some api.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2te1lt","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You&amp;#39;re not going to be coding in the middle of the amazon so you can connect from a crappy laptop to your server or some api.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyen05/laptop_gpu_for_agentic_coding_worth_it/n2te1lt/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752365496,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lyen05","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2tf4dj","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"SlowFail2433","can_mod_post":false,"created_utc":1752365893,"send_replies":true,"parent_id":"t3_1lyen05","score":1,"author_fullname":"t2_131eezppgs","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Want to discourage laptop GPU as much as possible","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2tf4dj","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Want to discourage laptop GPU as much as possible&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lyen05/laptop_gpu_for_agentic_coding_worth_it/n2tf4dj/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752365893,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lyen05","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),r=()=>e.jsx(l,{data:t});export{r as default};
