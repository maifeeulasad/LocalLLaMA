import{j as e}from"./index-CNyNkRpk.js";import{R as l}from"./RedditPostRenderer-Dza0u9i2.js";import"./index-BUchu_-K.js";const t=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Hi all,\\nI'm building a machine for gaming/ AI hobbyist and right now I'm debating myself on the GPU. My budget is around 750$ for the GPU.\\nRefurbished 7900xtx with 5 months warranty for 690$\\nUsed RTX3090 for 750$\\nNew 5070ti\\nNew RX9070XT\\n\\nI'm leaning towards a used GPU. I know ROCM and Vulkan have improved AMD inference massively and the warranty on 7900xtx is nice as well.\\n\\nWhat are your suggestions?","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"7900XTX vs RTX3090","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1llvz0g","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.7,"author_flair_background_color":null,"subreddit_type":"public","ups":4,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_67mdrsv5","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":4,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1751036181,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Hi all,\\nI&amp;#39;m building a machine for gaming/ AI hobbyist and right now I&amp;#39;m debating myself on the GPU. My budget is around 750$ for the GPU.\\nRefurbished 7900xtx with 5 months warranty for 690$\\nUsed RTX3090 for 750$\\nNew 5070ti\\nNew RX9070XT&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;m leaning towards a used GPU. I know ROCM and Vulkan have improved AMD inference massively and the warranty on 7900xtx is nice as well.&lt;/p&gt;\\n\\n&lt;p&gt;What are your suggestions?&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1llvz0g","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"_ballzdeep_","discussion_type":null,"num_comments":11,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1llvz0g/7900xtx_vs_rtx3090/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1llvz0g/7900xtx_vs_rtx3090/","subreddit_subscribers":492233,"created_utc":1751036181,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n03eeif","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"LagOps91","can_mod_post":false,"created_utc":1751043114,"send_replies":true,"parent_id":"t3_1llvz0g","score":6,"author_fullname":"t2_3wi6j7vwh","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"7900XTX works just fine for AI. It's true that the 3090 has better support, but since you want to do gaming as well, i can only recommend going with the 7900XTX as it's better in gaming performance.\\n\\nvulcan support has been improving over time for the 7900XTX and i can run 32b ai models with koboldcpp at 500 t/s prompt processing and 15-20ts at 16k context. that's already way faster than reading speed and i'm not sure how much better it would feel to have higher t/s. for chain of thought models that need a lot of thinking it would make a difference i suppose.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n03eeif","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;7900XTX works just fine for AI. It&amp;#39;s true that the 3090 has better support, but since you want to do gaming as well, i can only recommend going with the 7900XTX as it&amp;#39;s better in gaming performance.&lt;/p&gt;\\n\\n&lt;p&gt;vulcan support has been improving over time for the 7900XTX and i can run 32b ai models with koboldcpp at 500 t/s prompt processing and 15-20ts at 16k context. that&amp;#39;s already way faster than reading speed and i&amp;#39;m not sure how much better it would feel to have higher t/s. for chain of thought models that need a lot of thinking it would make a difference i suppose.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1llvz0g/7900xtx_vs_rtx3090/n03eeif/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751043114,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1llvz0g","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n03in2f","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"nobodyhasusedthislol","can_mod_post":false,"created_utc":1751044301,"send_replies":true,"parent_id":"t3_1llvz0g","score":3,"author_fullname":"t2_tysoz7zt","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Both will do 1440p ultra at &gt;60fps in almost every game currently.\\n\\nAnd even then, medium-high settings really isn’t an issue.\\n\\nBoth will do a single LLM at the same time perfectly fine - I have a 6900 xt and, despite bring RDNA2 (bad ray tracing/ai), for a single user at a time, memory bandwidth will be the bottleneck.\\n\\nGet the 7900XTX if you’re SURE you’ll never need any real amount of concurrent users and won’t need to fine tune, and you’re fine with less ray tracing performance and performance for extreme productivity workloads like effect-filled 4k+ video editing, just because it’s cheaper.\\n\\nGet the 3090 if you don’t care about the extra money or want something more well-rounded (again, just for simple ollama use, ALL you need to care about is VRAM, for multiple users such as server hosting is when you’d need the 3090.)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n03in2f","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Both will do 1440p ultra at &amp;gt;60fps in almost every game currently.&lt;/p&gt;\\n\\n&lt;p&gt;And even then, medium-high settings really isn’t an issue.&lt;/p&gt;\\n\\n&lt;p&gt;Both will do a single LLM at the same time perfectly fine - I have a 6900 xt and, despite bring RDNA2 (bad ray tracing/ai), for a single user at a time, memory bandwidth will be the bottleneck.&lt;/p&gt;\\n\\n&lt;p&gt;Get the 7900XTX if you’re SURE you’ll never need any real amount of concurrent users and won’t need to fine tune, and you’re fine with less ray tracing performance and performance for extreme productivity workloads like effect-filled 4k+ video editing, just because it’s cheaper.&lt;/p&gt;\\n\\n&lt;p&gt;Get the 3090 if you don’t care about the extra money or want something more well-rounded (again, just for simple ollama use, ALL you need to care about is VRAM, for multiple users such as server hosting is when you’d need the 3090.)&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1llvz0g/7900xtx_vs_rtx3090/n03in2f/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751044301,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1llvz0g","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n03fazr","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"My_Unbiased_Opinion","can_mod_post":false,"created_utc":1751043366,"send_replies":true,"parent_id":"t1_n02zxyx","score":3,"author_fullname":"t2_esiyl0yb","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yeah. IMHO, because the OP is gaming, I would recommend XTX. It's just much faster card in gaming. Inference via common inference engines is quite performant as well.. ","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n03fazr","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yeah. IMHO, because the OP is gaming, I would recommend XTX. It&amp;#39;s just much faster card in gaming. Inference via common inference engines is quite performant as well.. &lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1llvz0g","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1llvz0g/7900xtx_vs_rtx3090/n03fazr/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751043366,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n02zxyx","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Biomass23","can_mod_post":false,"created_utc":1751039041,"send_replies":true,"parent_id":"t3_1llvz0g","score":6,"author_fullname":"t2_j0xufqz","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I have both.  I didn't do any benchmarking, but it feels like the 7900 XTX is faster than the 3090, and slower than a 4090.  Either are a good choice.  The 24GB of VRAM would be preferred over a faster GPU with less VRAM.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n02zxyx","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I have both.  I didn&amp;#39;t do any benchmarking, but it feels like the 7900 XTX is faster than the 3090, and slower than a 4090.  Either are a good choice.  The 24GB of VRAM would be preferred over a faster GPU with less VRAM.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1llvz0g/7900xtx_vs_rtx3090/n02zxyx/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751039041,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1llvz0g","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n04ce2e","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Willing_Landscape_61","can_mod_post":false,"created_utc":1751052800,"send_replies":true,"parent_id":"t1_n03ecbz","score":2,"author_fullname":"t2_8lvrytgw","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"\\" Multigpu does work on AMD but is more of a hassle. \\"\\nThis might be of interest to me. Would you have some reference for me to read on multi GPU AMD setups?\\nThx ","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n04ce2e","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;&amp;quot; Multigpu does work on AMD but is more of a hassle. &amp;quot;\\nThis might be of interest to me. Would you have some reference for me to read on multi GPU AMD setups?\\nThx &lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1llvz0g","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1llvz0g/7900xtx_vs_rtx3090/n04ce2e/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751052800,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n03ecbz","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"My_Unbiased_Opinion","can_mod_post":false,"created_utc":1751043096,"send_replies":true,"parent_id":"t3_1llvz0g","score":6,"author_fullname":"t2_esiyl0yb","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I have both cards. If you are gaming and need LLM inference only, then get the XTX. Don't consider the 3090. The XTX is MUCH faster in gaming. And for inference speeds, they are similar. \\n\\n\\nThe 3090 wins when you are doing more experimental AI stuff where CUDA support comes first for these things. But with llama.cpp and Ollama, XTX is solid. \\n\\n\\n7900XTX is the right answer here. However, if you want to expand to more GPUs, then stick with Nvidia. Multigpu does work on AMD but is more of a hassle. ","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n03ecbz","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I have both cards. If you are gaming and need LLM inference only, then get the XTX. Don&amp;#39;t consider the 3090. The XTX is MUCH faster in gaming. And for inference speeds, they are similar. &lt;/p&gt;\\n\\n&lt;p&gt;The 3090 wins when you are doing more experimental AI stuff where CUDA support comes first for these things. But with llama.cpp and Ollama, XTX is solid. &lt;/p&gt;\\n\\n&lt;p&gt;7900XTX is the right answer here. However, if you want to expand to more GPUs, then stick with Nvidia. Multigpu does work on AMD but is more of a hassle. &lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1llvz0g/7900xtx_vs_rtx3090/n03ecbz/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751043096,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1llvz0g","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n03nwss","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"djdeniro","can_mod_post":false,"created_utc":1751045765,"send_replies":true,"parent_id":"t3_1llvz0g","score":2,"author_fullname":"t2_1epwrhsm","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"7900 xtx amazing, but for VLLM it support only FP16 and AWQ, FP8 works bad.\\n\\nAnyway now i have 4x7900xtx and one 7800xt.\\n\\ni buy 1 Refurbished from UAE and it was broken after one week of low usage, ask seller to test VRAM for 30-40 minutes before buy it\\n\\nVulkan has very good speed for output, but prompt tokens will slow, ROCM works very well for prompt speed, and 10-15% slower than Vulkan. You will got slower speed than 3090 for rocm, and same with Vulkan.\\n\\nAnd if you plan to make build with 2x GPU, 3090 will faster with nvlink. and same with VLLM tensor parallelism\\n\\nMy friend buy used 3090, and it got high temperature of ram, in 7900 xtx it depends of model, XFX model has good temperature, saphire  has high temperature, maybe 5-20% more. than XFX\\n\\nhttps://preview.redd.it/n36injnbbi9f1.jpeg?width=960&amp;format=pjpg&amp;auto=webp&amp;s=3df03f81bd707443d27229c9832884bd7209876b","edited":1751046188,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n03nwss","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;7900 xtx amazing, but for VLLM it support only FP16 and AWQ, FP8 works bad.&lt;/p&gt;\\n\\n&lt;p&gt;Anyway now i have 4x7900xtx and one 7800xt.&lt;/p&gt;\\n\\n&lt;p&gt;i buy 1 Refurbished from UAE and it was broken after one week of low usage, ask seller to test VRAM for 30-40 minutes before buy it&lt;/p&gt;\\n\\n&lt;p&gt;Vulkan has very good speed for output, but prompt tokens will slow, ROCM works very well for prompt speed, and 10-15% slower than Vulkan. You will got slower speed than 3090 for rocm, and same with Vulkan.&lt;/p&gt;\\n\\n&lt;p&gt;And if you plan to make build with 2x GPU, 3090 will faster with nvlink. and same with VLLM tensor parallelism&lt;/p&gt;\\n\\n&lt;p&gt;My friend buy used 3090, and it got high temperature of ram, in 7900 xtx it depends of model, XFX model has good temperature, saphire  has high temperature, maybe 5-20% more. than XFX&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://preview.redd.it/n36injnbbi9f1.jpeg?width=960&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=3df03f81bd707443d27229c9832884bd7209876b\\"&gt;https://preview.redd.it/n36injnbbi9f1.jpeg?width=960&amp;amp;format=pjpg&amp;amp;auto=webp&amp;amp;s=3df03f81bd707443d27229c9832884bd7209876b&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1llvz0g/7900xtx_vs_rtx3090/n03nwss/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751045765,"media_metadata":{"n36injnbbi9f1":{"status":"valid","e":"Image","m":"image/jpeg","p":[{"y":144,"x":108,"u":"https://preview.redd.it/n36injnbbi9f1.jpeg?width=108&amp;crop=smart&amp;auto=webp&amp;s=96ef8bbd5e01cc3cafc03ac9cd690f0a96b88a17"},{"y":288,"x":216,"u":"https://preview.redd.it/n36injnbbi9f1.jpeg?width=216&amp;crop=smart&amp;auto=webp&amp;s=53bcf8f21a8179b790166bd2c4600f92e55db855"},{"y":426,"x":320,"u":"https://preview.redd.it/n36injnbbi9f1.jpeg?width=320&amp;crop=smart&amp;auto=webp&amp;s=f9d5065638e2a9257d4c8cd58537b1e435b9517f"},{"y":853,"x":640,"u":"https://preview.redd.it/n36injnbbi9f1.jpeg?width=640&amp;crop=smart&amp;auto=webp&amp;s=08cce0d96e9eb730a3204c5579de9992fff492e7"},{"y":1280,"x":960,"u":"https://preview.redd.it/n36injnbbi9f1.jpeg?width=960&amp;crop=smart&amp;auto=webp&amp;s=3513b0c6a42d2aa46d97dd9db4fe507d79b65c40"}],"s":{"y":1280,"x":960,"u":"https://preview.redd.it/n36injnbbi9f1.jpeg?width=960&amp;format=pjpg&amp;auto=webp&amp;s=3df03f81bd707443d27229c9832884bd7209876b"},"id":"n36injnbbi9f1"}},"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1llvz0g","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n04caji","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Marksta","can_mod_post":false,"created_utc":1751052769,"send_replies":true,"parent_id":"t3_1llvz0g","score":2,"author_fullname":"t2_559a1","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Linking to my comment answering this earlier today from an LLM point of view: https://www.reddit.com/r/LocalLLaMA/comments/1lls5ru/optimal_poor_mans_gpu_for_local_inference/n029zq2/\\n\\nIf gaming is the focus, then just use gaming benchmarks and buy what's best for your sort of games. The goal is totally opposite, you don't desperately need VRAM above all else for gaming. If it's AMD that'll get you most FPS, then Llama.cpp Vulkan is good enough for hobbyist use.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n04caji","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Linking to my comment answering this earlier today from an LLM point of view: &lt;a href=\\"https://www.reddit.com/r/LocalLLaMA/comments/1lls5ru/optimal_poor_mans_gpu_for_local_inference/n029zq2/\\"&gt;https://www.reddit.com/r/LocalLLaMA/comments/1lls5ru/optimal_poor_mans_gpu_for_local_inference/n029zq2/&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;If gaming is the focus, then just use gaming benchmarks and buy what&amp;#39;s best for your sort of games. The goal is totally opposite, you don&amp;#39;t desperately need VRAM above all else for gaming. If it&amp;#39;s AMD that&amp;#39;ll get you most FPS, then Llama.cpp Vulkan is good enough for hobbyist use.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1llvz0g/7900xtx_vs_rtx3090/n04caji/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751052769,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1llvz0g","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n02zc7m","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"k_means_clusterfuck","can_mod_post":false,"created_utc":1751038874,"send_replies":true,"parent_id":"t3_1llvz0g","score":3,"author_fullname":"t2_4bby1cv5","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"3090","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n02zc7m","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;3090&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1llvz0g/7900xtx_vs_rtx3090/n02zc7m/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751038874,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1llvz0g","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n03fpog","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"a_beautiful_rhind","can_mod_post":false,"created_utc":1751043480,"send_replies":true,"parent_id":"t3_1llvz0g","score":1,"author_fullname":"t2_h5utwre7","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Does 7900 support fp8 or BF16? You will fight more with non llama.cpp things from what I can tell.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n03fpog","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Does 7900 support fp8 or BF16? You will fight more with non llama.cpp things from what I can tell.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1llvz0g/7900xtx_vs_rtx3090/n03fpog/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751043480,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1llvz0g","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n030gap","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"kevin_1994","can_mod_post":false,"created_utc":1751039183,"send_replies":true,"parent_id":"t3_1llvz0g","score":-1,"author_fullname":"t2_o015g","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Get the 3090. \\n\\nMany frameworks \\"support\\" ROCm and Vulkan, but as second class citizens. For Vulkan specifically, you're also going to get a big hit compared to native CUDA. And there are also many frameworks that only support NVIDIA at this time, and it seems unlikely to change.\\n\\nAnother thing to consider is that there are tons of cheap NVIDIA cards you can expand your system with in the future such as 3060, P100, even 2000 series cards you can often find on marketplace for &lt; $200. Mixing AMD and NVIDIA is apparently possible in Vulkan, but is going to be a huge pain in the ass for most use cases.\\n\\nSidenote: while 7900xtx appears faster than 3090 for gaming on paper, imo 3090 + DLSS4 is better. ymmv","edited":1751039429,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n030gap","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Get the 3090. &lt;/p&gt;\\n\\n&lt;p&gt;Many frameworks &amp;quot;support&amp;quot; ROCm and Vulkan, but as second class citizens. For Vulkan specifically, you&amp;#39;re also going to get a big hit compared to native CUDA. And there are also many frameworks that only support NVIDIA at this time, and it seems unlikely to change.&lt;/p&gt;\\n\\n&lt;p&gt;Another thing to consider is that there are tons of cheap NVIDIA cards you can expand your system with in the future such as 3060, P100, even 2000 series cards you can often find on marketplace for &amp;lt; $200. Mixing AMD and NVIDIA is apparently possible in Vulkan, but is going to be a huge pain in the ass for most use cases.&lt;/p&gt;\\n\\n&lt;p&gt;Sidenote: while 7900xtx appears faster than 3090 for gaming on paper, imo 3090 + DLSS4 is better. ymmv&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":true,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1llvz0g/7900xtx_vs_rtx3090/n030gap/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751039183,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1llvz0g","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":1,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-1}}],"before":null}}]`),o=()=>e.jsx(l,{data:t});export{o as default};
