import{j as e}from"./index-xfnGEtuL.js";import{R as l}from"./RedditPostRenderer-DAZRIZZK.js";import"./index-BaNn5-RR.js";const a=JSON.parse('[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Mamba-2 support in llama.cpp landed","link_flair_richtext":[{"e":"text","t":"News"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":70,"top_awarded_type":null,"hide_score":false,"name":"t3_1lq1jyr","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.97,"author_flair_background_color":null,"ups":113,"total_awards_received":0,"media_embed":{},"thumbnail_width":140,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_a2gtk","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"News","can_mod_post":false,"score":113,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"https://external-preview.redd.it/ygyWPNq8dkq2um7AlKnQuPRca4C5R9wcoTedIaY7KTk.png?width=140&amp;height=70&amp;crop=140:70,smart&amp;auto=webp&amp;s=7512f1621ccc42771158d4a83dba439854c266e0","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"post_hint":"link","content_categories":null,"is_self":false,"subreddit_type":"public","created":1751476448,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"github.com","allow_live_comments":false,"selftext_html":null,"likes":null,"suggested_sort":null,"banned_at_utc":null,"url_overridden_by_dest":"https://github.com/ggml-org/llama.cpp/pull/9126#issuecomment-3027064556","view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"preview":{"images":[{"source":{"url":"https://external-preview.redd.it/ygyWPNq8dkq2um7AlKnQuPRca4C5R9wcoTedIaY7KTk.png?auto=webp&amp;s=4510a608e1ff971f852ec57d8f07668f2b8ab0d6","width":1200,"height":600},"resolutions":[{"url":"https://external-preview.redd.it/ygyWPNq8dkq2um7AlKnQuPRca4C5R9wcoTedIaY7KTk.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=d7a6abcb15d265565c74fee896eff28e14b9a9f0","width":108,"height":54},{"url":"https://external-preview.redd.it/ygyWPNq8dkq2um7AlKnQuPRca4C5R9wcoTedIaY7KTk.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=5355fc411298a6af9630c201bb9f61dddf8fb479","width":216,"height":108},{"url":"https://external-preview.redd.it/ygyWPNq8dkq2um7AlKnQuPRca4C5R9wcoTedIaY7KTk.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=4e602810402906fef613cd3cd6f104bb341ab3f8","width":320,"height":160},{"url":"https://external-preview.redd.it/ygyWPNq8dkq2um7AlKnQuPRca4C5R9wcoTedIaY7KTk.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=c83c80ef04abfb36fdb066d346ea91753a7d280d","width":640,"height":320},{"url":"https://external-preview.redd.it/ygyWPNq8dkq2um7AlKnQuPRca4C5R9wcoTedIaY7KTk.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=04f4978664f7d2eb57a2e9fe0c534b61e0bcd9f1","width":960,"height":480},{"url":"https://external-preview.redd.it/ygyWPNq8dkq2um7AlKnQuPRca4C5R9wcoTedIaY7KTk.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=e45bd94615d08f9378ef6a9d9775ca74a0f112d6","width":1080,"height":540}],"variants":{},"id":"ygyWPNq8dkq2um7AlKnQuPRca4C5R9wcoTedIaY7KTk"}],"enabled":false},"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"f8cc0dfe-c1eb-11ed-8d79-72bc34fff7d9","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"mod_note":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"num_reports":null,"removal_reason":null,"link_flair_background_color":"#cc3600","id":"1lq1jyr","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"pkmxtw","discussion_type":null,"num_comments":10,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lq1jyr/mamba2_support_in_llamacpp_landed/","stickied":false,"url":"https://github.com/ggml-org/llama.cpp/pull/9126#issuecomment-3027064556","subreddit_subscribers":494001,"created_utc":1751476448,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1341lf","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"compilade","can_mod_post":false,"send_replies":true,"parent_id":"t1_n119kgq","score":2,"author_fullname":"t2_vsi08kvjl","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"\\n&gt; sorry for the low effort question\\n\\nIt\'s alright, at least the question is on topic.\\n\\n&gt; you seem really up to date\\n\\nOf course, I wrote the Mamba-2 PR linked in OP ;)\\n\\n&gt; do you have a mamba model you\'d recommend for fill-in-middle?\\n\\nI don\'t really know; I\'ve mostly focused on implementation because it was interesting. I don\'t know which models are good for FIM, because I didn\'t try LLM-assisted coding yet.\\n\\nBut what I know is that recurrent models in `llama.cpp` can\'t currently rollback their state (but might eventually), and so with fill-in-middle, assume the whole context will be reprocessed every time (there is CUDA support for both Mamba-1 and Mamba-2, so the speed could still be  acceptable depending on your hardware and/or context size. At least for recurrent models, VRAM usage is constant for any context size).\\n\\nMamba-Codestral-7B-v0.1 was trained on code, and does seem to have FIM tokens in its vocab (`[PREFIX]`, `[MIDDLE]`, and `[SUFFIX]`); this might require using an appropriate template. There doesn\'t seem to be an official template for that model (or at least I didn\'t find it; if you find a good template, do share).","edited":false,"author_flair_css_class":null,"name":"t1_n1341lf","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;sorry for the low effort question&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;It&amp;#39;s alright, at least the question is on topic.&lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;you seem really up to date&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;Of course, I wrote the Mamba-2 PR linked in OP ;)&lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;do you have a mamba model you&amp;#39;d recommend for fill-in-middle?&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;I don&amp;#39;t really know; I&amp;#39;ve mostly focused on implementation because it was interesting. I don&amp;#39;t know which models are good for FIM, because I didn&amp;#39;t try LLM-assisted coding yet.&lt;/p&gt;\\n\\n&lt;p&gt;But what I know is that recurrent models in &lt;code&gt;llama.cpp&lt;/code&gt; can&amp;#39;t currently rollback their state (but might eventually), and so with fill-in-middle, assume the whole context will be reprocessed every time (there is CUDA support for both Mamba-1 and Mamba-2, so the speed could still be  acceptable depending on your hardware and/or context size. At least for recurrent models, VRAM usage is constant for any context size).&lt;/p&gt;\\n\\n&lt;p&gt;Mamba-Codestral-7B-v0.1 was trained on code, and does seem to have FIM tokens in its vocab (&lt;code&gt;[PREFIX]&lt;/code&gt;, &lt;code&gt;[MIDDLE]&lt;/code&gt;, and &lt;code&gt;[SUFFIX]&lt;/code&gt;); this might require using an appropriate template. There doesn&amp;#39;t seem to be an official template for that model (or at least I didn&amp;#39;t find it; if you find a good template, do share).&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lq1jyr","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":"light","treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lq1jyr/mamba2_support_in_llamacpp_landed/n1341lf/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751526097,"author_flair_text":"llama.cpp","collapsed":false,"created_utc":1751526097,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n119kgq","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MengerianMango","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0zkz25","score":3,"author_fullname":"t2_mcvyi","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Hey sorry for the low effort question but you seem really up to date: do you have a mamba model you\'d recommend for fill-in-middle? Are any of them being developed with that in mind? Any i can use now or that I should be watching for support to be added?\\n\\nThanks","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n119kgq","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Hey sorry for the low effort question but you seem really up to date: do you have a mamba model you&amp;#39;d recommend for fill-in-middle? Are any of them being developed with that in mind? Any i can use now or that I should be watching for support to be added?&lt;/p&gt;\\n\\n&lt;p&gt;Thanks&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lq1jyr","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lq1jyr/mamba2_support_in_llamacpp_landed/n119kgq/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751499626,"author_flair_text":null,"treatment_tags":[],"created_utc":1751499626,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n0zkz25","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"compilade","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0zbmbf","score":28,"author_fullname":"t2_vsi08kvjl","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Note that only pure Mamba-2 models are supported for now. Which means `mistralai/Mamba-Codestral-7B-v0.1` should work, and `state-spaces/mamba2-2.7b` too.\\n\\nHybrid models will be supported later, but it seems like Granite-4.0 and Falcon-H1 are the most actively worked on currently, see https://github.com/ggml-org/llama.cpp/pull/13550 and https://github.com/ggml-org/llama.cpp/pull/14238","edited":1751481595,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n0zkz25","is_submitter":false,"collapsed":false,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Note that only pure Mamba-2 models are supported for now. Which means &lt;code&gt;mistralai/Mamba-Codestral-7B-v0.1&lt;/code&gt; should work, and &lt;code&gt;state-spaces/mamba2-2.7b&lt;/code&gt; too.&lt;/p&gt;\\n\\n&lt;p&gt;Hybrid models will be supported later, but it seems like Granite-4.0 and Falcon-H1 are the most actively worked on currently, see &lt;a href=\\"https://github.com/ggml-org/llama.cpp/pull/13550\\"&gt;https://github.com/ggml-org/llama.cpp/pull/13550&lt;/a&gt; and &lt;a href=\\"https://github.com/ggml-org/llama.cpp/pull/14238\\"&gt;https://github.com/ggml-org/llama.cpp/pull/14238&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lq1jyr","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lq1jyr/mamba2_support_in_llamacpp_landed/n0zkz25/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751481040,"author_flair_text":"llama.cpp","treatment_tags":[],"created_utc":1751481040,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":28}}],"before":null}},"user_reports":[],"saved":false,"id":"n0zbmbf","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"Saffron4609","can_mod_post":false,"created_utc":1751478319,"send_replies":true,"parent_id":"t1_n0z6w39","score":19,"author_fullname":"t2_pwzie27h","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"The Nemotron-H models look pretty strong and I think are hybrid Mamba-2. There\'s also Codestral Mamba.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0zbmbf","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;The Nemotron-H models look pretty strong and I think are hybrid Mamba-2. There&amp;#39;s also Codestral Mamba.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lq1jyr","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lq1jyr/mamba2_support_in_llamacpp_landed/n0zbmbf/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751478319,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":19}}],"before":null}},"user_reports":[],"saved":false,"id":"n0z6w39","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"pseudonerv","can_mod_post":false,"created_utc":1751476994,"send_replies":true,"parent_id":"t3_1lq1jyr","score":20,"author_fullname":"t2_eerln","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Any good mamba-2 models worth trying?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0z6w39","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Any good mamba-2 models worth trying?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lq1jyr/mamba2_support_in_llamacpp_landed/n0z6w39/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751476994,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lq1jyr","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":20}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1359e9","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"compilade","can_mod_post":false,"created_utc":1751526782,"send_replies":true,"parent_id":"t1_n12n97y","score":2,"author_fullname":"t2_vsi08kvjl","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Nice!\\n\\nNote that for Mamba-2 (and also Mamba-1) there isn\'t really any difference between `_S`, `_M` and `_L` variants of quants (except for i-quants which are actually different types), because mixes have not yet been distinguished for the tensors used in state-space models.\\n\\nThis is why some of the model files with different quant mix types have the exact same size (and tensor types if you look at the tensor list).\\n\\n(Quantization should still work, this only means some variants are the same)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1359e9","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Nice!&lt;/p&gt;\\n\\n&lt;p&gt;Note that for Mamba-2 (and also Mamba-1) there isn&amp;#39;t really any difference between &lt;code&gt;_S&lt;/code&gt;, &lt;code&gt;_M&lt;/code&gt; and &lt;code&gt;_L&lt;/code&gt; variants of quants (except for i-quants which are actually different types), because mixes have not yet been distinguished for the tensors used in state-space models.&lt;/p&gt;\\n\\n&lt;p&gt;This is why some of the model files with different quant mix types have the exact same size (and tensor types if you look at the tensor list).&lt;/p&gt;\\n\\n&lt;p&gt;(Quantization should still work, this only means some variants are the same)&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lq1jyr","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lq1jyr/mamba2_support_in_llamacpp_landed/n1359e9/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751526782,"author_flair_text":"llama.cpp","treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n12n97y","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"GL-AI","can_mod_post":false,"created_utc":1751517539,"send_replies":true,"parent_id":"t3_1lq1jyr","score":3,"author_fullname":"t2_1sr5yw3yg0","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I made some Mamba Codestral imatrix [GGUFs](https://huggingface.co/gabriellarson/Mamba-Codestral-7B-v0.1-GGUF). Results have been hit or miss. I\'m not sure what samplers are best so if anyone wants to try and mess around with them let me know what you find. Also make sure to use --chat-template Mistral","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n12n97y","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I made some Mamba Codestral imatrix &lt;a href=\\"https://huggingface.co/gabriellarson/Mamba-Codestral-7B-v0.1-GGUF\\"&gt;GGUFs&lt;/a&gt;. Results have been hit or miss. I&amp;#39;m not sure what samplers are best so if anyone wants to try and mess around with them let me know what you find. Also make sure to use --chat-template Mistral&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lq1jyr/mamba2_support_in_llamacpp_landed/n12n97y/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751517539,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lq1jyr","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"2b12e2b8-fdc0-11ee-9a03-6e2f48afd456","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n12ii15","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"xXWarMachineRoXx","can_mod_post":false,"created_utc":1751515437,"send_replies":true,"parent_id":"t3_1lq1jyr","score":1,"author_fullname":"t2_6zhl6n94","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I just re watched the mamba explainer video","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n12ii15","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"Llama 3"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I just re watched the mamba explainer video&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lq1jyr/mamba2_support_in_llamacpp_landed/n12ii15/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751515437,"author_flair_text":"Llama 3","treatment_tags":[],"link_id":"t3_1lq1jyr","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#c7b594","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0zgv0n","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":"LOW_SCORE","no_follow":true,"author":"No_Edge2098","can_mod_post":false,"created_utc":1751479817,"send_replies":true,"parent_id":"t3_1lq1jyr","score":-8,"author_fullname":"t2_uaotuj04","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":true,"body":"&gt;","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0zgv0n","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;/blockquote&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":"comment score below threshold","distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lq1jyr/mamba2_support_in_llamacpp_landed/n0zgv0n/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751479817,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lq1jyr","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-8}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n101evi","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":"LOW_SCORE","no_follow":true,"author":"AppearanceHeavy6724","can_mod_post":false,"created_utc":1751485985,"send_replies":true,"parent_id":"t3_1lq1jyr","score":-6,"author_fullname":"t2_uz37qfx5","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":true,"body":"[https://www.youtube.com/watch?v=ZNys8Ua-BaU](https://www.youtube.com/watch?v=ZNys8Ua-BaU)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n101evi","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;&lt;a href=\\"https://www.youtube.com/watch?v=ZNys8Ua-BaU\\"&gt;https://www.youtube.com/watch?v=ZNys8Ua-BaU&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":"comment score below threshold","distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lq1jyr/mamba2_support_in_llamacpp_landed/n101evi/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751485985,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lq1jyr","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-6}}],"before":null}}]'),s=()=>e.jsx(l,{data:a});export{s as default};
