import{j as e}from"./index-Cd3v0jxz.js";import{R as l}from"./RedditPostRenderer-cI96YLhy.js";import"./index-DGBoKyQm.js";const t=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Recently started experimenting with training my own models and I wanted to ask if anyone knew. I would figure that if you were designing a model to be run at a precision lower than fp16 you would just quantize the model and then train over the quantized model instead of training in fp16 with “fake quantization”. I’m sure there’s a reason for it, I would just like to know if anyone more experienced than I could explain it. Thanks!","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"What is the point of QAT?","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1ltmou4","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.4,"author_flair_background_color":null,"subreddit_type":"public","ups":0,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_1flwpwd3","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":0,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1751868472,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Recently started experimenting with training my own models and I wanted to ask if anyone knew. I would figure that if you were designing a model to be run at a precision lower than fp16 you would just quantize the model and then train over the quantized model instead of training in fp16 with “fake quantization”. I’m sure there’s a reason for it, I would just like to know if anyone more experienced than I could explain it. Thanks!&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":true,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1ltmou4","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"WyattTheSkid","discussion_type":null,"num_comments":6,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1ltmou4/what_is_the_point_of_qat/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1ltmou4/what_is_the_point_of_qat/","subreddit_subscribers":496034,"created_utc":1751868472,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1rmtu1","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"WyattTheSkid","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1rmc22","score":1,"author_fullname":"t2_1flwpwd3","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Yeah pretty much. Afaik, fp16 has been widely considered to be precise enough for llms but I’m wondering if the extra depth would make a difference for QAT. I’m very intrigued with this training method as Gemma 3 is the only model ive ever enjoyed using in 4 bit","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1rmtu1","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yeah pretty much. Afaik, fp16 has been widely considered to be precise enough for llms but I’m wondering if the extra depth would make a difference for QAT. I’m very intrigued with this training method as Gemma 3 is the only model ive ever enjoyed using in 4 bit&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ltmou4","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltmou4/what_is_the_point_of_qat/n1rmtu1/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751870915,"author_flair_text":null,"treatment_tags":[],"created_utc":1751870915,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1rmc22","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MengerianMango","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1rm4a1","score":2,"author_fullname":"t2_mcvyi","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"So you're saying keep gradients in fp32 rather than fp16? Doubt it matters.\\n\\nI'm not an expert, mostly just watch YouTubes. I like 3blue1brown and julia turk.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n1rmc22","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;So you&amp;#39;re saying keep gradients in fp32 rather than fp16? Doubt it matters.&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;m not an expert, mostly just watch YouTubes. I like 3blue1brown and julia turk.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ltmou4","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltmou4/what_is_the_point_of_qat/n1rmc22/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751870638,"author_flair_text":null,"treatment_tags":[],"created_utc":1751870638,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1rmvpw","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"WyattTheSkid","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1rmgkt","score":1,"author_fullname":"t2_1flwpwd3","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I will watch that when I get home from work thank you!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1rmvpw","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I will watch that when I get home from work thank you!&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ltmou4","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltmou4/what_is_the_point_of_qat/n1rmvpw/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751870944,"author_flair_text":null,"treatment_tags":[],"created_utc":1751870944,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n1rmgkt","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MengerianMango","can_mod_post":false,"send_replies":true,"parent_id":"t1_n1rm4a1","score":2,"author_fullname":"t2_mcvyi","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"https://youtu.be/WBm0nyDkVYM?si=zHk8vPjPs9L-Z_ef\\n\\nYou'll probably find this useful. There's a section on QAT.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n1rmgkt","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;&lt;a href=\\"https://youtu.be/WBm0nyDkVYM?si=zHk8vPjPs9L-Z_ef\\"&gt;https://youtu.be/WBm0nyDkVYM?si=zHk8vPjPs9L-Z_ef&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;You&amp;#39;ll probably find this useful. There&amp;#39;s a section on QAT.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ltmou4","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltmou4/what_is_the_point_of_qat/n1rmgkt/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751870709,"author_flair_text":null,"treatment_tags":[],"created_utc":1751870709,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n1rm4a1","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"WyattTheSkid","can_mod_post":false,"created_utc":1751870516,"send_replies":true,"parent_id":"t1_n1rluuo","score":0,"author_fullname":"t2_1flwpwd3","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Oh ok that makes sense actually. Do you think training in fp32 with QAT targeting FP4 would make a noticeable difference opposed to fp16?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1rm4a1","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Oh ok that makes sense actually. Do you think training in fp32 with QAT targeting FP4 would make a noticeable difference opposed to fp16?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1ltmou4","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltmou4/what_is_the_point_of_qat/n1rm4a1/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751870516,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}},"user_reports":[],"saved":false,"id":"n1rluuo","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"MengerianMango","can_mod_post":false,"created_utc":1751870370,"send_replies":true,"parent_id":"t3_1ltmou4","score":7,"author_fullname":"t2_mcvyi","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"QAT keeps weights in quantized form but calculates gradients in full fp16 on the forward pass, so that it can make more accurately informed decisions to change the quantized weights.\\n\\nI think most models are trained in fp16 first (or maybe fp8). QAT is a post processing step.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1rluuo","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;QAT keeps weights in quantized form but calculates gradients in full fp16 on the forward pass, so that it can make more accurately informed decisions to change the quantized weights.&lt;/p&gt;\\n\\n&lt;p&gt;I think most models are trained in fp16 first (or maybe fp8). QAT is a post processing step.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1ltmou4/what_is_the_point_of_qat/n1rluuo/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751870370,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1ltmou4","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":7}}],"before":null}}]`),o=()=>e.jsx(l,{data:t});export{o as default};
