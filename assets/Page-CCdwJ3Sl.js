import{j as e}from"./index-DOAmItP2.js";import{R as l}from"./RedditPostRenderer-KKgzpPpv.js";import"./index-YSfz60vQ.js";const t=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"I can get RTX 2080 TI 22GBs for around 350 USD per. Are they a good deal for running LLMs locally using LMStudio?\\n\\nThe plan is to get a cheap CPU with a desktop motherboard that has 4 PCIE slots. \\n\\nI will likely get a Ryzen 5 3600 with an ATX B450 board and 4 sticks of 16gb DDR4 ram totalling 64gb.\\n\\nI think some B450 boards have 4 slots? One concern of mine is that some of the slots will probably be PCIE 3.0 x1 \\n\\nThen I’ll probably start with 2 gpus and maybe add more in the future.\\n\\nAre there any issues with this plan? I’ll reply to comments as best I can if clarification is needed.\\n\\nI got the idea because I wanted a strix halo machine for ai but I realised that with such a cheap 22gb card, it’ll end up cheaper than the 118gb Strix Halo machine. 4x 22gb should get me 88gbs\\n\\nThough the plan right now is to get two gpus. The total cost should end up less than 1000 usd.\\n\\nTwo gpus for 350 each\\nCPU and motherboard for 80\\n64gb Ram for 60\\nPsu for 100\\nCheapo Chinese case for 20 dollars\\n\\n","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"RTX 2080 TI 22gb Build","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lpz46u","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":1,"author_flair_background_color":null,"subreddit_type":"public","ups":2,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_rn6co7q5m","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":2,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1751470793,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;I can get RTX 2080 TI 22GBs for around 350 USD per. Are they a good deal for running LLMs locally using LMStudio?&lt;/p&gt;\\n\\n&lt;p&gt;The plan is to get a cheap CPU with a desktop motherboard that has 4 PCIE slots. &lt;/p&gt;\\n\\n&lt;p&gt;I will likely get a Ryzen 5 3600 with an ATX B450 board and 4 sticks of 16gb DDR4 ram totalling 64gb.&lt;/p&gt;\\n\\n&lt;p&gt;I think some B450 boards have 4 slots? One concern of mine is that some of the slots will probably be PCIE 3.0 x1 &lt;/p&gt;\\n\\n&lt;p&gt;Then I’ll probably start with 2 gpus and maybe add more in the future.&lt;/p&gt;\\n\\n&lt;p&gt;Are there any issues with this plan? I’ll reply to comments as best I can if clarification is needed.&lt;/p&gt;\\n\\n&lt;p&gt;I got the idea because I wanted a strix halo machine for ai but I realised that with such a cheap 22gb card, it’ll end up cheaper than the 118gb Strix Halo machine. 4x 22gb should get me 88gbs&lt;/p&gt;\\n\\n&lt;p&gt;Though the plan right now is to get two gpus. The total cost should end up less than 1000 usd.&lt;/p&gt;\\n\\n&lt;p&gt;Two gpus for 350 each\\nCPU and motherboard for 80\\n64gb Ram for 60\\nPsu for 100\\nCheapo Chinese case for 20 dollars&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1lpz46u","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"opoot_","discussion_type":null,"num_comments":8,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lpz46u/rtx_2080_ti_22gb_build/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lpz46u/rtx_2080_ti_22gb_build/","subreddit_subscribers":494001,"created_utc":1751470793,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0yom2m","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"opoot_","can_mod_post":false,"created_utc":1751471829,"send_replies":true,"parent_id":"t1_n0yo8jv","score":1,"author_fullname":"t2_rn6co7q5m","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Chinese platform called TaoBao","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0yom2m","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Chinese platform called TaoBao&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lpz46u","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpz46u/rtx_2080_ti_22gb_build/n0yom2m/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751471829,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0yo8jv","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Due_Orange_3723","can_mod_post":false,"created_utc":1751471725,"send_replies":true,"parent_id":"t3_1lpz46u","score":1,"author_fullname":"t2_v3f2lhjd","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Where are you finding them for $350?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0yo8jv","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Where are you finding them for $350?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpz46u/rtx_2080_ti_22gb_build/n0yo8jv/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751471725,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lpz46u","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0yyddq","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"meatyminus","can_mod_post":false,"created_utc":1751474619,"send_replies":true,"parent_id":"t3_1lpz46u","score":1,"author_fullname":"t2_ogou6","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"They are usable but setup things to run with multiple GPUs is not that easy. But if you want to run small models or generate image/video/audio, they are great. I'm currently use 2x2080ti + 64gb ddr5 + r7 7900x + b650e-e. My order have nvlink but the gap of 2 cards is too large to use, so normally I just run multiple models at the same time in both.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0yyddq","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;They are usable but setup things to run with multiple GPUs is not that easy. But if you want to run small models or generate image/video/audio, they are great. I&amp;#39;m currently use 2x2080ti + 64gb ddr5 + r7 7900x + b650e-e. My order have nvlink but the gap of 2 cards is too large to use, so normally I just run multiple models at the same time in both.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpz46u/rtx_2080_ti_22gb_build/n0yyddq/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751474619,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lpz46u","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0zcfe4","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"a_beautiful_rhind","can_mod_post":false,"created_utc":1751478545,"send_replies":true,"parent_id":"t3_1lpz46u","score":1,"author_fullname":"t2_h5utwre7","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"A lot of stuff wants ampere now and pytorch flash attention doesn't work on these. \\n\\nThey have nvlink support so you can somewhat get around the 1x issue. Driver support isn't getting dropped any time soon so there's that.\\n\\nMaybe get a better board that you can at least do 8x for every card.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0zcfe4","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;A lot of stuff wants ampere now and pytorch flash attention doesn&amp;#39;t work on these. &lt;/p&gt;\\n\\n&lt;p&gt;They have nvlink support so you can somewhat get around the 1x issue. Driver support isn&amp;#39;t getting dropped any time soon so there&amp;#39;s that.&lt;/p&gt;\\n\\n&lt;p&gt;Maybe get a better board that you can at least do 8x for every card.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpz46u/rtx_2080_ti_22gb_build/n0zcfe4/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751478545,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lpz46u","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1021rk","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AppearanceHeavy6724","can_mod_post":false,"created_utc":1751486171,"send_replies":true,"parent_id":"t1_n0zfvfb","score":1,"author_fullname":"t2_uz37qfx5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"&gt;However, for $350 if I already had an establish task and stack that supported the old architecture, it does seem tempting. \\n\\nImho it makes way more sense to buy 2x3060.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1021rk","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;However, for $350 if I already had an establish task and stack that supported the old architecture, it does seem tempting. &lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;Imho it makes way more sense to buy 2x3060.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lpz46u","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpz46u/rtx_2080_ti_22gb_build/n1021rk/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751486171,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0zfvfb","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Hot_Turnip_3309","can_mod_post":false,"created_utc":1751479529,"send_replies":true,"parent_id":"t3_1lpz46u","score":2,"author_fullname":"t2_161yq4x22a","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"with 2080ti, you are not going to get BF16, only FP16 half precision. Which is basically FP8 in terms of accuracy.  I would not, in 2025, buy anything that doesn't support BF16. However, for $350 if I already had an establish task and stack that supported the old architecture, it does seem tempting.\\n\\nIf you can get something that support BF16","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0zfvfb","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;with 2080ti, you are not going to get BF16, only FP16 half precision. Which is basically FP8 in terms of accuracy.  I would not, in 2025, buy anything that doesn&amp;#39;t support BF16. However, for $350 if I already had an establish task and stack that supported the old architecture, it does seem tempting.&lt;/p&gt;\\n\\n&lt;p&gt;If you can get something that support BF16&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpz46u/rtx_2080_ti_22gb_build/n0zfvfb/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751479529,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lpz46u","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n1029hb","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AppearanceHeavy6724","can_mod_post":false,"created_utc":1751486234,"send_replies":true,"parent_id":"t3_1lpz46u","score":1,"author_fullname":"t2_uz37qfx5","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Prompt processing will probably be ass, which starts getting good only from 30xx series.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n1029hb","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Prompt processing will probably be ass, which starts getting good only from 30xx series.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lpz46u/rtx_2080_ti_22gb_build/n1029hb/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751486234,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lpz46u","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),n=()=>e.jsx(l,{data:t});export{n as default};
