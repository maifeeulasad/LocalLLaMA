import{j as e}from"./index-CNyNkRpk.js";import{R as l}from"./RedditPostRenderer-Dza0u9i2.js";import"./index-BUchu_-K.js";const a=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Hey everyone, good news for AMD GPU users! It seems AMD is getting serious about boosting support for their graphics cards in llama.cpp\\n\\nWord is, someone from AMD dropped a pull request to tweak the code, aimed at adapting the project for use with AMD graphics cards.   \\nDiscussions with the project leaders are planned in the near future to explore opportunities for further enhancements.  \\n[https://github.com/ggml-org/llama.cpp/pull/14624](https://github.com/ggml-org/llama.cpp/pull/14624)","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"AMD's Pull Request for llama.cpp: Enhancing GPU Support","link_flair_richtext":[{"e":"text","t":"Discussion"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lwta86","quarantine":false,"link_flair_text_color":"light","upvote_ratio":0.96,"author_flair_background_color":null,"subreddit_type":"public","ups":362,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_i7v1u","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Discussion","can_mod_post":false,"score":362,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"post_hint":"self","content_categories":null,"is_self":true,"mod_note":null,"created":1752194746,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Hey everyone, good news for AMD GPU users! It seems AMD is getting serious about boosting support for their graphics cards in llama.cpp&lt;/p&gt;\\n\\n&lt;p&gt;Word is, someone from AMD dropped a pull request to tweak the code, aimed at adapting the project for use with AMD graphics cards.&lt;br/&gt;\\nDiscussions with the project leaders are planned in the near future to explore opportunities for further enhancements.&lt;br/&gt;\\n&lt;a href=\\"https://github.com/ggml-org/llama.cpp/pull/14624\\"&gt;https://github.com/ggml-org/llama.cpp/pull/14624&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"preview":{"images":[{"source":{"url":"https://external-preview.redd.it/Nv6JEmpzvB3dldFAieW1ex5iixHjB2uRtht6aYJ1wHE.png?auto=webp&amp;s=7e2a03c13fb4ce4c8558a9462d8d8db18654140b","width":1200,"height":600},"resolutions":[{"url":"https://external-preview.redd.it/Nv6JEmpzvB3dldFAieW1ex5iixHjB2uRtht6aYJ1wHE.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=4049b21c0ac9b7c3089ec2e3df2e59d8659989da","width":108,"height":54},{"url":"https://external-preview.redd.it/Nv6JEmpzvB3dldFAieW1ex5iixHjB2uRtht6aYJ1wHE.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=64c95decda4b1ae5bbb895753cfeceaa35e24a90","width":216,"height":108},{"url":"https://external-preview.redd.it/Nv6JEmpzvB3dldFAieW1ex5iixHjB2uRtht6aYJ1wHE.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=fa643aece72bc03f667bdb9cc869c4aa0b4e21c6","width":320,"height":160},{"url":"https://external-preview.redd.it/Nv6JEmpzvB3dldFAieW1ex5iixHjB2uRtht6aYJ1wHE.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=0f1dc1a1002471d9dfa784ab140da65e1281030d","width":640,"height":320},{"url":"https://external-preview.redd.it/Nv6JEmpzvB3dldFAieW1ex5iixHjB2uRtht6aYJ1wHE.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=8339bbe8f014bcbb3649aaeea0714cc8ef76827c","width":960,"height":480},{"url":"https://external-preview.redd.it/Nv6JEmpzvB3dldFAieW1ex5iixHjB2uRtht6aYJ1wHE.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=d6cedcf5e6a230d8742e6b27dc0eda7e78cf0f16","width":1080,"height":540}],"variants":{},"id":"Nv6JEmpzvB3dldFAieW1ex5iixHjB2uRtht6aYJ1wHE"}],"enabled":false},"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"5f921ea4-c7bc-11ed-9c23-3a00622979b4","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#646d73","id":"1lwta86","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"Rrraptr","discussion_type":null,"num_comments":58,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lwta86/amds_pull_request_for_llamacpp_enhancing_gpu/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lwta86/amds_pull_request_for_llamacpp_enhancing_gpu/","subreddit_subscribers":498115,"created_utc":1752194746,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2irkzz","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"SkyFeistyLlama8","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2ioy4q","score":15,"author_fullname":"t2_1hgbaqgbnq","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Giving those Qualcomm engineers the benefit of the doubt, the OpenCL backend had been abandoned for a while. I wish there was proper Vulkan support for Adreno because that would be cross-platform.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2irkzz","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Giving those Qualcomm engineers the benefit of the doubt, the OpenCL backend had been abandoned for a while. I wish there was proper Vulkan support for Adreno because that would be cross-platform.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lwta86","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwta86/amds_pull_request_for_llamacpp_enhancing_gpu/n2irkzz/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752226908,"author_flair_text":null,"treatment_tags":[],"created_utc":1752226908,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":15}}],"before":null}},"user_reports":[],"saved":false,"id":"n2ioy4q","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"spaceman_","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2h2gxv","score":13,"author_fullname":"t2_9neub","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I think it's odd that the upstream project allowed Qualcomm to nuke support for other GPUs in the OpenCL backend the way they did. OpenCL was a universal backend, and now it's Adreno-first &amp; only second class support for Intel iGPUs, nothing else.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2ioy4q","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I think it&amp;#39;s odd that the upstream project allowed Qualcomm to nuke support for other GPUs in the OpenCL backend the way they did. OpenCL was a universal backend, and now it&amp;#39;s Adreno-first &amp;amp; only second class support for Intel iGPUs, nothing else.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lwta86","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwta86/amds_pull_request_for_llamacpp_enhancing_gpu/n2ioy4q/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752225403,"author_flair_text":null,"treatment_tags":[],"created_utc":1752225403,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":13}},{"kind":"t1","data":{"total_awards_received":0,"approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"ups":-11,"removal_reason":null,"link_id":"t3_1lwta86","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2ia5b7","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"SashaUsesReddit","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2i9la1","score":1,"author_fullname":"t2_57wafqev","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yeah... not one in enterprise cares about llama.cpp support. It drives no revenue.\\n\\nEnterprise support (funding) is for less hobby grade applications","edited":false,"author_flair_css_class":null,"name":"t1_n2ia5b7","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yeah... not one in enterprise cares about llama.cpp support. It drives no revenue.&lt;/p&gt;\\n\\n&lt;p&gt;Enterprise support (funding) is for less hobby grade applications&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lwta86","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwta86/amds_pull_request_for_llamacpp_enhancing_gpu/n2ia5b7/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752216965,"author_flair_text":null,"collapsed":false,"created_utc":1752216965,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2i9la1","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"SkyFeistyLlama8","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2hsfyg","score":7,"author_fullname":"t2_1hgbaqgbnq","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Come on. Qualcomm engineers still can't get the Hexagon HTP NPU working on llama.cpp. I can't get it working with ONNX Runtime either. Maybe it was a different team from Qualcomm that worked on the Adreno OpenCL GPU backend because the NPU team hasn't done a damn thing for llama.cpp.\\n\\nYou basically need to rebuild a model's weights and activations to make it compatible for Hexagon. There's one madlad enthusiast \\"chraac\\" who has been laboring away on integrating Hexagon support into llama.cpp and he's nowhere near reaching his goal.\\n\\nMicrosoft needed months to port old models like Deepseek Distill Qwen 7B and 14B to the NPU using ONNX Runtime and some parts of the model still run on CPU, and that was probably with full support from Qualcomm.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2i9la1","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Come on. Qualcomm engineers still can&amp;#39;t get the Hexagon HTP NPU working on llama.cpp. I can&amp;#39;t get it working with ONNX Runtime either. Maybe it was a different team from Qualcomm that worked on the Adreno OpenCL GPU backend because the NPU team hasn&amp;#39;t done a damn thing for llama.cpp.&lt;/p&gt;\\n\\n&lt;p&gt;You basically need to rebuild a model&amp;#39;s weights and activations to make it compatible for Hexagon. There&amp;#39;s one madlad enthusiast &amp;quot;chraac&amp;quot; who has been laboring away on integrating Hexagon support into llama.cpp and he&amp;#39;s nowhere near reaching his goal.&lt;/p&gt;\\n\\n&lt;p&gt;Microsoft needed months to port old models like Deepseek Distill Qwen 7B and 14B to the NPU using ONNX Runtime and some parts of the model still run on CPU, and that was probably with full support from Qualcomm.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lwta86","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwta86/amds_pull_request_for_llamacpp_enhancing_gpu/n2i9la1/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752216668,"author_flair_text":null,"treatment_tags":[],"created_utc":1752216668,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":7}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2i65o3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"umtausch","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2hsfyg","score":2,"author_fullname":"t2_1ph0uduj","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"You’ve not worked with Qualcomm engineers apparently 😏","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2i65o3","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You’ve not worked with Qualcomm engineers apparently 😏&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lwta86","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwta86/amds_pull_request_for_llamacpp_enhancing_gpu/n2i65o3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752214842,"author_flair_text":null,"treatment_tags":[],"created_utc":1752214842,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n2hsfyg","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":"DELETED","no_follow":true,"author":"[deleted]","can_mod_post":false,"created_utc":1752208205,"send_replies":true,"parent_id":"t1_n2h2gxv","score":-11,"approved_by":null,"report_reasons":null,"all_awardings":[],"subreddit_id":"t5_81eyvm","body":"[deleted]","edited":false,"author_flair_css_class":null,"downs":0,"is_submitter":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;[deleted]&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"associated_award":null,"stickied":false,"subreddit_type":"public","can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":"dark","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwta86/amds_pull_request_for_llamacpp_enhancing_gpu/n2hsfyg/","num_reports":null,"locked":false,"name":"t1_n2hsfyg","created":1752208205,"subreddit":"LocalLLaMA","author_flair_text":null,"treatment_tags":[],"collapsed":true,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":"","collapsed_because_crowd_control":null,"mod_reports":[],"mod_note":null,"distinguished":null}}],"before":null}},"user_reports":[],"saved":false,"id":"n2h2gxv","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"SkyFeistyLlama8","can_mod_post":false,"created_utc":1752198169,"send_replies":true,"parent_id":"t1_n2gtvaz","score":74,"author_fullname":"t2_1hgbaqgbnq","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"It took Qualcomm engineers working on the Adreno OpenCL backend to get that GPU working properly on llama.cpp. It's been almost flawless ever since. ARM engineers also helped out with ARM vector instructions for the CPU backend.\\n\\nWhat took AMD so damned long rofl","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2h2gxv","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;It took Qualcomm engineers working on the Adreno OpenCL backend to get that GPU working properly on llama.cpp. It&amp;#39;s been almost flawless ever since. ARM engineers also helped out with ARM vector instructions for the CPU backend.&lt;/p&gt;\\n\\n&lt;p&gt;What took AMD so damned long rofl&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lwta86","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwta86/amds_pull_request_for_llamacpp_enhancing_gpu/n2h2gxv/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752198169,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":74}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2h999f","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"-p-e-w-","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2h3hru","score":39,"author_fullname":"t2_dkgrhaet","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"They have been involved for almost a year, with Nvidia engineers submitting some highly technical PRs related to scheduling on the GPU (CUDA graphs).","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2h999f","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;They have been involved for almost a year, with Nvidia engineers submitting some highly technical PRs related to scheduling on the GPU (CUDA graphs).&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lwta86","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwta86/amds_pull_request_for_llamacpp_enhancing_gpu/n2h999f/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752200551,"author_flair_text":null,"treatment_tags":[],"created_utc":1752200551,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":39}}],"before":null}},"user_reports":[],"saved":false,"id":"n2h3hru","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"fallingdowndizzyvr","can_mod_post":false,"created_utc":1752198529,"send_replies":true,"parent_id":"t1_n2gtvaz","score":6,"author_fullname":"t2_o65i6kx","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"When did Nvidia get involved?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2h3hru","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;When did Nvidia get involved?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lwta86","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwta86/amds_pull_request_for_llamacpp_enhancing_gpu/n2h3hru/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752198529,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}}],"before":null}},"user_reports":[],"saved":false,"id":"n2gtvaz","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"emprahsFury","can_mod_post":false,"created_utc":1752195183,"send_replies":false,"parent_id":"t3_1lwta86","score":155,"author_fullname":"t2_177r8n","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"And all it took was every other major doing it first","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2gtvaz","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;And all it took was every other major doing it first&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwta86/amds_pull_request_for_llamacpp_enhancing_gpu/n2gtvaz/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752195183,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lwta86","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":155}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2j7gnn","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"SilentLennie","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2i4k6u","score":10,"author_fullname":"t2_4y34t","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"That's.... gonna be a while.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2j7gnn","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;That&amp;#39;s.... gonna be a while.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lwta86","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwta86/amds_pull_request_for_llamacpp_enhancing_gpu/n2j7gnn/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752234487,"author_flair_text":null,"treatment_tags":[],"created_utc":1752234487,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":10}}],"before":null}},"user_reports":[],"saved":false,"id":"n2i4k6u","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"ttkciar","can_mod_post":false,"created_utc":1752214011,"send_replies":true,"parent_id":"t1_n2gwo4j","score":18,"author_fullname":"t2_cpegz","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I'm glad that whenever affordable MI300s find their way onto eBay, I can buy one knowing llama.cpp will work well with it.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2i4k6u","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;m glad that whenever affordable MI300s find their way onto eBay, I can buy one knowing llama.cpp will work well with it.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lwta86","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwta86/amds_pull_request_for_llamacpp_enhancing_gpu/n2i4k6u/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752214011,"author_flair_text":"llama.cpp","treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":18}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2jl7rs","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"a_slay_nub","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2ih5e3","score":9,"author_fullname":"t2_u8o4d","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"No serious inference provider is using llama.cpp unless they want to lose lots of money","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2jl7rs","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;No serious inference provider is using llama.cpp unless they want to lose lots of money&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lwta86","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwta86/amds_pull_request_for_llamacpp_enhancing_gpu/n2jl7rs/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752239460,"author_flair_text":null,"treatment_tags":[],"created_utc":1752239460,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":9}}],"before":null}},"user_reports":[],"saved":false,"id":"n2ih5e3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"abkibaarnsit","can_mod_post":false,"created_utc":1752220883,"send_replies":true,"parent_id":"t1_n2gwo4j","score":6,"author_fullname":"t2_ki1ez","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Even Inference providers use llama.cpp?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2ih5e3","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Even Inference providers use llama.cpp?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lwta86","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwta86/amds_pull_request_for_llamacpp_enhancing_gpu/n2ih5e3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752220883,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2ll3bb","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"FullstackSensei","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2kwxwq","score":1,"author_fullname":"t2_17n3nqtj56","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I wouldn't make such a blanket statement. We have no idea what sort of optimizations whoever use MI cards have done on llama.cpp without releasing the source. The fact that AMD engineers want to setup a call tells you there's a lot more going behind the scenes.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2ll3bb","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I wouldn&amp;#39;t make such a blanket statement. We have no idea what sort of optimizations whoever use MI cards have done on llama.cpp without releasing the source. The fact that AMD engineers want to setup a call tells you there&amp;#39;s a lot more going behind the scenes.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lwta86","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwta86/amds_pull_request_for_llamacpp_enhancing_gpu/n2ll3bb/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752260132,"author_flair_text":null,"treatment_tags":[],"created_utc":1752260132,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2kwxwq","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"CommunityTough1","can_mod_post":false,"created_utc":1752253320,"send_replies":true,"parent_id":"t1_n2gwo4j","score":2,"author_fullname":"t2_1iuzpxw7eg","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Well that's dumb considering the fact that nobody who runs serious cloud inference is using llama.cpp","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2kwxwq","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Well that&amp;#39;s dumb considering the fact that nobody who runs serious cloud inference is using llama.cpp&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lwta86","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwta86/amds_pull_request_for_llamacpp_enhancing_gpu/n2kwxwq/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752253320,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"more","data":{"count":0,"name":"t1__","id":"_","parent_id":"t1_n2lfeqd","depth":10,"children":[]}}],"before":null}},"user_reports":[],"saved":false,"id":"n2lfeqd","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"fallingdowndizzyvr","can_mod_post":false,"created_utc":1752258482,"send_replies":true,"parent_id":"t1_n2ldomv","score":1,"author_fullname":"t2_o65i6kx","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Thank Turing the principals are.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2lfeqd","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thank Turing the principals are.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwta86/amds_pull_request_for_llamacpp_enhancing_gpu/n2lfeqd/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752258482,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lwta86","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":9,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2ldomv","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"HiddenoO","can_mod_post":false,"created_utc":1752257982,"send_replies":true,"parent_id":"t1_n2l9w3j","score":1,"author_fullname":"t2_8127x","approved_by":null,"mod_note":null,"all_awardings":[],"body":"Thank god all software is created equal.","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n2ldomv","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thank god all software is created equal.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwta86/amds_pull_request_for_llamacpp_enhancing_gpu/n2ldomv/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752257982,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lwta86","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":8,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2l9w3j","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"fallingdowndizzyvr","can_mod_post":false,"created_utc":1752256886,"send_replies":true,"parent_id":"t1_n2isih4","score":0,"author_fullname":"t2_o65i6kx","approved_by":null,"mod_note":null,"all_awardings":[],"body":"&gt; You cannot just use some browser engine example and then suggest the same would apply to microarchitectures because \\"that's how it works in tech\\".\\n\\nIt makes perfect sense. Since that's how it works in tech. How do you think they design GPUs? Do you think it's a guy in a room with a little chisel? No. It's a design program. A compiler for silicon as it were. it's software.","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n2l9w3j","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;You cannot just use some browser engine example and then suggest the same would apply to microarchitectures because &amp;quot;that&amp;#39;s how it works in tech&amp;quot;.&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;It makes perfect sense. Since that&amp;#39;s how it works in tech. How do you think they design GPUs? Do you think it&amp;#39;s a guy in a room with a little chisel? No. It&amp;#39;s a design program. A compiler for silicon as it were. it&amp;#39;s software.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lwta86","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwta86/amds_pull_request_for_llamacpp_enhancing_gpu/n2l9w3j/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752256886,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":7,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}},"user_reports":[],"saved":false,"id":"n2isih4","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"HiddenoO","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2i2w79","score":3,"author_fullname":"t2_8127x","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Leaving aside speculation about the similarity of RDNA and CDNA, your comparison makes no sense.\\n\\nYou cannot just use some browser engine example and then suggest the same would apply to microarchitectures because \\"that's how it works in tech\\".\\n\\nNot to mention, your example doesn't generalize as well as you're suggesting in general because it heavily depends on the actual tech stacks in question. You could have two tech stacks with no common origin at all and it could make sense merging them if their non-overlapping features are implemented very modular.","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n2isih4","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Leaving aside speculation about the similarity of RDNA and CDNA, your comparison makes no sense.&lt;/p&gt;\\n\\n&lt;p&gt;You cannot just use some browser engine example and then suggest the same would apply to microarchitectures because &amp;quot;that&amp;#39;s how it works in tech&amp;quot;.&lt;/p&gt;\\n\\n&lt;p&gt;Not to mention, your example doesn&amp;#39;t generalize as well as you&amp;#39;re suggesting in general because it heavily depends on the actual tech stacks in question. You could have two tech stacks with no common origin at all and it could make sense merging them if their non-overlapping features are implemented very modular.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lwta86","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwta86/amds_pull_request_for_llamacpp_enhancing_gpu/n2isih4/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752227419,"author_flair_text":null,"treatment_tags":[],"created_utc":1752227419,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":6,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n2i2w79","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":"LOW_SCORE","no_follow":true,"author":"fallingdowndizzyvr","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2hztfd","score":-9,"author_fullname":"t2_o65i6kx","approved_by":null,"mod_note":null,"all_awardings":[],"body":"&gt; 'They can't be all that different' - like... sure - they have enough shared lineage that with a lot of talented engineers and money they can cherry-pick the best features from both into a new unified architecture.\\n\\nUh huh. They have to be similar to merge or it would be a waste of time. Since if they are too different it would be better to just pick one and ditch the other. Which happens all the time in tech. Are you in tech?\\n\\nHere, this is a relevant example. Remember how Webkit forked from KHTML? Well they diverged so much that at some point KDE ditched their branch altogether and adopted Webkit. Since that made much more sense than trying to merge them back together.\\n\\nThat's how it works in tech. It happens all the time.\\n\\n&gt;  That doesn't mean that 'RDNA 3 is related to CDNA 3' just because they have the same number at the end.\\n\\n\\"Variant\\tCDNA 3 (datacenter)\\"\\n\\nhttps://en.wikipedia.org/wiki/RDNA_3","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n2i2w79","is_submitter":false,"collapsed":true,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;&amp;#39;They can&amp;#39;t be all that different&amp;#39; - like... sure - they have enough shared lineage that with a lot of talented engineers and money they can cherry-pick the best features from both into a new unified architecture.&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;Uh huh. They have to be similar to merge or it would be a waste of time. Since if they are too different it would be better to just pick one and ditch the other. Which happens all the time in tech. Are you in tech?&lt;/p&gt;\\n\\n&lt;p&gt;Here, this is a relevant example. Remember how Webkit forked from KHTML? Well they diverged so much that at some point KDE ditched their branch altogether and adopted Webkit. Since that made much more sense than trying to merge them back together.&lt;/p&gt;\\n\\n&lt;p&gt;That&amp;#39;s how it works in tech. It happens all the time.&lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;That doesn&amp;#39;t mean that &amp;#39;RDNA 3 is related to CDNA 3&amp;#39; just because they have the same number at the end.&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;&amp;quot;Variant    CDNA 3 (datacenter)&amp;quot;&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://en.wikipedia.org/wiki/RDNA_3\\"&gt;https://en.wikipedia.org/wiki/RDNA_3&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":"comment score below threshold","link_id":"t3_1lwta86","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwta86/amds_pull_request_for_llamacpp_enhancing_gpu/n2i2w79/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752213163,"author_flair_text":null,"treatment_tags":[],"created_utc":1752213163,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-9}}],"before":null}},"user_reports":[],"saved":false,"id":"n2hztfd","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"qualverse","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2hvek5","score":17,"author_fullname":"t2_kqw73","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"'They can't be all that different' - like... *sure -* they have enough shared lineage that with a lot of talented engineers and money they can cherry-pick the best features from both into a new unified architecture. That doesn't mean that 'RDNA 3 is related to CDNA 3' just because they have the same number at the end. It's actually less of a logical stretch to say RDNA 3 is related to CDNA 2, since they both came out at the same time, but still completely wrong.\\n\\nAnd sidenote, 'pick one and ditch the other' is really dumb. They could only choose RDNA to 'pick' because CDNA does not actually support graphics, but that would completely sacrifice all of CDNA's benefits like the XCD chiplets, a massively better memory subsystem, and Matrix cores.","edited":false,"author_flair_css_class":null,"name":"t1_n2hztfd","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;&amp;#39;They can&amp;#39;t be all that different&amp;#39; - like... &lt;em&gt;sure -&lt;/em&gt; they have enough shared lineage that with a lot of talented engineers and money they can cherry-pick the best features from both into a new unified architecture. That doesn&amp;#39;t mean that &amp;#39;RDNA 3 is related to CDNA 3&amp;#39; just because they have the same number at the end. It&amp;#39;s actually less of a logical stretch to say RDNA 3 is related to CDNA 2, since they both came out at the same time, but still completely wrong.&lt;/p&gt;\\n\\n&lt;p&gt;And sidenote, &amp;#39;pick one and ditch the other&amp;#39; is really dumb. They could only choose RDNA to &amp;#39;pick&amp;#39; because CDNA does not actually support graphics, but that would completely sacrifice all of CDNA&amp;#39;s benefits like the XCD chiplets, a massively better memory subsystem, and Matrix cores.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lwta86","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwta86/amds_pull_request_for_llamacpp_enhancing_gpu/n2hztfd/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752211635,"author_flair_text":null,"collapsed":false,"created_utc":1752211635,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":17}}],"before":null}},"user_reports":[],"saved":false,"id":"n2hvek5","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":"LOW_SCORE","no_follow":true,"author":"fallingdowndizzyvr","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2hrgty","score":-13,"author_fullname":"t2_o65i6kx","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":true,"body":"Ah... they can't be all that different since AMD has said that both CDNA and RDNA are being merged back together to form UDNA.\\n\\nhttps://www.tomshardware.com/pc-components/cpus/amd-announces-unified-udna-gpu-architecture-bringing-rdna-and-cdna-together-to-take-on-nvidias-cuda-ecosystem\\n\\nIf they were radically different, that wouldn't make any sense. It would make much more sense to pick one and ditch the other.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2hvek5","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Ah... they can&amp;#39;t be all that different since AMD has said that both CDNA and RDNA are being merged back together to form UDNA.&lt;/p&gt;\\n\\n&lt;p&gt;&lt;a href=\\"https://www.tomshardware.com/pc-components/cpus/amd-announces-unified-udna-gpu-architecture-bringing-rdna-and-cdna-together-to-take-on-nvidias-cuda-ecosystem\\"&gt;https://www.tomshardware.com/pc-components/cpus/amd-announces-unified-udna-gpu-architecture-bringing-rdna-and-cdna-together-to-take-on-nvidias-cuda-ecosystem&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;If they were radically different, that wouldn&amp;#39;t make any sense. It would make much more sense to pick one and ditch the other.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":"comment score below threshold","distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lwta86","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwta86/amds_pull_request_for_llamacpp_enhancing_gpu/n2hvek5/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752209537,"author_flair_text":null,"treatment_tags":[],"created_utc":1752209537,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-13}}],"before":null}},"user_reports":[],"saved":false,"id":"n2hrgty","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"qualverse","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2h4auv","score":30,"author_fullname":"t2_kqw73","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"This is actually not true. They're mostly entirely separate architectures that diverged several years ago and have different matrix multiply instruction sets (MFMA vs WMMA).","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2hrgty","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;This is actually not true. They&amp;#39;re mostly entirely separate architectures that diverged several years ago and have different matrix multiply instruction sets (MFMA vs WMMA).&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lwta86","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwta86/amds_pull_request_for_llamacpp_enhancing_gpu/n2hrgty/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752207782,"author_flair_text":null,"treatment_tags":[],"created_utc":1752207782,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":30}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2h5zyp","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"mindwip","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2h4auv","score":7,"author_fullname":"t2_9ojbybyf","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"And maybe zero day support for udna? Or what ever the new one is for next generation.\\n\\nI see this as good.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2h5zyp","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;And maybe zero day support for udna? Or what ever the new one is for next generation.&lt;/p&gt;\\n\\n&lt;p&gt;I see this as good.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lwta86","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwta86/amds_pull_request_for_llamacpp_enhancing_gpu/n2h5zyp/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752199402,"author_flair_text":null,"treatment_tags":[],"created_utc":1752199402,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":7}}],"before":null}},"user_reports":[],"saved":false,"id":"n2h4auv","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":"LOW_SCORE","no_follow":true,"author":"fallingdowndizzyvr","can_mod_post":false,"created_utc":1752198811,"send_replies":true,"parent_id":"t1_n2gwo4j","score":-7,"author_fullname":"t2_o65i6kx","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"RDNA 3 is related to CDNA 3. RDNA 3 is the architecture for the 7000 series cards like the 7900xtx. What benefits CDNA 3 will probably benefit RNDA 3.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2h4auv","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;RDNA 3 is related to CDNA 3. RDNA 3 is the architecture for the 7000 series cards like the 7900xtx. What benefits CDNA 3 will probably benefit RNDA 3.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":"comment score below threshold","distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lwta86","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwta86/amds_pull_request_for_llamacpp_enhancing_gpu/n2h4auv/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752198811,"author_flair_text":null,"treatment_tags":[],"collapsed":true,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":1,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":-7}}],"before":null}},"user_reports":[],"saved":false,"id":"n2gwo4j","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"FullstackSensei","can_mod_post":false,"created_utc":1752196151,"send_replies":true,"parent_id":"t3_1lwta86","score":93,"author_fullname":"t2_17n3nqtj56","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Not to rain on anyone's parade, but this PR is not about llama.cpp support for graphics cards. Literally in the title \\"for CDNA 3\\", which is the MI300-series cards. Given that, I doubt the call he's asking for will be to discuss feature support for any \\"graphics cards\\" and will very probably focus on MI300 cards.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2gwo4j","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Not to rain on anyone&amp;#39;s parade, but this PR is not about llama.cpp support for graphics cards. Literally in the title &amp;quot;for CDNA 3&amp;quot;, which is the MI300-series cards. Given that, I doubt the call he&amp;#39;s asking for will be to discuss feature support for any &amp;quot;graphics cards&amp;quot; and will very probably focus on MI300 cards.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwta86/amds_pull_request_for_llamacpp_enhancing_gpu/n2gwo4j/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752196151,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lwta86","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":93}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"7dba5c08-72f1-11ee-9b6f-ca195bc297d4","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"7dba5c08-72f1-11ee-9b6f-ca195bc297d4","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2jc5yr","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"PraxisOG","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2iuss8","score":1,"author_fullname":"t2_3f9vjjno","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Thx!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2jc5yr","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"Llama 70B"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thx!&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lwta86","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwta86/amds_pull_request_for_llamacpp_enhancing_gpu/n2jc5yr/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752236308,"author_flair_text":"Llama 70B","treatment_tags":[],"created_utc":1752236308,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2iuss8","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"METr_X","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2ispep","score":7,"author_fullname":"t2_16hgrw","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I think that llama.cpp actually has their own implementation that still works with the MI50.\\n\\nThis is mostly relevant when you want to use flash attention with vLLM or something like ComfyUI. If you want to get that to work [this comment on the Level1 forum](https://forum.level1techs.com/t/deepseek-deep-dive-r1-at-home/225826/106) is a good starting point.","edited":1752228854,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n2iuss8","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I think that llama.cpp actually has their own implementation that still works with the MI50.&lt;/p&gt;\\n\\n&lt;p&gt;This is mostly relevant when you want to use flash attention with vLLM or something like ComfyUI. If you want to get that to work &lt;a href=\\"https://forum.level1techs.com/t/deepseek-deep-dive-r1-at-home/225826/106\\"&gt;this comment on the Level1 forum&lt;/a&gt; is a good starting point.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lwta86","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwta86/amds_pull_request_for_llamacpp_enhancing_gpu/n2iuss8/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752228636,"author_flair_text":null,"treatment_tags":[],"created_utc":1752228636,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":7}}],"before":null}},"user_reports":[],"saved":false,"id":"n2ispep","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"PraxisOG","can_mod_post":false,"created_utc":1752227523,"send_replies":true,"parent_id":"t1_n2idul5","score":4,"author_fullname":"t2_3f9vjjno","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Are there instructions for that? I'm hoping to put together a MI50 server soon","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2ispep","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"Llama 70B"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Are there instructions for that? I&amp;#39;m hoping to put together a MI50 server soon&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lwta86","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwta86/amds_pull_request_for_llamacpp_enhancing_gpu/n2ispep/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752227523,"author_flair_text":"Llama 70B","treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2j4lb7","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"BoeJonDaker","can_mod_post":false,"created_utc":1752233282,"send_replies":true,"parent_id":"t1_n2idul5","score":4,"author_fullname":"t2_6kenqrfm","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"&gt; I really want to like AMD but they make it so damn hard.\\n\\nAs a shareholder, I say the same thing.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2j4lb7","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;I really want to like AMD but they make it so damn hard.&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;As a shareholder, I say the same thing.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lwta86","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwta86/amds_pull_request_for_llamacpp_enhancing_gpu/n2j4lb7/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752233282,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}}],"before":null}},"user_reports":[],"saved":false,"id":"n2idul5","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"METr_X","can_mod_post":false,"created_utc":1752219019,"send_replies":true,"parent_id":"t3_1lwta86","score":17,"author_fullname":"t2_16hgrw","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"&gt;We would like to get on call to discuss some future PR plans for \\\\[...\\\\] **flash attention** changes, etc.\\n\\nI'm really excited to see what this is going to bring. But I'm skeptical. Especially given that the FlashAttention-2 ROCm backend dropped support for not only the MI50 and MI60 but also the MI100 which is a \\\\~4 year old card!!! And meanwhile even Nvidia supports their server gpus for \\\\~10 years.\\n\\nI really want to like AMD but they make it so damn hard.\\n\\nEdit: I should add that it would be zero effort for them to add it back in. You literally just have to change 10 lines of code and recompile the whole thing. But AMD specifically chose **not** to do that.","edited":1752219699,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2idul5","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;blockquote&gt;\\n&lt;p&gt;We would like to get on call to discuss some future PR plans for [...] &lt;strong&gt;flash attention&lt;/strong&gt; changes, etc.&lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;I&amp;#39;m really excited to see what this is going to bring. But I&amp;#39;m skeptical. Especially given that the FlashAttention-2 ROCm backend dropped support for not only the MI50 and MI60 but also the MI100 which is a ~4 year old card!!! And meanwhile even Nvidia supports their server gpus for ~10 years.&lt;/p&gt;\\n\\n&lt;p&gt;I really want to like AMD but they make it so damn hard.&lt;/p&gt;\\n\\n&lt;p&gt;Edit: I should add that it would be zero effort for them to add it back in. You literally just have to change 10 lines of code and recompile the whole thing. But AMD specifically chose &lt;strong&gt;not&lt;/strong&gt; to do that.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwta86/amds_pull_request_for_llamacpp_enhancing_gpu/n2idul5/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752219019,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lwta86","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":17}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2hmvwz","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"cowmix","can_mod_post":false,"created_utc":1752205844,"send_replies":true,"parent_id":"t1_n2ham3u","score":3,"author_fullname":"t2_5dbvs","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"werd.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2hmvwz","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;werd.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lwta86","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwta86/amds_pull_request_for_llamacpp_enhancing_gpu/n2hmvwz/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752205844,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n2ham3u","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"xjE4644Eyc","can_mod_post":false,"created_utc":1752201043,"send_replies":true,"parent_id":"t3_1lwta86","score":13,"author_fullname":"t2_14i2kd","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Fingers crossed for Strix Halo support","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2ham3u","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Fingers crossed for Strix Halo support&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwta86/amds_pull_request_for_llamacpp_enhancing_gpu/n2ham3u/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752201043,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lwta86","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":13}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2lad68","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"fallingdowndizzyvr","can_mod_post":false,"created_utc":1752257022,"send_replies":true,"parent_id":"t1_n2ihb8c","score":4,"author_fullname":"t2_o65i6kx","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"They are already working on that for llama.cpp. As per the occasional lemonade post.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2lad68","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;They are already working on that for llama.cpp. As per the occasional lemonade post.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lwta86","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwta86/amds_pull_request_for_llamacpp_enhancing_gpu/n2lad68/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752257022,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}}],"before":null}},"user_reports":[],"saved":false,"id":"n2ihb8c","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"IcyUse33","can_mod_post":false,"created_utc":1752220977,"send_replies":true,"parent_id":"t3_1lwta86","score":13,"author_fullname":"t2_ua8a2ccha","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"NPU support first.\\n\\nMany base model laptops have 50 TOPs of power sitting there doing nothing.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2ihb8c","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;NPU support first.&lt;/p&gt;\\n\\n&lt;p&gt;Many base model laptops have 50 TOPs of power sitting there doing nothing.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwta86/amds_pull_request_for_llamacpp_enhancing_gpu/n2ihb8c/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752220977,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lwta86","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":13}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2jodk9","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"My_Unbiased_Opinion","can_mod_post":false,"created_utc":1752240488,"send_replies":true,"parent_id":"t1_n2h3a3s","score":2,"author_fullname":"t2_esiyl0yb","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"missed opportunity for \\"deepseks\\"","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2jodk9","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;missed opportunity for &amp;quot;deepseks&amp;quot;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lwta86","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwta86/amds_pull_request_for_llamacpp_enhancing_gpu/n2jodk9/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752240488,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n2h3a3s","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"fallingdowndizzyvr","can_mod_post":false,"created_utc":1752198454,"send_replies":true,"parent_id":"t3_1lwta86","score":29,"author_fullname":"t2_o65i6kx","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Sweet. Funny how the AMD dev's username is \\"deepsek\\".","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2h3a3s","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Sweet. Funny how the AMD dev&amp;#39;s username is &amp;quot;deepsek&amp;quot;.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwta86/amds_pull_request_for_llamacpp_enhancing_gpu/n2h3a3s/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752198454,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lwta86","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":29}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2guxdm","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"jacek2023","can_mod_post":false,"created_utc":1752195546,"send_replies":true,"parent_id":"t3_1lwta86","score":26,"author_fullname":"t2_vqgbql9w","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"A great move by AMD","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2guxdm","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;A great move by AMD&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwta86/amds_pull_request_for_llamacpp_enhancing_gpu/n2guxdm/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752195546,"author_flair_text":"llama.cpp","treatment_tags":[],"link_id":"t3_1lwta86","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":26}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2ifa61","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"waiting_for_zban","can_mod_post":false,"created_utc":1752219832,"send_replies":true,"parent_id":"t3_1lwta86","score":6,"author_fullname":"t2_13yxr6ze7l","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I hope AMD delivers though, for the sake of our Ryzen AI 395+ Turbo Max Premium Ultra","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2ifa61","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I hope AMD delivers though, for the sake of our Ryzen AI 395+ Turbo Max Premium Ultra&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwta86/amds_pull_request_for_llamacpp_enhancing_gpu/n2ifa61/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752219832,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lwta86","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":6}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2hp958","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"thebadslime","can_mod_post":false,"created_utc":1752206833,"send_replies":true,"parent_id":"t3_1lwta86","score":4,"author_fullname":"t2_i5os0v0","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I am so thankful everytime I use this that it's not python.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2hp958","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I am so thankful everytime I use this that it&amp;#39;s not python.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwta86/amds_pull_request_for_llamacpp_enhancing_gpu/n2hp958/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752206833,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lwta86","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2gz44c","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"sunshinecheung","can_mod_post":false,"created_utc":1752196996,"send_replies":true,"parent_id":"t3_1lwta86","score":1,"author_fullname":"t2_u398xzta","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"pls enhancing gaming gpu","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2gz44c","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;pls enhancing gaming gpu&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwta86/amds_pull_request_for_llamacpp_enhancing_gpu/n2gz44c/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752196996,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lwta86","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2jom3k","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"My_Unbiased_Opinion","can_mod_post":false,"created_utc":1752240565,"send_replies":true,"parent_id":"t1_n2jbc93","score":2,"author_fullname":"t2_esiyl0yb","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"P40s are gonna be inflated forever from now on. Don't rush to sell.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2jom3k","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;P40s are gonna be inflated forever from now on. Don&amp;#39;t rush to sell.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lwta86","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwta86/amds_pull_request_for_llamacpp_enhancing_gpu/n2jom3k/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752240565,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n2jbc93","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"muxxington","can_mod_post":false,"created_utc":1752236001,"send_replies":true,"parent_id":"t3_1lwta86","score":1,"author_fullname":"t2_1ktdmsvo","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"OT: Is there a right time to sell my 5xP40 and buy 5xMI50 instead? If so, when is that time?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2jbc93","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;OT: Is there a right time to sell my 5xP40 and buy 5xMI50 instead? If so, when is that time?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwta86/amds_pull_request_for_llamacpp_enhancing_gpu/n2jbc93/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752236001,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lwta86","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2lfk1g","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"vulcan4d","can_mod_post":false,"created_utc":1752258524,"send_replies":true,"parent_id":"t3_1lwta86","score":1,"author_fullname":"t2_a5y20","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Probably will need the latest rocm so they don't have to support old GPUs, don't get too excited.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2lfk1g","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Probably will need the latest rocm so they don&amp;#39;t have to support old GPUs, don&amp;#39;t get too excited.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwta86/amds_pull_request_for_llamacpp_enhancing_gpu/n2lfk1g/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752258524,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lwta86","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"total_awards_received":0,"approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"ups":-6,"removal_reason":null,"link_id":"t3_1lwta86","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"total_awards_received":0,"approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"ups":-5,"removal_reason":null,"link_id":"t3_1lwta86","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"total_awards_received":0,"approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"ups":-5,"removal_reason":null,"link_id":"t3_1lwta86","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2j9dfk","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"SeymourBits","can_mod_post":false,"created_utc":1752235244,"send_replies":true,"parent_id":"t1_n2j6b7a","score":2,"author_fullname":"t2_hb7wj","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"You “really doubt” what? That AMD is not really dedicated to AI? That AMD is playing marketing games? That this late to the party llama support is just an earnings talking point?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2j9dfk","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You “really doubt” what? That AMD is not really dedicated to AI? That AMD is playing marketing games? That this late to the party llama support is just an earnings talking point?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwta86/amds_pull_request_for_llamacpp_enhancing_gpu/n2j9dfk/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752235244,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lwta86","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":9,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n2j6b7a","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Aphid_red","can_mod_post":false,"created_utc":1752234014,"send_replies":true,"parent_id":"t1_n2j2n29","score":1,"author_fullname":"t2_csn2q","approved_by":null,"mod_note":null,"all_awardings":[],"body":"I really doubt that. While perhaps HBM is too scarce to feature it on products other than the MI300 right now, GDDR is a commodity. \\n\\nAt least a simple band-aid fix that will move chips is to make sure clam-shelled versions of GPUs with wider busses are available at consumer prices. It's useless to sell them at over double the price of the single-sided variant. \\n\\nDon't just release a 16GB variant of a 128-bit card, also a 32GB for a 256-bit and a 48GB for a 384-bit. Keeping up and using 3GB GDDR7 chips would be nice (that would've meant a 72GB version of the 9070 potentially), though I suspect they'll come next generation. \\n\\nThese memory chips are *not* expensive. We're talking $2 to $10 per chip. You can make big margins on them and still undercut the competition by an order of magnitude for the same memory size GPU. AI is not some magic complicated voodoo, it's just matrix multiply. Don't need any of the pro features... local AI users just need moar memory to not keep using second hand GPUs from over five years ago!\\n\\nIf AMD wanted to get really creative, they could also stack a GPU with a whole bunch of DDR5 slots so it can access a bunch of RAM without having to go through the PCIe bus. In fact, their strix halo kind-of does that, but since it has still only 2 channels and low tdp, it ends up pretty anemic and not viable for larger models.  \\n  \\n I was thinking more of what their epyc servers and threadrippers do to get MBW: 12 lanes of DDR5, that would get you 768GB on a single card for around $4000 total, with \\\\~500GB/s speeds. In fact, just make a socket SP5 APU! \\n\\nJust feels like so many missed opportunities.","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n2j6b7a","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I really doubt that. While perhaps HBM is too scarce to feature it on products other than the MI300 right now, GDDR is a commodity. &lt;/p&gt;\\n\\n&lt;p&gt;At least a simple band-aid fix that will move chips is to make sure clam-shelled versions of GPUs with wider busses are available at consumer prices. It&amp;#39;s useless to sell them at over double the price of the single-sided variant. &lt;/p&gt;\\n\\n&lt;p&gt;Don&amp;#39;t just release a 16GB variant of a 128-bit card, also a 32GB for a 256-bit and a 48GB for a 384-bit. Keeping up and using 3GB GDDR7 chips would be nice (that would&amp;#39;ve meant a 72GB version of the 9070 potentially), though I suspect they&amp;#39;ll come next generation. &lt;/p&gt;\\n\\n&lt;p&gt;These memory chips are &lt;em&gt;not&lt;/em&gt; expensive. We&amp;#39;re talking $2 to $10 per chip. You can make big margins on them and still undercut the competition by an order of magnitude for the same memory size GPU. AI is not some magic complicated voodoo, it&amp;#39;s just matrix multiply. Don&amp;#39;t need any of the pro features... local AI users just need moar memory to not keep using second hand GPUs from over five years ago!&lt;/p&gt;\\n\\n&lt;p&gt;If AMD wanted to get really creative, they could also stack a GPU with a whole bunch of DDR5 slots so it can access a bunch of RAM without having to go through the PCIe bus. In fact, their strix halo kind-of does that, but since it has still only 2 channels and low tdp, it ends up pretty anemic and not viable for larger models.  &lt;/p&gt;\\n\\n&lt;p&gt;I was thinking more of what their epyc servers and threadrippers do to get MBW: 12 lanes of DDR5, that would get you 768GB on a single card for around $4000 total, with ~500GB/s speeds. In fact, just make a socket SP5 APU! &lt;/p&gt;\\n\\n&lt;p&gt;Just feels like so many missed opportunities.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwta86/amds_pull_request_for_llamacpp_enhancing_gpu/n2j6b7a/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752234014,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lwta86","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":8,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2j2n29","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"SeymourBits","can_mod_post":false,"created_utc":1752232430,"send_replies":true,"parent_id":"t1_n2ipeji","score":3,"author_fullname":"t2_hb7wj","approved_by":null,"mod_note":null,"all_awardings":[],"body":"AMD is spread too thin with CPU and chipset design to ever get close to leading in the GPU or AI space. Nvidia is highly focused on AI performance and dedicated to maintaining and increasing their substantial lead. As a result, AMD plays various shady games with marketing and product line naming which is designed to confuse purchasers, but that’s all they have. This “token llama support” is likely just related to the upcoming earnings report where it can be played up as a talking point on the call.","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n2j2n29","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;AMD is spread too thin with CPU and chipset design to ever get close to leading in the GPU or AI space. Nvidia is highly focused on AI performance and dedicated to maintaining and increasing their substantial lead. As a result, AMD plays various shady games with marketing and product line naming which is designed to confuse purchasers, but that’s all they have. This “token llama support” is likely just related to the upcoming earnings report where it can be played up as a talking point on the call.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lwta86","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwta86/amds_pull_request_for_llamacpp_enhancing_gpu/n2j2n29/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752232430,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":7,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n2ipeji","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Aphid_red","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2iehxi","score":2,"author_fullname":"t2_csn2q","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Even now AMD's efforts are still being rather useless for local generation. \\n\\nThe commits they're writing for llama.cpp *only work for the MI300X*. \\n\\nWhen you can't even buy a single MI300X, which is when you would use llama.cpp: it's not any good with more than one card because no tensor parallel. \\n\\nYou can only buy 8 for the price of a house (and the power consumption of multiple houses let alone the jet engine noise), In what world is that useful for local AI? When you're either a multimillionaire or a large business, maybe. But not for everyone else. Where's the *workstation* gpu with HBM? They're not making any.  \\n\\nImagine them showing up with a competitor to the RTX 6000 for a similar price but with a stack of HBM3 instead? AMD could *smoke* nvidia for local AI, but chooses actively not to compete at all.","edited":false,"gildings":{},"author_flair_css_class":null,"name":"t1_n2ipeji","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Even now AMD&amp;#39;s efforts are still being rather useless for local generation. &lt;/p&gt;\\n\\n&lt;p&gt;The commits they&amp;#39;re writing for llama.cpp &lt;em&gt;only work for the MI300X&lt;/em&gt;. &lt;/p&gt;\\n\\n&lt;p&gt;When you can&amp;#39;t even buy a single MI300X, which is when you would use llama.cpp: it&amp;#39;s not any good with more than one card because no tensor parallel. &lt;/p&gt;\\n\\n&lt;p&gt;You can only buy 8 for the price of a house (and the power consumption of multiple houses let alone the jet engine noise), In what world is that useful for local AI? When you&amp;#39;re either a multimillionaire or a large business, maybe. But not for everyone else. Where&amp;#39;s the &lt;em&gt;workstation&lt;/em&gt; gpu with HBM? They&amp;#39;re not making any.  &lt;/p&gt;\\n\\n&lt;p&gt;Imagine them showing up with a competitor to the RTX 6000 for a similar price but with a stack of HBM3 instead? AMD could &lt;em&gt;smoke&lt;/em&gt; nvidia for local AI, but chooses actively not to compete at all.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lwta86","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwta86/amds_pull_request_for_llamacpp_enhancing_gpu/n2ipeji/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752225666,"author_flair_text":null,"treatment_tags":[],"created_utc":1752225666,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":6,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n2iehxi","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"randomfoo2","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2iajpw","score":5,"author_fullname":"t2_eztox","approved_by":null,"mod_note":null,"all_awardings":[],"body":"I've been using ROCm since launch (last decade), but you really only need to rewind 1 year to when inference support even on CDNA was quite a different picture. The SGLang launch was a disaster (I was there at the Advancing AI announcement - it didn't build on my MI300X when they were on stage announcing it). It took over two months last fall for AMD to fix a 100% crasher to do basic multi-GPU training across a wide variety of frameworks (including bare metal) and the triaging teams did not have GPU resources to even replicate the error. This type of dysfunction/silliness has been publicly/widely documented by Semianalysis, but also widely shared amongst both independent/open source developers and amongst devs at hyperscalers using these systems.\\n\\nI'm glad you're having a good experience **now** using AMD hardware but no one should pretend/imagine that AMD's current market position isn't driven by the fact that their software was just plain dogshit for quite a long while. Even now both their MFUs and MBW efficiency severely lags behind their hardware specs would suggest (and of course what Nvidia provides) and one only needs to look up a level at RCCL or hipBLASLt or two to Megatron, bnb, FA to see how far AMD still has to go (or simply look at the lack of IR, the lack of Windows supported, state of support for released hardware (gfx1150, gfx1151, gfx120x).\\n\\nIt seems like you're all over this thread but either have rose tinted glasses or are simply unaware, but you seem to be echoing the mindset/arguments that led AMD spending the beginning of the AI boom (2022-2024) being largely irrelevant in the  AI accelerator space, so I'm glad that this isn't something that's shared by AMD AI leadership anymore.\\n\\nAs for llama.cpp - AMD marketing frequently cites LM Studio and Ollama in their AI writeups but has very pointedly  provided zero support to Johannes or the llama.cpp team. Hence why the HIP backend is worse than Metal, CUDA, Vulkan, SYCL, IPEX-LLM (and probably others, lol). Anyone who doesn't see a direct relationship between edge/client/academic/developer platform support to directly drive server/production sales deserves what they get.","edited":false,"gildings":{},"downs":0,"author_flair_css_class":null,"name":"t1_n2iehxi","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;ve been using ROCm since launch (last decade), but you really only need to rewind 1 year to when inference support even on CDNA was quite a different picture. The SGLang launch was a disaster (I was there at the Advancing AI announcement - it didn&amp;#39;t build on my MI300X when they were on stage announcing it). It took over two months last fall for AMD to fix a 100% crasher to do basic multi-GPU training across a wide variety of frameworks (including bare metal) and the triaging teams did not have GPU resources to even replicate the error. This type of dysfunction/silliness has been publicly/widely documented by Semianalysis, but also widely shared amongst both independent/open source developers and amongst devs at hyperscalers using these systems.&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;m glad you&amp;#39;re having a good experience &lt;strong&gt;now&lt;/strong&gt; using AMD hardware but no one should pretend/imagine that AMD&amp;#39;s current market position isn&amp;#39;t driven by the fact that their software was just plain dogshit for quite a long while. Even now both their MFUs and MBW efficiency severely lags behind their hardware specs would suggest (and of course what Nvidia provides) and one only needs to look up a level at RCCL or hipBLASLt or two to Megatron, bnb, FA to see how far AMD still has to go (or simply look at the lack of IR, the lack of Windows supported, state of support for released hardware (gfx1150, gfx1151, gfx120x).&lt;/p&gt;\\n\\n&lt;p&gt;It seems like you&amp;#39;re all over this thread but either have rose tinted glasses or are simply unaware, but you seem to be echoing the mindset/arguments that led AMD spending the beginning of the AI boom (2022-2024) being largely irrelevant in the  AI accelerator space, so I&amp;#39;m glad that this isn&amp;#39;t something that&amp;#39;s shared by AMD AI leadership anymore.&lt;/p&gt;\\n\\n&lt;p&gt;As for llama.cpp - AMD marketing frequently cites LM Studio and Ollama in their AI writeups but has very pointedly  provided zero support to Johannes or the llama.cpp team. Hence why the HIP backend is worse than Metal, CUDA, Vulkan, SYCL, IPEX-LLM (and probably others, lol). Anyone who doesn&amp;#39;t see a direct relationship between edge/client/academic/developer platform support to directly drive server/production sales deserves what they get.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"link_id":"t3_1lwta86","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwta86/amds_pull_request_for_llamacpp_enhancing_gpu/n2iehxi/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752219384,"author_flair_text":null,"treatment_tags":[],"created_utc":1752219384,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":5,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}}],"before":null}},"user_reports":[],"saved":false,"id":"n2iajpw","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":"DELETED","no_follow":true,"author":"[deleted]","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2i9pk3","score":-5,"approved_by":null,"report_reasons":null,"all_awardings":[],"subreddit_id":"t5_81eyvm","body":"[deleted]","edited":false,"author_flair_css_class":null,"collapsed":true,"downs":0,"is_submitter":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;[deleted]&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"associated_award":null,"stickied":false,"subreddit_type":"public","can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":"dark","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwta86/amds_pull_request_for_llamacpp_enhancing_gpu/n2iajpw/","num_reports":null,"locked":false,"name":"t1_n2iajpw","created":1752217179,"subreddit":"LocalLLaMA","author_flair_text":null,"treatment_tags":[],"created_utc":1752217179,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":"","collapsed_because_crowd_control":null,"mod_reports":[],"mod_note":null,"distinguished":null}}],"before":null}},"user_reports":[],"saved":false,"id":"n2i9pk3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"randomfoo2","can_mod_post":false,"send_replies":true,"parent_id":"t1_n2i8h7f","score":10,"author_fullname":"t2_eztox","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Having non-functional and non-competitive hardware due to poor software doesn’t make revenue either, as AMD’s GPU divisions have finally realized first hand over the past several years.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2i9pk3","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Having non-functional and non-competitive hardware due to poor software doesn’t make revenue either, as AMD’s GPU divisions have finally realized first hand over the past several years.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lwta86","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwta86/amds_pull_request_for_llamacpp_enhancing_gpu/n2i9pk3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752216731,"author_flair_text":null,"treatment_tags":[],"created_utc":1752216731,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":10}}],"before":null}},"user_reports":[],"saved":false,"id":"n2i8h7f","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":"DELETED","no_follow":true,"author":"[deleted]","can_mod_post":false,"created_utc":1752216074,"send_replies":true,"parent_id":"t1_n2i7gxc","score":-5,"approved_by":null,"report_reasons":null,"all_awardings":[],"subreddit_id":"t5_81eyvm","body":"[deleted]","edited":false,"author_flair_css_class":null,"downs":0,"is_submitter":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;[deleted]&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"associated_award":null,"stickied":false,"subreddit_type":"public","can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":"dark","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwta86/amds_pull_request_for_llamacpp_enhancing_gpu/n2i8h7f/","num_reports":null,"locked":false,"name":"t1_n2i8h7f","created":1752216074,"subreddit":"LocalLLaMA","author_flair_text":null,"treatment_tags":[],"collapsed":true,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":"","collapsed_because_crowd_control":null,"mod_reports":[],"mod_note":null,"distinguished":null}}],"before":null}},"user_reports":[],"saved":false,"id":"n2i7gxc","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"randomfoo2","can_mod_post":false,"created_utc":1752215537,"send_replies":true,"parent_id":"t1_n2hu38y","score":9,"author_fullname":"t2_eztox","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Llama.cpp is used by LM Studio and Ollama - it is the #1 way most desktop/edge users will use local LLMs. It’s also tends to have far faster bs=1 tg, supports a plethora of quant sizes and allows CPU offloading of layers. I run vLLM and SGLang in prod but these are all things they are far weaker in than llama.cpp and simply not on their radar (nor should it be).\\n\\nAll other major edge inference hardware platforms recognize that llama.cpp is key to adoption. That it took AMD this long says more about AMD than anything else.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2i7gxc","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Llama.cpp is used by LM Studio and Ollama - it is the #1 way most desktop/edge users will use local LLMs. It’s also tends to have far faster bs=1 tg, supports a plethora of quant sizes and allows CPU offloading of layers. I run vLLM and SGLang in prod but these are all things they are far weaker in than llama.cpp and simply not on their radar (nor should it be).&lt;/p&gt;\\n\\n&lt;p&gt;All other major edge inference hardware platforms recognize that llama.cpp is key to adoption. That it took AMD this long says more about AMD than anything else.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lwta86","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwta86/amds_pull_request_for_llamacpp_enhancing_gpu/n2i7gxc/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752215537,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":9}}],"before":null}},"user_reports":[],"saved":false,"id":"n2hu38y","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":"DELETED","no_follow":true,"author":"[deleted]","can_mod_post":false,"send_replies":true,"parent_id":"t3_1lwta86","score":-6,"approved_by":null,"report_reasons":null,"all_awardings":[],"subreddit_id":"t5_81eyvm","body":"[deleted]","edited":false,"downs":0,"author_flair_css_class":null,"collapsed":true,"is_submitter":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;[deleted]&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"associated_award":null,"stickied":false,"subreddit_type":"public","can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":"dark","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lwta86/amds_pull_request_for_llamacpp_enhancing_gpu/n2hu38y/","num_reports":null,"locked":false,"name":"t1_n2hu38y","created":1752208940,"subreddit":"LocalLLaMA","author_flair_text":null,"treatment_tags":[],"created_utc":1752208940,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"","collapsed_because_crowd_control":null,"mod_reports":[],"mod_note":null,"distinguished":null}}],"before":null}}]`),o=()=>e.jsx(l,{data:a});export{o as default};
