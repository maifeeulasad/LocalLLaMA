import{j as l}from"./index-BOnf-UhU.js";import{R as e}from"./RedditPostRenderer-Ce39qICS.js";import"./index-CDase6VD.js";const a=JSON.parse('[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"I’m running llama.cpp on Ubuntu 22.04 with ROCm 6.2. I cloned the repo and built it like this:\\n\\nHIPCXX=\\"$(hipconfig -l)/clang\\" HIP_PATH=\\"$(hipconfig -R)\\" \\\\\\n    cmake -S . -B build -DGGML_HIP=ON -DAMDGPU_TARGETS=gfx1030 -DCMAKE_BUILD_TYPE=Release \\\\\\n    &amp;&amp; cmake --build build --config Release -- -j 16\\n\\nThen I run the model:\\n\\n./build/bin/llama-cli -hf ggml-org/gemma-3-1b-it-GGUF\\n\\nBut I’m only getting around 10 tokens/sec. When I check system usage:\\n- GPU utilization is stuck at 1%\\n- VRAM usage is 0\\n- CPU is at 100%\\n\\nLooks like it’s not using the GPU at all.\\nrocm-smi can list all 4 GPUs\\nllama.cpp also able to list 4 GPU devices\\nMachine is not plugged in into any monitor, just ssh remotely\\n\\nAnyone have experience running llama.cpp with ROCm or on multiple AMD GPUs? Any specific flags or build settings I might be missing?\\n\\n","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"llama.cpp on ROCm only running at 10 tokens/sec, GPU at 1% util. What am I missing?","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1m6khbt","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.5,"author_flair_background_color":null,"subreddit_type":"public","ups":0,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_ip4e2mp8","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":0,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1753205572,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;I’m running llama.cpp on Ubuntu 22.04 with ROCm 6.2. I cloned the repo and built it like this:&lt;/p&gt;\\n\\n&lt;p&gt;HIPCXX=&amp;quot;$(hipconfig -l)/clang&amp;quot; HIP_PATH=&amp;quot;$(hipconfig -R)&amp;quot; \\\\\\n    cmake -S . -B build -DGGML_HIP=ON -DAMDGPU_TARGETS=gfx1030 -DCMAKE_BUILD_TYPE=Release \\\\\\n    &amp;amp;&amp;amp; cmake --build build --config Release -- -j 16&lt;/p&gt;\\n\\n&lt;p&gt;Then I run the model:&lt;/p&gt;\\n\\n&lt;p&gt;./build/bin/llama-cli -hf ggml-org/gemma-3-1b-it-GGUF&lt;/p&gt;\\n\\n&lt;p&gt;But I’m only getting around 10 tokens/sec. When I check system usage:\\n- GPU utilization is stuck at 1%\\n- VRAM usage is 0\\n- CPU is at 100%&lt;/p&gt;\\n\\n&lt;p&gt;Looks like it’s not using the GPU at all.\\nrocm-smi can list all 4 GPUs\\nllama.cpp also able to list 4 GPU devices\\nMachine is not plugged in into any monitor, just ssh remotely&lt;/p&gt;\\n\\n&lt;p&gt;Anyone have experience running llama.cpp with ROCm or on multiple AMD GPUs? Any specific flags or build settings I might be missing?&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":true,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1m6khbt","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"Reasonable_Can_5793","discussion_type":null,"num_comments":11,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1m6khbt/llamacpp_on_rocm_only_running_at_10_tokenssec_gpu/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1m6khbt/llamacpp_on_rocm_only_running_at_10_tokenssec_gpu/","subreddit_subscribers":503254,"created_utc":1753205572,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4kuedb","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"dinerburgeryum","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4krzmp","score":2,"author_fullname":"t2_1j53p3yv3e","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"No problem. It\'s a powerful tool but it takes some getting used to.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n4kuedb","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;No problem. It&amp;#39;s a powerful tool but it takes some getting used to.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m6khbt","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6khbt/llamacpp_on_rocm_only_running_at_10_tokenssec_gpu/n4kuedb/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753211558,"author_flair_text":null,"treatment_tags":[],"created_utc":1753211558,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4l2k81","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Willing_Landscape_61","can_mod_post":false,"send_replies":true,"parent_id":"t1_n4krzmp","score":1,"author_fullname":"t2_8lvrytgw","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Can you share pp and tg speed for various models plz?","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n4l2k81","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Can you share pp and tg speed for various models plz?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m6khbt","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6khbt/llamacpp_on_rocm_only_running_at_10_tokenssec_gpu/n4l2k81/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753213907,"author_flair_text":null,"treatment_tags":[],"created_utc":1753213907,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n4krzmp","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Reasonable_Can_5793","can_mod_post":false,"created_utc":1753210861,"send_replies":true,"parent_id":"t1_n4kb2yr","score":4,"author_fullname":"t2_ip4e2mp8","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"TYSM! This helped out, finally selling some usage and incredible inference speed now!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4krzmp","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;TYSM! This helped out, finally selling some usage and incredible inference speed now!&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m6khbt","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6khbt/llamacpp_on_rocm_only_running_at_10_tokenssec_gpu/n4krzmp/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753210861,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":4}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4khcgw","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Ulterior-Motive_","can_mod_post":false,"created_utc":1753207878,"send_replies":true,"parent_id":"t1_n4kb2yr","score":2,"author_fullname":"t2_127atw4awd","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"This, OPs only specifying the model, which by default, will only use the CPU.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4khcgw","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;This, OPs only specifying the model, which by default, will only use the CPU.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m6khbt","unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6khbt/llamacpp_on_rocm_only_running_at_10_tokenssec_gpu/n4khcgw/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753207878,"author_flair_text":"llama.cpp","treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n4kb2yr","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"dinerburgeryum","can_mod_post":false,"created_utc":1753206164,"send_replies":true,"parent_id":"t3_1m6khbt","score":12,"author_fullname":"t2_1j53p3yv3e","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"You\'ve gotta specify GPU offload with `-ngl 99` too.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4kb2yr","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You&amp;#39;ve gotta specify GPU offload with &lt;code&gt;-ngl 99&lt;/code&gt; too.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6khbt/llamacpp_on_rocm_only_running_at_10_tokenssec_gpu/n4kb2yr/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753206164,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m6khbt","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":12}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4kyi5z","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"steezy13312","can_mod_post":false,"created_utc":1753212747,"send_replies":true,"parent_id":"t1_n4ka6wl","score":2,"author_fullname":"t2_rfjj2","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Note that `-DGGML_HIP_ROCWMMA_FATTN=ON` only applies if you\'re on RDNA3 hardware (or older CDNA GPUs): https://github.com/ROCm/rocWMMA","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4kyi5z","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Note that &lt;code&gt;-DGGML_HIP_ROCWMMA_FATTN=ON&lt;/code&gt; only applies if you&amp;#39;re on RDNA3 hardware (or older CDNA GPUs): &lt;a href=\\"https://github.com/ROCm/rocWMMA\\"&gt;https://github.com/ROCm/rocWMMA&lt;/a&gt;&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m6khbt","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6khbt/llamacpp_on_rocm_only_running_at_10_tokenssec_gpu/n4kyi5z/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753212747,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n4ka6wl","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"mhogag","can_mod_post":false,"created_utc":1753205925,"send_replies":true,"parent_id":"t3_1m6khbt","score":2,"author_fullname":"t2_5u5dmme","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"For me, it needed quite specific CMake flags and I found it helpful and reliable to also specify the CC, CXX, etc. environment variables.\\n\\nHere\'s what I used a while back when I built it:\\n\\n\\\\`CC=/opt/rocm/llvm/bin/clang CXX=/opt/rocm/bin/hipcc HIPCXX=\\"$(hipconfig -l)/clang\\" HIP\\\\_PATH=\\"$(hipconfig -R)\\" cmake -S . -B build -DGGML\\\\_HIP=ON -DGPU\\\\_TARGETS=gfx1100 -DCMAKE\\\\_BUILD\\\\_TYPE=Release\\\\`\\n\\n(You can also add \\\\`-DGGML\\\\_HIP\\\\_ROCWMMA\\\\_FATTN=ON\\\\` if you want flash-attention.)\\n\\nThen, build it normally using \\\\`cmake --build build --config Release -- -j 8\\\\`","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4ka6wl","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;For me, it needed quite specific CMake flags and I found it helpful and reliable to also specify the CC, CXX, etc. environment variables.&lt;/p&gt;\\n\\n&lt;p&gt;Here&amp;#39;s what I used a while back when I built it:&lt;/p&gt;\\n\\n&lt;p&gt;`CC=/opt/rocm/llvm/bin/clang CXX=/opt/rocm/bin/hipcc HIPCXX=&amp;quot;$(hipconfig -l)/clang&amp;quot; HIP_PATH=&amp;quot;$(hipconfig -R)&amp;quot; cmake -S . -B build -DGGML_HIP=ON -DGPU_TARGETS=gfx1100 -DCMAKE_BUILD_TYPE=Release`&lt;/p&gt;\\n\\n&lt;p&gt;(You can also add `-DGGML_HIP_ROCWMMA_FATTN=ON` if you want flash-attention.)&lt;/p&gt;\\n\\n&lt;p&gt;Then, build it normally using `cmake --build build --config Release -- -j 8`&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6khbt/llamacpp_on_rocm_only_running_at_10_tokenssec_gpu/n4ka6wl/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753205925,"author_flair_text":"llama.cpp","treatment_tags":[],"link_id":"t3_1m6khbt","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"total_awards_received":0,"approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"ups":1,"removal_reason":null,"link_id":"t3_1m6khbt","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4knw9f","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"nsfnd","can_mod_post":false,"created_utc":1753209709,"send_replies":true,"parent_id":"t1_n4kf4d1","score":2,"author_fullname":"t2_rldv9","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I agree 100%. Fug\'em for selling hardware and giving half ass baked software.\\n\\nThat being said, I think GGML\\\\_HIP\\\\_ROCWMMA\\\\_FATTN flag makes a lot of difference. So rocm runs faster than vulkan now, on my system at least.\\n\\nWith this model unsloth/Devstral-Small-2507-IQ4\\\\_XS.gguf;\\n\\n    ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    yes\\n    ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: yes\\n    ggml_cuda_init: found 1 ROCm devices:\\n      Device 0: AMD Radeon RX 7900 XTX, gfx1100 (0x1100), VMM: yes, Wave Size: 32\\n    | model                          |       size |     params | backend    | ngl | fa |            test |                  t/s |\\n    | ------------------------------ | ---------: | ---------: | ---------- | --: | -: | --------------: | -------------------: |\\n    | llama 13B IQ4_XS - 4.25 bpw    |  11.89 GiB |    23.57 B | ROCm       |  99 |  1 |           pp512 |       1191.33 ± 5.07 |\\n    | llama 13B IQ4_XS - 4.25 bpw    |  11.89 GiB |    23.57 B | ROCm       |  99 |  1 |           tg128 |         51.29 ± 0.02 |\\n\\n    ggml_vulkan: Found 1 Vulkan devices:\\n    ggml_vulkan: 0 = AMD Radeon RX 7900 XTX (RADV NAVI31) (radv) | uma: 0 | fp16: 1 | bf16: 0 | warp size: 64 | shared memory: 65536 | int dot: 1 | matrix cores: KHR_coopmat\\n    | model                          |       size |     params | backend    | ngl | fa |            test |                  t/s |\\n    | ------------------------------ | ---------: | ---------: | ---------- | --: | -: | --------------: | -------------------: |\\n    | llama 13B IQ4_XS - 4.25 bpw    |  11.89 GiB |    23.57 B | Vulkan     |  99 |  1 |           pp512 |        525.46 ± 2.37 |\\n    | llama 13B IQ4_XS - 4.25 bpw    |  11.89 GiB |    23.57 B | Vulkan     |  99 |  1 |           tg128 |         37.27 ± 0.11 |","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4knw9f","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I agree 100%. Fug&amp;#39;em for selling hardware and giving half ass baked software.&lt;/p&gt;\\n\\n&lt;p&gt;That being said, I think GGML_HIP_ROCWMMA_FATTN flag makes a lot of difference. So rocm runs faster than vulkan now, on my system at least.&lt;/p&gt;\\n\\n&lt;p&gt;With this model unsloth/Devstral-Small-2507-IQ4_XS.gguf;&lt;/p&gt;\\n\\n&lt;pre&gt;&lt;code&gt;ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    yes\\nggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: yes\\nggml_cuda_init: found 1 ROCm devices:\\n  Device 0: AMD Radeon RX 7900 XTX, gfx1100 (0x1100), VMM: yes, Wave Size: 32\\n| model                          |       size |     params | backend    | ngl | fa |            test |                  t/s |\\n| ------------------------------ | ---------: | ---------: | ---------- | --: | -: | --------------: | -------------------: |\\n| llama 13B IQ4_XS - 4.25 bpw    |  11.89 GiB |    23.57 B | ROCm       |  99 |  1 |           pp512 |       1191.33 ± 5.07 |\\n| llama 13B IQ4_XS - 4.25 bpw    |  11.89 GiB |    23.57 B | ROCm       |  99 |  1 |           tg128 |         51.29 ± 0.02 |\\n\\nggml_vulkan: Found 1 Vulkan devices:\\nggml_vulkan: 0 = AMD Radeon RX 7900 XTX (RADV NAVI31) (radv) | uma: 0 | fp16: 1 | bf16: 0 | warp size: 64 | shared memory: 65536 | int dot: 1 | matrix cores: KHR_coopmat\\n| model                          |       size |     params | backend    | ngl | fa |            test |                  t/s |\\n| ------------------------------ | ---------: | ---------: | ---------- | --: | -: | --------------: | -------------------: |\\n| llama 13B IQ4_XS - 4.25 bpw    |  11.89 GiB |    23.57 B | Vulkan     |  99 |  1 |           pp512 |        525.46 ± 2.37 |\\n| llama 13B IQ4_XS - 4.25 bpw    |  11.89 GiB |    23.57 B | Vulkan     |  99 |  1 |           tg128 |         37.27 ± 0.11 |\\n&lt;/code&gt;&lt;/pre&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m6khbt","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6khbt/llamacpp_on_rocm_only_running_at_10_tokenssec_gpu/n4knw9f/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753209709,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n4kf4d1","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":"DELETED","no_follow":true,"author":"[deleted]","can_mod_post":false,"send_replies":true,"parent_id":"t3_1m6khbt","score":1,"approved_by":null,"report_reasons":null,"all_awardings":[],"subreddit_id":"t5_81eyvm","body":"[deleted]","edited":1753208025,"downs":0,"author_flair_css_class":null,"collapsed":true,"is_submitter":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;[deleted]&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"associated_award":null,"stickied":false,"subreddit_type":"public","can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":"dark","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6khbt/llamacpp_on_rocm_only_running_at_10_tokenssec_gpu/n4kf4d1/","num_reports":null,"locked":false,"name":"t1_n4kf4d1","created":1753207261,"subreddit":"LocalLLaMA","author_flair_text":null,"treatment_tags":[],"created_utc":1753207261,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"","collapsed_because_crowd_control":null,"mod_reports":[],"mod_note":null,"distinguished":null}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"ed89e5c6-72f1-11ee-9954-1697022cd89d","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4kldux","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"nsfnd","can_mod_post":false,"created_utc":1753209002,"send_replies":true,"parent_id":"t1_n4ka71s","score":3,"author_fullname":"t2_rldv9","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"its shite on amd.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4kldux","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;its shite on amd.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1m6khbt","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6khbt/llamacpp_on_rocm_only_running_at_10_tokenssec_gpu/n4kldux/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753209002,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}},"user_reports":[],"saved":false,"id":"n4ka71s","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"segmond","can_mod_post":false,"created_utc":1753205926,"send_replies":true,"parent_id":"t3_1m6khbt","score":0,"author_fullname":"t2_ah13x","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"vllm","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4ka71s","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"llama.cpp"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;vllm&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6khbt/llamacpp_on_rocm_only_running_at_10_tokenssec_gpu/n4ka71s/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753205926,"author_flair_text":"llama.cpp","treatment_tags":[],"link_id":"t3_1m6khbt","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n4k9677","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"No-Assist-4041","can_mod_post":false,"created_utc":1753205652,"send_replies":true,"parent_id":"t3_1m6khbt","score":0,"author_fullname":"t2_ly870h93f","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Target release instead of debug, also why not update to a newer version of ROCm? (E.g. 6.4.1)","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n4k9677","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Target release instead of debug, also why not update to a newer version of ROCm? (E.g. 6.4.1)&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1m6khbt/llamacpp_on_rocm_only_running_at_10_tokenssec_gpu/n4k9677/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1753205652,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1m6khbt","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":0}}],"before":null}}]'),_=()=>l.jsx(e,{data:a});export{_ as default};
