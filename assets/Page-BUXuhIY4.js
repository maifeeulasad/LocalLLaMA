import{j as e}from"./index-F0NXdzZX.js";import{R as l}from"./RedditPostRenderer-CoEZB1dt.js";import"./index-DrtravXm.js";const a=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Hello r/LocalLLaMA!\\n\\nI have a potentially challenging question for you all. I'm searching for a local vision LLM that's small and efficient enough to process a video stream in near real-time. I'm realistic – I know handling 60 FPS isn't feasible right now. But is there a solution that could process, say, 5-10 frames per minute, providing a short, precise description of each frame's content and not eating all the PC resources at the same time?\\n\\nHave any of you experimented with something like this locally? Is there any hope for \\"real-time\\" visual understanding on consumer hardware?","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Local vision LLM for (not really)real time processing.","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lqtxdp","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.75,"author_flair_background_color":null,"subreddit_type":"public","ups":2,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_1zyh18yq","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":2,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1751559945,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Hello &lt;a href=\\"/r/LocalLLaMA\\"&gt;r/LocalLLaMA&lt;/a&gt;!&lt;/p&gt;\\n\\n&lt;p&gt;I have a potentially challenging question for you all. I&amp;#39;m searching for a local vision LLM that&amp;#39;s small and efficient enough to process a video stream in near real-time. I&amp;#39;m realistic – I know handling 60 FPS isn&amp;#39;t feasible right now. But is there a solution that could process, say, 5-10 frames per minute, providing a short, precise description of each frame&amp;#39;s content and not eating all the PC resources at the same time?&lt;/p&gt;\\n\\n&lt;p&gt;Have any of you experimented with something like this locally? Is there any hope for &amp;quot;real-time&amp;quot; visual understanding on consumer hardware?&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1lqtxdp","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"RIPT1D3_Z","discussion_type":null,"num_comments":9,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lqtxdp/local_vision_llm_for_not_reallyreal_time/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lqtxdp/local_vision_llm_for_not_reallyreal_time/","subreddit_subscribers":494198,"created_utc":1751559945,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n16r2rm","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"RIPT1D3_Z","can_mod_post":false,"created_utc":1751573250,"send_replies":true,"parent_id":"t1_n15krk7","score":2,"author_fullname":"t2_1zyh18yq","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"4070 TI SUPER + Ryzen 9 7900x with 64gb ram\\n\\nSeems like I need to rent a GPU to be able to handle streaming and inference simultaneously.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n16r2rm","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;4070 TI SUPER + Ryzen 9 7900x with 64gb ram&lt;/p&gt;\\n\\n&lt;p&gt;Seems like I need to rent a GPU to be able to handle streaming and inference simultaneously.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lqtxdp","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqtxdp/local_vision_llm_for_not_reallyreal_time/n16r2rm/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751573250,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n15krk7","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Kv603","can_mod_post":false,"created_utc":1751561059,"send_replies":true,"parent_id":"t3_1lqtxdp","score":3,"author_fullname":"t2_uuj6dq7","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"What kind of GPU resources can you dedicate to this?  Have you checked out https://github.com/brendanmckeag/gemma-captioner ?\\n\\n&gt; Have any of you experimented with something like this locally? \\n\\nMost \\"vision\\" models take many seconds per frame on a small consumer GPU.\\n\\nFor example, captioning a single frame using gemma3:4b on an RTX4060  takes ~5 seconds with a prompt of \\"*Provide a short, single-line description of this image*\\".\\n\\nI found that reducing the image size (in terms of pixels and also via jpg compression) didn't save significant clock time.   Cropping away extraneous static elements (walls, ceiling, etc) did help a bit, while just resizing or just JPG compressing did not.","edited":1751561817,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n15krk7","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;What kind of GPU resources can you dedicate to this?  Have you checked out &lt;a href=\\"https://github.com/brendanmckeag/gemma-captioner\\"&gt;https://github.com/brendanmckeag/gemma-captioner&lt;/a&gt; ?&lt;/p&gt;\\n\\n&lt;blockquote&gt;\\n&lt;p&gt;Have any of you experimented with something like this locally? &lt;/p&gt;\\n&lt;/blockquote&gt;\\n\\n&lt;p&gt;Most &amp;quot;vision&amp;quot; models take many seconds per frame on a small consumer GPU.&lt;/p&gt;\\n\\n&lt;p&gt;For example, captioning a single frame using gemma3:4b on an RTX4060  takes ~5 seconds with a prompt of &amp;quot;&lt;em&gt;Provide a short, single-line description of this image&lt;/em&gt;&amp;quot;.&lt;/p&gt;\\n\\n&lt;p&gt;I found that reducing the image size (in terms of pixels and also via jpg compression) didn&amp;#39;t save significant clock time.   Cropping away extraneous static elements (walls, ceiling, etc) did help a bit, while just resizing or just JPG compressing did not.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqtxdp/local_vision_llm_for_not_reallyreal_time/n15krk7/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751561059,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lqtxdp","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n16rbhw","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"RIPT1D3_Z","can_mod_post":false,"created_utc":1751573323,"send_replies":true,"parent_id":"t1_n15jagp","score":1,"author_fullname":"t2_1zyh18yq","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I'll check it, thanks!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n16rbhw","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;ll check it, thanks!&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lqtxdp","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqtxdp/local_vision_llm_for_not_reallyreal_time/n16rbhw/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751573323,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n15jagp","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"No-Refrigerator-1672","can_mod_post":false,"created_utc":1751560640,"send_replies":true,"parent_id":"t3_1lqtxdp","score":2,"author_fullname":"t2_baavelp5","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Qwen 2.5 Omni is designed to handle real time visual and audio data streams. That's if you need 5-10 frames per second. If you want 5-10 frames per minute, then your smallest models that could do this would be Llama 3.1 11B, Qwen 2.5 VL 7B, Gemma 3 4B.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n15jagp","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Qwen 2.5 Omni is designed to handle real time visual and audio data streams. That&amp;#39;s if you need 5-10 frames per second. If you want 5-10 frames per minute, then your smallest models that could do this would be Llama 3.1 11B, Qwen 2.5 VL 7B, Gemma 3 4B.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqtxdp/local_vision_llm_for_not_reallyreal_time/n15jagp/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751560640,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lqtxdp","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n15jk0s","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Pedalnomica","can_mod_post":false,"created_utc":1751560716,"send_replies":true,"parent_id":"t3_1lqtxdp","score":1,"author_fullname":"t2_b0d7j6x9","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"With batching, I'd think a 3090 and one of the smaller Gemma 3s or Qwen2.5VLs should be fine.\\n\\n\\nThere's also models that take video inputs directly.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n15jk0s","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;With batching, I&amp;#39;d think a 3090 and one of the smaller Gemma 3s or Qwen2.5VLs should be fine.&lt;/p&gt;\\n\\n&lt;p&gt;There&amp;#39;s also models that take video inputs directly.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqtxdp/local_vision_llm_for_not_reallyreal_time/n15jk0s/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751560716,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lqtxdp","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n173752","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"RIPT1D3_Z","can_mod_post":false,"created_utc":1751576848,"send_replies":true,"parent_id":"t1_n172c6f","score":1,"author_fullname":"t2_1zyh18yq","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Full textual description it is, since the goal is to try to catch the dynamics of a scene as well. Not necessarily with the same model, tho.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n173752","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Full textual description it is, since the goal is to try to catch the dynamics of a scene as well. Not necessarily with the same model, tho.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lqtxdp","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqtxdp/local_vision_llm_for_not_reallyreal_time/n173752/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751576848,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n172c6f","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"MHTMakerspace","can_mod_post":false,"created_utc":1751576583,"send_replies":true,"parent_id":"t3_1lqtxdp","score":1,"author_fullname":"t2_dv0swue7","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"When you say \\"*a short, precise description of each frame's content\\",* do you need a full textual description as a normal english paragraph?   Or could you perhaps get by with a list of objects detected, such as from a computer vision coprocessor and a model akin to YOLOv8?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n172c6f","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;When you say &amp;quot;&lt;em&gt;a short, precise description of each frame&amp;#39;s content&amp;quot;,&lt;/em&gt; do you need a full textual description as a normal english paragraph?   Or could you perhaps get by with a list of objects detected, such as from a computer vision coprocessor and a model akin to YOLOv8?&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqtxdp/local_vision_llm_for_not_reallyreal_time/n172c6f/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751576583,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lqtxdp","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n18o7vb","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"townofsalemfangay","can_mod_post":false,"created_utc":1751596889,"send_replies":true,"parent_id":"t3_1lqtxdp","score":1,"author_fullname":"t2_122dsg","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"You really need to start exploring pipelines that leverage computer vision, not just multi-modal LLMs, especially when you're aiming for high-frame, low-latency parsing. Vision-specific architectures can often outperform general-purpose LLMs in these real-time scenarios and it's not even close tbh.\\n\\nHowever, an interesting recent project I saw on X, was using Gemini API as a basketball coach. You can see it [here](https://x.com/aaditsh/status/1939435960825180286).","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n18o7vb","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You really need to start exploring pipelines that leverage computer vision, not just multi-modal LLMs, especially when you&amp;#39;re aiming for high-frame, low-latency parsing. Vision-specific architectures can often outperform general-purpose LLMs in these real-time scenarios and it&amp;#39;s not even close tbh.&lt;/p&gt;\\n\\n&lt;p&gt;However, an interesting recent project I saw on X, was using Gemini API as a basketball coach. You can see it &lt;a href=\\"https://x.com/aaditsh/status/1939435960825180286\\"&gt;here&lt;/a&gt;.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqtxdp/local_vision_llm_for_not_reallyreal_time/n18o7vb/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751596889,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lqtxdp","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n18qbg5","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"HRudy94","can_mod_post":false,"created_utc":1751597705,"send_replies":true,"parent_id":"t3_1lqtxdp","score":1,"author_fullname":"t2_12e33e","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Gemma 3 though yeah you're not getting 60fps any time soon xD","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n18qbg5","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Gemma 3 though yeah you&amp;#39;re not getting 60fps any time soon xD&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lqtxdp/local_vision_llm_for_not_reallyreal_time/n18qbg5/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751597705,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lqtxdp","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),n=()=>e.jsx(l,{data:a});export{n as default};
