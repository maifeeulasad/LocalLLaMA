import{j as e}from"./index-CqAPCjw5.js";import{R as t}from"./RedditPostRenderer-4oBDAtGr.js";import"./index-D3Sdy_Op.js";const l=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"hey, I am building a small tool for myself to load up links, files, pdfs, photos, text and later recall them by text, cuz i anxious about losing this links, and presume i am going to need them later, and i dont like managers with folders to organise those links because at some point it is whole another job.\\n\\nI am thinking about super simple solution:  \\n\\\\- use firecrawl to get the markdown content;  \\n\\\\- get vector / save into databse;  \\n\\\\- when text input comes I fill it with additional context for better vector search performance;  \\n\\\\- load N results  \\n\\\\- filter with gpt\\n\\nbut the last time I was doing it, it wasn't working really great, so i was wondering maybe there is better solution for this?\\n\\n","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"What are the best practices for vector search + filtering with LLM?","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lzz13f","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":1,"author_flair_background_color":null,"subreddit_type":"public","ups":2,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_zx18p","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":2,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1752528242,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;hey, I am building a small tool for myself to load up links, files, pdfs, photos, text and later recall them by text, cuz i anxious about losing this links, and presume i am going to need them later, and i dont like managers with folders to organise those links because at some point it is whole another job.&lt;/p&gt;\\n\\n&lt;p&gt;I am thinking about super simple solution:&lt;br/&gt;\\n- use firecrawl to get the markdown content;&lt;br/&gt;\\n- get vector / save into databse;&lt;br/&gt;\\n- when text input comes I fill it with additional context for better vector search performance;&lt;br/&gt;\\n- load N results&lt;br/&gt;\\n- filter with gpt&lt;/p&gt;\\n\\n&lt;p&gt;but the last time I was doing it, it wasn&amp;#39;t working really great, so i was wondering maybe there is better solution for this?&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1lzz13f","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"andrewshvv","discussion_type":null,"num_comments":2,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lzz13f/what_are_the_best_practices_for_vector_search/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lzz13f/what_are_the_best_practices_for_vector_search/","subreddit_subscribers":499295,"created_utc":1752528242,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"fe89e94a-13f2-11f0-a9de-6262c74956cf","likes":null,"replies":"","user_reports":[],"saved":false,"id":"n370htc","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Asleep-Ratio7535","can_mod_post":false,"created_utc":1752547536,"send_replies":true,"parent_id":"t3_1lzz13f","score":1,"author_fullname":"t2_1lfyddwf0c","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I'm not sure, but I'm looking into this right now as well. For filtering, NER or CER would be good enough, and it's much faster compared to embedding.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n370htc","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"Llama 4"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;m not sure, but I&amp;#39;m looking into this right now as well. For filtering, NER or CER would be good enough, and it&amp;#39;s much faster compared to embedding.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzz13f/what_are_the_best_practices_for_vector_search/n370htc/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752547536,"author_flair_text":"Llama 4","treatment_tags":[],"link_id":"t3_1lzz13f","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#b0ae9b","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n37t58w","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"HistorianPotential48","can_mod_post":false,"created_utc":1752560373,"send_replies":true,"parent_id":"t3_1lzz13f","score":1,"author_fullname":"t2_4dzthia7","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"check vector lengths. are these trimmed in your database table? what embedding models are you using?\\n\\n\\"fill it with additional context\\" - how, and what's the queries end up look like? It might not work best for vector searches, or for some models like Multilingual-e5-large, it needs user to format query like \\\\\`query: thick thigh femboy\\\\\`, so check your embedding model specifications too.\\n\\nthe description in your post is not clear enough to debug. one way i'd suggest is to pause and check by each step, see where did this flow exactly went wrong.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n37t58w","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;check vector lengths. are these trimmed in your database table? what embedding models are you using?&lt;/p&gt;\\n\\n&lt;p&gt;&amp;quot;fill it with additional context&amp;quot; - how, and what&amp;#39;s the queries end up look like? It might not work best for vector searches, or for some models like Multilingual-e5-large, it needs user to format query like \`query: thick thigh femboy\`, so check your embedding model specifications too.&lt;/p&gt;\\n\\n&lt;p&gt;the description in your post is not clear enough to debug. one way i&amp;#39;d suggest is to pause and check by each step, see where did this flow exactly went wrong.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lzz13f/what_are_the_best_practices_for_vector_search/n37t58w/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752560373,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lzz13f","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),s=()=>e.jsx(t,{data:l});export{s as default};
