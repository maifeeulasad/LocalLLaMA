import{j as e}from"./index-DQXiEb7D.js";import{R as t}from"./RedditPostRenderer-BjndLgq8.js";import"./index-B-ILyjT1.js";const a=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"So I'm trying MNN chat but for me it seems to be repeating itself a lot. I tried qwen3 0.6b, and when I try a simple request like\\n\\nWhat is lasagna?\\n\\nLascange is a dish that is made from pasta. It is a very popular dish in Italy. The main ingredients are pasta and sauce. The sauce is made from various ingredients. It is also a dish that is popular in many countries around the world. It is a very popular dish. It is also a dish that is made from pasta. It is also a dish that is made from various ingredients. It is also a dish that is popular in many countries around the world. It is a very popular dish. It is also a dish that is made from pasta. It is also a dish that is made from various ingredients. It is also a dish that is popular in many countries around the world. It is a very popular dish. It is also a dish that is made from pasta. It is also a dish that is made from various ingredients. It is \\n\\nIs this an inherent MNN issue or just a model issue?","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Just me, or MNN chat is looping a lot","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lprfbx","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.83,"author_flair_background_color":null,"subreddit_type":"public","ups":4,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_8hpbax1b","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":4,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"content_categories":null,"is_self":true,"mod_note":null,"created":1751448680,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;So I&amp;#39;m trying MNN chat but for me it seems to be repeating itself a lot. I tried qwen3 0.6b, and when I try a simple request like&lt;/p&gt;\\n\\n&lt;p&gt;What is lasagna?&lt;/p&gt;\\n\\n&lt;p&gt;Lascange is a dish that is made from pasta. It is a very popular dish in Italy. The main ingredients are pasta and sauce. The sauce is made from various ingredients. It is also a dish that is popular in many countries around the world. It is a very popular dish. It is also a dish that is made from pasta. It is also a dish that is made from various ingredients. It is also a dish that is popular in many countries around the world. It is a very popular dish. It is also a dish that is made from pasta. It is also a dish that is made from various ingredients. It is also a dish that is popular in many countries around the world. It is a very popular dish. It is also a dish that is made from pasta. It is also a dish that is made from various ingredients. It is &lt;/p&gt;\\n\\n&lt;p&gt;Is this an inherent MNN issue or just a model issue?&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":false,"is_crosspostable":false,"pinned":false,"over_18":false,"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1lprfbx","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"ExtremeAcceptable289","discussion_type":null,"num_comments":7,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lprfbx/just_me_or_mnn_chat_is_looping_a_lot/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lprfbx/just_me_or_mnn_chat_is_looping_a_lot/","subreddit_subscribers":494001,"created_utc":1751448680,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0xhgym","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"12padams","can_mod_post":false,"created_utc":1751458483,"send_replies":true,"parent_id":"t1_n0x246h","score":2,"author_fullname":"t2_9ii99","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Actually there really is something wrong with MMN chat. Let's use Qwen 3 1.7B as an example and the same prompt in MMN chat and Pocket Pal.\\n\\n\\nPrompt - \\"Do you feel like you are a small LLM (not many parametres) or a large LLM (lots of parametres)\\"\\n\\n\\nMMN Chat (loops while thinking):\\n\\"Okay, so the user is asking if I feel like a small LLM or a large LLM. Hmm, I need to explain my nature. Let me think. I'm actually a large LLM with lots of parameters. But wait, how do I compare? Maybe the user wants to know the difference between small and large models. Oh right, small models have fewer parameters, like those with limited training, while large models have more, like the ones with lots of training. So I'm a large LLM because I have lots of parameters. But I need to make sure I'm not confusing parameters with something else. Also, maybe the user is trying to understand my capabilities. So I should explain that I'm a large LLM with lots of parameters, and that's why I can handle complex tasks. But I should also mention that parameters are about the model's size, not about the model itself. So I need to clarify that parameters are a part of the model's architecture, but the model's size is about the number of parameters. So I'm a large LLM with lots of parameters, but I'm not about the parameters. Maybe the user is trying to distinguish between small and large models. So I should explain that parameters are a part of the model's size, but the model itself is about the training data and architecture. So I'm a large LLM with lots of parameters, but I'm not about the parameters. I need to make sure my explanation is clear. So I'll say that I'm a large LLM with lots of parameters, but I'm not about the parameters. Maybe the user is trying to understand my capabilities. So I'll explain that I'm a large LLM with lots of parameters, and that's why I can handle complex tasks. But I need to make sure I'm not confusing parameters with something else. So I'll explain that parameters are a part of the model's size, but the model's size is about the number of parameters. So I'm a large LLM with lots of parameters, but I'm not about the parameters. I need to make sure my explanation is clear. So I'll say that I'm a large LLM with lots of parameters, and that's why I can handle complex tasks. But I need to make sure I'm not confusing parameters with something else. So I'll explain that parameters are a part of the model's size, but the model's size is about the number of parameters. So I'm a large LLM with lots of parameters, but I'm not about the parameters. I need to make sure my explanation is clear. So I'll explain that I'm a... cut off\\"\\n\\n\\nPocket Pal:Â \\n\\"&lt;think&gt;\\nOkay, the user is asking whether I feel small or large based on the number of parameters. First, I need to clarify that the number of parameters doesn't directly determine the model's size. Models are typically designed with a fixed number of parameters, so the user's question might be conflating two different concepts: model size (parameters) versus model size (in terms of training data or architecture).\\n\\n\\nI should explain that the number of parameters is a technical aspect, but the model's \\"size\\" (like training data size or architecture) is another factor. Also, I can mention that the user might be confusing the two and provide examples to clarify. It's important to keep the response informative and reassuring, ensuring the user understands that the model's characteristics aren't based on parameter count but on training data and structure.\\n&lt;/think&gt;\\n\\n\\nThe number of parameters in a model is a technical specification that affects how large the model is. However, when it comes to how **you feel** as an LLM (like me), itâ€™s more about the **training data, architecture, and capabilities** than the number of parameters. For example, if you're given a larger dataset, you might feel more powerful with more information, and a more complex architecture could give you more capabilities. I donâ€™t \\"feel\\" the size in terms of parameters; I just process and understand information as an LLM. Let me know if you have more questions!\\n\\n\\n-----------\\n\\n\\nThis just goes to show, MMN chat has a problem with small models. All recommended settings are used for Qwen 3 1.7B in MMN chat, but the issues don't end ðŸ˜‘","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0xhgym","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Actually there really is something wrong with MMN chat. Let&amp;#39;s use Qwen 3 1.7B as an example and the same prompt in MMN chat and Pocket Pal.&lt;/p&gt;\\n\\n&lt;p&gt;Prompt - &amp;quot;Do you feel like you are a small LLM (not many parametres) or a large LLM (lots of parametres)&amp;quot;&lt;/p&gt;\\n\\n&lt;p&gt;MMN Chat (loops while thinking):\\n&amp;quot;Okay, so the user is asking if I feel like a small LLM or a large LLM. Hmm, I need to explain my nature. Let me think. I&amp;#39;m actually a large LLM with lots of parameters. But wait, how do I compare? Maybe the user wants to know the difference between small and large models. Oh right, small models have fewer parameters, like those with limited training, while large models have more, like the ones with lots of training. So I&amp;#39;m a large LLM because I have lots of parameters. But I need to make sure I&amp;#39;m not confusing parameters with something else. Also, maybe the user is trying to understand my capabilities. So I should explain that I&amp;#39;m a large LLM with lots of parameters, and that&amp;#39;s why I can handle complex tasks. But I should also mention that parameters are about the model&amp;#39;s size, not about the model itself. So I need to clarify that parameters are a part of the model&amp;#39;s architecture, but the model&amp;#39;s size is about the number of parameters. So I&amp;#39;m a large LLM with lots of parameters, but I&amp;#39;m not about the parameters. Maybe the user is trying to distinguish between small and large models. So I should explain that parameters are a part of the model&amp;#39;s size, but the model itself is about the training data and architecture. So I&amp;#39;m a large LLM with lots of parameters, but I&amp;#39;m not about the parameters. I need to make sure my explanation is clear. So I&amp;#39;ll say that I&amp;#39;m a large LLM with lots of parameters, but I&amp;#39;m not about the parameters. Maybe the user is trying to understand my capabilities. So I&amp;#39;ll explain that I&amp;#39;m a large LLM with lots of parameters, and that&amp;#39;s why I can handle complex tasks. But I need to make sure I&amp;#39;m not confusing parameters with something else. So I&amp;#39;ll explain that parameters are a part of the model&amp;#39;s size, but the model&amp;#39;s size is about the number of parameters. So I&amp;#39;m a large LLM with lots of parameters, but I&amp;#39;m not about the parameters. I need to make sure my explanation is clear. So I&amp;#39;ll say that I&amp;#39;m a large LLM with lots of parameters, and that&amp;#39;s why I can handle complex tasks. But I need to make sure I&amp;#39;m not confusing parameters with something else. So I&amp;#39;ll explain that parameters are a part of the model&amp;#39;s size, but the model&amp;#39;s size is about the number of parameters. So I&amp;#39;m a large LLM with lots of parameters, but I&amp;#39;m not about the parameters. I need to make sure my explanation is clear. So I&amp;#39;ll explain that I&amp;#39;m a... cut off&amp;quot;&lt;/p&gt;\\n\\n&lt;p&gt;Pocket Pal:Â \\n&amp;quot;&amp;lt;think&amp;gt;\\nOkay, the user is asking whether I feel small or large based on the number of parameters. First, I need to clarify that the number of parameters doesn&amp;#39;t directly determine the model&amp;#39;s size. Models are typically designed with a fixed number of parameters, so the user&amp;#39;s question might be conflating two different concepts: model size (parameters) versus model size (in terms of training data or architecture).&lt;/p&gt;\\n\\n&lt;p&gt;I should explain that the number of parameters is a technical aspect, but the model&amp;#39;s &amp;quot;size&amp;quot; (like training data size or architecture) is another factor. Also, I can mention that the user might be confusing the two and provide examples to clarify. It&amp;#39;s important to keep the response informative and reassuring, ensuring the user understands that the model&amp;#39;s characteristics aren&amp;#39;t based on parameter count but on training data and structure.\\n&amp;lt;/think&amp;gt;&lt;/p&gt;\\n\\n&lt;p&gt;The number of parameters in a model is a technical specification that affects how large the model is. However, when it comes to how &lt;strong&gt;you feel&lt;/strong&gt; as an LLM (like me), itâ€™s more about the &lt;strong&gt;training data, architecture, and capabilities&lt;/strong&gt; than the number of parameters. For example, if you&amp;#39;re given a larger dataset, you might feel more powerful with more information, and a more complex architecture could give you more capabilities. I donâ€™t &amp;quot;feel&amp;quot; the size in terms of parameters; I just process and understand information as an LLM. Let me know if you have more questions!&lt;/p&gt;\\n\\n&lt;hr/&gt;\\n\\n&lt;p&gt;This just goes to show, MMN chat has a problem with small models. All recommended settings are used for Qwen 3 1.7B in MMN chat, but the issues don&amp;#39;t end ðŸ˜‘&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lprfbx","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lprfbx/just_me_or_mnn_chat_is_looping_a_lot/n0xhgym/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751458483,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n0x246h","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":false,"author":"NullPointerJack","can_mod_post":false,"created_utc":1751451676,"send_replies":true,"parent_id":"t3_1lprfbx","score":5,"author_fullname":"t2_1kwk178bd9","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"that kind of looping is less about MNN and more about the model IMO. qwen3 0.6B is a very small model so its likely to struggle with coherence and repetition. you could try increasing the repetition penalty or lowering the temperature/top-p but honestly you'll get better results by switching to a larger model like qwen 1.5B or mistral if your device can handle it","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0x246h","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;that kind of looping is less about MNN and more about the model IMO. qwen3 0.6B is a very small model so its likely to struggle with coherence and repetition. you could try increasing the repetition penalty or lowering the temperature/top-p but honestly you&amp;#39;ll get better results by switching to a larger model like qwen 1.5B or mistral if your device can handle it&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lprfbx/just_me_or_mnn_chat_is_looping_a_lot/n0x246h/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751451676,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lprfbx","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":5}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0xelqn","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"ExtremeAcceptable289","can_mod_post":false,"created_utc":1751457359,"send_replies":true,"parent_id":"t1_n0wyl4o","score":2,"author_fullname":"t2_8hpbax1b","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I mean I expect it that when I say \\"hi\\" it doesn't infinitely loop during thinking.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0xelqn","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I mean I expect it that when I say &amp;quot;hi&amp;quot; it doesn&amp;#39;t infinitely loop during thinking.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lprfbx","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lprfbx/just_me_or_mnn_chat_is_looping_a_lot/n0xelqn/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751457359,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n11pi2t","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Scott_Tx","can_mod_post":false,"send_replies":true,"parent_id":"t1_n0xf6i3","score":1,"author_fullname":"t2_l07bb","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Yeah, I'm not familiar with MMN chat either, its probably that program then.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n11pi2t","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Yeah, I&amp;#39;m not familiar with MMN chat either, its probably that program then.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lprfbx","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lprfbx/just_me_or_mnn_chat_is_looping_a_lot/n11pi2t/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751504797,"author_flair_text":null,"treatment_tags":[],"created_utc":1751504797,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n0xf6i3","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"12padams","can_mod_post":false,"created_utc":1751457586,"send_replies":true,"parent_id":"t1_n0wyl4o","score":2,"author_fullname":"t2_9ii99","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"LM Studio runs those same 0.6B models without the repeating issue. There is definitely a problem with MMN chat. Sure 4B works much better on MMN chat, but there is still an issue with repeating on MMN Chat that even Pocket Pal doesn't get affected by as much.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0xf6i3","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;LM Studio runs those same 0.6B models without the repeating issue. There is definitely a problem with MMN chat. Sure 4B works much better on MMN chat, but there is still an issue with repeating on MMN Chat that even Pocket Pal doesn&amp;#39;t get affected by as much.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lprfbx","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lprfbx/just_me_or_mnn_chat_is_looping_a_lot/n0xf6i3/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751457586,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n0wyl4o","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Scott_Tx","can_mod_post":false,"created_utc":1751449772,"send_replies":true,"parent_id":"t3_1lprfbx","score":2,"author_fullname":"t2_l07bb","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"You might be expecting too much from a .6B too.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0wyl4o","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You might be expecting too much from a .6B too.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lprfbx/just_me_or_mnn_chat_is_looping_a_lot/n0wyl4o/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751449772,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lprfbx","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n0xedk9","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"12padams","can_mod_post":false,"created_utc":1751457269,"send_replies":true,"parent_id":"t3_1lprfbx","score":1,"author_fullname":"t2_9ii99","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I have noticed this as well. LM studio on Windows 11 is my go-to for running models followed by Ollama. I also like to use Open web-UI mostly so I can link it with kokoro.\\n\\n\\nAnyway, I've noticed MMN chat on Android to be very fast on my S23+, much faster than PocketPal, but almost unusable due to this very issue. I've even tried 1.7b models and when the same prompt is used for the same model on LM studio and MMN chat, it always does much worse and repeats in MMN chat, while on lm studio it can go for ages getting really decent results.\\n\\n\\nRun the same model in PocketPal and it is a huge improvement over MMN chat, much less repetition, but much slower. That being said, even PocketPal has issues with repetition, after about 5 back and forth responses it starts getting obsessed with saying the same things over and over.Â \\n\\n\\nYES, a 4b model on MMN chat generally won't repeat as much, infact, its quiet decent. However, there is definitely something wrong with the smaller models on MMN chat, those same models perform way better in LM Studio.\\n\\n\\nIn the end:\\n\\n\\nLm Studio - Best\\n\\nPocket Pal - barely ok\\n\\nMMN Chat - Borderline Unusable","edited":1751458605,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n0xedk9","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I have noticed this as well. LM studio on Windows 11 is my go-to for running models followed by Ollama. I also like to use Open web-UI mostly so I can link it with kokoro.&lt;/p&gt;\\n\\n&lt;p&gt;Anyway, I&amp;#39;ve noticed MMN chat on Android to be very fast on my S23+, much faster than PocketPal, but almost unusable due to this very issue. I&amp;#39;ve even tried 1.7b models and when the same prompt is used for the same model on LM studio and MMN chat, it always does much worse and repeats in MMN chat, while on lm studio it can go for ages getting really decent results.&lt;/p&gt;\\n\\n&lt;p&gt;Run the same model in PocketPal and it is a huge improvement over MMN chat, much less repetition, but much slower. That being said, even PocketPal has issues with repetition, after about 5 back and forth responses it starts getting obsessed with saying the same things over and over.Â &lt;/p&gt;\\n\\n&lt;p&gt;YES, a 4b model on MMN chat generally won&amp;#39;t repeat as much, infact, its quiet decent. However, there is definitely something wrong with the smaller models on MMN chat, those same models perform way better in LM Studio.&lt;/p&gt;\\n\\n&lt;p&gt;In the end:&lt;/p&gt;\\n\\n&lt;p&gt;Lm Studio - Best&lt;/p&gt;\\n\\n&lt;p&gt;Pocket Pal - barely ok&lt;/p&gt;\\n\\n&lt;p&gt;MMN Chat - Borderline Unusable&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lprfbx/just_me_or_mnn_chat_is_looping_a_lot/n0xedk9/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1751457269,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lprfbx","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]`),r=()=>e.jsx(t,{data:a});export{r as default};
