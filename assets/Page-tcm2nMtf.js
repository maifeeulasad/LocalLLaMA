import{j as l}from"./index-DQXiEb7D.js";import{R as e}from"./RedditPostRenderer-BjndLgq8.js";import"./index-B-ILyjT1.js";const t=JSON.parse('[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"I’m trying to repurpose my desktop as a local LLM server. Specs are:\\n\\n* Ryzen 7 (1st gen)\\n* 48GB RAM\\n* RTX 3060 12GB\\n\\nI want to be able to run LLMs locally and connect to this machine from other devices on my LAN to offload inference tasks.\\n\\nHere’s what I’ve tried so far:\\n\\n* Installed Ubuntu Server, but had a rough time getting the GPU drivers working properly. `nvidia-smi` was hit or miss.\\n* Tried setting up [vLLM](https://github.com/vllm-project/vllm), but `vllm serve` throws various errors I couldn’t resolve.\\n* Attempted running things in Docker with NVIDIA container toolkit and passthrough - still hit compatibility issues and weird driver errors.\\n\\nAfter 5+ hours of debugging and failure, I’m feeling stuck.\\n\\nCan anyone share their setup that *actually works*? I’m looking for:\\n\\n1. A reliable way to get the RTX 3060 working with GPU acceleration.\\n2. A minimal stack (OS, drivers, runtime) that just works with something like vLLM or Ollama or anything similar.\\n3. Bonus: A way to expose the model server over LAN for remote clients.\\n\\nThanks in advance","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Need help setting up a local LLM server with RTX 3060","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lvm3kv","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":1,"author_flair_background_color":null,"subreddit_type":"public","ups":3,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_54sxk5i5","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":3,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"post_hint":"self","content_categories":null,"is_self":true,"mod_note":null,"created":1752075816,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;I’m trying to repurpose my desktop as a local LLM server. Specs are:&lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;Ryzen 7 (1st gen)&lt;/li&gt;\\n&lt;li&gt;48GB RAM&lt;/li&gt;\\n&lt;li&gt;RTX 3060 12GB&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;p&gt;I want to be able to run LLMs locally and connect to this machine from other devices on my LAN to offload inference tasks.&lt;/p&gt;\\n\\n&lt;p&gt;Here’s what I’ve tried so far:&lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;Installed Ubuntu Server, but had a rough time getting the GPU drivers working properly. &lt;code&gt;nvidia-smi&lt;/code&gt; was hit or miss.&lt;/li&gt;\\n&lt;li&gt;Tried setting up &lt;a href=\\"https://github.com/vllm-project/vllm\\"&gt;vLLM&lt;/a&gt;, but &lt;code&gt;vllm serve&lt;/code&gt; throws various errors I couldn’t resolve.&lt;/li&gt;\\n&lt;li&gt;Attempted running things in Docker with NVIDIA container toolkit and passthrough - still hit compatibility issues and weird driver errors.&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;p&gt;After 5+ hours of debugging and failure, I’m feeling stuck.&lt;/p&gt;\\n\\n&lt;p&gt;Can anyone share their setup that &lt;em&gt;actually works&lt;/em&gt;? I’m looking for:&lt;/p&gt;\\n\\n&lt;ol&gt;\\n&lt;li&gt;A reliable way to get the RTX 3060 working with GPU acceleration.&lt;/li&gt;\\n&lt;li&gt;A minimal stack (OS, drivers, runtime) that just works with something like vLLM or Ollama or anything similar.&lt;/li&gt;\\n&lt;li&gt;Bonus: A way to expose the model server over LAN for remote clients.&lt;/li&gt;\\n&lt;/ol&gt;\\n\\n&lt;p&gt;Thanks in advance&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":true,"is_crosspostable":false,"pinned":false,"over_18":false,"preview":{"images":[{"source":{"url":"https://external-preview.redd.it/t0Z5DepX83mfh-NfvU1dN5vFN1CnaCZ7ZvGJ-syn1EA.png?auto=webp&amp;s=cab466f000a9569548ebd3ae8abfd85c32ee31f1","width":1200,"height":600},"resolutions":[{"url":"https://external-preview.redd.it/t0Z5DepX83mfh-NfvU1dN5vFN1CnaCZ7ZvGJ-syn1EA.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=38d7d905f6a5896e20f689af4bc05612592cc000","width":108,"height":54},{"url":"https://external-preview.redd.it/t0Z5DepX83mfh-NfvU1dN5vFN1CnaCZ7ZvGJ-syn1EA.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=568fa566629c29f2e0bb183fde6d812148f6062a","width":216,"height":108},{"url":"https://external-preview.redd.it/t0Z5DepX83mfh-NfvU1dN5vFN1CnaCZ7ZvGJ-syn1EA.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=adeec7f214bf0e350cda96ec601ac78d5bc9f67a","width":320,"height":160},{"url":"https://external-preview.redd.it/t0Z5DepX83mfh-NfvU1dN5vFN1CnaCZ7ZvGJ-syn1EA.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=9aabc62c14eb9789f323eecff0f6dff014c9b9b8","width":640,"height":320},{"url":"https://external-preview.redd.it/t0Z5DepX83mfh-NfvU1dN5vFN1CnaCZ7ZvGJ-syn1EA.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=b945a80304a01af644621d72f808cfcac8d23284","width":960,"height":480},{"url":"https://external-preview.redd.it/t0Z5DepX83mfh-NfvU1dN5vFN1CnaCZ7ZvGJ-syn1EA.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=71dcd25b6ef4ccfb096088be00e7b463415d2f2c","width":1080,"height":540}],"variants":{},"id":"t0Z5DepX83mfh-NfvU1dN5vFN1CnaCZ7ZvGJ-syn1EA"}],"enabled":false},"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1lvm3kv","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"Watch-D0g","discussion_type":null,"num_comments":15,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lvm3kv/need_help_setting_up_a_local_llm_server_with_rtx/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lvm3kv/need_help_setting_up_a_local_llm_server_with_rtx/","subreddit_subscribers":497025,"created_utc":1752075816,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n27amkj","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Herdnerfer","can_mod_post":false,"send_replies":true,"parent_id":"t1_n27153f","score":2,"author_fullname":"t2_6hed7","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I had to install the cuda stuff, but that’s all I recall doing. I doubt pop is any different than Ubuntu, was just a random distro recommended to me when I started.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n27amkj","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I had to install the cuda stuff, but that’s all I recall doing. I doubt pop is any different than Ubuntu, was just a random distro recommended to me when I started.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lvm3kv","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lvm3kv/need_help_setting_up_a_local_llm_server_with_rtx/n27amkj/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752078784,"author_flair_text":null,"treatment_tags":[],"created_utc":1752078784,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n27153f","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Watch-D0g","can_mod_post":false,"created_utc":1752076177,"send_replies":true,"parent_id":"t1_n270khg","score":2,"author_fullname":"t2_54sxk5i5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"(I havent used Pop linux before)  \\nDid you have to fiddle with nvidia drivers ? or the OS installation auto installed those ?","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n27153f","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;(I havent used Pop linux before)&lt;br/&gt;\\nDid you have to fiddle with nvidia drivers ? or the OS installation auto installed those ?&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lvm3kv","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lvm3kv/need_help_setting_up_a_local_llm_server_with_rtx/n27153f/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752076177,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}}],"before":null}},"user_reports":[],"saved":false,"id":"n270khg","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Herdnerfer","can_mod_post":false,"created_utc":1752076022,"send_replies":true,"parent_id":"t3_1lvm3kv","score":3,"author_fullname":"t2_6hed7","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I’m running ollama and open webui in dockers on Pop Linux with 2 3060s and it runs great for me.  Even have an instance of Comfyui running for local image generation in webui.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n270khg","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I’m running ollama and open webui in dockers on Pop Linux with 2 3060s and it runs great for me.  Even have an instance of Comfyui running for local image generation in webui.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lvm3kv/need_help_setting_up_a_local_llm_server_with_rtx/n270khg/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752076022,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lvm3kv","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2787jz","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Watch-D0g","can_mod_post":false,"created_utc":1752078119,"send_replies":true,"parent_id":"t1_n271xrg","score":1,"author_fullname":"t2_54sxk5i5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Okay thankyou","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2787jz","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Okay thankyou&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lvm3kv","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lvm3kv/need_help_setting_up_a_local_llm_server_with_rtx/n2787jz/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752078119,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n271xrg","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"AppearanceHeavy6724","can_mod_post":false,"created_utc":1752076392,"send_replies":true,"parent_id":"t3_1lvm3kv","score":1,"author_fullname":"t2_uz37qfx5","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"You do not need vLLM if all you have is single 3060. Use llama.cpp. The best way it to compile it yourself.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n271xrg","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;You do not need vLLM if all you have is single 3060. Use llama.cpp. The best way it to compile it yourself.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lvm3kv/need_help_setting_up_a_local_llm_server_with_rtx/n271xrg/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752076392,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lvm3kv","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n278ik7","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Watch-D0g","can_mod_post":false,"created_utc":1752078202,"send_replies":true,"parent_id":"t1_n272gy8","score":1,"author_fullname":"t2_54sxk5i5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I used to use LM Studio on this machine when it was on windows. then i decided to have it as a dedicated server, took the route of installing Ubuntu 24 Server, and started facing a lot of GPU issues.   \\ni might just install win11 back lol","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n278ik7","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I used to use LM Studio on this machine when it was on windows. then i decided to have it as a dedicated server, took the route of installing Ubuntu 24 Server, and started facing a lot of GPU issues.&lt;br/&gt;\\ni might just install win11 back lol&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lvm3kv","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lvm3kv/need_help_setting_up_a_local_llm_server_with_rtx/n278ik7/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752078202,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n272gy8","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"dkeiz","can_mod_post":false,"created_utc":1752076538,"send_replies":true,"parent_id":"t3_1lvm3kv","score":2,"author_fullname":"t2_1d1fsbe7","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"lm studio works, but you should initial run model with large context.  \\nSame for ollama - use larger context run command.  \\nUse small models - 7B 4-5bit quant. \\n\\nAll this for win11, but its shouldnt be a problem.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n272gy8","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;lm studio works, but you should initial run model with large context.&lt;br/&gt;\\nSame for ollama - use larger context run command.&lt;br/&gt;\\nUse small models - 7B 4-5bit quant. &lt;/p&gt;\\n\\n&lt;p&gt;All this for win11, but its shouldnt be a problem.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lvm3kv/need_help_setting_up_a_local_llm_server_with_rtx/n272gy8/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752076538,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lvm3kv","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2bef1m","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Watch-D0g","can_mod_post":false,"created_utc":1752126911,"send_replies":true,"parent_id":"t1_n278spa","score":1,"author_fullname":"t2_54sxk5i5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Thanks!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2bef1m","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thanks!&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lvm3kv","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lvm3kv/need_help_setting_up_a_local_llm_server_with_rtx/n2bef1m/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752126911,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n278spa","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Conscious_Cut_6144","can_mod_post":false,"created_utc":1752078280,"send_replies":true,"parent_id":"t3_1lvm3kv","score":2,"author_fullname":"t2_9hl4ymvj","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Get chatgpt to walk you through this, it\'s very good at it.\\n\\nBut here are your steps:\\n\\n1. wipe out all nvidia stuff, drivers cuda etc. and reboot\\n2. sudo apt install nvidia-driver-535 2.1 reboot 2.2 nvidia-smi (should show 535 and your gpu)\\n3. sudo apt install nvidia-cuda-toolkit 3.1 reboot again 3.2 nvcc --version (should say something like 12.4)\\n4. mkdir vllm, cd vllm, python3 -m venv myenv, source myenv/bin/activate\\n5. pip install vllm\\n6. vllm serve Qwen/Qwen3-8B-AWQ --max-model-len 8000\\n\\n  \\nAnd for a bonus:  \\ncd \\\\~    \\ngit clone [https://github.com/ggml-org/llama.cpp.git](https://github.com/ggml-org/llama.cpp.git)  \\ncd llama.cpp  \\ncmake -B build -DGGML\\\\_CUDA=ON  \\ncmake --build build --config Release -j$(nproc)  \\ncd build/bin  \\n./llama-server -m Qwen3-8B-Q4\\\\_K\\\\_M.gguf --port 8000 --host [0.0.0.0](http://0.0.0.0) \\\\-fa -ngl 99 -c 8000  \\n(you have to download that gguf first)","edited":1752078675,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n278spa","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Get chatgpt to walk you through this, it&amp;#39;s very good at it.&lt;/p&gt;\\n\\n&lt;p&gt;But here are your steps:&lt;/p&gt;\\n\\n&lt;ol&gt;\\n&lt;li&gt;wipe out all nvidia stuff, drivers cuda etc. and reboot&lt;/li&gt;\\n&lt;li&gt;sudo apt install nvidia-driver-535 2.1 reboot 2.2 nvidia-smi (should show 535 and your gpu)&lt;/li&gt;\\n&lt;li&gt;sudo apt install nvidia-cuda-toolkit 3.1 reboot again 3.2 nvcc --version (should say something like 12.4)&lt;/li&gt;\\n&lt;li&gt;mkdir vllm, cd vllm, python3 -m venv myenv, source myenv/bin/activate&lt;/li&gt;\\n&lt;li&gt;pip install vllm&lt;/li&gt;\\n&lt;li&gt;vllm serve Qwen/Qwen3-8B-AWQ --max-model-len 8000&lt;/li&gt;\\n&lt;/ol&gt;\\n\\n&lt;p&gt;And for a bonus:&lt;br/&gt;\\ncd ~&lt;br/&gt;\\ngit clone &lt;a href=\\"https://github.com/ggml-org/llama.cpp.git\\"&gt;https://github.com/ggml-org/llama.cpp.git&lt;/a&gt;&lt;br/&gt;\\ncd llama.cpp&lt;br/&gt;\\ncmake -B build -DGGML_CUDA=ON&lt;br/&gt;\\ncmake --build build --config Release -j$(nproc)&lt;br/&gt;\\ncd build/bin&lt;br/&gt;\\n./llama-server -m Qwen3-8B-Q4_K_M.gguf --port 8000 --host &lt;a href=\\"http://0.0.0.0\\"&gt;0.0.0.0&lt;/a&gt; -fa -ngl 99 -c 8000&lt;br/&gt;\\n(you have to download that gguf first)&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lvm3kv/need_help_setting_up_a_local_llm_server_with_rtx/n278spa/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752078280,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lvm3kv","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2begs6","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Watch-D0g","can_mod_post":false,"created_utc":1752126935,"send_replies":true,"parent_id":"t1_n281gym","score":1,"author_fullname":"t2_54sxk5i5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"okay","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2begs6","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;okay&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lvm3kv","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lvm3kv/need_help_setting_up_a_local_llm_server_with_rtx/n2begs6/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752126935,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n281gym","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"fizzy1242","can_mod_post":false,"created_utc":1752086061,"send_replies":true,"parent_id":"t3_1lvm3kv","score":2,"author_fullname":"t2_16zcsx","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"for minimal/easiest setup, download prebuilt koboldCPP and a model that can fit as .gguf","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n281gym","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;for minimal/easiest setup, download prebuilt koboldCPP and a model that can fit as .gguf&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lvm3kv/need_help_setting_up_a_local_llm_server_with_rtx/n281gym/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752086061,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lvm3kv","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n2behu8","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Watch-D0g","can_mod_post":false,"created_utc":1752126950,"send_replies":true,"parent_id":"t1_n2a8tgs","score":1,"author_fullname":"t2_54sxk5i5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"got it thanks","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2behu8","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;got it thanks&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lvm3kv","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lvm3kv/need_help_setting_up_a_local_llm_server_with_rtx/n2behu8/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752126950,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n2a8tgs","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"SwissyLDN","can_mod_post":false,"created_utc":1752110067,"send_replies":true,"parent_id":"t3_1lvm3kv","score":2,"author_fullname":"t2_2pl78ro4","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Pop!_OS has CUDA drivers pre-installed so it saves you messing around with GPU drivers","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n2a8tgs","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Pop!_OS has CUDA drivers pre-installed so it saves you messing around with GPU drivers&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lvm3kv/need_help_setting_up_a_local_llm_server_with_rtx/n2a8tgs/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752110067,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lvm3kv","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":2}},{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"richtext","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":"50c36eba-fdca-11ee-9735-92a88d7e3b87","likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n278oyl","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Watch-D0g","can_mod_post":false,"created_utc":1752078251,"send_replies":true,"parent_id":"t1_n273j04","score":1,"author_fullname":"t2_54sxk5i5","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"Perfect i will try out MX Linux, thanks!","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n278oyl","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Perfect i will try out MX Linux, thanks!&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lvm3kv","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lvm3kv/need_help_setting_up_a_local_llm_server_with_rtx/n278oyl/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752078251,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n273j04","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Glittering_Mouse_883","can_mod_post":false,"created_utc":1752076828,"send_replies":true,"parent_id":"t3_1lvm3kv","score":1,"author_fullname":"t2_sad822hq9","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"I set up something with similar hardware using MX Linux. Nvidia driver installation is a breeze on MX with an Nvidia driver install utility right in the start menu.\\n\\n\\nNext I went to the ollama website and ran the install script in the command line. I was up and running in minutes.\\n\\n\\nOnce you get that working as a proof of concept you can go ahead and do llama.cpp as someone else suggested. Or just wril with ollama and use openwebui to host a web interface to your ollama.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n273j04","is_submitter":false,"downs":0,"author_flair_richtext":[{"e":"text","t":"Ollama"}],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I set up something with similar hardware using MX Linux. Nvidia driver installation is a breeze on MX with an Nvidia driver install utility right in the start menu.&lt;/p&gt;\\n\\n&lt;p&gt;Next I went to the ollama website and ran the install script in the command line. I was up and running in minutes.&lt;/p&gt;\\n\\n&lt;p&gt;Once you get that working as a proof of concept you can go ahead and do llama.cpp as someone else suggested. Or just wril with ollama and use openwebui to host a web interface to your ollama.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":"light","score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lvm3kv/need_help_setting_up_a_local_llm_server_with_rtx/n273j04/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752076828,"author_flair_text":"Ollama","treatment_tags":[],"link_id":"t3_1lvm3kv","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":"#bbbdbf","collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}}]'),o=()=>l.jsx(e,{data:t});export{o as default};
