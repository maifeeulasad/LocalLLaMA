import{j as e}from"./index-BlGsFJYy.js";import{R as t}from"./RedditPostRenderer-B6uvq_Zl.js";import"./index-DDvVtNwD.js";const l=JSON.parse(`[{"kind":"Listing","data":{"after":null,"dist":1,"modhash":"","geo_filter":"","children":[{"kind":"t3","data":{"approved_at_utc":null,"subreddit":"LocalLLaMA","selftext":"Google has the on-device model Gemma 3 1B that I am using for my scam detection Android app. Google has instructions for RAG here - [https://ai.google.dev/edge/mediapipe/solutions/genai/rag/android](https://ai.google.dev/edge/mediapipe/solutions/genai/rag/android)\\n\\nBut that gets too slow for loading even 1000 chunks. Anybody knows how to compute the chunk embeddings offline, store it in sqlite and then load that into the Gemma 3 instead?","user_reports":[],"saved":false,"mod_reason_title":null,"gilded":0,"clicked":false,"title":"Computing embeddings offline for Gemma 3 1B (on-device model)","link_flair_richtext":[{"e":"text","t":"Question | Help"}],"subreddit_name_prefixed":"r/LocalLLaMA","hidden":false,"pwls":6,"link_flair_css_class":"","downs":0,"thumbnail_height":null,"top_awarded_type":null,"hide_score":false,"name":"t3_1lz4sk3","quarantine":false,"link_flair_text_color":"dark","upvote_ratio":0.83,"author_flair_background_color":null,"subreddit_type":"public","ups":8,"total_awards_received":0,"media_embed":{},"thumbnail_width":null,"author_flair_template_id":null,"is_original_content":false,"author_fullname":"t2_1inf92eupz","secure_media":null,"is_reddit_media_domain":false,"is_meta":false,"category":null,"secure_media_embed":{},"link_flair_text":"Question | Help","can_mod_post":false,"score":8,"approved_by":null,"is_created_from_ads_ui":false,"author_premium":false,"thumbnail":"self","edited":false,"author_flair_css_class":null,"author_flair_richtext":[],"gildings":{},"post_hint":"self","content_categories":null,"is_self":true,"mod_note":null,"created":1752443051,"link_flair_type":"richtext","wls":6,"removed_by_category":null,"banned_by":null,"author_flair_type":"text","domain":"self.LocalLLaMA","allow_live_comments":false,"selftext_html":"&lt;!-- SC_OFF --&gt;&lt;div class=\\"md\\"&gt;&lt;p&gt;Google has the on-device model Gemma 3 1B that I am using for my scam detection Android app. Google has instructions for RAG here - &lt;a href=\\"https://ai.google.dev/edge/mediapipe/solutions/genai/rag/android\\"&gt;https://ai.google.dev/edge/mediapipe/solutions/genai/rag/android&lt;/a&gt;&lt;/p&gt;\\n\\n&lt;p&gt;But that gets too slow for loading even 1000 chunks. Anybody knows how to compute the chunk embeddings offline, store it in sqlite and then load that into the Gemma 3 instead?&lt;/p&gt;\\n&lt;/div&gt;&lt;!-- SC_ON --&gt;","likes":null,"suggested_sort":null,"banned_at_utc":null,"view_count":null,"archived":false,"no_follow":true,"is_crosspostable":false,"pinned":false,"over_18":false,"preview":{"images":[{"source":{"url":"https://external-preview.redd.it/iaG91J8UPyw0LumfZ8FtQViH0YMYP9q-z6paL0E-fpE.png?auto=webp&amp;s=b78731184d9920fa4900b6590e113d2772fa64ed","width":1440,"height":900},"resolutions":[{"url":"https://external-preview.redd.it/iaG91J8UPyw0LumfZ8FtQViH0YMYP9q-z6paL0E-fpE.png?width=108&amp;crop=smart&amp;auto=webp&amp;s=a1cc13c1cb1062998d0e6a2cc88bc3272f2368f7","width":108,"height":67},{"url":"https://external-preview.redd.it/iaG91J8UPyw0LumfZ8FtQViH0YMYP9q-z6paL0E-fpE.png?width=216&amp;crop=smart&amp;auto=webp&amp;s=1812be5c0e49c65e85787f4dbb2922a543943e79","width":216,"height":135},{"url":"https://external-preview.redd.it/iaG91J8UPyw0LumfZ8FtQViH0YMYP9q-z6paL0E-fpE.png?width=320&amp;crop=smart&amp;auto=webp&amp;s=ca7983e470f1e5cbc5edcd5c5e1c7e5b70227953","width":320,"height":200},{"url":"https://external-preview.redd.it/iaG91J8UPyw0LumfZ8FtQViH0YMYP9q-z6paL0E-fpE.png?width=640&amp;crop=smart&amp;auto=webp&amp;s=293ebb5606c7edf7f2570aa914eb4ddb55f1e615","width":640,"height":400},{"url":"https://external-preview.redd.it/iaG91J8UPyw0LumfZ8FtQViH0YMYP9q-z6paL0E-fpE.png?width=960&amp;crop=smart&amp;auto=webp&amp;s=b1bd156ecd3df7024382f9e145cda17bcaf6bc79","width":960,"height":600},{"url":"https://external-preview.redd.it/iaG91J8UPyw0LumfZ8FtQViH0YMYP9q-z6paL0E-fpE.png?width=1080&amp;crop=smart&amp;auto=webp&amp;s=a3b1fd853b19889a23a601c33fae7d2323e8bdb0","width":1080,"height":675}],"variants":{},"id":"iaG91J8UPyw0LumfZ8FtQViH0YMYP9q-z6paL0E-fpE"}],"enabled":false},"all_awardings":[],"awarders":[],"media_only":false,"link_flair_template_id":"2c9831e6-bf92-11ed-98e6-d2b8bcc02ae1","can_gild":false,"spoiler":false,"locked":false,"author_flair_text":null,"treatment_tags":[],"visited":false,"removed_by":null,"num_reports":null,"distinguished":null,"subreddit_id":"t5_81eyvm","author_is_blocked":false,"mod_reason_by":null,"removal_reason":null,"link_flair_background_color":"#5a74cc","id":"1lz4sk3","is_robot_indexable":true,"num_duplicates":0,"report_reasons":null,"author":"Basic-Donut1740","discussion_type":null,"num_comments":5,"send_replies":true,"media":null,"contest_mode":false,"author_patreon_flair":false,"author_flair_text_color":null,"permalink":"/r/LocalLLaMA/comments/1lz4sk3/computing_embeddings_offline_for_gemma_3_1b/","stickied":false,"url":"https://www.reddit.com/r/LocalLLaMA/comments/1lz4sk3/computing_embeddings_offline_for_gemma_3_1b/","subreddit_subscribers":498850,"created_utc":1752443051,"num_crossposts":0,"mod_reports":[],"is_video":false}}],"before":null}},{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"likes":null,"replies":{"kind":"Listing","data":{"after":null,"dist":null,"modhash":"","geo_filter":"","children":[{"kind":"t1","data":{"subreddit_id":"t5_81eyvm","approved_at_utc":null,"author_is_blocked":false,"comment_type":null,"awarders":[],"mod_reason_by":null,"banned_by":null,"author_flair_type":"text","total_awards_received":0,"subreddit":"LocalLLaMA","author_flair_template_id":null,"distinguished":null,"likes":null,"replies":"","user_reports":[],"saved":false,"id":"n337eiu","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"SkyFeistyLlama8","can_mod_post":false,"send_replies":true,"parent_id":"t1_n336eys","score":1,"author_fullname":"t2_1hgbaqgbnq","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"BERT is supposed to be good at individual messages but I haven't used it much. I don't know about conversations. You might have to break them down into question-response pairs.\\n\\nI've done zero LLM work on Android or mobile so I can't be much help to you there.","edited":false,"author_flair_css_class":null,"name":"t1_n337eiu","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;BERT is supposed to be good at individual messages but I haven&amp;#39;t used it much. I don&amp;#39;t know about conversations. You might have to break them down into question-response pairs.&lt;/p&gt;\\n\\n&lt;p&gt;I&amp;#39;ve done zero LLM work on Android or mobile so I can&amp;#39;t be much help to you there.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"link_id":"t3_1lz4sk3","associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"top_awarded_type":null,"unrepliable_reason":null,"author_flair_text_color":null,"treatment_tags":[],"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lz4sk3/computing_embeddings_offline_for_gemma_3_1b/n337eiu/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752505985,"author_flair_text":null,"collapsed":false,"created_utc":1752505985,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":4,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n336eys","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Basic-Donut1740","can_mod_post":false,"send_replies":true,"parent_id":"t1_n32sfu4","score":1,"author_fullname":"t2_1inf92eupz","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Thanks for the detailed information, its helpful. Would BERT be good to classify text messages? Not just the incoming message but entire conversations. Do you have any suggestion on which one to try for Android? I appreciate the help.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n336eys","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Thanks for the detailed information, its helpful. Would BERT be good to classify text messages? Not just the incoming message but entire conversations. Do you have any suggestion on which one to try for Android? I appreciate the help.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lz4sk3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lz4sk3/computing_embeddings_offline_for_gemma_3_1b/n336eys/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752505701,"author_flair_text":null,"treatment_tags":[],"created_utc":1752505701,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":3,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n32sfu4","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"SkyFeistyLlama8","can_mod_post":false,"send_replies":true,"parent_id":"t1_n30sc8i","score":1,"author_fullname":"t2_1hgbaqgbnq","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I've read of others doing finetuning on small models like Phi 4B to get it to encode certain limited knowledge and to output in a certain syntax. If you have a lot of data, finetuning might not be enough and RAG would still be the way to go.\\n\\nFor RAG on a phone, you need:\\n\\n- a fast embedding model, keep it small\\n- a fast vector database to store text chunks and embedding vectors\\n- a fast method to calculate vector similarities\\n- finally, a light and fast SLM like Gemma 1B to output the answer to the query\\n\\nIf your app's purpose is mostly classification, why not try existing BERT models? Those are very fast and easy to finetune. LLMs and the RAG pipeline could be overkill.","edited":false,"top_awarded_type":null,"downs":0,"author_flair_css_class":null,"name":"t1_n32sfu4","is_submitter":false,"collapsed":false,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I&amp;#39;ve read of others doing finetuning on small models like Phi 4B to get it to encode certain limited knowledge and to output in a certain syntax. If you have a lot of data, finetuning might not be enough and RAG would still be the way to go.&lt;/p&gt;\\n\\n&lt;p&gt;For RAG on a phone, you need:&lt;/p&gt;\\n\\n&lt;ul&gt;\\n&lt;li&gt;a fast embedding model, keep it small&lt;/li&gt;\\n&lt;li&gt;a fast vector database to store text chunks and embedding vectors&lt;/li&gt;\\n&lt;li&gt;a fast method to calculate vector similarities&lt;/li&gt;\\n&lt;li&gt;finally, a light and fast SLM like Gemma 1B to output the answer to the query&lt;/li&gt;\\n&lt;/ul&gt;\\n\\n&lt;p&gt;If your app&amp;#39;s purpose is mostly classification, why not try existing BERT models? Those are very fast and easy to finetune. LLMs and the RAG pipeline could be overkill.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lz4sk3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lz4sk3/computing_embeddings_offline_for_gemma_3_1b/n32sfu4/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752501583,"author_flair_text":null,"treatment_tags":[],"created_utc":1752501583,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":2,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n30sc8i","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"Basic-Donut1740","can_mod_post":false,"created_utc":1752467180,"send_replies":true,"parent_id":"t1_n30rv6i","score":1,"author_fullname":"t2_1inf92eupz","removal_reason":null,"approved_by":null,"mod_note":null,"all_awardings":[],"body":"I am using the Gecko embedder (recommended by Google) that runs on the phone. But its slow. I will check the IBM model to see if it has Android support.\\n\\nGot it. Sounds like fine-tuning is a better approach for me.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n30sc8i","is_submitter":true,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;I am using the Gecko embedder (recommended by Google) that runs on the phone. But its slow. I will check the IBM model to see if it has Android support.&lt;/p&gt;\\n\\n&lt;p&gt;Got it. Sounds like fine-tuning is a better approach for me.&lt;/p&gt;\\n&lt;/div&gt;","gildings":{},"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"link_id":"t3_1lz4sk3","unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lz4sk3/computing_embeddings_offline_for_gemma_3_1b/n30sc8i/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752467180,"author_flair_text":null,"treatment_tags":[],"collapsed":false,"subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":1,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":1}}],"before":null}},"user_reports":[],"saved":false,"id":"n30rv6i","banned_at_utc":null,"mod_reason_title":null,"gilded":0,"archived":false,"collapsed_reason_code":null,"no_follow":true,"author":"SkyFeistyLlama8","can_mod_post":false,"created_utc":1752466962,"send_replies":true,"parent_id":"t3_1lz4sk3","score":3,"author_fullname":"t2_1hgbaqgbnq","approved_by":null,"mod_note":null,"all_awardings":[],"collapsed":false,"body":"Why not use a smaller embedding model that can run on the phone? I've been using IBM's granite-embedding-125m-english on a laptop and I'm getting very good results.\\n\\nYou need to compute cosine similarity or another vector similarity value to find the most likely chunks among the 1000 chunks. Then you load those matching chunks into Gemma 3 1B. You can't load all 1000 chunks because that's a huge number of context tokens. Your phone can't handle that amount of prompt processing.","edited":false,"top_awarded_type":null,"author_flair_css_class":null,"name":"t1_n30rv6i","is_submitter":false,"downs":0,"author_flair_richtext":[],"author_patreon_flair":false,"body_html":"&lt;div class=\\"md\\"&gt;&lt;p&gt;Why not use a smaller embedding model that can run on the phone? I&amp;#39;ve been using IBM&amp;#39;s granite-embedding-125m-english on a laptop and I&amp;#39;m getting very good results.&lt;/p&gt;\\n\\n&lt;p&gt;You need to compute cosine similarity or another vector similarity value to find the most likely chunks among the 1000 chunks. Then you load those matching chunks into Gemma 3 1B. You can&amp;#39;t load all 1000 chunks because that&amp;#39;s a huge number of context tokens. Your phone can&amp;#39;t handle that amount of prompt processing.&lt;/p&gt;\\n&lt;/div&gt;","removal_reason":null,"collapsed_reason":null,"distinguished":null,"associated_award":null,"stickied":false,"author_premium":false,"can_gild":false,"gildings":{},"unrepliable_reason":null,"author_flair_text_color":null,"score_hidden":false,"permalink":"/r/LocalLLaMA/comments/1lz4sk3/computing_embeddings_offline_for_gemma_3_1b/n30rv6i/","subreddit_type":"public","locked":false,"report_reasons":null,"created":1752466962,"author_flair_text":null,"treatment_tags":[],"link_id":"t3_1lz4sk3","subreddit_name_prefixed":"r/LocalLLaMA","controversiality":0,"depth":0,"author_flair_background_color":null,"collapsed_because_crowd_control":null,"mod_reports":[],"num_reports":null,"ups":3}}],"before":null}}]`),s=()=>e.jsx(t,{data:l});export{s as default};
